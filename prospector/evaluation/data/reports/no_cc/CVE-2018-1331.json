{
    "advisory_record": {
        "cve_id": "CVE-2018-1331",
        "description": "In Apache Storm 0.10.0 through 0.10.2, 1.0.0 through 1.0.6, 1.1.0 through 1.1.2, and 1.2.0 through 1.2.1, an attacker with access to a secure storm cluster in some cases could execute arbitrary code as a different user.",
        "reserved_timestamp": 1512604800,
        "published_timestamp": 1531180800,
        "updated_timestamp": 1531562221,
        "repository_url": null,
        "references": {
            "": 182,
            "http://storm.apache.org/2018/06/04/storm122-released.html": 2,
            "http://www.openwall.com/lists/oss-security/2018/07/10/4": 2,
            "http://storm.apache.org/2018/06/04/storm113-released.html": 2,
            "http://www.securityfocus.com/bid/104732": 2,
            "http://www.securitytracker.com/id/1041273": 2,
            "https://issues.apache.org/jira/browse/STORM-3026": 2,
            "https://issues.apache.org/jira/browse/STORM-3027": 2,
            "https://issues.apache.org/jira/browse/STORM-3011": 2,
            "https://issues.apache.org/jira/browse/STORM-3039": 2,
            "https://issues.apache.org/jira/browse/STORM-2911": 2,
            "https://issues.apache.org/jira/browse/STORM-2978": 2,
            "https://issues.apache.org/jira/browse/STORM-2979": 2,
            "https://issues.apache.org/jira/browse/STORM-2981": 2,
            "https://issues.apache.org/jira/browse/STORM-2985": 2,
            "https://issues.apache.org/jira/browse/STORM-2989": 2,
            "https://issues.apache.org/jira/browse/STORM-2994": 2,
            "https://issues.apache.org/jira/browse/STORM-3043": 2,
            "https://issues.apache.org/jira/browse/STORM-3052": 2,
            "https://issues.apache.org/jira/browse/STORM-3059": 2,
            "https://issues.apache.org/jira/browse/STORM-2960": 2,
            "https://issues.apache.org/jira/browse/STORM-3060": 2,
            "https://issues.apache.org/jira/browse/STORM-2952": 2,
            "https://issues.apache.org/jira/browse/STORM-3005": 2,
            "https://www.apache.org/licenses/": 2,
            "https://www.apache.org/security/": 2,
            "https://www.apache.org/foundation/thanks.html": 2,
            "https://privacy.apache.org/policies/privacy-policy-public.html": 2,
            "https://www.apache.org": 2,
            "https://issues.apache.org/jira/browse/STORM-2896": 1,
            "https://issues.apache.org/jira/browse/STORM-2997": 1,
            "https://issues.apache.org/jira/browse/STORM-3006": 1,
            "https://issues.apache.org/jira/browse/STORM-3022": 1,
            "https://issues.apache.org/jira/browse/STORM-3069": 1,
            "https://issues.apache.org/jira/browse/STORM-2967": 1,
            "https://issues.apache.org/jira/browse/STORM-2968": 1,
            "https://issues.apache.org/jira/browse/STORM-2988": 1,
            "https://issues.apache.org/jira/browse/STORM-2993": 1,
            "https://issues.apache.org/jira/browse/STORM-2841": 1
        },
        "affected_products": [
            "Storm",
            "Apache Storm",
            "Apache"
        ],
        "versions": {
            "status": "affected",
            "version": "0.10.0 through 0.10.2"
        },
        "files": [],
        "keywords": [
            "cluster",
            "user",
            "attacker",
            "case",
            "execute",
            "apache",
            "storm",
            "access",
            "code"
        ],
        "files_extension": [],
        "has_fixing_commit": false
    },
    "commits": [
        {
            "commit_id": "5e3c966b23bc3b6d924a52ea62a32b194978af24",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513658495,
            "hunks": 2,
            "message": "[STORM-2690] resurrect invocation of ISupervisor.assigned() & make Supervisor.launchDaemon() accessible This commit fixes the storm-mesos integration for the interaction between the Storm core's Supervisor daemon and the MesosSupervisor that implements the ISupervisor interface.",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "index 6e92a9cb9..884efcb0e 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "@@ -139,2 +139,3 @@ public class ReadClusterState implements Runnable, AutoCloseable {",
                "             HashSet<Integer> allPorts = new HashSet<>(assignedPorts);",
                "+            iSuper.assigned(allPorts);",
                "             allPorts.addAll(slots.keySet());",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java",
                "index e11ee4b05..ee5a55c7b 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java",
                "@@ -213,3 +213,3 @@ public class Supervisor implements DaemonCommon, AutoCloseable {",
                "      */",
                "-    private void launchDaemon() {",
                "+    public void launchDaemon() {",
                "         LOG.info(\"Starting supervisor for storm version '{}'.\", VersionInfo.getVersion());"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2690": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "a21dcf6c16f794e1ac1287b9fcd3b4e8a9418824"
                ],
                [
                    "no-tag",
                    "a884c6f2e7df01edc4fcaa7d1b392f09f4ff3c2a"
                ],
                [
                    "no-tag",
                    "a4215f37a41030b96ce57338ff23aede12131ccd"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: access",
                    "relevance": 4
                },
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                },
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2690",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7afd6fbe4603e35114a84e836b02484fe8cda660",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1506533003,
            "hunks": 175,
            "message": "STORM-2759: Let users indicate if a blob should restart a worker * Closes #2363",
            "diff": [
                "diff --git a/log4j2/cluster.xml b/log4j2/cluster.xml",
                "index 35ef25760..2a12fbac4 100644",
                "--- a/log4j2/cluster.xml",
                "+++ b/log4j2/cluster.xml",
                "@@ -23,3 +23,3 @@",
                " <appenders>",
                "-    <RollingFile name=\"A1\" immediateFlush=\"false\"",
                "+    <RollingFile name=\"A1\" ",
                "                  fileName=\"${sys:storm.log.dir}/${sys:logfile.name}\"",
                "@@ -34,3 +34,3 @@",
                "     </RollingFile>",
                "-    <RollingFile name=\"WEB-ACCESS\" immediateFlush=\"false\"",
                "+    <RollingFile name=\"WEB-ACCESS\" ",
                "                  fileName=\"${sys:storm.log.dir}/access-web-${sys:daemon.name}.log\"",
                "@@ -45,3 +45,3 @@",
                "     </RollingFile>",
                "-    <RollingFile name=\"THRIFT-ACCESS\" immediateFlush=\"false\"",
                "+    <RollingFile name=\"THRIFT-ACCESS\" ",
                "                  fileName=\"${sys:storm.log.dir}/access-${sys:logfile.name}\"",
                "diff --git a/storm-client/pom.xml b/storm-client/pom.xml",
                "index b37f45879..6ecb04da1 100644",
                "--- a/storm-client/pom.xml",
                "+++ b/storm-client/pom.xml",
                "@@ -253,3 +253,3 @@",
                "                     <excludes>**/generated/**</excludes>",
                "-                    <maxAllowedViolations>10785</maxAllowedViolations>",
                "+                    <maxAllowedViolations>10473</maxAllowedViolations>",
                "                 </configuration>",
                "diff --git a/storm-client/src/jvm/org/apache/storm/Config.java b/storm-client/src/jvm/org/apache/storm/Config.java",
                "index 6be0c2167..a73e79148 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/Config.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/Config.java",
                "@@ -93,7 +93,8 @@ public class Config extends HashMap<String, Object> {",
                "      * A map with blobstore keys mapped to each filename the worker will have access to in the",
                "-     * launch directory to the blob by local file name and uncompress flag. Both localname and",
                "-     * uncompress flag are optional. It uses the key is localname is not specified. Each topology",
                "-     * will have different map of blobs.  Example: topology.blobstore.map: {\"blobstorekey\" :",
                "+     * launch directory to the blob by local file name, uncompress flag, and if the worker",
                "+     * should restart when the blob is updated. localname, workerRestart, and",
                "+     * uncompress are optional. If localname is not specified the name of the key is used instead.",
                "+     * Each topologywill have different map of blobs.  Example: topology.blobstore.map: {\"blobstorekey\" :",
                "      * {\"localname\": \"myblob\", \"uncompress\": false}, \"blobstorearchivekey\" :",
                "-     * {\"localname\": \"myarchive\", \"uncompress\": true}}",
                "+     * {\"localname\": \"myarchive\", \"uncompress\": true, \"workerRestart\": true}}",
                "      */",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/CuratorUtils.java b/storm-client/src/jvm/org/apache/storm/utils/CuratorUtils.java",
                "index f36003ddf..a3c255868 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/utils/CuratorUtils.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/CuratorUtils.java",
                "@@ -47,3 +47,3 @@ public class CuratorUtils {",
                "     public static CuratorFramework newCurator(Map<String, Object> conf, List<String> servers, Object port, String root, ZookeeperAuthInfo auth) {",
                "-        List<String> serverPorts = new ArrayList<String>();",
                "+        List<String> serverPorts = new ArrayList<>();",
                "         for (String zkServer : servers) {",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/NimbusClient.java b/storm-client/src/jvm/org/apache/storm/utils/NimbusClient.java",
                "index c61940023..2f4f6dd79 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/utils/NimbusClient.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/NimbusClient.java",
                "@@ -17,4 +17,10 @@",
                "  */",
                "+",
                " package org.apache.storm.utils;",
                "+import com.google.common.collect.Lists;",
                "+import java.security.Principal;",
                "+import java.util.HashMap;",
                "+import java.util.List;",
                "+import java.util.Map;",
                " import org.apache.storm.Config;",
                "@@ -25,3 +31,2 @@ import org.apache.storm.security.auth.ThriftClient;",
                " import org.apache.storm.security.auth.ThriftConnectionType;",
                "-import com.google.common.collect.Lists;",
                " import org.apache.thrift.transport.TTransportException;",
                "@@ -30,7 +35,2 @@ import org.slf4j.LoggerFactory;",
                "-import java.security.Principal;",
                "-import java.util.HashMap;",
                "-import java.util.List;",
                "-import java.util.Map;",
                "-",
                " public class NimbusClient extends ThriftClient {",
                "@@ -51,3 +51,3 @@ public class NimbusClient extends ThriftClient {",
                "      * @return true of new clients will be overridden to connect to a local cluster",
                "-     * and not the configured remote cluster.",
                "+     *     and not the configured remote cluster.",
                "      */",
                "@@ -88,3 +88,25 @@ public class NimbusClient extends ThriftClient {",
                "     }",
                "-    ",
                "+",
                "+    private static String oldLeader = \"\";",
                "+",
                "+    /**",
                "+     * Check to see if we should log the leader we are connecting to or not.  This typically happens when the leader changes or if debug",
                "+     * logging is enabled. The code remembers the last leader it was called with, but it should be transparent to the caller.",
                "+     * @param leader the leader we are trying to connect to.",
                "+     * @return true if it should be logged else false.",
                "+     */",
                "+    private static synchronized boolean shouldLogLeader(String leader) {",
                "+        assert leader != null;",
                "+        if (LOG.isDebugEnabled()) {",
                "+            //If debug logging is turned on we should just log the leader all the time....",
                "+            return true;",
                "+        }",
                "+        //Only log if the leader has changed.  It is not interesting otherwise.",
                "+        if (oldLeader.equals(leader)) {",
                "+            return false;",
                "+        }",
                "+        oldLeader = leader;",
                "+        return true;",
                "+    }",
                "+",
                "     public static NimbusClient getConfiguredClientAs(Map conf, String asUser) {",
                "@@ -104,4 +126,4 @@ public class NimbusClient extends ThriftClient {",
                "             if (asUser != null && !asUser.isEmpty()) {",
                "-                LOG.warn(\"You have specified a doAsUser as param {} and a doAsParam as config, config will take precedence.\"",
                "-                        , asUser, conf.get(Config.STORM_DO_AS_USER));",
                "+                LOG.warn(\"You have specified a doAsUser as param {} and a doAsParam as config, config will take precedence.\",",
                "+                    asUser, conf.get(Config.STORM_DO_AS_USER));",
                "             }",
                "@@ -111,3 +133,3 @@ public class NimbusClient extends ThriftClient {",
                "         List<String> seeds;",
                "-        if(conf.containsKey(Config.NIMBUS_HOST)) {",
                "+        if (conf.containsKey(Config.NIMBUS_HOST)) {",
                "             LOG.warn(\"Using deprecated config {} for backward compatibility. Please update your storm.yaml so it only has config {}\",",
                "@@ -128,3 +150,5 @@ public class NimbusClient extends ThriftClient {",
                "                     String leaderNimbus = nimbusSummary.get_host() + \":\" + nimbusSummary.get_port();",
                "-                    LOG.info(\"Found leader nimbus : {}\", leaderNimbus);",
                "+                    if (shouldLogLeader(leaderNimbus)) {",
                "+                        LOG.info(\"Found leader nimbus : {}\", leaderNimbus);",
                "+                    }",
                "                     if (nimbusSummary.get_host().equals(host) && nimbusSummary.get_port() == port) {",
                "@@ -149,9 +173,8 @@ public class NimbusClient extends ThriftClient {",
                "             }",
                "-            throw new NimbusLeaderNotFoundException(\"Could not find a nimbus leader, please try \" +",
                "-                                                            \"again after some time.\");",
                "+            throw new NimbusLeaderNotFoundException(\"Could not find a nimbus leader, please try again after some time.\");",
                "         }",
                "         throw new NimbusLeaderNotFoundException(",
                "-                \"Could not find leader nimbus from seed hosts \" + seeds + \". \" +",
                "-                        \"Did you specify a valid list of nimbus hosts for config \" +",
                "-                        Config.NIMBUS_SEEDS + \"?\");",
                "+                \"Could not find leader nimbus from seed hosts \" + seeds + \". \"",
                "+                    + \"Did you specify a valid list of nimbus hosts for config \"",
                "+                    + Config.NIMBUS_SEEDS + \"?\");",
                "     }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/Utils.java b/storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "index 37c29630c..4ad2ee2f7 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "@@ -404,6 +404,9 @@ public class Utils {",
                "     public static boolean exceptionCauseIsInstanceOf(Class klass, Throwable throwable) {",
                "-        Throwable t = throwable;",
                "+        return unwrapTo(klass, throwable) != null;",
                "+    }",
                "+",
                "+    public static <T extends Throwable> T unwrapTo(Class<T> klass, Throwable t) {",
                "         while (t != null) {",
                "             if (klass.isInstance(t)) {",
                "-                return true;",
                "+                return (T)t;",
                "             }",
                "@@ -411,3 +414,10 @@ public class Utils {",
                "         }",
                "-        return false;",
                "+        return null;",
                "+    }",
                "+",
                "+    public static <T extends Throwable> void unwrapAndThrow(Class<T> klass, Throwable t) throws T {",
                "+        T ret = unwrapTo(klass, t);",
                "+        if (ret != null) {",
                "+            throw ret;",
                "+        }",
                "     }",
                "@@ -1102,17 +1112,2 @@ public class Utils {",
                "-    public static long getVersionFromBlobVersionFile(File versionFile) {",
                "-        long currentVersion = 0;",
                "-        if (versionFile.exists() && !(versionFile.isDirectory())) {",
                "-            try (BufferedReader br = new BufferedReader(new FileReader(versionFile))) {",
                "-                String line = br.readLine();",
                "-                currentVersion = Long.parseLong(line);",
                "-            } catch (IOException e) {",
                "-                throw new RuntimeException(e);",
                "-            }",
                "-            return currentVersion;",
                "-        } else {",
                "-            return -1;",
                "-        }",
                "-    }",
                "-",
                "     public static boolean checkDirExists(String dir) {",
                "diff --git a/storm-server/pom.xml b/storm-server/pom.xml",
                "index 02104beea..9a1b4c0c0 100644",
                "--- a/storm-server/pom.xml",
                "+++ b/storm-server/pom.xml",
                "@@ -132,3 +132,3 @@",
                "                 <configuration>",
                "-                    <maxAllowedViolations>3590</maxAllowedViolations>",
                "+                    <maxAllowedViolations>2699</maxAllowedViolations>",
                "                 </configuration>",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java",
                "index 08d32f14a..e11ee4b05 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java",
                "@@ -110,3 +110,3 @@ public class Supervisor implements DaemonCommon, AutoCloseable {",
                "             this.localState = ServerConfigUtils.supervisorState(conf);",
                "-            this.asyncLocalizer = new AsyncLocalizer(conf, currAssignment, localState.getLocalAssignmentsMap());",
                "+            this.asyncLocalizer = new AsyncLocalizer(conf);",
                "         } catch (IOException e) {",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/SupervisorUtils.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/SupervisorUtils.java",
                "index 33574c3d2..81421b24c 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/SupervisorUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/SupervisorUtils.java",
                "@@ -71,3 +71,3 @@ public class SupervisorUtils {",
                "      */",
                "-    public static Boolean shouldUncompressBlob(Map<String, Object> blobInfo) {",
                "+    public static boolean shouldUncompressBlob(Map<String, Object> blobInfo) {",
                "         return ObjectReader.getBoolean(blobInfo.get(\"uncompress\"), false);",
                "@@ -75,2 +75,13 @@ public class SupervisorUtils {",
                "+    /**",
                "+     * Given the blob information returns the value of the workerRestart field, handling it either being a string or a boolean value, or",
                "+     * if it's not specified then returns false",
                "+     *",
                "+     * @param blobInfo the info for the blob.",
                "+     * @return true if the blob needs a worker restart by way of the callback else false.",
                "+     */",
                "+    public static boolean blobNeedsWorkerRestart(Map<String, Object> blobInfo) {",
                "+        return ObjectReader.getBoolean(blobInfo.get(\"workerRestart\"), false);",
                "+    }",
                "+",
                "     /**",
                "@@ -85,3 +96,4 @@ public class SupervisorUtils {",
                "             for (Map.Entry<String, Map<String, Object>> map : blobstoreMap.entrySet()) {",
                "-                LocalResource localResource = new LocalResource(map.getKey(), shouldUncompressBlob(map.getValue()));",
                "+                Map<String, Object> blobConf = map.getValue();",
                "+                LocalResource localResource = new LocalResource(map.getKey(), shouldUncompressBlob(blobConf), blobNeedsWorkerRestart(blobConf));",
                "                 localResourceList.add(localResource);",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java b/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java",
                "index 4b4750246..71f3495e7 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java",
                "@@ -22,9 +22,4 @@ import com.google.common.annotations.VisibleForTesting;",
                " import com.google.common.util.concurrent.ThreadFactoryBuilder;",
                "-import java.io.BufferedWriter;",
                " import java.io.File;",
                "-import java.io.FileOutputStream;",
                "-import java.io.FileWriter;",
                " import java.io.IOException;",
                "-import java.io.PrintWriter;",
                "-import java.net.URLDecoder;",
                " import java.nio.file.DirectoryStream;",
                "@@ -41,3 +36,2 @@ import java.util.Map;",
                " import java.util.Set;",
                "-import java.util.concurrent.Callable;",
                " import java.util.concurrent.CompletableFuture;",
                "@@ -47,3 +41,2 @@ import java.util.concurrent.ExecutionException;",
                " import java.util.concurrent.Executors;",
                "-import java.util.concurrent.Future;",
                " import java.util.concurrent.RejectedExecutionException;",
                "@@ -51,6 +44,3 @@ import java.util.concurrent.ScheduledExecutorService;",
                " import java.util.concurrent.TimeUnit;",
                "-import java.util.concurrent.atomic.AtomicReference;",
                " import java.util.function.Supplier;",
                "-import java.util.stream.Collectors;",
                "-import org.apache.commons.io.FileUtils;",
                " import org.apache.storm.Config;",
                "@@ -58,3 +48,2 @@ import org.apache.storm.DaemonConfig;",
                " import org.apache.storm.blobstore.ClientBlobStore;",
                "-import org.apache.storm.blobstore.InputStreamWithMeta;",
                " import org.apache.storm.daemon.supervisor.AdvancedFSOps;",
                "@@ -65,3 +54,2 @@ import org.apache.storm.generated.LocalAssignment;",
                " import org.apache.storm.generated.StormTopology;",
                "-import org.apache.storm.streams.Pair;",
                " import org.apache.storm.utils.ConfigUtils;",
                "@@ -70,3 +58,2 @@ import org.apache.storm.utils.ObjectReader;",
                " import org.apache.storm.utils.ServerUtils;",
                "-import org.apache.storm.utils.ShellUtils;",
                " import org.apache.storm.utils.Utils;",
                "@@ -76,4 +63,2 @@ import org.slf4j.LoggerFactory;",
                "-import static java.nio.file.StandardCopyOption.ATOMIC_MOVE;",
                "-",
                " /**",
                "@@ -83,10 +68,5 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     private static final Logger LOG = LoggerFactory.getLogger(AsyncLocalizer.class);",
                "-    public static final String FILECACHE = \"filecache\";",
                "-    public static final String USERCACHE = \"usercache\";",
                "-    // sub directories to store either files or uncompressed archives respectively",
                "-    public static final String FILESDIR = \"files\";",
                "-    public static final String ARCHIVESDIR = \"archives\";",
                "-    private static final String TO_UNCOMPRESS = \"_tmp_\";",
                "     private static final CompletableFuture<Void> ALL_DONE_FUTURE = new CompletableFuture<>();",
                "+",
                "     static {",
                "@@ -95,15 +75,8 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    private static Set<String> readDownloadedTopologyIds(Map<String, Object> conf) throws IOException {",
                "-        Set<String> stormIds = new HashSet<>();",
                "-        String path = ConfigUtils.supervisorStormDistRoot(conf);",
                "-        Collection<String> rets = ConfigUtils.readDirContents(path);",
                "-        for (String ret : rets) {",
                "-            stormIds.add(URLDecoder.decode(ret, \"UTF-8\"));",
                "-        }",
                "-        return stormIds;",
                "-    }",
                "-",
                "-    private final AtomicReference<Map<Long, LocalAssignment>> currAssignment;",
                "     private final boolean isLocalMode;",
                "-    private final Map<String, LocalDownloadedResource> blobPending;",
                "+    // track resources - user to resourceSet",
                "+    protected final ConcurrentMap<String, ConcurrentMap<String, LocalizedResource>> userFiles = new ConcurrentHashMap<>();",
                "+    protected final ConcurrentMap<String, ConcurrentMap<String, LocalizedResource>> userArchives = new ConcurrentHashMap<>();",
                "+    // topology to tracking of topology dir and resources",
                "+    private final Map<String, CompletableFuture<Void>> blobPending;",
                "     private final Map<String, Object> conf;",
                "@@ -114,7 +87,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    // track resources - user to resourceSet",
                "-    private final ConcurrentMap<String, LocalizedResourceSet> userRsrc = new ConcurrentHashMap<>();",
                "-",
                "-    private final String localBaseDir;",
                "-",
                "+    private final Path localBaseDir;",
                "     private final int blobDownloadRetries;",
                "@@ -123,3 +92,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     // cleanup",
                "-    private long cacheTargetSize;",
                "+    @VisibleForTesting",
                "+    protected long cacheTargetSize;",
                "     private final long cacheCleanupPeriod;",
                "@@ -127,5 +97,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     @VisibleForTesting",
                "-    AsyncLocalizer(Map<String, Object> conf, AdvancedFSOps ops, String baseDir,",
                "-                   AtomicReference<Map<Long, LocalAssignment>> currAssignment,",
                "-                   Map<Integer, LocalAssignment> portToAssignments) throws IOException {",
                "+    AsyncLocalizer(Map<String, Object> conf, AdvancedFSOps ops, String baseDir) throws IOException {",
                "@@ -134,3 +102,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         fsOps = ops;",
                "-        localBaseDir = baseDir;",
                "+        localBaseDir = Paths.get(baseDir);",
                "         // default cache size 10GB, converted to Bytes",
                "@@ -153,10 +121,6 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         blobPending = new HashMap<>();",
                "-        this.currAssignment = currAssignment;",
                "-",
                "-        recoverBlobReferences(portToAssignments);",
                "     }",
                "-    public AsyncLocalizer(Map<String, Object> conf, AtomicReference<Map<Long, LocalAssignment>> currAssignment,",
                "-                          Map<Integer, LocalAssignment> portToAssignments) throws IOException {",
                "-        this(conf, AdvancedFSOps.make(conf), ConfigUtils.supervisorLocalDir(conf), currAssignment, portToAssignments);",
                "+    public AsyncLocalizer(Map<String, Object> conf) throws IOException {",
                "+        this(conf, AdvancedFSOps.make(conf), ConfigUtils.supervisorLocalDir(conf));",
                "     }",
                "@@ -199,17 +163,43 @@ public class AsyncLocalizer implements AutoCloseable {",
                "+    private LocalizedResource getUserArchive(String user, String key) throws IOException {",
                "+        assert user != null : \"All user archives require a user present\";",
                "+        ConcurrentMap<String, LocalizedResource> keyToResource = userArchives.computeIfAbsent(user, (u) -> new ConcurrentHashMap<>());",
                "+        return keyToResource.computeIfAbsent(key, (k) -> new LocalizedResource(key, localBaseDir, true, fsOps, conf, user));",
                "+    }",
                "+",
                "+    private LocalizedResource getUserFile(String user, String key) throws IOException {",
                "+        assert user != null : \"All user archives require a user present\";",
                "+        ConcurrentMap<String, LocalizedResource> keyToResource = userFiles.computeIfAbsent(user, (u) -> new ConcurrentHashMap<>());",
                "+        return keyToResource.computeIfAbsent(key, (k) -> new LocalizedResource(key, localBaseDir, false, fsOps, conf, user));",
                "+    }",
                "+",
                "+    /**",
                "+     * Request that all of the blobs necessary for this topology be downloaded.",
                "+     * @param assignment the assignment that needs the blobs",
                "+     * @param port the port the assignment is a part of",
                "+     * @param cb a callback for when the blobs change.  This is only for blobs that are tied to the lifetime of the worker.",
                "+     * @return a Future that indicates when they are all downloaded.",
                "+     * @throws IOException if there was an error while trying doing it.",
                "+     */",
                "     public synchronized CompletableFuture<Void> requestDownloadTopologyBlobs(final LocalAssignment assignment, final int port,",
                "                                                                              final BlobChangingCallback cb) throws IOException {",
                "-        final String topologyId = assignment.get_topology_id();",
                "+        final PortAndAssignment pna = new PortAndAssignment(port, assignment);",
                "+        final String topologyId = pna.getToplogyId();",
                "-        CompletableFuture<Void> baseBlobs = requestDownloadBaseTopologyBlobs(assignment, port, cb);",
                "+        CompletableFuture<Void> baseBlobs = requestDownloadBaseTopologyBlobs(pna, cb);",
                "         return baseBlobs.thenComposeAsync((v) -> {",
                "-            LocalDownloadedResource localResource = blobPending.get(topologyId);",
                "+            CompletableFuture<Void> localResource = blobPending.get(topologyId);",
                "             if (localResource == null) {",
                "-                Supplier<Void> supplier = new DownloadBlobs(topologyId, assignment.get_owner());",
                "-                localResource = new LocalDownloadedResource(CompletableFuture.supplyAsync(supplier, execService));",
                "+                Supplier<Void> supplier = new DownloadBlobs(pna, cb);",
                "+                localResource = CompletableFuture.supplyAsync(supplier, execService);",
                "                 blobPending.put(topologyId, localResource);",
                "+            } else {",
                "+                try {",
                "+                    addReferencesToBlobs(pna, cb);",
                "+                } catch (Exception e) {",
                "+                    throw new RuntimeException(e);",
                "+                }",
                "             }",
                "-            CompletableFuture<Void> r = localResource.reserve(port, assignment);",
                "             LOG.debug(\"Reserved blobs {} {}\", topologyId, localResource);",
                "-            return r;",
                "+            return localResource;",
                "         });",
                "@@ -218,6 +208,5 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     @VisibleForTesting",
                "-    synchronized CompletableFuture<Void> requestDownloadBaseTopologyBlobs(final LocalAssignment assignment, final int port,",
                "-                                                                          BlobChangingCallback cb) throws IOException {",
                "-        PortAndAssignment pna = new PortAndAssignment(port, assignment);",
                "-        final String topologyId = assignment.get_topology_id();",
                "+    synchronized CompletableFuture<Void> requestDownloadBaseTopologyBlobs(PortAndAssignment pna, BlobChangingCallback cb)",
                "+        throws IOException {",
                "+        final String topologyId = pna.getToplogyId();",
                "@@ -235,2 +224,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "             ret = downloadOrUpdate(topoJar, topoCode, topoConf);",
                "+            topologyBasicDownloaded.put(topologyId, ret);",
                "         }",
                "@@ -242,8 +232,12 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     private CompletableFuture<Void> downloadOrUpdate(LocallyCachedBlob ... blobs) {",
                "-        CompletableFuture<Void> [] all = new CompletableFuture[blobs.length];",
                "-        for (int i = 0; i < blobs.length; i++) {",
                "-            final LocallyCachedBlob blob = blobs[i];",
                "+        return downloadOrUpdate(Arrays.asList(blobs));",
                "+    }",
                "+",
                "+    private CompletableFuture<Void> downloadOrUpdate(Collection<? extends LocallyCachedBlob> blobs) {",
                "+        CompletableFuture<Void> [] all = new CompletableFuture[blobs.size()];",
                "+        int i = 0;",
                "+        for (final LocallyCachedBlob blob: blobs) {",
                "             all[i] = CompletableFuture.runAsync(() -> {",
                "                 LOG.debug(\"STARTING download of {}\", blob);",
                "-                try (ClientBlobStore blobStore = ServerUtils.getClientBlobStoreForSupervisor(conf)) {",
                "+                try (ClientBlobStore blobStore = getClientBlobStore()) {",
                "                     boolean done = false;",
                "@@ -255,3 +249,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "                                 long remoteVersion = blob.getRemoteVersion(blobStore);",
                "-                                if (localVersion != remoteVersion) {",
                "+                                if (localVersion != remoteVersion || !blob.isFullyDownloaded()) {",
                "                                     try {",
                "@@ -279,2 +273,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "             }, execService);",
                "+            i++;",
                "         }",
                "@@ -283,41 +278,2 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    /**",
                "-     * For each of the downloaded topologies, adds references to the blobs that the topologies are using. This is used to reconstruct the",
                "-     * cache on restart.",
                "-     * @param topoId the topology id",
                "-     * @param user the User that owns this topology",
                "-     */",
                "-    private void addBlobReferences(String topoId, String user) throws IOException {",
                "-        Map<String, Object> topoConf = ConfigUtils.readSupervisorStormConf(conf, topoId);",
                "-        @SuppressWarnings(\"unchecked\")",
                "-        Map<String, Map<String, Object>> blobstoreMap = (Map<String, Map<String, Object>>) topoConf.get(Config.TOPOLOGY_BLOBSTORE_MAP);",
                "-        String topoName = (String) topoConf.get(Config.TOPOLOGY_NAME);",
                "-        List<LocalResource> localresources = SupervisorUtils.blobstoreMapToLocalresources(blobstoreMap);",
                "-        if (blobstoreMap != null) {",
                "-            addReferences(localresources, user, topoName);",
                "-        }",
                "-    }",
                "-",
                "-    /**",
                "-     * Pick up where we left off last time.",
                "-     * @param portToAssignments the current set of assignments for the supervisor.",
                "-     */",
                "-    private void recoverBlobReferences(Map<Integer, LocalAssignment> portToAssignments) throws IOException {",
                "-        Set<String> downloadedTopoIds = readDownloadedTopologyIds(conf);",
                "-        if (portToAssignments != null) {",
                "-            Map<String, LocalAssignment> assignments = new HashMap<>();",
                "-            for (LocalAssignment la : portToAssignments.values()) {",
                "-                assignments.put(la.get_topology_id(), la);",
                "-            }",
                "-            for (String topoId : downloadedTopoIds) {",
                "-                LocalAssignment la = assignments.get(topoId);",
                "-                if (la != null) {",
                "-                    addBlobReferences(topoId, la.get_owner());",
                "-                } else {",
                "-                    LOG.warn(\"Could not find an owner for topo {}\", topoId);",
                "-                }",
                "-            }",
                "-        }",
                "-    }",
                "-",
                "     /**",
                "@@ -326,53 +282,29 @@ public class AsyncLocalizer implements AutoCloseable {",
                "      */",
                "-    private void updateBlobs() {",
                "-        for (LocallyCachedBlob blob : topologyBlobs.values()) {",
                "-            if (blob.isUsed()) {",
                "-                try {",
                "-                    downloadOrUpdate(blob);",
                "-                } catch (Exception e) {",
                "-                    LOG.error(\"Could not update {}\", blob, e);",
                "-                }",
                "+    @VisibleForTesting",
                "+    void updateBlobs() {",
                "+        List<CompletableFuture<?>> futures = new ArrayList<>();",
                "+        futures.add(downloadOrUpdate(topologyBlobs.values()));",
                "+        if (symlinksDisabled) {",
                "+            LOG.warn(\"symlinks are disabled so blobs cannot be downloaded.\");",
                "+        } else {",
                "+            for (ConcurrentMap<String, LocalizedResource> map : userArchives.values()) {",
                "+                futures.add(downloadOrUpdate(map.values()));",
                "+            }",
                "+",
                "+            for (ConcurrentMap<String, LocalizedResource> map : userFiles.values()) {",
                "+                futures.add(downloadOrUpdate(map.values()));",
                "             }",
                "         }",
                "-        try {",
                "-            Map<String, String> topoIdToOwner = currAssignment.get().values().stream()",
                "-                .map((la) -> Pair.of(la.get_topology_id(), la.get_owner()))",
                "-                .distinct()",
                "-                .collect(Collectors.toMap(Pair::getFirst, Pair::getSecond));",
                "-            for (String topoId : readDownloadedTopologyIds(conf)) {",
                "-                String owner = topoIdToOwner.get(topoId);",
                "-                if (owner == null) {",
                "-                    //We got a case where the local assignment is not up to date, no point in going on...",
                "-                    LOG.warn(\"The blobs will not be updated for {} until the local assignment is updated...\", topoId);",
                "+        for (CompletableFuture<?> f: futures) {",
                "+            try {",
                "+                f.get();",
                "+            } catch (Exception e) {",
                "+                if (Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {",
                "+                    LOG.error(\"Network error while updating blobs, will retry again later\", e);",
                "+                } else if (Utils.exceptionCauseIsInstanceOf(NimbusLeaderNotFoundException.class, e)) {",
                "+                    LOG.error(\"Nimbus unavailable to update blobs, will retry again later\", e);",
                "                 } else {",
                "-                    String stormRoot = ConfigUtils.supervisorStormDistRoot(conf, topoId);",
                "-                    LOG.debug(\"Checking Blob updates for storm topology id {} With target_dir: {}\", topoId, stormRoot);",
                "-                    updateBlobsForTopology(conf, topoId, owner);",
                "+                    LOG.error(\"Could not update blob, will retry again later\", e);",
                "                 }",
                "             }",
                "-        } catch (Exception e) {",
                "-            if (Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {",
                "-                LOG.error(\"Network error while updating blobs, will retry again later\", e);",
                "-            } else if (Utils.exceptionCauseIsInstanceOf(NimbusLeaderNotFoundException.class, e)) {",
                "-                LOG.error(\"Nimbus unavailable to update blobs, will retry again later\", e);",
                "-            } else {",
                "-                throw Utils.wrapInRuntime(e);",
                "-            }",
                "-        }",
                "-    }",
                "-",
                "-    /**",
                "-     * Update each blob listed in the topology configuration if the latest version of the blob has not been downloaded.",
                "-     */",
                "-    private void updateBlobsForTopology(Map<String, Object> conf, String stormId, String user)",
                "-        throws IOException {",
                "-        Map<String, Object> topoConf = ConfigUtils.readSupervisorStormConf(conf, stormId);",
                "-        Map<String, Map<String, Object>> blobstoreMap = (Map<String, Map<String, Object>>) topoConf.get(Config.TOPOLOGY_BLOBSTORE_MAP);",
                "-        List<LocalResource> localresources = SupervisorUtils.blobstoreMapToLocalresources(blobstoreMap);",
                "-        try {",
                "-            updateBlobs(localresources, user);",
                "-        } catch (AuthorizationException authExp) {",
                "-            LOG.error(\"AuthorizationException error\", authExp);",
                "-        } catch (KeyNotFoundException knf) {",
                "-            LOG.error(\"KeyNotFoundException error\", knf);",
                "         }",
                "@@ -386,2 +318,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         execService.scheduleWithFixedDelay(this::updateBlobs, 30, 30, TimeUnit.SECONDS);",
                "+        LOG.debug(\"Scheduling cleanup every {} millis\", cacheCleanupPeriod);",
                "         execService.scheduleAtFixedRate(this::cleanup, cacheCleanupPeriod, cacheCleanupPeriod, TimeUnit.MILLISECONDS);",
                "@@ -396,10 +329,47 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    //ILocalizer",
                "+    private List<LocalResource> getLocalResources(PortAndAssignment pna) throws IOException {",
                "+        String topologyId = pna.getToplogyId();",
                "+        Map<String, Object> topoConf = ConfigUtils.readSupervisorStormConf(conf, topologyId);",
                "+",
                "+        @SuppressWarnings(\"unchecked\")",
                "+        Map<String, Map<String, Object>> blobstoreMap = (Map<String, Map<String, Object>>) topoConf.get(Config.TOPOLOGY_BLOBSTORE_MAP);",
                "+",
                "+        List<LocalResource> ret = new ArrayList<>();",
                "+        if (blobstoreMap != null) {",
                "+            List<LocalResource> tmp = SupervisorUtils.blobstoreMapToLocalresources(blobstoreMap);",
                "+            if (tmp != null) {",
                "+                ret.addAll(tmp);",
                "+            }",
                "+        }",
                "+",
                "+        StormTopology stormCode = ConfigUtils.readSupervisorTopology(conf, topologyId, fsOps);",
                "+        List<String> dependencies = new ArrayList<>();",
                "+        if (stormCode.is_set_dependency_jars()) {",
                "+            dependencies.addAll(stormCode.get_dependency_jars());",
                "+        }",
                "+        if (stormCode.is_set_dependency_artifacts()) {",
                "+            dependencies.addAll(stormCode.get_dependency_artifacts());",
                "+        }",
                "+        for (String dependency : dependencies) {",
                "+            ret.add(new LocalResource(dependency, false, true));",
                "+        }",
                "+        return ret;",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    void addReferencesToBlobs(PortAndAssignment pna, BlobChangingCallback cb)",
                "+        throws IOException, KeyNotFoundException, AuthorizationException {",
                "+        List<LocalResource> localResourceList = getLocalResources(pna);",
                "+        if (!localResourceList.isEmpty()) {",
                "+            getBlobs(localResourceList, pna, cb);",
                "+        }",
                "+    }",
                "+",
                "     private class DownloadBlobs implements Supplier<Void> {",
                "-        private final String topologyId;",
                "-        private final String topoOwner;",
                "+        private final PortAndAssignment pna;",
                "+        private final BlobChangingCallback cb;",
                "-        public DownloadBlobs(String topologyId, String topoOwner) {",
                "-            this.topologyId = topologyId;",
                "-            this.topoOwner = topoOwner;",
                "+        public DownloadBlobs(PortAndAssignment pna, BlobChangingCallback cb) {",
                "+            this.pna = pna;",
                "+            this.cb = cb;",
                "         }",
                "@@ -409,2 +379,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "             try {",
                "+                String topologyId = pna.getToplogyId();",
                "+                String topoOwner = pna.getOwner();",
                "                 String stormroot = ConfigUtils.supervisorStormDistRoot(conf, topologyId);",
                "@@ -413,25 +385,6 @@ public class AsyncLocalizer implements AutoCloseable {",
                "                 @SuppressWarnings(\"unchecked\")",
                "-                Map<String, Map<String, Object>> blobstoreMap = (Map<String, Map<String, Object>>) topoConf.get(Config.TOPOLOGY_BLOBSTORE_MAP);",
                "-                String topoName = (String) topoConf.get(Config.TOPOLOGY_NAME);",
                "-",
                "-                List<LocalResource> localResourceList = new ArrayList<>();",
                "-                if (blobstoreMap != null) {",
                "-                    List<LocalResource> tmp = SupervisorUtils.blobstoreMapToLocalresources(blobstoreMap);",
                "-                    if (tmp != null) {",
                "-                        localResourceList.addAll(tmp);",
                "-                    }",
                "-                }",
                "-",
                "-                StormTopology stormCode = ConfigUtils.readSupervisorTopology(conf, topologyId, fsOps);",
                "-                List<String> dependencies = new ArrayList<>();",
                "-                if (stormCode.is_set_dependency_jars()) {",
                "-                    dependencies.addAll(stormCode.get_dependency_jars());",
                "-                }",
                "-                if (stormCode.is_set_dependency_artifacts()) {",
                "-                    dependencies.addAll(stormCode.get_dependency_artifacts());",
                "-                }",
                "-                for (String dependency : dependencies) {",
                "-                    localResourceList.add(new LocalResource(dependency, false));",
                "-                }",
                "+                Map<String, Map<String, Object>> blobstoreMap =",
                "+                    (Map<String, Map<String, Object>>) topoConf.get(Config.TOPOLOGY_BLOBSTORE_MAP);",
                "+                List<LocalResource> localResourceList = getLocalResources(pna);",
                "                 if (!localResourceList.isEmpty()) {",
                "@@ -441,3 +394,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "                     }",
                "-                    List<LocalizedResource> localizedResources = getBlobs(localResourceList, topoOwner, topoName, userDir);",
                "+                    List<LocalizedResource> localizedResources = getBlobs(localResourceList, pna, cb);",
                "                     fsOps.setupBlobPermissions(userDir, topoOwner);",
                "@@ -447,3 +400,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "                             //The sym link we are pointing to",
                "-                            File rsrcFilePath = new File(localizedResource.getCurrentSymlinkPath());",
                "+                            File rsrcFilePath = localizedResource.getCurrentSymlinkPath().toFile();",
                "@@ -474,6 +427,14 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    public synchronized void recoverRunningTopology(final LocalAssignment assignment, final int port,",
                "+    /**",
                "+     * Do everything needed to recover the state in the AsyncLocalizer for a running topology.",
                "+     * @param currentAssignment the assignment for the topology.",
                "+     * @param port the port the assignment is on.",
                "+     * @param cb a callback for when the blobs are updated.  This will only be for blobs that",
                "+     *     indicate that if they change the worker should be restarted.",
                "+     * @throws IOException on any error trying to recover the state.",
                "+     */",
                "+    public synchronized void recoverRunningTopology(final LocalAssignment currentAssignment, final int port,",
                "                                                     final BlobChangingCallback cb) throws IOException {",
                "-        PortAndAssignment pna = new PortAndAssignment(port, assignment);",
                "-        final String topologyId = assignment.get_topology_id();",
                "+        final PortAndAssignment pna = new PortAndAssignment(port, currentAssignment);",
                "+        final String topologyId = pna.getToplogyId();",
                "@@ -488,8 +449,14 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-        LocalDownloadedResource localResource = blobPending.get(topologyId);",
                "+        CompletableFuture<Void> localResource = blobPending.get(topologyId);",
                "         if (localResource == null) {",
                "-            localResource = new LocalDownloadedResource(ALL_DONE_FUTURE);",
                "+            localResource = ALL_DONE_FUTURE;",
                "             blobPending.put(topologyId, localResource);",
                "         }",
                "-        localResource.reserve(port, assignment);",
                "+",
                "+        try {",
                "+            addReferencesToBlobs(pna, cb);",
                "+        } catch (KeyNotFoundException | AuthorizationException e) {",
                "+            LOG.error(\"Could not recover all blob references for {}\", pna);",
                "+        }",
                "+",
                "         LOG.debug(\"Recovered blobs {} {}\", topologyId, localResource);",
                "@@ -528,27 +495,8 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-        LocalDownloadedResource localResource = blobPending.get(topologyId);",
                "-        if (localResource == null || !localResource.release(port, assignment)) {",
                "-            LOG.warn(\"Released blob reference {} {} for something that we didn't have {}\", topologyId, port, localResource);",
                "-        } else if (localResource.isDone()){",
                "-            LOG.info(\"Released blob reference {} {} Cleaning up BLOB references...\", topologyId, port);",
                "-            blobPending.remove(topologyId);",
                "-            Map<String, Object> topoConf = ConfigUtils.readSupervisorStormConf(conf, topologyId);",
                "-            @SuppressWarnings(\"unchecked\")",
                "-            Map<String, Map<String, Object>> blobstoreMap = (Map<String, Map<String, Object>>) topoConf.get(Config.TOPOLOGY_BLOBSTORE_MAP);",
                "-            if (blobstoreMap != null) {",
                "-                String user = assignment.get_owner();",
                "-                String topoName = (String) topoConf.get(Config.TOPOLOGY_NAME);",
                "-",
                "-                for (Map.Entry<String, Map<String, Object>> entry : blobstoreMap.entrySet()) {",
                "-                    String key = entry.getKey();",
                "-                    Map<String, Object> blobInfo = entry.getValue();",
                "-                    try {",
                "-                        removeBlobReference(key, user, topoName, SupervisorUtils.shouldUncompressBlob(blobInfo));",
                "-                    } catch (Exception e) {",
                "-                        throw new IOException(e);",
                "-                    }",
                "-                }",
                "+        for (LocalResource lr : getLocalResources(pna)) {",
                "+            try {",
                "+                removeBlobReference(lr.getBlobName(), pna, lr.shouldUncompress());",
                "+            } catch (Exception e) {",
                "+                throw new IOException(e);",
                "             }",
                "-        } else {",
                "-            LOG.debug(\"Released blob reference {} {} still waiting on {}\", topologyId, port, localResource);",
                "         }",
                "@@ -556,22 +504,6 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    //From Localizer",
                "-",
                "-    @VisibleForTesting",
                "-    void setTargetCacheSize(long size) {",
                "-        cacheTargetSize = size;",
                "-    }",
                "-",
                "-    // For testing, be careful as it doesn't clone",
                "-    ConcurrentMap<String, LocalizedResourceSet> getUserResources() {",
                "-        return userRsrc;",
                "-    }",
                "-",
                "-    // baseDir/supervisor/usercache/",
                "-    private File getUserCacheDir() {",
                "-        return new File(localBaseDir, USERCACHE);",
                "-    }",
                "-",
                "     // baseDir/supervisor/usercache/user1/",
                "+    @VisibleForTesting",
                "     File getLocalUserDir(String userName) {",
                "-        return new File(getUserCacheDir(), userName);",
                "+        return LocalizedResource.getLocalUserDir(localBaseDir, userName).toFile();",
                "     }",
                "@@ -579,35 +511,10 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     // baseDir/supervisor/usercache/user1/filecache",
                "+    @VisibleForTesting",
                "     File getLocalUserFileCacheDir(String userName) {",
                "-        return new File(getLocalUserDir(userName), FILECACHE);",
                "+        return LocalizedResource.getLocalUserFileCacheDir(localBaseDir, userName).toFile();",
                "     }",
                "-    // baseDir/supervisor/usercache/user1/filecache/files",
                "-    private File getCacheDirForFiles(File dir) {",
                "-        return new File(dir, FILESDIR);",
                "-    }",
                "-",
                "-    // get the directory to put uncompressed archives in",
                "-    // baseDir/supervisor/usercache/user1/filecache/archives",
                "-    private File getCacheDirForArchives(File dir) {",
                "-        return new File(dir, ARCHIVESDIR);",
                "-    }",
                "-",
                "-    private void addLocalizedResourceInDir(String dir, LocalizedResourceSet lrsrcSet,",
                "-                                           boolean uncompress) {",
                "-        File[] lrsrcs = readCurrentBlobs(dir);",
                "-",
                "-        if (lrsrcs != null) {",
                "-            for (File rsrc : lrsrcs) {",
                "-                LOG.info(\"add localized in dir found: \" + rsrc);",
                "-                /// strip off .suffix",
                "-                String path = rsrc.getPath();",
                "-                int p = path.lastIndexOf('.');",
                "-                if (p > 0) {",
                "-                    path = path.substring(0, p);",
                "-                }",
                "-                LOG.debug(\"local file is: {} path is: {}\", rsrc.getPath(), path);",
                "-                LocalizedResource lrsrc = new LocalizedResource(new File(path).getName(), path,",
                "-                    uncompress);",
                "-                lrsrcSet.add(lrsrc.getKey(), lrsrc, uncompress);",
                "-            }",
                "+    private void recoverLocalizedArchivesForUser(String user) throws IOException {",
                "+        for (String key: LocalizedResource.getLocalizedArchiveKeys(localBaseDir, user)) {",
                "+            getUserArchive(user, key);",
                "         }",
                "@@ -615,10 +522,6 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    // Looks for files in the directory with .current suffix",
                "-    private File[] readCurrentBlobs(String location) {",
                "-        File dir = new File(location);",
                "-        File[] files = null;",
                "-        if (dir.exists()) {",
                "-            files = dir.listFiles((d, name) -> name.toLowerCase().endsWith(ServerUtils.DEFAULT_CURRENT_BLOB_SUFFIX));",
                "+    private void recoverLocalizedFilesForUser(String user) throws IOException {",
                "+        for (String key: LocalizedResource.getLocalizedFileKeys(localBaseDir, user)) {",
                "+            getUserFile(user, key);",
                "         }",
                "-        return files;",
                "     }",
                "@@ -628,21 +531,12 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         try {",
                "-            LOG.info(\"Reconstruct localized resource: \" + getUserCacheDir().getPath());",
                "-            Collection<File> users = ConfigUtils.readDirFiles(getUserCacheDir().getPath());",
                "+            LOG.info(\"Reconstruct localized resources\");",
                "+            Collection<String> users = LocalizedResource.getLocalizedUsers(localBaseDir);",
                "             if (!(users == null || users.isEmpty())) {",
                "-                for (File userDir : users) {",
                "-                    String user = userDir.getName();",
                "-                    LOG.debug(\"looking in: {} for user: {}\", userDir.getPath(), user);",
                "-                    LocalizedResourceSet newSet = new LocalizedResourceSet(user);",
                "-                    LocalizedResourceSet lrsrcSet = userRsrc.putIfAbsent(user, newSet);",
                "-                    if (lrsrcSet == null) {",
                "-                        lrsrcSet = newSet;",
                "-                    }",
                "-                    addLocalizedResourceInDir(getCacheDirForFiles(getLocalUserFileCacheDir(user)).getPath(),",
                "-                        lrsrcSet, false);",
                "-                    addLocalizedResourceInDir(",
                "-                        getCacheDirForArchives(getLocalUserFileCacheDir(user)).getPath(),",
                "-                        lrsrcSet, true);",
                "+                for (String user : users) {",
                "+                    LOG.debug(\"reconstructing resources owned by {}\", user);",
                "+                    recoverLocalizedFilesForUser(user);",
                "+                    recoverLocalizedArchivesForUser(user);",
                "                 }",
                "             } else {",
                "-                LOG.warn(\"No left over resources found for any user during reconstructing of local resources at: {}\", getUserCacheDir().getPath());",
                "+                LOG.debug(\"No left over resources found for any user\");",
                "             }",
                "@@ -654,13 +548,15 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     // ignores invalid user/topo/key",
                "-    synchronized void removeBlobReference(String key, String user, String topo,",
                "+    synchronized void removeBlobReference(String key, PortAndAssignment pna,",
                "                                           boolean uncompress) throws AuthorizationException, KeyNotFoundException {",
                "-        LocalizedResourceSet lrsrcSet = userRsrc.get(user);",
                "+        String user = pna.getOwner();",
                "+        String topo = pna.getToplogyId();",
                "+        ConcurrentMap<String, LocalizedResource> lrsrcSet = uncompress ? userArchives.get(user) : userFiles.get(user);",
                "         if (lrsrcSet != null) {",
                "-            LocalizedResource lrsrc = lrsrcSet.get(key, uncompress);",
                "+            LocalizedResource lrsrc = lrsrcSet.get(key);",
                "             if (lrsrc != null) {",
                "                 LOG.debug(\"removing blob reference to: {} for topo: {}\", key, topo);",
                "-                lrsrc.removeReference(topo);",
                "+                lrsrc.removeReference(pna);",
                "             } else {",
                "-                LOG.warn(\"trying to remove non-existent blob, key: \" + key + \" for user: \" + user +",
                "-                    \" topo: \" + topo);",
                "+                LOG.warn(\"trying to remove non-existent blob, key: \" + key + \" for user: \" + user",
                "+                    + \" topo: \" + topo);",
                "             }",
                "@@ -672,52 +568,2 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    synchronized void addReferences(List<LocalResource> localresource, String user,",
                "-                                    String topo) {",
                "-        LocalizedResourceSet lrsrcSet = userRsrc.get(user);",
                "-        if (lrsrcSet != null) {",
                "-            for (LocalResource blob : localresource) {",
                "-                LocalizedResource lrsrc = lrsrcSet.get(blob.getBlobName(), blob.shouldUncompress());",
                "-                if (lrsrc != null) {",
                "-                    lrsrc.addReference(topo);",
                "-                    LOG.debug(\"added reference for topo: {} key: {}\", topo, blob);",
                "-                } else {",
                "-                    LOG.warn(\"trying to add reference to non-existent blob, key: \" + blob + \" topo: \" + topo);",
                "-                }",
                "-            }",
                "-        } else {",
                "-            LOG.warn(\"trying to add reference to non-existent local resource set, \" +",
                "-                \"user: \" + user + \" topo: \" + topo);",
                "-        }",
                "-    }",
                "-",
                "-    /**",
                "-     * This function either returns the blob in the existing cache or if it doesn't exist in the",
                "-     * cache, it will download the blob and will block until the download is complete.",
                "-     */",
                "-    LocalizedResource getBlob(LocalResource localResource, String user, String topo,",
                "-                                     File userFileDir) throws AuthorizationException, KeyNotFoundException, IOException {",
                "-        ArrayList<LocalResource> arr = new ArrayList<>();",
                "-        arr.add(localResource);",
                "-        List<LocalizedResource> results = getBlobs(arr, user, topo, userFileDir);",
                "-        if (results.isEmpty() || results.size() != 1) {",
                "-            throw new IOException(\"Unknown error getting blob: \" + localResource + \", for user: \" + user +",
                "-                \", topo: \" + topo);",
                "-        }",
                "-        return results.get(0);",
                "-    }",
                "-",
                "-    private boolean isLocalizedResourceDownloaded(LocalizedResource lrsrc) {",
                "-        File rsrcFileCurrent = new File(lrsrc.getCurrentSymlinkPath());",
                "-        File rsrcFileWithVersion = new File(lrsrc.getFilePathWithVersion());",
                "-        File versionFile = new File(lrsrc.getVersionFilePath());",
                "-        return (rsrcFileWithVersion.exists() && rsrcFileCurrent.exists() && versionFile.exists());",
                "-    }",
                "-",
                "-    private boolean isLocalizedResourceUpToDate(LocalizedResource lrsrc,",
                "-                                                ClientBlobStore blobstore) throws AuthorizationException, KeyNotFoundException {",
                "-        String localFile = lrsrc.getFilePath();",
                "-        long nimbusBlobVersion = ServerUtils.nimbusVersionOfBlob(lrsrc.getKey(), blobstore);",
                "-        long currentBlobVersion = ServerUtils.localVersionOfBlob(localFile);",
                "-        return (nimbusBlobVersion == currentBlobVersion);",
                "-    }",
                "-",
                "     protected ClientBlobStore getClientBlobStore() {",
                "@@ -726,73 +572,2 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    /**",
                "-     * This function updates blobs on the supervisor. It uses a separate thread pool and runs",
                "-     * asynchronously of the download and delete.",
                "-     */",
                "-    List<LocalizedResource> updateBlobs(List<LocalResource> localResources,",
                "-                                        String user) throws AuthorizationException, KeyNotFoundException, IOException {",
                "-        LocalizedResourceSet lrsrcSet = userRsrc.get(user);",
                "-        ArrayList<LocalizedResource> results = new ArrayList<>();",
                "-        ArrayList<Callable<LocalizedResource>> updates = new ArrayList<>();",
                "-",
                "-        if (lrsrcSet == null) {",
                "-            // resource set must have been removed",
                "-            return results;",
                "-        }",
                "-        ClientBlobStore blobstore = null;",
                "-        try {",
                "-            blobstore = getClientBlobStore();",
                "-            for (LocalResource localResource: localResources) {",
                "-                String key = localResource.getBlobName();",
                "-                LocalizedResource lrsrc = lrsrcSet.get(key, localResource.shouldUncompress());",
                "-                if (lrsrc == null) {",
                "-                    LOG.warn(\"blob requested for update doesn't exist: {}\", key);",
                "-                } else if ((boolean) conf.getOrDefault(Config.DISABLE_SYMLINKS, false)) {",
                "-                    LOG.warn(\"symlinks are disabled so blobs cannot be downloaded.\");",
                "-                } else {",
                "-                    // update it if either the version isn't the latest or if any local blob files are missing",
                "-                    if (!isLocalizedResourceUpToDate(lrsrc, blobstore) ||",
                "-                        !isLocalizedResourceDownloaded(lrsrc)) {",
                "-                        LOG.debug(\"updating blob: {}\", key);",
                "-                        updates.add(new DownloadBlob(this, conf, key, new File(lrsrc.getFilePath()), user,",
                "-                            lrsrc.isUncompressed(), true));",
                "-                    }",
                "-                }",
                "-            }",
                "-        } finally {",
                "-            if(blobstore != null) {",
                "-                blobstore.shutdown();",
                "-            }",
                "-        }",
                "-        try {",
                "-            List<Future<LocalizedResource>> futures = execService.invokeAll(updates);",
                "-            for (Future<LocalizedResource> futureRsrc : futures) {",
                "-                try {",
                "-                    LocalizedResource lrsrc = futureRsrc.get();",
                "-                    // put the resource just in case it was removed at same time by the cleaner",
                "-                    LocalizedResourceSet newSet = new LocalizedResourceSet(user);",
                "-                    LocalizedResourceSet newlrsrcSet = userRsrc.putIfAbsent(user, newSet);",
                "-                    if (newlrsrcSet == null) {",
                "-                        newlrsrcSet = newSet;",
                "-                    }",
                "-                    newlrsrcSet.putIfAbsent(lrsrc.getKey(), lrsrc, lrsrc.isUncompressed());",
                "-                    results.add(lrsrc);",
                "-                }",
                "-                catch (ExecutionException e) {",
                "-                    LOG.error(\"Error updating blob: \", e);",
                "-                    if (e.getCause() instanceof AuthorizationException) {",
                "-                        throw (AuthorizationException)e.getCause();",
                "-                    }",
                "-                    if (e.getCause() instanceof KeyNotFoundException) {",
                "-                        throw (KeyNotFoundException)e.getCause();",
                "-                    }",
                "-                }",
                "-            }",
                "-        } catch (RejectedExecutionException re) {",
                "-            LOG.error(\"Error updating blobs : \", re);",
                "-        } catch (InterruptedException ie) {",
                "-            throw new IOException(\"Interrupted Exception\", ie);",
                "-        }",
                "-        return results;",
                "-    }",
                "-",
                "     /**",
                "@@ -800,6 +575,5 @@ public class AsyncLocalizer implements AutoCloseable {",
                "      * cache, it downloads them in parallel (up to SUPERVISOR_BLOBSTORE_DOWNLOAD_THREAD_COUNT)",
                "-     * and will block until all of them have been downloaded",
                "+     * and will block until all of them have been downloaded.",
                "      */",
                "-    synchronized List<LocalizedResource> getBlobs(List<LocalResource> localResources,",
                "-                                                  String user, String topo, File userFileDir)",
                "+    synchronized List<LocalizedResource> getBlobs(List<LocalResource> localResources, PortAndAssignment pna, BlobChangingCallback cb)",
                "         throws AuthorizationException, KeyNotFoundException, IOException {",
                "@@ -808,13 +582,7 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         }",
                "-        LocalizedResourceSet newSet = new LocalizedResourceSet(user);",
                "-        LocalizedResourceSet lrsrcSet = userRsrc.putIfAbsent(user, newSet);",
                "-        if (lrsrcSet == null) {",
                "-            lrsrcSet = newSet;",
                "-        }",
                "+        String user = pna.getOwner();",
                "         ArrayList<LocalizedResource> results = new ArrayList<>();",
                "-        ArrayList<Callable<LocalizedResource>> downloads = new ArrayList<>();",
                "+        List<CompletableFuture<?>> futures = new ArrayList<>();",
                "-        ClientBlobStore blobstore = null;",
                "         try {",
                "-            blobstore = getClientBlobStore();",
                "             for (LocalResource localResource: localResources) {",
                "@@ -822,15 +590,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "                 boolean uncompress = localResource.shouldUncompress();",
                "-                LocalizedResource lrsrc = lrsrcSet.get(key, localResource.shouldUncompress());",
                "-                boolean isUpdate = false;",
                "-                if ((lrsrc != null) && (lrsrc.isUncompressed() == localResource.shouldUncompress()) &&",
                "-                    (isLocalizedResourceDownloaded(lrsrc))) {",
                "-                    if (isLocalizedResourceUpToDate(lrsrc, blobstore)) {",
                "-                        LOG.debug(\"blob already exists: {}\", key);",
                "-                        lrsrc.addReference(topo);",
                "-                        results.add(lrsrc);",
                "-                        continue;",
                "-                    }",
                "-                    LOG.debug(\"blob exists but isn't up to date: {}\", key);",
                "-                    isUpdate = true;",
                "-                }",
                "+                LocalizedResource lrsrc = uncompress ? getUserArchive(user, key) : getUserFile(user, key);",
                "@@ -839,34 +595,14 @@ public class AsyncLocalizer implements AutoCloseable {",
                "                 LOG.debug(\"fetching blob: {}\", key);",
                "-                File downloadDir = getCacheDirForFiles(userFileDir);",
                "-                File localFile = new File(downloadDir, key);",
                "-                if (uncompress) {",
                "-                    // for compressed file, download to archives dir",
                "-                    downloadDir = getCacheDirForArchives(userFileDir);",
                "-                    localFile = new File(downloadDir, key);",
                "-                }",
                "-                downloadDir.mkdir();",
                "-                downloads.add(new DownloadBlob(this, conf, key, localFile, user, uncompress,",
                "-                    isUpdate));",
                "-            }",
                "-        } finally {",
                "-            if(blobstore !=null) {",
                "-                blobstore.shutdown();",
                "-            }",
                "-        }",
                "-        try {",
                "-            List<Future<LocalizedResource>> futures = execService.invokeAll(downloads);",
                "-            for (Future<LocalizedResource> futureRsrc: futures) {",
                "-                LocalizedResource lrsrc = futureRsrc.get();",
                "-                lrsrc.addReference(topo);",
                "-                lrsrcSet.add(lrsrc.getKey(), lrsrc, lrsrc.isUncompressed());",
                "+                futures.add(downloadOrUpdate(lrsrc));",
                "                 results.add(lrsrc);",
                "+                lrsrc.addReference(pna, localResource.needsCallback() ? cb : null);",
                "             }",
                "-        } catch (ExecutionException e) {",
                "-            if (e.getCause() instanceof AuthorizationException)",
                "-                throw (AuthorizationException)e.getCause();",
                "-            else if (e.getCause() instanceof KeyNotFoundException) {",
                "-                throw (KeyNotFoundException)e.getCause();",
                "-            } else {",
                "-                throw new IOException(\"Error getting blobs\", e);",
                "+",
                "+            for (CompletableFuture<?> futureRsrc: futures) {",
                "+                futureRsrc.get();",
                "             }",
                "+        } catch (ExecutionException e) {",
                "+            Utils.unwrapAndThrow(AuthorizationException.class, e);",
                "+            Utils.unwrapAndThrow(KeyNotFoundException.class, e);",
                "+            throw new IOException(\"Error getting blobs\", e);",
                "         } catch (RejectedExecutionException re) {",
                "@@ -879,197 +615,2 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    static class DownloadBlob implements Callable<LocalizedResource> {",
                "-",
                "-        private AsyncLocalizer localizer;",
                "-        private Map conf;",
                "-        private String key;",
                "-        private File localFile;",
                "-        private String user;",
                "-        private boolean uncompress;",
                "-        private boolean isUpdate;",
                "-",
                "-        DownloadBlob(AsyncLocalizer localizer, Map<String, Object> conf, String key, File localFile,",
                "-                     String user, boolean uncompress, boolean update) {",
                "-            this.localizer = localizer;",
                "-            this.conf = conf;",
                "-            this.key = key;",
                "-            this.localFile = localFile;",
                "-            this.user = user;",
                "-            this.uncompress = uncompress;",
                "-            isUpdate = update;",
                "-        }",
                "-",
                "-        @Override",
                "-        public LocalizedResource call()",
                "-            throws AuthorizationException, KeyNotFoundException, IOException  {",
                "-            return localizer.downloadBlob(conf, key, localFile, user, uncompress,",
                "-                isUpdate);",
                "-        }",
                "-    }",
                "-",
                "-    private LocalizedResource downloadBlob(Map<String, Object> conf, String key, File localFile,",
                "-                                           String user, boolean uncompress, boolean isUpdate)",
                "-        throws AuthorizationException, KeyNotFoundException, IOException {",
                "-        ClientBlobStore blobstore = null;",
                "-        try {",
                "-            blobstore = getClientBlobStore();",
                "-            long nimbusBlobVersion = ServerUtils.nimbusVersionOfBlob(key, blobstore);",
                "-            long oldVersion = ServerUtils.localVersionOfBlob(localFile.toString());",
                "-            FileOutputStream out = null;",
                "-            PrintWriter writer = null;",
                "-            int numTries = 0;",
                "-            String localizedPath = localFile.toString();",
                "-            String localFileWithVersion = ServerUtils.constructBlobWithVersionFileName(localFile.toString(),",
                "-                nimbusBlobVersion);",
                "-            String localVersionFile = ServerUtils.constructVersionFileName(localFile.toString());",
                "-            String downloadFile = localFileWithVersion;",
                "-            if (uncompress) {",
                "-                // we need to download to temp file and then unpack into the one requested",
                "-                downloadFile = new File(localFile.getParent(), TO_UNCOMPRESS + localFile.getName()).toString();",
                "-            }",
                "-            while (numTries < blobDownloadRetries) {",
                "-                out = new FileOutputStream(downloadFile);",
                "-                numTries++;",
                "-                try {",
                "-                    if (!ServerUtils.canUserReadBlob(blobstore.getBlobMeta(key), user, conf)) {",
                "-                        throw new AuthorizationException(user + \" does not have READ access to \" + key);",
                "-                    }",
                "-                    InputStreamWithMeta in = blobstore.getBlob(key);",
                "-                    byte[] buffer = new byte[1024];",
                "-                    int len;",
                "-                    while ((len = in.read(buffer)) >= 0) {",
                "-                        out.write(buffer, 0, len);",
                "-                    }",
                "-                    out.close();",
                "-                    in.close();",
                "-                    if (uncompress) {",
                "-                        ServerUtils.unpack(new File(downloadFile), new File(localFileWithVersion));",
                "-                        LOG.debug(\"uncompressed \" + downloadFile + \" to: \" + localFileWithVersion);",
                "-                    }",
                "-",
                "-                    // Next write the version.",
                "-                    LOG.info(\"Blob: \" + key + \" updated with new Nimbus-provided version: \" +",
                "-                        nimbusBlobVersion + \" local version was: \" + oldVersion);",
                "-                    // The false parameter ensures overwriting the version file, not appending",
                "-                    writer = new PrintWriter(",
                "-                        new BufferedWriter(new FileWriter(localVersionFile, false)));",
                "-                    writer.println(nimbusBlobVersion);",
                "-                    writer.close();",
                "-",
                "-                    try {",
                "-                        setBlobPermissions(conf, user, localFileWithVersion);",
                "-                        setBlobPermissions(conf, user, localVersionFile);",
                "-",
                "-                        // Update the key.current symlink. First create tmp symlink and do",
                "-                        // move of tmp to current so that the operation is atomic.",
                "-                        String tmp_uuid_local = java.util.UUID.randomUUID().toString();",
                "-                        LOG.debug(\"Creating a symlink @\" + localFile + \".\" + tmp_uuid_local + \" , \" +",
                "-                            \"linking to: \" + localFile + \".\" + nimbusBlobVersion);",
                "-                        File uuid_symlink = new File(localFile + \".\" + tmp_uuid_local);",
                "-",
                "-                        Files.createSymbolicLink(uuid_symlink.toPath(),",
                "-                            Paths.get(ServerUtils.constructBlobWithVersionFileName(localFile.toString(),",
                "-                                nimbusBlobVersion)));",
                "-                        File current_symlink = new File(ServerUtils.constructBlobCurrentSymlinkName(",
                "-                            localFile.toString()));",
                "-                        Files.move(uuid_symlink.toPath(), current_symlink.toPath(), ATOMIC_MOVE);",
                "-                    } catch (IOException e) {",
                "-                        // if we fail after writing the version file but before we move current link we need to",
                "-                        // restore the old version to the file",
                "-                        try {",
                "-                            PrintWriter restoreWriter = new PrintWriter(",
                "-                                new BufferedWriter(new FileWriter(localVersionFile, false)));",
                "-                            restoreWriter.println(oldVersion);",
                "-                            restoreWriter.close();",
                "-                        } catch (IOException ignore) {}",
                "-                        throw e;",
                "-                    }",
                "-",
                "-                    String oldBlobFile = localFile + \".\" + oldVersion;",
                "-                    try {",
                "-                        // Remove the old version. Note that if a number of processes have that file open,",
                "-                        // the OS will keep the old blob file around until they all close the handle and only",
                "-                        // then deletes it. No new process will open the old blob, since the users will open the",
                "-                        // blob through the \"blob.current\" symlink, which always points to the latest version of",
                "-                        // a blob. Remove the old version after the current symlink is updated as to not affect",
                "-                        // anyone trying to read it.",
                "-                        if ((oldVersion != -1) && (oldVersion != nimbusBlobVersion)) {",
                "-                            LOG.info(\"Removing an old blob file:\" + oldBlobFile);",
                "-                            Files.delete(Paths.get(oldBlobFile));",
                "-                        }",
                "-                    } catch (IOException e) {",
                "-                        // At this point we have downloaded everything and moved symlinks.  If the remove of",
                "-                        // old fails just log an error",
                "-                        LOG.error(\"Exception removing old blob version: \" + oldBlobFile);",
                "-                    }",
                "-",
                "-                    break;",
                "-                } catch (AuthorizationException ae) {",
                "-                    // we consider this non-retriable exceptions",
                "-                    if (out != null) {",
                "-                        out.close();",
                "-                    }",
                "-                    new File(downloadFile).delete();",
                "-                    throw ae;",
                "-                } catch (IOException | KeyNotFoundException e) {",
                "-                    if (out != null) {",
                "-                        out.close();",
                "-                    }",
                "-                    if (writer != null) {",
                "-                        writer.close();",
                "-                    }",
                "-                    new File(downloadFile).delete();",
                "-                    if (uncompress) {",
                "-                        try {",
                "-                            FileUtils.deleteDirectory(new File(localFileWithVersion));",
                "-                        } catch (IOException ignore) {}",
                "-                    }",
                "-                    if (!isUpdate) {",
                "-                        // don't want to remove existing version file if its an update",
                "-                        new File(localVersionFile).delete();",
                "-                    }",
                "-",
                "-                    if (numTries < blobDownloadRetries) {",
                "-                        LOG.error(\"Failed to download blob, retrying\", e);",
                "-                    } else {",
                "-                        throw e;",
                "-                    }",
                "-                }",
                "-            }",
                "-            return new LocalizedResource(key, localizedPath, uncompress);",
                "-        } finally {",
                "-            if(blobstore != null) {",
                "-                blobstore.shutdown();",
                "-            }",
                "-        }",
                "-    }",
                "-",
                "-    private void setBlobPermissions(Map<String, Object> conf, String user, String path)",
                "-        throws IOException {",
                "-",
                "-        if (!ObjectReader.getBoolean(conf.get(Config.SUPERVISOR_RUN_WORKER_AS_USER), false)) {",
                "-            return;",
                "-        }",
                "-        String wlCommand = ObjectReader.getString(conf.get(Config.SUPERVISOR_WORKER_LAUNCHER), \"\");",
                "-        if (wlCommand.isEmpty()) {",
                "-            String stormHome = System.getProperty(\"storm.home\");",
                "-            wlCommand = stormHome + \"/bin/worker-launcher\";",
                "-        }",
                "-        List<String> command = new ArrayList<>(Arrays.asList(wlCommand, user, \"blob\", path));",
                "-",
                "-        String[] commandArray = command.toArray(new String[command.size()]);",
                "-        ShellUtils.ShellCommandExecutor shExec = new ShellUtils.ShellCommandExecutor(commandArray);",
                "-        LOG.info(\"Setting blob permissions, command: {}\", Arrays.toString(commandArray));",
                "-",
                "-        try {",
                "-            shExec.execute();",
                "-            LOG.debug(\"output: {}\", shExec.getOutput());",
                "-        } catch (ShellUtils.ExitCodeException e) {",
                "-            int exitCode = shExec.getExitCode();",
                "-            LOG.warn(\"Exit code from worker-launcher is : \" + exitCode, e);",
                "-            LOG.debug(\"output: {}\", shExec.getOutput());",
                "-            throw new IOException(\"Setting blob permissions failed\" +",
                "-                \" (exitCode=\" + exitCode + \") with output: \" + shExec.getOutput(), e);",
                "-        }",
                "-    }",
                "-",
                "     private interface ConsumePathAndId {",
                "@@ -1096,5 +637,10 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         // need one large set of all and then clean via LRU",
                "-        for (LocalizedResourceSet t : userRsrc.values()) {",
                "-            toClean.addResources(t);",
                "-            LOG.debug(\"Resources to be cleaned after adding {} : {}\", t.getUser(), toClean);",
                "+        for (Map.Entry<String, ConcurrentMap<String, LocalizedResource>> t : userArchives.entrySet()) {",
                "+            toClean.addResources(t.getValue());",
                "+            LOG.debug(\"Resources to be cleaned after adding {} archives : {}\", t.getKey(), toClean);",
                "+        }",
                "+",
                "+        for (Map.Entry<String, ConcurrentMap<String, LocalizedResource>> t : userFiles.entrySet()) {",
                "+            toClean.addResources(t.getValue());",
                "+            LOG.debug(\"Resources to be cleaned after adding {} files : {}\", t.getKey(), toClean);",
                "         }",
                "@@ -1102,3 +648,5 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         toClean.addResources(topologyBlobs);",
                "-        toClean.cleanup();",
                "+        try (ClientBlobStore store = getClientBlobStore()) {",
                "+            toClean.cleanup(store);",
                "+        }",
                "@@ -1111,2 +659,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         topologyBasicDownloaded.keySet().removeIf(topoId -> !safeTopologyIds.contains(topoId));",
                "+        blobPending.keySet().removeIf(topoId -> !safeTopologyIds.contains(topoId));",
                "@@ -1123,15 +672,17 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         LOG.debug(\"Resource cleanup: {}\", toClean);",
                "-        for (LocalizedResourceSet t : userRsrc.values()) {",
                "-            if (t.getSize() == 0) {",
                "-                String user = t.getUser();",
                "+        Set<String> allUsers = new HashSet<>(userArchives.keySet());",
                "+        allUsers.addAll(userFiles.keySet());",
                "+        for (String user: allUsers) {",
                "+            ConcurrentMap<String, LocalizedResource> filesForUser = userFiles.get(user);",
                "+            ConcurrentMap<String, LocalizedResource> archivesForUser = userArchives.get(user);",
                "+            if ((filesForUser == null || filesForUser.size() == 0)",
                "+                && (archivesForUser == null || archivesForUser.size() == 0)) {",
                "                 LOG.debug(\"removing empty set: {}\", user);",
                "-                File userFileCacheDir = getLocalUserFileCacheDir(user);",
                "-                getCacheDirForFiles(userFileCacheDir).delete();",
                "-                getCacheDirForArchives(userFileCacheDir).delete();",
                "-                getLocalUserFileCacheDir(user).delete();",
                "-                boolean dirsRemoved = getLocalUserDir(user).delete();",
                "-                // to catch race with update thread",
                "-                if (dirsRemoved) {",
                "-                    userRsrc.remove(user);",
                "+                try {",
                "+                    LocalizedResource.completelyRemoveUnusedUser(localBaseDir, user);",
                "+                    userFiles.remove(user);",
                "+                    userArchives.remove(user);",
                "+                } catch (IOException e) {",
                "+                    LOG.error(\"Error trying to delete cached user files\", e);",
                "                 }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocalDownloadedResource.java b/storm-server/src/main/java/org/apache/storm/localizer/LocalDownloadedResource.java",
                "deleted file mode 100644",
                "index 2d2e87983..000000000",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocalDownloadedResource.java",
                "+++ /dev/null",
                "@@ -1,86 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-",
                "-package org.apache.storm.localizer;",
                "-",
                "-import java.util.HashSet;",
                "-import java.util.Set;",
                "-import java.util.concurrent.CompletableFuture;",
                "-",
                "-import org.apache.storm.generated.LocalAssignment;",
                "-import org.slf4j.Logger;",
                "-import org.slf4j.LoggerFactory;",
                "-",
                "-/**",
                "- * Used for accounting to keep track of who is waiting for specific resources to be downloaded.",
                "- */",
                "-public class LocalDownloadedResource {",
                "-    private static final Logger LOG = LoggerFactory.getLogger(LocalDownloadedResource.class);",
                "-    private final CompletableFuture<Void> pending;",
                "-    private final Set<PortAndAssignment> references;",
                "-    private boolean isDone;",
                "-    ",
                "-    ",
                "-    public LocalDownloadedResource(CompletableFuture<Void> pending) {",
                "-        this.pending = pending;",
                "-        references = new HashSet<>();",
                "-        isDone = false;",
                "-    }",
                "-",
                "-    /**",
                "-     * Reserve the resources",
                "-     * @param port the port this is for",
                "-     * @param la the assignment this is for",
                "-     * @return a future that can be used to track it being downloaded.",
                "-     */",
                "-    public synchronized CompletableFuture<Void> reserve(int port, LocalAssignment la) {",
                "-        PortAndAssignment pna = new PortAndAssignment(port, la);",
                "-        if (!references.add(pna)) {",
                "-            LOG.warn(\"Resources {} already reserved {} for this topology\", pna, references);",
                "-        }",
                "-        return pending;",
                "-    }",
                "-    ",
                "-    /**",
                "-     * Release a port from the reference count, and update isDone if all is done.",
                "-     * @param port the port to release",
                "-     * @param la the assignment to release",
                "-     * @return true if the port was being counted else false",
                "-     */",
                "-    public synchronized boolean release(int port, LocalAssignment la) {",
                "-        PortAndAssignment pna = new PortAndAssignment(port, la);",
                "-        boolean ret = references.remove(pna);",
                "-        if (ret && references.isEmpty()) {",
                "-            isDone = true;",
                "-        }",
                "-        return ret;",
                "-    }",
                "-    ",
                "-    /**",
                "-     * Is this has been cleaned up completely.",
                "-     * @return true if it is done else false",
                "-     */",
                "-    public synchronized boolean isDone() {",
                "-        return isDone;",
                "-    }",
                "-",
                "-    @Override",
                "-    public String toString() {",
                "-        return references.toString();",
                "-    }",
                "-}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocalResource.java b/storm-server/src/main/java/org/apache/storm/localizer/LocalResource.java",
                "index f38f6f676..862349b66 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocalResource.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocalResource.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.localizer;",
                "@@ -20,25 +21,37 @@ package org.apache.storm.localizer;",
                " /**",
                "- * Local Resource requested by the topology",
                "+ * Local Resource requested by the topology.",
                "  */",
                " public class LocalResource {",
                "-  private String _blobKey;",
                "-  private boolean _uncompress;",
                "+    private final boolean needsCallback;",
                "+    private final String blobKey;",
                "+    private final boolean uncompress;",
                "+",
                "+    /**",
                "+     * Constructor.",
                "+     * @param keyname the key of the blob to download.",
                "+     * @param uncompress should the blob be uncompressed or not.",
                "+     * @param needsCallback if the blobs changes should a callback happen so the worker is restarted.",
                "+     */",
                "+    public LocalResource(String keyname, boolean uncompress, boolean needsCallback) {",
                "+        blobKey = keyname;",
                "+        this.uncompress = uncompress;",
                "+        this.needsCallback = needsCallback;",
                "+    }",
                "-  public LocalResource(String keyname, boolean uncompress) {",
                "-    _blobKey = keyname;",
                "-    _uncompress = uncompress;",
                "-  }",
                "+    public String getBlobName() {",
                "+        return blobKey;",
                "+    }",
                "-  public String getBlobName() {",
                "-    return _blobKey;",
                "-  }",
                "+    public boolean shouldUncompress() {",
                "+        return uncompress;",
                "+    }",
                "-  public boolean shouldUncompress() {",
                "-    return _uncompress;",
                "-  }",
                "+    public boolean needsCallback() {",
                "+        return needsCallback;",
                "+    }",
                "-  @Override",
                "-  public String toString() {",
                "-    return \"Key: \" + _blobKey + \" uncompress: \" + _uncompress;",
                "-  }",
                "+    @Override",
                "+    public String toString() {",
                "+        return \"Key: \" + blobKey + \" uncompress: \" + uncompress;",
                "+    }",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java b/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java",
                "index 7241976b2..4f01c304f 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java",
                "@@ -17,5 +17,40 @@",
                "  */",
                "+",
                " package org.apache.storm.localizer;",
                "+import static java.nio.file.StandardCopyOption.ATOMIC_MOVE;",
                "+",
                "+import com.google.common.annotations.VisibleForTesting;",
                "+import java.io.BufferedReader;",
                "+import java.io.BufferedWriter;",
                "+import java.io.FileOutputStream;",
                "+import java.io.FileReader;",
                "+import java.io.FileWriter;",
                "+import java.io.IOException;",
                "+import java.io.PrintWriter;",
                "+import java.nio.file.DirectoryStream;",
                "+import java.nio.file.Files;",
                "+import java.nio.file.NoSuchFileException;",
                "+import java.nio.file.Path;",
                "+import java.nio.file.Paths;",
                "+import java.util.ArrayList;",
                "+import java.util.Arrays;",
                "+import java.util.Collection;",
                "+import java.util.Collections;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import java.util.regex.Matcher;",
                "+import java.util.regex.Pattern;",
                "+import java.util.stream.Collectors;",
                "+import org.apache.commons.io.FileUtils;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.blobstore.ClientBlobStore;",
                "+import org.apache.storm.blobstore.InputStreamWithMeta;",
                "+import org.apache.storm.daemon.supervisor.IAdvancedFSOps;",
                "+import org.apache.storm.generated.AuthorizationException;",
                "+import org.apache.storm.generated.KeyNotFoundException;",
                "+import org.apache.storm.generated.ReadableBlobMeta;",
                "+import org.apache.storm.utils.ObjectReader;",
                " import org.apache.storm.utils.ServerUtils;",
                "+import org.apache.storm.utils.ShellUtils;",
                " import org.slf4j.Logger;",
                "@@ -23,7 +58,2 @@ import org.slf4j.LoggerFactory;",
                "-import java.io.File;",
                "-import java.util.HashSet;",
                "-import java.util.Set;",
                "-import java.util.concurrent.atomic.AtomicLong;",
                "-",
                " /**",
                "@@ -33,111 +63,386 @@ import java.util.concurrent.atomic.AtomicLong;",
                "  */",
                "-public class LocalizedResource {",
                "-  public static final Logger LOG = LoggerFactory.getLogger(LocalizedResource.class);",
                "-",
                "-  // filesystem path to the resource",
                "-  private final String _localPath;",
                "-  private final String _versionFilePath;",
                "-  private final String _symlinkPath;",
                "-  private final String _key;",
                "-  private final boolean _uncompressed;",
                "-  // _size of the resource",
                "-  private long _size = -1;",
                "-  // queue of topologies referencing resource",
                "-  private final Set<String> _ref;",
                "-  // last access time of the resource -> increment when topology finishes using it",
                "-  private final AtomicLong _lastAccessTime = new AtomicLong(currentTime());",
                "-",
                "-  public LocalizedResource(String key, String fileLoc, boolean uncompressed) {",
                "-    _ref = new HashSet<String>();",
                "-    _localPath = fileLoc;",
                "-    _versionFilePath = ServerUtils.constructVersionFileName(fileLoc);",
                "-    _symlinkPath = ServerUtils.constructBlobCurrentSymlinkName(fileLoc);",
                "-    _uncompressed = uncompressed;",
                "-    _key = key;",
                "-    // we trust that the file exists",
                "-    _size = ServerUtils.getDU(new File(getFilePathWithVersion()));",
                "-    LOG.debug(\"size of {} is: {}\", fileLoc, _size);",
                "-  }",
                "-",
                "-  // create local resource and add reference",
                "-  public LocalizedResource(String key, String fileLoc, boolean uncompressed, String topo) {",
                "-    this(key, fileLoc, uncompressed);",
                "-    _ref.add(topo);",
                "-  }",
                "-",
                "-  public boolean isUncompressed() {",
                "-    return _uncompressed;",
                "-  }",
                "-",
                "-  public String getKey() {",
                "-    return _key;",
                "-  }",
                "-",
                "-  public String getCurrentSymlinkPath() {",
                "-    return _symlinkPath;",
                "-  }",
                "-",
                "-  public String getVersionFilePath() {",
                "-    return _versionFilePath;",
                "-  }",
                "-",
                "-  public String getFilePathWithVersion() {",
                "-    long version = ServerUtils.localVersionOfBlob(_localPath);",
                "-    return ServerUtils.constructBlobWithVersionFileName(_localPath, version);",
                "-  }",
                "-",
                "-  public String getFilePath() {",
                "-    return _localPath;",
                "-  }",
                "-",
                "-  public void addReference(String topo) {",
                "-    _ref.add(topo);",
                "-  }",
                "-",
                "-  public void removeReference(String topo) {",
                "-    if (!_ref.remove(topo)) {",
                "-      LOG.warn(\"Tried to remove a reference to a topology that doesn't use this resource\");",
                "-    }",
                "-    setTimestamp();",
                "-  }",
                "-",
                "-  // The last access time is only valid if the resource doesn't have any references.",
                "-  public long getLastAccessTime() {",
                "-    return _lastAccessTime.get();",
                "-  }",
                "-",
                "-  // for testing",
                "-  protected void setSize(long size) {",
                "-    _size = size;",
                "-  }",
                "-",
                "-  public long getSize() {",
                "-    return _size;",
                "-  }",
                "-",
                "-  private void setTimestamp() {",
                "-    _lastAccessTime.set(currentTime());",
                "-  }",
                "-",
                "-  public int getRefCount() {",
                "-    return _ref.size();",
                "-  }",
                "-",
                "-  private long currentTime() {",
                "-    return System.nanoTime();",
                "-  }",
                "-",
                "-  @Override",
                "-  public boolean equals(Object other) {",
                "-    if (other instanceof LocalizedResource) {",
                "-        LocalizedResource l = (LocalizedResource)other;",
                "-        return _key.equals(l._key) && _uncompressed == l._uncompressed && _localPath.equals(l._localPath);",
                "-    }",
                "-    return false;",
                "-  }",
                "-",
                "-  @Override",
                "-  public int hashCode() {",
                "-     return _key.hashCode() + Boolean.hashCode(_uncompressed) + _localPath.hashCode();",
                "-  }",
                "+public class LocalizedResource extends LocallyCachedBlob {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(LocalizedResource.class);",
                "+    @VisibleForTesting",
                "+    static final String CURRENT_BLOB_SUFFIX = \".current\";",
                "+    @VisibleForTesting",
                "+    static final String BLOB_VERSION_SUFFIX = \".version\";",
                "+    @VisibleForTesting",
                "+    static final String FILECACHE = \"filecache\";",
                "+    @VisibleForTesting",
                "+    static final String USERCACHE = \"usercache\";",
                "+    // sub directories to store either files or uncompressed archives respectively",
                "+    @VisibleForTesting",
                "+    static final String FILESDIR = \"files\";",
                "+    @VisibleForTesting",
                "+    static final String ARCHIVESDIR = \"archives\";",
                "+    private static final String TO_UNCOMPRESS = \"_tmp_\";",
                "+",
                "+    private static Path constructVersionFileName(Path baseDir, String key) {",
                "+        return baseDir.resolve(key + BLOB_VERSION_SUFFIX);",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    static long localVersionOfBlob(Path versionFile) {",
                "+        long currentVersion = -1;",
                "+        if (Files.exists(versionFile) && !(Files.isDirectory(versionFile))) {",
                "+            try (BufferedReader br = new BufferedReader(new FileReader(versionFile.toFile()))) {",
                "+                String line = br.readLine();",
                "+                currentVersion = Long.parseLong(line);",
                "+            } catch (IOException e) {",
                "+                throw new RuntimeException(e);",
                "+            }",
                "+        }",
                "+        return currentVersion;",
                "+    }",
                "+",
                "+    private static Path constructBlobCurrentSymlinkName(Path baseDir, String key) {",
                "+        return baseDir.resolve(key + CURRENT_BLOB_SUFFIX);",
                "+    }",
                "+",
                "+    private static Path constructBlobWithVersionFileName(Path baseDir, String key, long version) {",
                "+        return baseDir.resolve(key + \".\" + version);",
                "+    }",
                "+",
                "+    static Collection<String> getLocalizedUsers(Path localBaseDir) throws IOException {",
                "+        Path userCacheDir = getUserCacheDir(localBaseDir);",
                "+        if (!Files.exists(userCacheDir)) {",
                "+            return Collections.emptyList();",
                "+        }",
                "+        return Files.list(userCacheDir).map((p) -> p.getFileName().toString()).collect(Collectors.toList());",
                "+    }",
                "+",
                "+    static void completelyRemoveUnusedUser(Path localBaseDir, String user) throws IOException {",
                "+        Path userFileCacheDir = getLocalUserFileCacheDir(localBaseDir, user);",
                "+        // baseDir/supervisor/usercache/user1/filecache/files",
                "+        Files.deleteIfExists(getCacheDirForFiles(userFileCacheDir));",
                "+        // baseDir/supervisor/usercache/user1/filecache/archives",
                "+        Files.deleteIfExists(getCacheDirForArchives(userFileCacheDir));",
                "+        // baseDir/supervisor/usercache/user1/filecache",
                "+        Files.deleteIfExists(userFileCacheDir);",
                "+        // baseDir/supervisor/usercache/user1",
                "+        Files.deleteIfExists(getLocalUserDir(localBaseDir, user));",
                "+    }",
                "+",
                "+    static List<String> getLocalizedArchiveKeys(Path localBaseDir, String user) throws IOException {",
                "+        Path dir = getCacheDirForArchives(getLocalUserFileCacheDir(localBaseDir, user));",
                "+        return readKeysFromDir(dir);",
                "+    }",
                "+",
                "+    static List<String> getLocalizedFileKeys(Path localBaseDir, String user) throws IOException {",
                "+        Path dir = getCacheDirForFiles(getLocalUserFileCacheDir(localBaseDir, user));",
                "+        return readKeysFromDir(dir);",
                "+    }",
                "+",
                "+    // Looks for files in the directory with .current suffix",
                "+    private static List<String> readKeysFromDir(Path dir) throws IOException {",
                "+        if (!Files.exists(dir)) {",
                "+            return Collections.emptyList();",
                "+        }",
                "+        return Files.list(dir)",
                "+            .map((p) -> p.getFileName().toString())",
                "+            .filter((name) -> name.toLowerCase().endsWith(CURRENT_BLOB_SUFFIX))",
                "+            .map((key) -> {",
                "+                int p = key.lastIndexOf('.');",
                "+                if (p > 0) {",
                "+                    key = key.substring(0, p);",
                "+                }",
                "+                return key;",
                "+            })",
                "+            .collect(Collectors.toList());",
                "+    }",
                "+",
                "+    // baseDir/supervisor/usercache/",
                "+    private static Path getUserCacheDir(Path localBaseDir) {",
                "+        return localBaseDir.resolve(USERCACHE);",
                "+    }",
                "+",
                "+    // baseDir/supervisor/usercache/user1/",
                "+    static Path getLocalUserDir(Path localBaseDir, String userName) {",
                "+        return getUserCacheDir(localBaseDir).resolve(userName);",
                "+    }",
                "+",
                "+    // baseDir/supervisor/usercache/user1/filecache",
                "+    static Path getLocalUserFileCacheDir(Path localBaseDir, String userName) {",
                "+        return getLocalUserDir(localBaseDir, userName).resolve(FILECACHE);",
                "+    }",
                "+",
                "+    // baseDir/supervisor/usercache/user1/filecache/files",
                "+    private static Path getCacheDirForFiles(Path dir) {",
                "+        return dir.resolve(FILESDIR);",
                "+    }",
                "+",
                "+    // get the directory to put uncompressed archives in",
                "+    // baseDir/supervisor/usercache/user1/filecache/archives",
                "+    private static Path getCacheDirForArchives(Path dir) {",
                "+        return dir.resolve(ARCHIVESDIR);",
                "+    }",
                "+",
                "+    // filesystem path to the resource",
                "+    private final Path baseDir;",
                "+    private final Path versionFilePath;",
                "+    private final Path symlinkPath;",
                "+    private final boolean uncompressed;",
                "+    private final IAdvancedFSOps fsOps;",
                "+    private final String user;",
                "+    // size of the resource",
                "+    private long size = -1;",
                "+    private final Map<String, Object> conf;",
                "+",
                "+    LocalizedResource(String key, Path localBaseDir, boolean uncompressed, IAdvancedFSOps fsOps, Map<String, Object> conf,",
                "+                             String user) {",
                "+        super(key + (uncompressed ? \" archive\" : \" file\"), key);",
                "+        Path base = getLocalUserFileCacheDir(localBaseDir, user);",
                "+        this.baseDir = uncompressed ? getCacheDirForArchives(base) : getCacheDirForFiles(base);",
                "+        this.conf = conf;",
                "+        this.user = user;",
                "+        this.fsOps = fsOps;",
                "+        versionFilePath = constructVersionFileName(baseDir, key);",
                "+        symlinkPath = constructBlobCurrentSymlinkName(baseDir, key);",
                "+        this.uncompressed = uncompressed;",
                "+        //Set the size in case we are recovering an already downloaded object",
                "+        setSize();",
                "+    }",
                "+",
                "+    Path getCurrentSymlinkPath() {",
                "+        return symlinkPath;",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    Path getFilePathWithVersion() {",
                "+        return constructBlobWithVersionFileName(baseDir, getKey(), getLocalVersion());",
                "+    }",
                "+",
                "+    private void setSize() {",
                "+        // we trust that the file exists",
                "+        Path withVersion = getFilePathWithVersion();",
                "+        size = ServerUtils.getDU(withVersion.toFile());",
                "+        LOG.debug(\"size of {} is: {}\", withVersion, size);",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    protected void setSize(long size) {",
                "+        this.size = size;",
                "+    }",
                "+",
                "+    @Override",
                "+    public long getLocalVersion() {",
                "+        return localVersionOfBlob(versionFilePath);",
                "+    }",
                "+",
                "+    @Override",
                "+    public long getRemoteVersion(ClientBlobStore store) throws KeyNotFoundException, AuthorizationException {",
                "+        return ServerUtils.nimbusVersionOfBlob(getKey(), store);",
                "+    }",
                "+",
                "+    @Override",
                "+    public long downloadToTempLocation(ClientBlobStore store) throws IOException, KeyNotFoundException, AuthorizationException {",
                "+        String key = getKey();",
                "+        ReadableBlobMeta meta = store.getBlobMeta(key);",
                "+        if (!ServerUtils.canUserReadBlob(meta, user, conf)) {",
                "+            throw new AuthorizationException(user + \" does not have READ access to \" + key);",
                "+        }",
                "+        long version;",
                "+        Path downloadFile;",
                "+        Path finalLocation;",
                "+        try (InputStreamWithMeta in = store.getBlob(key)) {",
                "+            version = in.getVersion();",
                "+            finalLocation = constructBlobWithVersionFileName(baseDir, getKey(), version);",
                "+            if (uncompressed) {",
                "+                // we need to download to temp file and then unpack into the one requested",
                "+                downloadFile = tmpOutputLocation();",
                "+            } else {",
                "+                downloadFile = finalLocation;",
                "+            }",
                "+            byte[] buffer = new byte[1024];",
                "+            int len;",
                "+            LOG.debug(\"Downloading {} to {}\", key, downloadFile);",
                "+            Path parent = downloadFile.getParent();",
                "+            if (!Files.exists(parent)) {",
                "+                Files.createDirectory(parent);",
                "+            }",
                "+            try (FileOutputStream out = new FileOutputStream(downloadFile.toFile())) {",
                "+                while ((len = in.read(buffer)) >= 0) {",
                "+                    out.write(buffer, 0, len);",
                "+                }",
                "+            }",
                "+        }",
                "+        if (uncompressed) {",
                "+            ServerUtils.unpack(downloadFile.toFile(), finalLocation.toFile());",
                "+            LOG.debug(\"Uncompressed {} to: {}\", downloadFile, finalLocation);",
                "+        }",
                "+        setBlobPermissions(conf, user, finalLocation);",
                "+        return version;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void commitNewVersion(long version) throws IOException {",
                "+        String key = getKey();",
                "+        LOG.info(\"Blob: {} updated to version {} from version {}\", key, version, getLocalVersion());",
                "+        Path localVersionFile = versionFilePath;",
                "+        // The false parameter ensures overwriting the version file, not appending",
                "+        try (PrintWriter writer = new PrintWriter(",
                "+            new BufferedWriter(new FileWriter(localVersionFile.toFile(), false)))) {",
                "+            writer.println(version);",
                "+        }",
                "+        setBlobPermissions(conf, user, localVersionFile);",
                "+",
                "+        // Update the key.current symlink. First create tmp symlink and do",
                "+        // move of tmp to current so that the operation is atomic.",
                "+        Path tmpSymlink = tmpSymlinkLocation();",
                "+        Path targetOfSymlink = constructBlobWithVersionFileName(baseDir, getKey(), version);",
                "+        LOG.debug(\"Creating a symlink @{} linking to: {}\", tmpSymlink, targetOfSymlink);",
                "+        Files.createSymbolicLink(tmpSymlink, targetOfSymlink);",
                "+",
                "+        Path currentSymLink = getCurrentSymlinkPath();",
                "+        Files.move(tmpSymlink, currentSymLink, ATOMIC_MOVE);",
                "+        //Update the size of the objects",
                "+        setSize();",
                "+    }",
                "+",
                "+    private void setBlobPermissions(Map<String, Object> conf, String user, Path path)",
                "+        throws IOException {",
                "+",
                "+        if (!ObjectReader.getBoolean(conf.get(Config.SUPERVISOR_RUN_WORKER_AS_USER), false)) {",
                "+            return;",
                "+        }",
                "+        String wlCommand = ObjectReader.getString(conf.get(Config.SUPERVISOR_WORKER_LAUNCHER), \"\");",
                "+        if (wlCommand.isEmpty()) {",
                "+            String stormHome = System.getProperty(\"storm.home\");",
                "+            wlCommand = stormHome + \"/bin/worker-launcher\";",
                "+        }",
                "+        List<String> command = new ArrayList<>(Arrays.asList(wlCommand, user, \"blob\", path.toString()));",
                "+",
                "+        String[] commandArray = command.toArray(new String[command.size()]);",
                "+        ShellUtils.ShellCommandExecutor shExec = new ShellUtils.ShellCommandExecutor(commandArray);",
                "+        LOG.debug(\"Setting blob permissions, command: {}\", Arrays.toString(commandArray));",
                "+",
                "+        try {",
                "+            shExec.execute();",
                "+            LOG.debug(\"output: {}\", shExec.getOutput());",
                "+        } catch (ShellUtils.ExitCodeException e) {",
                "+            int exitCode = shExec.getExitCode();",
                "+            LOG.warn(\"Exit code from worker-launcher is: {}\", exitCode, e);",
                "+            LOG.debug(\"output: {}\", shExec.getOutput());",
                "+            throw new IOException(\"Setting blob permissions failed\"",
                "+                + \" (exitCode=\" + exitCode + \") with output: \" + shExec.getOutput(), e);",
                "+        }",
                "+    }",
                "+",
                "+    private Path tmpOutputLocation() {",
                "+        return baseDir.resolve(Paths.get(LocalizedResource.TO_UNCOMPRESS + getKey()));",
                "+    }",
                "+",
                "+    private Path tmpSymlinkLocation() {",
                "+        return baseDir.resolve(Paths.get(LocalizedResource.TO_UNCOMPRESS + getKey() + CURRENT_BLOB_SUFFIX));",
                "+    }",
                "+",
                "+    private static final Pattern VERSION_FILE_PATTERN = Pattern.compile(\"^(.+)\\\\.(\\\\d+)$\");",
                "+",
                "+    @Override",
                "+    public void cleanupOrphanedData() throws IOException {",
                "+        //There are a few possible files that we would want to clean up",
                "+        //baseDir + \"/\" + \"_tmp_\" + baseName",
                "+        //baseDir + \"/\" + \"_tmp_\" + baseName + \".current\"",
                "+        //baseDir + \"/\" + baseName.<VERSION>",
                "+        //baseDir + \"/\" + baseName.current",
                "+        //baseDir + \"/\" + baseName.version",
                "+        //In general we always want to delete the _tmp_ files if they are there.",
                "+",
                "+        Path tmpOutput = tmpOutputLocation();",
                "+        Files.deleteIfExists(tmpOutput);",
                "+        Path tmpSym = tmpSymlinkLocation();",
                "+        Files.deleteIfExists(tmpSym);",
                "+",
                "+        try {",
                "+            String baseName = getKey();",
                "+            long version = getLocalVersion();",
                "+            Path current = getCurrentSymlinkPath();",
                "+",
                "+            //If .current and .version do not match, we roll back the .version file to match",
                "+            // what .current is pointing to.",
                "+            if (Files.exists(current) && Files.isSymbolicLink(current)) {",
                "+                Path versionFile = Files.readSymbolicLink(current);",
                "+                Matcher m = VERSION_FILE_PATTERN.matcher(versionFile.getFileName().toString());",
                "+                if (m.matches()) {",
                "+                    long foundVersion = Long.valueOf(m.group(2));",
                "+                    if (foundVersion != version) {",
                "+                        LOG.error(\"{} does not match the version file so fix the version file\", current);",
                "+                        //The versions are different so roll back to whatever current is",
                "+                        try (PrintWriter restoreWriter = new PrintWriter(",
                "+                            new BufferedWriter(new FileWriter(versionFilePath.toFile(), false)))) {",
                "+                            restoreWriter.println(foundVersion);",
                "+                        }",
                "+                        version = foundVersion;",
                "+                    }",
                "+                }",
                "+            }",
                "+",
                "+            // Finally delete any baseName.<VERSION> files that are not pointed to by the current version",
                "+            final long finalVersion = version;",
                "+            LOG.debug(\"Looking to clean up after {} in {}\", getKey(), baseDir);",
                "+            try (DirectoryStream<Path> ds = fsOps.newDirectoryStream(baseDir, (path) -> {",
                "+                Matcher m = VERSION_FILE_PATTERN.matcher(path.getFileName().toString());",
                "+                if (m.matches()) {",
                "+                    long foundVersion = Long.valueOf(m.group(2));",
                "+                    return m.group(1).equals(baseName) && foundVersion != finalVersion;",
                "+                }",
                "+                return false;",
                "+            })) {",
                "+                for (Path p : ds) {",
                "+                    LOG.info(\"Cleaning up old localized resource file {}\", p);",
                "+                    if (Files.isDirectory(p)) {",
                "+                        FileUtils.deleteDirectory(p.toFile());",
                "+                    } else {",
                "+                        fsOps.deleteIfExists(p.toFile());",
                "+                    }",
                "+                }",
                "+            }",
                "+        } catch (NoSuchFileException e) {",
                "+            LOG.warn(\"Nothing to cleanup with badeDir {} even though we expected there to be something there\", baseDir);",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public void completelyRemove() throws IOException {",
                "+        Path fileWithVersion = getFilePathWithVersion();",
                "+        Path currentSymLink = getCurrentSymlinkPath();",
                "+",
                "+        if (uncompressed) {",
                "+            if (Files.exists(fileWithVersion)) {",
                "+                // this doesn't follow symlinks, which is what we want",
                "+                FileUtils.deleteDirectory(fileWithVersion.toFile());",
                "+            }",
                "+        } else {",
                "+            Files.deleteIfExists(fileWithVersion);",
                "+        }",
                "+        Files.deleteIfExists(currentSymLink);",
                "+        Files.deleteIfExists(versionFilePath);",
                "+    }",
                "+",
                "+    @Override",
                "+    public long getSizeOnDisk() {",
                "+        return size;",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean isFullyDownloaded() {",
                "+        return Files.exists(getFilePathWithVersion())",
                "+            && Files.exists(getCurrentSymlinkPath())",
                "+            && Files.exists(versionFilePath);",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean equals(Object other) {",
                "+        if (other instanceof LocalizedResource) {",
                "+            LocalizedResource l = (LocalizedResource)other;",
                "+            return getKey().equals(l.getKey()) && uncompressed == l.uncompressed && baseDir.equals(l.baseDir);",
                "+        }",
                "+        return false;",
                "+    }",
                "+",
                "+    @Override",
                "+    public int hashCode() {",
                "+        return getKey().hashCode() + Boolean.hashCode(uncompressed) + baseDir.hashCode();",
                "+    }",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java b/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java",
                "index 826bf9869..936dbc11e 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.localizer;",
                "@@ -20,11 +21,2 @@ package org.apache.storm.localizer;",
                " import com.google.common.annotations.VisibleForTesting;",
                "-import java.nio.file.Path;",
                "-import java.util.concurrent.ConcurrentHashMap;",
                "-import org.apache.commons.io.FileUtils;",
                "-import org.slf4j.Logger;",
                "-import org.slf4j.LoggerFactory;",
                "-",
                "-import java.io.File;",
                "-import java.io.IOException;",
                "-import java.nio.file.Files;",
                " import java.util.Comparator;",
                "@@ -34,2 +26,8 @@ import java.util.SortedMap;",
                " import java.util.TreeMap;",
                "+import java.util.concurrent.ConcurrentMap;",
                "+import org.apache.storm.blobstore.ClientBlobStore;",
                "+import org.apache.storm.generated.AuthorizationException;",
                "+import org.apache.storm.generated.KeyNotFoundException;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "@@ -41,3 +39,2 @@ public class LocalizedResourceRetentionSet {",
                "     public static final Logger LOG = LoggerFactory.getLogger(LocalizedResourceRetentionSet.class);",
                "-    private long delSize;",
                "     private long currentSize;",
                "@@ -46,4 +43,3 @@ public class LocalizedResourceRetentionSet {",
                "     @VisibleForTesting",
                "-    final SortedMap<ComparableResource, CleanableResourceSet> noReferences;",
                "-    private int resourceCount = 0;",
                "+    final SortedMap<LocallyCachedBlob, Map<String, ? extends LocallyCachedBlob>> noReferences;",
                "@@ -53,3 +49,3 @@ public class LocalizedResourceRetentionSet {",
                "-    LocalizedResourceRetentionSet(long targetSize, Comparator<? super ComparableResource> cmp) {",
                "+    LocalizedResourceRetentionSet(long targetSize, Comparator<? super LocallyCachedBlob> cmp) {",
                "         this(targetSize, new TreeMap<>(cmp));",
                "@@ -58,3 +54,3 @@ public class LocalizedResourceRetentionSet {",
                "     LocalizedResourceRetentionSet(long targetSize,",
                "-                                  SortedMap<ComparableResource, CleanableResourceSet> retain) {",
                "+                                  SortedMap<LocallyCachedBlob, Map<String, ? extends LocallyCachedBlob>> retain) {",
                "         this.noReferences = retain;",
                "@@ -68,27 +64,12 @@ public class LocalizedResourceRetentionSet {",
                "-    protected void addResourcesForSet(Iterator<LocalizedResource> setIter, LocalizedResourceSet set) {",
                "-        CleanableLocalizedResourceSet cleanset = new CleanableLocalizedResourceSet(set);",
                "-        for (Iterator<LocalizedResource> iter = setIter; setIter.hasNext(); ) {",
                "-            LocalizedResource lrsrc = iter.next();",
                "-            currentSize += lrsrc.getSize();",
                "-            resourceCount ++;",
                "-            if (lrsrc.getRefCount() > 0) {",
                "-                // always retain resources in use",
                "-                continue;",
                "-            }",
                "-            noReferences.put(new LocalizedBlobComparableResource(lrsrc), cleanset);",
                "-        }",
                "-    }",
                "-",
                "-    public void addResources(LocalizedResourceSet set) {",
                "-        addResourcesForSet(set.getLocalFilesIterator(), set);",
                "-        addResourcesForSet(set.getLocalArchivesIterator(), set);",
                "-    }",
                "-",
                "-    public void addResources(ConcurrentHashMap<String, LocallyCachedBlob> blobs) {",
                "-        CleanableLocalizedLocallyCachedBlob set = new CleanableLocalizedLocallyCachedBlob(blobs);",
                "+    /**",
                "+     * Add blobs to be checked if they can be deleted.",
                "+     * @param blobs a map of blob name to the blob object.  The blobs in this map will be deleted from the map",
                "+     *     if they are deleted on disk too.",
                "+     */",
                "+    public void addResources(ConcurrentMap<String, ? extends LocallyCachedBlob> blobs) {",
                "         for (LocallyCachedBlob b: blobs.values()) {",
                "             currentSize += b.getSizeOnDisk();",
                "-            resourceCount ++;",
                "             if (b.isUsed()) {",
                "+                LOG.debug(\"NOT going to clean up {}, {} depends on it\", b.getKey(), b.getDependencies());",
                "                 // always retain resources in use",
                "@@ -96,4 +77,4 @@ public class LocalizedResourceRetentionSet {",
                "             }",
                "-            LocallyCachedBlobComparableResource cb = new LocallyCachedBlobComparableResource(b);",
                "-            noReferences.put(cb, set);",
                "+            LOG.debug(\"Possibly going to clean up {} ts {} size {}\", b.getKey(), b.getLastUsed(), b.getSizeOnDisk());",
                "+            noReferences.put(b, blobs);",
                "         }",
                "@@ -101,18 +82,26 @@ public class LocalizedResourceRetentionSet {",
                "-    public void cleanup() {",
                "+    /**",
                "+     * Actually cleanup the blobs to try and get below the target cache size.",
                "+     * @param store the blobs store client used to check if the blob has been deleted from the blobstore.  If it has, the blob will be",
                "+     *     deleted even if the cache is not over the target size.",
                "+     */",
                "+    public void cleanup(ClientBlobStore store) {",
                "         LOG.debug(\"cleanup target size: {} current size is: {}\", targetSize, currentSize);",
                "-        for (Iterator<Map.Entry<ComparableResource, CleanableResourceSet>> i =",
                "-             noReferences.entrySet().iterator();",
                "-             currentSize - delSize > targetSize && i.hasNext();) {",
                "-            Map.Entry<ComparableResource, CleanableResourceSet> rsrc = i.next();",
                "-            ComparableResource resource = rsrc.getKey();",
                "-            CleanableResourceSet set = rsrc.getValue();",
                "-            if (resource != null && set.remove(resource)) {",
                "-                if (set.deleteUnderlyingResource(resource)) {",
                "-                    delSize += resource.getSize();",
                "-                    LOG.info(\"deleting: {} with size of: {}\", resource.getNameForDebug(), resource.getSize());",
                "+        long bytesOver = currentSize - targetSize;",
                "+        //First delete everything that no longer exists...",
                "+        for (Iterator<Map.Entry<LocallyCachedBlob, Map<String, ? extends LocallyCachedBlob>>> i = noReferences.entrySet().iterator();",
                "+             i.hasNext();) {",
                "+            Map.Entry<LocallyCachedBlob, Map<String, ? extends LocallyCachedBlob>> rsrc = i.next();",
                "+            LocallyCachedBlob resource = rsrc.getKey();",
                "+            try {",
                "+                resource.getRemoteVersion(store);",
                "+            } catch (AuthorizationException e) {",
                "+                //Ignored",
                "+            } catch (KeyNotFoundException e) {",
                "+                //The key was removed so we should delete it too.",
                "+                Map<String, ? extends LocallyCachedBlob> set = rsrc.getValue();",
                "+                if (removeBlob(resource, set)) {",
                "+                    bytesOver -= resource.getSizeOnDisk();",
                "+                    LOG.info(\"Deleted blob: {} (KEY NOT FOUND).\", resource.getKey());",
                "                     i.remove();",
                "-                } else {",
                "-                    // since it failed to delete add it back so it gets retried",
                "-                    set.add(resource.getKey(), resource);",
                "                 }",
                "@@ -120,86 +109,13 @@ public class LocalizedResourceRetentionSet {",
                "         }",
                "-    }",
                "-",
                "-    @VisibleForTesting",
                "-    public boolean deleteResource(CleanableResourceSet set, ComparableResource resource) {",
                "-        return set.deleteUnderlyingResource(resource);",
                "-    }",
                "-",
                "-    public long getCurrentSize() {",
                "-        return currentSize;",
                "-    }",
                "-",
                "-    public int getResourceCount() {",
                "-        return resourceCount;",
                "-    }",
                "-",
                "-    @Override",
                "-    public String toString() {",
                "-        StringBuilder sb = new StringBuilder();",
                "-        sb.append(\"Cache: \").append(currentSize).append(\", \");",
                "-        sb.append(\"Deleted: \").append(delSize);",
                "-        return sb.toString();",
                "-    }",
                "-",
                "-    interface ComparableResource {",
                "-        long getLastAccessTime();",
                "-",
                "-        long getSize();",
                "-",
                "-        String getNameForDebug();",
                "-",
                "-        String getKey();",
                "-    }",
                "-",
                "-    interface CleanableResourceSet {",
                "-        boolean remove(ComparableResource resource);",
                "-",
                "-        void add(String key, ComparableResource resource);",
                "-",
                "-        boolean deleteUnderlyingResource(ComparableResource resource);",
                "-    }",
                "-",
                "-    public static class LocallyCachedBlobComparableResource implements ComparableResource {",
                "-        private final LocallyCachedBlob blob;",
                "-",
                "-        public LocallyCachedBlobComparableResource(LocallyCachedBlob blob) {",
                "-            this.blob = blob;",
                "-        }",
                "-",
                "-        @Override",
                "-        public long getLastAccessTime() {",
                "-            return blob.getLastUsed();",
                "-        }",
                "-",
                "-        @Override",
                "-        public long getSize() {",
                "-            return blob.getSizeOnDisk();",
                "-        }",
                "-",
                "-        @Override",
                "-        public String getNameForDebug() {",
                "-            return blob.getKey();",
                "-        }",
                "-",
                "-        @Override",
                "-        public String getKey() {",
                "-            return blob.getKey();",
                "-        }",
                "-",
                "-        @Override",
                "-        public String toString() {",
                "-            return blob.toString();",
                "-        }",
                "-        @Override",
                "-        public boolean equals(Object other) {",
                "-            if (other instanceof LocallyCachedBlobComparableResource) {",
                "-                return blob.equals(((LocallyCachedBlobComparableResource) other).blob);",
                "+        for (Iterator<Map.Entry<LocallyCachedBlob, Map<String, ? extends LocallyCachedBlob>>> i = noReferences.entrySet().iterator();",
                "+             bytesOver > 0 && i.hasNext();) {",
                "+            Map.Entry<LocallyCachedBlob, Map<String, ? extends LocallyCachedBlob>> rsrc = i.next();",
                "+            LocallyCachedBlob resource = rsrc.getKey();",
                "+            Map<String, ? extends LocallyCachedBlob> set = rsrc.getValue();",
                "+            if (removeBlob(resource, set)) {",
                "+                bytesOver -= resource.getSizeOnDisk();",
                "+                LOG.info(\"Deleted blob: {} (OVER SIZE LIMIT).\", resource.getKey());",
                "+                i.remove();",
                "             }",
                "-            return false;",
                "-        }",
                "-",
                "-        @Override",
                "-        public int hashCode() {",
                "-            return blob.hashCode();",
                "         }",
                "@@ -207,77 +123,12 @@ public class LocalizedResourceRetentionSet {",
                "-    private static class CleanableLocalizedLocallyCachedBlob implements CleanableResourceSet {",
                "-        private final ConcurrentHashMap<String, LocallyCachedBlob> blobs;",
                "-",
                "-        public CleanableLocalizedLocallyCachedBlob(ConcurrentHashMap<String, LocallyCachedBlob> blobs) {",
                "-            this.blobs = blobs;",
                "-        }",
                "-",
                "-        @Override",
                "-        public boolean remove(ComparableResource resource) {",
                "-            if (!(resource instanceof LocallyCachedBlobComparableResource)) {",
                "-                throw new IllegalStateException(resource + \" must be a LocallyCachedBlobComparableResource\");",
                "-            }",
                "-            LocallyCachedBlob blob = ((LocallyCachedBlobComparableResource)resource).blob;",
                "-            synchronized (blob) {",
                "-                if (!blob.isUsed()) {",
                "-                    try {",
                "-                        blob.completelyRemove();",
                "-                    } catch (Exception e) {",
                "-                        LOG.warn(\"Tried to remove {} but failed with\", blob, e);",
                "-                    }",
                "-                    blobs.remove(blob.getKey());",
                "-                    return true;",
                "+    private boolean removeBlob(LocallyCachedBlob blob, Map<String, ? extends LocallyCachedBlob> blobs) {",
                "+        synchronized (blob) {",
                "+            if (!blob.isUsed()) {",
                "+                try {",
                "+                    blob.completelyRemove();",
                "+                } catch (Exception e) {",
                "+                    LOG.warn(\"Tried to remove {} but failed with\", blob, e);",
                "                 }",
                "-                return false;",
                "-            }",
                "-        }",
                "-",
                "-        @Override",
                "-        public void add(String key, ComparableResource resource) {",
                "-            ///NOOP not used",
                "-        }",
                "-",
                "-        @Override",
                "-        public boolean deleteUnderlyingResource(ComparableResource resource) {",
                "-            //NOOP not used",
                "-            return true;",
                "-        }",
                "-    }",
                "-",
                "-    private static class LocalizedBlobComparableResource implements ComparableResource {",
                "-        private final LocalizedResource resource;",
                "-",
                "-        private LocalizedBlobComparableResource(LocalizedResource resource) {",
                "-            this.resource = resource;",
                "-        }",
                "-",
                "-        @Override",
                "-        public long getLastAccessTime() {",
                "-            return resource.getLastAccessTime();",
                "-        }",
                "-",
                "-        @Override",
                "-        public long getSize() {",
                "-            return resource.getSize();",
                "-        }",
                "-",
                "-        @Override",
                "-        public String getNameForDebug() {",
                "-            return resource.getFilePath();",
                "-        }",
                "-",
                "-        @Override",
                "-        public String getKey() {",
                "-            return resource.getKey();",
                "-        }",
                "-",
                "-        @Override",
                "-        public String toString() {",
                "-            return resource.getKey() + \" at \" + resource.getFilePathWithVersion();",
                "-        }",
                "-",
                "-        @Override",
                "-        public boolean equals(Object other) {",
                "-            if (other instanceof LocalizedBlobComparableResource) {",
                "-                return resource.equals(((LocalizedBlobComparableResource) other).resource);",
                "+                blobs.remove(blob.getKey());",
                "+                return true;",
                "             }",
                "@@ -285,66 +136,12 @@ public class LocalizedResourceRetentionSet {",
                "         }",
                "-",
                "-        @Override",
                "-        public int hashCode() {",
                "-            return resource.hashCode();",
                "-        }",
                "     }",
                "-    private static class CleanableLocalizedResourceSet implements CleanableResourceSet {",
                "-        private final LocalizedResourceSet set;",
                "-",
                "-        public CleanableLocalizedResourceSet(LocalizedResourceSet set) {",
                "-            this.set = set;",
                "-        }",
                "-",
                "-        @Override",
                "-        public boolean remove(ComparableResource resource) {",
                "-            if (!(resource instanceof LocalizedBlobComparableResource)) {",
                "-                throw new IllegalStateException(resource + \" must be a LocalizedBlobComparableResource\");",
                "-            }",
                "-            return set.remove(((LocalizedBlobComparableResource)resource).resource);",
                "-        }",
                "-",
                "-        @Override",
                "-        public void add(String key, ComparableResource resource) {",
                "-            if (!(resource instanceof LocalizedBlobComparableResource)) {",
                "-                throw new IllegalStateException(resource + \" must be a LocalizedBlobComparableResource\");",
                "-            }",
                "-            LocalizedResource r = ((LocalizedBlobComparableResource)resource).resource;",
                "-            set.add(key, r, r.isUncompressed());",
                "-        }",
                "-",
                "-        @Override",
                "-        public boolean deleteUnderlyingResource(ComparableResource resource) {",
                "-            if (resource instanceof LocalizedBlobComparableResource) {",
                "-                LocalizedResource lr = ((LocalizedBlobComparableResource) resource).resource;",
                "-                try {",
                "-                    Path fileWithVersion = new File(lr.getFilePathWithVersion()).toPath();",
                "-                    Path currentSymLink = new File(lr.getCurrentSymlinkPath()).toPath();",
                "-                    Path versionFile = new File(lr.getVersionFilePath()).toPath();",
                "-",
                "-                    if (lr.isUncompressed()) {",
                "-                        if (Files.exists(fileWithVersion)) {",
                "-                            // this doesn't follow symlinks, which is what we want",
                "-                            FileUtils.deleteDirectory(fileWithVersion.toFile());",
                "-                        }",
                "-                    } else {",
                "-                        Files.deleteIfExists(fileWithVersion);",
                "-                    }",
                "-                    Files.deleteIfExists(currentSymLink);",
                "-                    Files.deleteIfExists(versionFile);",
                "-                    return true;",
                "-                } catch (IOException e) {",
                "-                    LOG.warn(\"Could not delete: {}\", resource.getNameForDebug(), e);",
                "-                }",
                "-                return false;",
                "-            }  else {",
                "-                throw new IllegalArgumentException(\"Don't know how to handle a \" + resource.getClass());",
                "-            }",
                "-        }",
                "+    @Override",
                "+    public String toString() {",
                "+        return \"Cache: \" + currentSize;",
                "     }",
                "-    static class LRUComparator implements Comparator<ComparableResource> {",
                "-        public int compare(ComparableResource r1, ComparableResource r2) {",
                "-            long ret = r1.getLastAccessTime() - r2.getLastAccessTime();",
                "+    static class LRUComparator implements Comparator<LocallyCachedBlob> {",
                "+        public int compare(LocallyCachedBlob r1, LocallyCachedBlob r2) {",
                "+            long ret = r1.getLastUsed() - r2.getLastUsed();",
                "             if (0 == ret) {",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceSet.java b/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceSet.java",
                "deleted file mode 100644",
                "index 62d5b2fcf..000000000",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceSet.java",
                "+++ /dev/null",
                "@@ -1,101 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-package org.apache.storm.localizer;",
                "-",
                "-import org.slf4j.Logger;",
                "-import org.slf4j.LoggerFactory;",
                "-",
                "-import java.util.Iterator;",
                "-import java.util.concurrent.ConcurrentHashMap;",
                "-import java.util.concurrent.ConcurrentMap;",
                "-",
                "-/**",
                "- * Set of localized resources for a specific user.",
                "- */",
                "-public class LocalizedResourceSet {",
                "-",
                "-  public static final Logger LOG = LoggerFactory.getLogger(LocalizedResourceSet.class);",
                "-  // Key to LocalizedResource mapping for files",
                "-  private final ConcurrentMap<String, LocalizedResource> _localrsrcFiles;",
                "-  // Key to LocalizedResource mapping for files to be uncompressed",
                "-  private final ConcurrentMap<String, LocalizedResource> _localrsrcArchives;",
                "-  private String _user;",
                "-",
                "-  LocalizedResourceSet(String user) {",
                "-    this._localrsrcFiles = new ConcurrentHashMap<String, LocalizedResource>();",
                "-    this._localrsrcArchives = new ConcurrentHashMap<String, LocalizedResource>();",
                "-    _user = user;",
                "-  }",
                "-",
                "-  public String getUser() {",
                "-    return _user;",
                "-  }",
                "-",
                "-  public int getSize() {",
                "-    return _localrsrcFiles.size() + _localrsrcArchives.size();",
                "-  }",
                "-",
                "-  public LocalizedResource get(String name, boolean uncompress) {",
                "-    if (uncompress) {",
                "-      return _localrsrcArchives.get(name);",
                "-    }",
                "-    return _localrsrcFiles.get(name);",
                "-  }",
                "-",
                "-  public void putIfAbsent(String resourceName, LocalizedResource updatedResource,",
                "-                            boolean uncompress) {",
                "-    if (uncompress) {",
                "-      _localrsrcArchives.putIfAbsent(resourceName, updatedResource);",
                "-    } else {",
                "-      _localrsrcFiles.putIfAbsent(resourceName, updatedResource);",
                "-    }",
                "-  }",
                "-",
                "-  public void add(String resourceName, LocalizedResource newResource, boolean uncompress) {",
                "-    if (uncompress) {",
                "-      _localrsrcArchives.put(resourceName, newResource);",
                "-    } else {",
                "-      _localrsrcFiles.put(resourceName, newResource);",
                "-    }",
                "-  }",
                "-",
                "-  public boolean exists(String resourceName, boolean uncompress) {",
                "-    if (uncompress) {",
                "-      return _localrsrcArchives.containsKey(resourceName);",
                "-    }",
                "-    return _localrsrcFiles.containsKey(resourceName);",
                "-  }",
                "-",
                "-  public boolean remove(LocalizedResource resource) {",
                "-    LocalizedResource lrsrc = null;",
                "-    if (resource.isUncompressed()) {",
                "-      lrsrc = _localrsrcArchives.remove(resource.getKey());",
                "-    } else {",
                "-      lrsrc = _localrsrcFiles.remove(resource.getKey());",
                "-    }",
                "-    return (lrsrc != null);",
                "-  }",
                "-",
                "-  public Iterator<LocalizedResource> getLocalFilesIterator() {",
                "-    return _localrsrcFiles.values().iterator();",
                "-  }",
                "-",
                "-  public Iterator<LocalizedResource> getLocalArchivesIterator() {",
                "-    return _localrsrcArchives.values().iterator();",
                "-  }",
                "-}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java",
                "index a287e959d..1f7ee006d 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java",
                "@@ -21,2 +21,3 @@ package org.apache.storm.localizer;",
                " import java.io.IOException;",
                "+import java.io.OutputStream;",
                " import java.nio.file.Files;",
                "@@ -24,2 +25,3 @@ import java.nio.file.LinkOption;",
                " import java.nio.file.Path;",
                "+import java.util.Collection;",
                " import java.util.HashMap;",
                "@@ -29,6 +31,10 @@ import java.util.concurrent.CountDownLatch;",
                " import java.util.concurrent.TimeUnit;",
                "+import java.util.function.Function;",
                " import org.apache.storm.blobstore.BlobStore;",
                " import org.apache.storm.blobstore.ClientBlobStore;",
                "+import org.apache.storm.blobstore.InputStreamWithMeta;",
                "+import org.apache.storm.daemon.supervisor.IAdvancedFSOps;",
                " import org.apache.storm.generated.AuthorizationException;",
                " import org.apache.storm.generated.KeyNotFoundException;",
                "+import org.apache.storm.utils.Time;",
                " import org.slf4j.Logger;",
                "@@ -43,5 +49,6 @@ public abstract class LocallyCachedBlob {",
                "     // A callback that does nothing.",
                "-    private static final BlobChangingCallback NOOP_CB = (assignment, port, resource, go) -> {};",
                "+    private static final BlobChangingCallback NOOP_CB = (assignment, port, resource, go) -> {",
                "+    };",
                "-    private long lastUsed = System.currentTimeMillis();",
                "+    private long lastUsed = Time.currentTimeMillis();",
                "     private final Map<PortAndAssignment, BlobChangingCallback> references = new HashMap<>();",
                "@@ -53,4 +60,5 @@ public abstract class LocallyCachedBlob {",
                "      * Create a new LocallyCachedBlob.",
                "+     *",
                "      * @param blobDescription a description of the blob this represents.  Typically it should at least be the blob key, but ideally also",
                "-     * include if it is an archive or not, what user or topology it is for, or if it is a storm.jar etc.",
                "+     *     include if it is an archive or not, what user or topology it is for, or if it is a storm.jar etc.",
                "      */",
                "@@ -62,5 +70,4 @@ public abstract class LocallyCachedBlob {",
                "     /**",
                "-     * Get the version of the blob cached locally.  If the version is unknown or it has not been downloaded NOT_DOWNLOADED_VERSION",
                "-     * should be returned.",
                "-     * PRECONDITION: this can only be called with a lock on this instance held.",
                "+     * Get the version of the blob cached locally.  If the version is unknown or it has not been downloaded NOT_DOWNLOADED_VERSION should be",
                "+     * returned. PRECONDITION: this can only be called with a lock on this instance held.",
                "      */",
                "@@ -69,4 +76,3 @@ public abstract class LocallyCachedBlob {",
                "     /**",
                "-     * Get the version of the blob in the blob store.",
                "-     * PRECONDITION: this can only be called with a lock on this instance held.",
                "+     * Get the version of the blob in the blob store. PRECONDITION: this can only be called with a lock on this instance held.",
                "      */",
                "@@ -77,2 +83,3 @@ public abstract class LocallyCachedBlob {",
                "      * PRECONDITION: this can only be called with a lock on this instance held.",
                "+     *",
                "      * @param store the store to us to download the data.",
                "@@ -82,2 +89,31 @@ public abstract class LocallyCachedBlob {",
                "+    protected static long downloadToTempLocation(ClientBlobStore store, String key, long currentVersion, IAdvancedFSOps fsOps,",
                "+                                          Function<Long, Path> getTempPath)",
                "+        throws KeyNotFoundException, AuthorizationException, IOException {",
                "+        try (InputStreamWithMeta in = store.getBlob(key)) {",
                "+            long newVersion = in.getVersion();",
                "+            if (newVersion == currentVersion) {",
                "+                LOG.warn(\"The version did not change, but going to download again {} {}\", currentVersion, key);",
                "+            }",
                "+            Path tmpLocation = getTempPath.apply(newVersion);",
                "+            long totalRead = 0;",
                "+            //Make sure the parent directory is there and ready to go",
                "+            fsOps.forceMkdir(tmpLocation.getParent());",
                "+            try (OutputStream outStream = fsOps.getOutputStream(tmpLocation.toFile())) {",
                "+                byte[] buffer = new byte[4096];",
                "+                int read = 0;",
                "+                while ((read = in.read(buffer)) > 0) {",
                "+                    outStream.write(buffer, 0, read);",
                "+                    totalRead += read;",
                "+                }",
                "+            }",
                "+            long expectedSize = in.getFileLength();",
                "+            if (totalRead != expectedSize) {",
                "+                throw new IOException(\"We expected to download \" + expectedSize + \" bytes but found we got \" + totalRead);",
                "+            }",
                "+",
                "+            return newVersion;",
                "+        }",
                "+    }",
                "+",
                "     /**",
                "@@ -111,23 +147,2 @@ public abstract class LocallyCachedBlob {",
                "-    /**",
                "-     * Updates the last updated time.  This should be called when references are added or removed.",
                "-     */",
                "-    private synchronized void touch() {",
                "-        lastUsed = System.currentTimeMillis();",
                "-    }",
                "-",
                "-    /**",
                "-     * Get the last time that this used for LRU calculations.",
                "-     */",
                "-    public synchronized long getLastUsed() {",
                "-        return lastUsed;",
                "-    }",
                "-",
                "-    /**",
                "-     * Return true if this blob is actively being used, else false (meaning it can be deleted, but might not be).",
                "-     */",
                "-    public synchronized boolean isUsed() {",
                "-        return !references.isEmpty();",
                "-    }",
                "-",
                "     /**",
                "@@ -137,3 +152,3 @@ public abstract class LocallyCachedBlob {",
                "      */",
                "-    protected long getSizeOnDisk(Path p) throws IOException {",
                "+    protected static long getSizeOnDisk(Path p) throws IOException {",
                "         if (!Files.exists(p)) {",
                "@@ -157,2 +172,24 @@ public abstract class LocallyCachedBlob {",
                "+    /**",
                "+     * Updates the last updated time.  This should be called when references are added or removed.",
                "+     */",
                "+    protected synchronized void touch() {",
                "+        lastUsed = Time.currentTimeMillis();",
                "+        LOG.debug(\"Setting {} ts to {}\", blobKey, lastUsed);",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the last time that this used for LRU calculations.",
                "+     */",
                "+    public synchronized long getLastUsed() {",
                "+        return lastUsed;",
                "+    }",
                "+",
                "+    /**",
                "+     * Return true if this blob is actively being used, else false (meaning it can be deleted, but might not be).",
                "+     */",
                "+    public synchronized boolean isUsed() {",
                "+        return !references.isEmpty();",
                "+    }",
                "+",
                "     /**",
                "@@ -179,2 +216,3 @@ public abstract class LocallyCachedBlob {",
                "         }",
                "+        touch();",
                "     }",
                "@@ -219,2 +257,9 @@ public abstract class LocallyCachedBlob {",
                "     }",
                "+",
                "+",
                "+    public Collection<PortAndAssignment> getDependencies() {",
                "+        return references.keySet();",
                "+    }",
                "+",
                "+    public abstract boolean isFullyDownloaded();",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java",
                "index 35371b537..68415e1ca 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java",
                "@@ -117,3 +117,3 @@ public class LocallyCachedTopologyBlob extends LocallyCachedBlob {",
                "         }",
                "-    };",
                "+    }",
                "@@ -124,3 +124,2 @@ public class LocallyCachedTopologyBlob extends LocallyCachedBlob {",
                "     private final AdvancedFSOps fsOps;",
                "-    private final Map<String, Object> conf;",
                "     private volatile long version = NOT_DOWNLOADED_VERSION;",
                "@@ -141,3 +140,2 @@ public class LocallyCachedTopologyBlob extends LocallyCachedBlob {",
                "         this.fsOps = fsOps;",
                "-        this.conf = conf;",
                "         topologyBasicBlobsRootDir = Paths.get(ConfigUtils.supervisorStormDistRoot(conf, topologyId));",
                "@@ -205,27 +203,7 @@ public class LocallyCachedTopologyBlob extends LocallyCachedBlob {",
                "-        long newVersion;",
                "-        Path tmpLocation;",
                "-        String key = type.getKey(topologyId);",
                "-        try (InputStreamWithMeta in = store.getBlob(key)) {",
                "-            newVersion = in.getVersion();",
                "-            long expectedSize = in.getFileLength();",
                "-            if (newVersion == version) {",
                "-                throw new RuntimeException(\"The version did not change, but we tried to download it. \" + version + \" \" + key);",
                "-            }",
                "-            tmpLocation = topologyBasicBlobsRootDir.resolve(type.getTempFileName(newVersion));",
                "-            long totalRead = 0;",
                "-            //Make sure the parent directory is there and ready to go",
                "-            fsOps.forceMkdir(tmpLocation.getParent());",
                "-            try (OutputStream outStream = fsOps.getOutputStream(tmpLocation.toFile())) {",
                "-                byte [] buffer = new byte[4096];",
                "-                int read = 0;",
                "-                while ((read = in.read(buffer)) > 0) {",
                "-                    outStream.write(buffer, 0, read);",
                "-                    totalRead += read;",
                "-                }",
                "-            }",
                "-            if (totalRead != expectedSize) {",
                "-                throw new IOException(\"We expected to download \" + expectedSize + \" bytes but found we got \" + totalRead);",
                "-            }",
                "-        }",
                "+",
                "+        long newVersion = downloadToTempLocation(store, type.getKey(topologyId), version, fsOps,",
                "+            (version) -> topologyBasicBlobsRootDir.resolve(type.getTempFileName(version)));",
                "+",
                "+        Path tmpLocation = topologyBasicBlobsRootDir.resolve(type.getTempFileName(newVersion));",
                "@@ -249,6 +227,6 @@ public class LocallyCachedTopologyBlob extends LocallyCachedBlob {",
                "                     String shortenedName = name.replace(toRemove, \"\");",
                "-                    Path aFile = dest.resolve(shortenedName);",
                "-                    LOG.debug(\"EXTRACTING {} SHORTENED to {} into {}\", name, shortenedName, aFile);",
                "-                    fsOps.forceMkdir(aFile.getParent());",
                "-                    try (FileOutputStream out = new FileOutputStream(aFile.toFile());",
                "+                    Path targetFile = dest.resolve(shortenedName);",
                "+                    LOG.debug(\"EXTRACTING {} SHORTENED to {} into {}\", name, shortenedName, targetFile);",
                "+                    fsOps.forceMkdir(targetFile.getParent());",
                "+                    try (FileOutputStream out = new FileOutputStream(targetFile.toFile());",
                "                          InputStream in = jarFile.getInputStream(entry)) {",
                "@@ -261,2 +239,17 @@ public class LocallyCachedTopologyBlob extends LocallyCachedBlob {",
                "+    @Override",
                "+    public boolean isFullyDownloaded() {",
                "+        Path versionFile = topologyBasicBlobsRootDir.resolve(type.getVersionFileName());",
                "+        boolean ret = Files.exists(versionFile);",
                "+        Path dest = topologyBasicBlobsRootDir.resolve(type.getFileName());",
                "+        if (!(isLocalMode && type == TopologyBlobType.TOPO_JAR)) {",
                "+            ret = ret && Files.exists(dest);",
                "+        }",
                "+        if (type.needsExtraction()) {",
                "+            Path extractionDest = topologyBasicBlobsRootDir.resolve(type.getExtractionDir());",
                "+            ret = ret && Files.exists(extractionDest);",
                "+        }",
                "+        return ret;",
                "+    }",
                "+",
                "     @Override",
                "@@ -327,2 +320,3 @@ public class LocallyCachedTopologyBlob extends LocallyCachedBlob {",
                "         }",
                "+        touch();",
                "     }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/PortAndAssignment.java b/storm-server/src/main/java/org/apache/storm/localizer/PortAndAssignment.java",
                "index 081c81174..bd92173e8 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/PortAndAssignment.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/PortAndAssignment.java",
                "@@ -23,3 +23,3 @@ import org.apache.storm.generated.LocalAssignment;",
                " /**",
                "- * A Port and a LocalAssignment used to reference count Resources",
                "+ * A Port and a LocalAssignment used to reference count resources.",
                "  */",
                "@@ -47,2 +47,6 @@ class PortAndAssignment {",
                "+    public String getOwner() {",
                "+        return assignment.get_owner();",
                "+    }",
                "+",
                "     @Override",
                "@@ -54,3 +58,3 @@ class PortAndAssignment {",
                "     public String toString() {",
                "-        return \"{\" + port + \" \" + assignment + \"}\";",
                "+        return \"{\" + assignment.get_topology_id() + \" on \" + port + \"}\";",
                "     }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "index 6a4454a0a..340db1cba 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "@@ -84,6 +84,3 @@ public class ServerUtils {",
                "     public static final boolean IS_ON_WINDOWS = \"Windows_NT\".equals(System.getenv(\"OS\"));",
                "-    public static final String CURRENT_BLOB_SUFFIX_ID = \"current\";",
                "-    public static final String DEFAULT_CURRENT_BLOB_SUFFIX = \".\" + CURRENT_BLOB_SUFFIX_ID;",
                "-    public static final String DEFAULT_BLOB_VERSION_SUFFIX = \".version\";",
                "     public static final int SIGKILL = 9;",
                "@@ -168,10 +165,2 @@ public class ServerUtils {",
                "-    public static String constructVersionFileName(String fileName) {",
                "-        return fileName + DEFAULT_BLOB_VERSION_SUFFIX;",
                "-    }",
                "-",
                "-    public static String constructBlobCurrentSymlinkName(String fileName) {",
                "-        return fileName + DEFAULT_CURRENT_BLOB_SUFFIX;",
                "-    }",
                "-",
                "     /**",
                "@@ -208,10 +197,2 @@ public class ServerUtils {",
                "-    public static long localVersionOfBlob(String localFile) {",
                "-        return Utils.getVersionFromBlobVersionFile(new File(localFile + DEFAULT_BLOB_VERSION_SUFFIX));",
                "-    }",
                "-",
                "-    public static String constructBlobWithVersionFileName(String fileName, long version) {",
                "-        return fileName + \".\" + version;",
                "-    }",
                "-",
                "     public static ClientBlobStore getClientBlobStoreForSupervisor(Map<String, Object> conf) {"
            ],
            "changed_files": [
                "log4j2/cluster.xml",
                "storm-client/pom.xml",
                "storm-client/src/jvm/org/apache/storm/Config.java",
                "storm-client/src/jvm/org/apache/storm/utils/CuratorUtils.java",
                "storm-client/src/jvm/org/apache/storm/utils/NimbusClient.java",
                "storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "storm-server/pom.xml",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/SupervisorUtils.java",
                "storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocalDownloadedResource.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocalResource.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceSet.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java",
                "storm-server/src/main/java/org/apache/storm/localizer/PortAndAssignment.java",
                "storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2759": ""
            },
            "ghissue_refs": {
                "2363": "STORM-2438: added in rebalance changes to support RAS #2345 STORM-2811: Fix NPE in Nimbus when killing the same topology multiple\u2026 #2415 STORM-2809: Always create the resources directory so we can check for it #2417"
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: user",
                    "relevance": 4
                },
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2759",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2363",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f9637c6e680eca1577df795afc1e3e5add6cb44f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510692805,
            "hunks": 6,
            "message": "[STORM-2815] UI HTTP server should return 403 if the user is unauthorized * also dealing with http status code in exceptionToJson function This closes #2421",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/ui/core.clj b/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "index d12ff7498..8db46e8a9 100644",
                "--- a/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "@@ -1588,3 +1588,4 @@",
                "       (catch Exception ex",
                "-        (json-response (UIHelpers/exceptionToJson ex) ((:query-params request) \"callback\") :status 500)))))",
                "+        (let [status-code (if (instance? AuthorizationException ex) 403 500)]",
                "+          (json-response (UIHelpers/exceptionToJson ex status-code) ((:query-params request) \"callback\") :status status-code))))))",
                "diff --git a/storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java b/storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java",
                "index 75559984d..f29c1f6ff 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java",
                "@@ -34,2 +34,3 @@ import org.apache.storm.logging.filters.AccessLoggingFilter;",
                " import org.apache.storm.utils.ObjectReader;",
                "+import org.eclipse.jetty.http.HttpStatus;",
                " import org.eclipse.jetty.http.HttpVersion;",
                "@@ -294,6 +295,6 @@ public class UIHelpers {",
                "-    public static Map exceptionToJson(Exception ex) {",
                "+    public static Map exceptionToJson(Exception ex, int statusCode) {",
                "         StringWriter sw = new StringWriter();",
                "         ex.printStackTrace(new PrintWriter(sw));",
                "-        return ImmutableMap.of(\"error\", \"Internal Server Error\", \"errorMessage\", sw.toString());",
                "+        return ImmutableMap.of(\"error\", statusCode + \" \" + HttpStatus.getMessage(statusCode), \"errorMessage\", sw.toString());",
                "     }",
                "diff --git a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java",
                "index 59570eefc..74d70a07b 100644",
                "--- a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java",
                "+++ b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java",
                "@@ -127,4 +127,5 @@ public class LogviewerResponseBuilder {",
                "     public static Response buildExceptionJsonResponse(Exception ex, String callback) {",
                "-        return new JsonResponseBuilder().setData(UIHelpers.exceptionToJson(ex))",
                "-                .setCallback(callback).setStatus(500).build();",
                "+        int statusCode = 500;",
                "+        return new JsonResponseBuilder().setData(UIHelpers.exceptionToJson(ex, statusCode))",
                "+                .setCallback(callback).setStatus(statusCode).build();",
                "     }",
                "diff --git a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/webapp/LogviewerResource.java b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/webapp/LogviewerResource.java",
                "index 6478ca674..2e63013f6 100644",
                "--- a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/webapp/LogviewerResource.java",
                "+++ b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/webapp/LogviewerResource.java",
                "@@ -233,4 +233,5 @@ public class LogviewerResource {",
                "             LOG.error(e.getMessage(), e);",
                "-            return new JsonResponseBuilder().setData(UIHelpers.exceptionToJson(e)).setCallback(callback)",
                "-                    .setStatus(400).build();",
                "+            int statusCode = 400;",
                "+            return new JsonResponseBuilder().setData(UIHelpers.exceptionToJson(e, statusCode)).setCallback(callback)",
                "+                    .setStatus(statusCode).build();",
                "         }"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/ui/core.clj",
                "storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java",
                "storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java",
                "storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/webapp/LogviewerResource.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2815": ""
            },
            "ghissue_refs": {
                "2421": "[STORM-2815] UI HTTP server should return 403 if the user is unauthorized (1.x) #2425"
            },
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "ad13f457bf465f14e34133c013da1b18088036dd"
                ],
                [
                    "no-tag",
                    "6b22638d44481a161938aefa60c8545ca41cdeed"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: code, user",
                    "relevance": 4
                },
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2815",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2421",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "466a7ad74da27c1250eedf412a487db409e42c19",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515523883,
            "hunks": 0,
            "message": "Merge branch 'patch-1' of https://github.com/aqeelvn/storm into PULL-2508 Updated how to get the storm-starter code This closes #2508",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "PULL-2508": ""
            },
            "ghissue_refs": {
                "2508": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: code",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: PULL-2508",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2508",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "05890efcf0a509f6fabd2e89c4027eaffd497d68",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516616288,
            "hunks": 1,
            "message": "STORM-2903: (follow up) Log token identifier and username associated with token identifier",
            "diff": [
                "diff --git a/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java b/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java",
                "index eb383f4f9..7b2fc2db0 100644",
                "--- a/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java",
                "+++ b/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java",
                "@@ -226,2 +226,8 @@ public abstract class AbstractAutoCreds implements IAutoCredentials, ICredential",
                "+                                TokenIdentifier tokenId = token.decodeIdentifier();",
                "+                                if (tokenId != null) {",
                "+                                    LOG.debug(\"Token identifier : {}\", tokenId);",
                "+                                    LOG.debug(\"Username in token identifier : {}\", tokenId.getUser());",
                "+                                }",
                "+",
                "                                 UserGroupInformation.getCurrentUser().addToken(token);"
            ],
            "changed_files": [
                "external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2903": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "841586151e241beb87a42d55f5a7a500428b0c99"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: user",
                    "relevance": 4
                },
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2903",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "17173a51a86f633ef489b928d4060181a2ddc5cf",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514930079,
            "hunks": 0,
            "message": "Merge branch 'STORM-2874-1.x' of https://github.com/srdo/storm into STORM-2874-1.x STORM-2874 1.x: Minor refactoring of backpressure test code This closes #2489",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2874": ""
            },
            "ghissue_refs": {
                "2489": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: code",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2874",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2489",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7418bc7db9ffc783ed50d98ecd8c644be16bd860",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514930079,
            "hunks": 2,
            "message": "Merge branch 'STORM-2874-1.x' of https://github.com/srdo/storm into STORM-2874-1.x STORM-2874 1.x: Minor refactoring of backpressure test code This closes #2489",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java b/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java",
                "index f3b5a66cc..3c3ae6faa 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java",
                "@@ -23,3 +23,3 @@ import org.slf4j.LoggerFactory;",
                "-public class WorkerBackpressureThread extends Thread {",
                "+public final class WorkerBackpressureThread extends Thread {",
                "@@ -68,11 +68,13 @@ public class WorkerBackpressureThread extends Thread {",
                "     }",
                "-}",
                "+   ",
                "+    private static class BackpressureUncaughtExceptionHandler implements Thread.UncaughtExceptionHandler {",
                "+",
                "+        private static final Logger LOG = LoggerFactory.getLogger(BackpressureUncaughtExceptionHandler.class);",
                "-class BackpressureUncaughtExceptionHandler implements Thread.UncaughtExceptionHandler {",
                "-    private static final Logger LOG = LoggerFactory.getLogger(BackpressureUncaughtExceptionHandler.class);",
                "-    @Override",
                "-    public void uncaughtException(Thread t, Throwable e) {",
                "-        // note that exception that happens during connecting to ZK has been ignored in the callback implementation",
                "-        LOG.error(\"Received error or exception in WorkerBackpressureThread.. terminating the worker...\", e);",
                "-        Runtime.getRuntime().exit(1);",
                "+        @Override",
                "+        public void uncaughtException(Thread t, Throwable e) {",
                "+            // note that exception that happens during connecting to ZK has been ignored in the callback implementation",
                "+            LOG.error(\"Received error or exception in WorkerBackpressureThread.. terminating the worker...\", e);",
                "+            Runtime.getRuntime().exit(1);",
                "+        }",
                "     }"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2874": ""
            },
            "ghissue_refs": {
                "2489": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: code",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2874",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2489",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6799c5a30dedc39cda9a641ecbf8bb81aa4eec17",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510756380,
            "hunks": 3,
            "message": "[STORM-2814] Logviewer HTTP server should return 403 if the user is unauthorized",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/logviewer.clj b/storm-core/src/clj/org/apache/storm/daemon/logviewer.clj",
                "index d7b21de8a..a59cbb943 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/logviewer.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/logviewer.clj",
                "@@ -851,3 +851,3 @@",
                "               (json-response (exception->json ex) callback :status 500))))",
                "-        (json-response (unauthorized-user-json user) callback :status 401))",
                "+        (json-response (unauthorized-user-json user) callback :status 403))",
                "       (json-response {\"error\" \"Not Found\"",
                "diff --git a/storm-core/src/clj/org/apache/storm/ui/helpers.clj b/storm-core/src/clj/org/apache/storm/ui/helpers.clj",
                "index 94b426ba3..78c78bef4 100644",
                "--- a/storm-core/src/clj/org/apache/storm/ui/helpers.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/ui/helpers.clj",
                "@@ -38,3 +38,4 @@",
                "             [compojure.handler :as handler])",
                "-  (:require [metrics.meters :refer [defmeter mark!]]))",
                "+  (:require [metrics.meters :refer [defmeter mark!]]",
                "+            [ring.util.response :as resp]))",
                "@@ -135,3 +136,4 @@",
                " (defn unauthorized-user-html [user]",
                "-  [[:h2 \"User '\" (escape-html user) \"' is not authorized.\"]])",
                "+  (-> (resp/response (str \"User '\" (escape-html user) \"' is not authorized.\"))",
                "+      (resp/status 403)))"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/logviewer.clj",
                "storm-core/src/clj/org/apache/storm/ui/helpers.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2814": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "6ecef8890a81c0a719049bc077ea97a376b18d95"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: user",
                    "relevance": 4
                },
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2814",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6d43128de81b440262ce701f6e7ba14e95f39ab9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515607396,
            "hunks": 0,
            "message": "Merge branch 'STORM-2885' of https://github.com/revans2/incubator-storm into STORM-2885 STORM-2885: Avoid conflicts with nimbusDaemon LocalCluster Tests This closes #2507",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2885": ""
            },
            "ghissue_refs": {
                "2507": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: cluster",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2885",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2507",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "4d492874282b7e9f94c6ce089d5b614d7dc4fba6",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1495302363,
            "hunks": 32,
            "message": "STORM-2525: Fix flaky integration tests",
            "diff": [
                "diff --git a/integration-test/config/Vagrantfile b/integration-test/config/Vagrantfile",
                "index 740d0b02c..def461d70 100644",
                "--- a/integration-test/config/Vagrantfile",
                "+++ b/integration-test/config/Vagrantfile",
                "@@ -21,3 +21,3 @@ require 'uri'",
                " VAGRANTFILE_API_VERSION = \"2\"",
                "-STORM_BOX_TYPE = \"hashicorp/precise64\"",
                "+STORM_BOX_TYPE = \"ubuntu/xenial64\"",
                " STORM_ZIP = Dir.glob(\"../../storm-dist/binary/**/*.zip\")",
                "@@ -55,4 +55,4 @@ Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|",
                "-  config.vm.synced_folder \"../../\", \"/home/vagrant/build/vagrant/storm\"",
                "-  config.vm.synced_folder \"~/.m2\", \"/home/vagrant/.m2\"",
                "+  config.vm.synced_folder \"../../\", \"/home/ubuntu/build/vagrant/storm\"",
                "+  config.vm.synced_folder \"~/.m2\", \"/home/ubuntu/.m2\"",
                "@@ -64,3 +64,3 @@ Vagrant.configure(VAGRANTFILE_API_VERSION) do |config|",
                "     node1.vm.hostname = \"node1\"",
                "-    node1.vm.provision :shell, :inline => \"echo run integration test; whoami; env; cd /home/vagrant/build/vagrant/storm/; pwd; bash integration-test/run-it.sh\", privileged: false",
                "+    node1.vm.provision :shell, :inline => \"echo run integration test; whoami; env; cd /home/ubuntu/build/vagrant/storm/; pwd; bash integration-test/run-it.sh\", privileged: false",
                "     #node1.vm.provision :shell, :inline => \"sudo ln -fs /vagrant/etc-hosts /etc/hosts\"",
                "diff --git a/integration-test/config/cluster.xml b/integration-test/config/cluster.xml",
                "deleted file mode 100644",
                "index 97968e462..000000000",
                "--- a/integration-test/config/cluster.xml",
                "+++ /dev/null",
                "@@ -1,101 +0,0 @@",
                "-<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
                "-",
                "-<!--",
                "-  ~ Licensed to the Apache Software Foundation (ASF) under one or more",
                "-  ~ contributor license agreements.  See the NOTICE file distributed with",
                "-  ~ this work for additional information regarding copyright ownership.",
                "-  ~ The ASF licenses this file to You under the Apache License, Version 2.0",
                "-  ~ (the \"License\"); you may not use this file except in compliance with",
                "-  ~ the License.  You may obtain a copy of the License at",
                "-  ~",
                "-  ~     http://www.apache.org/licenses/LICENSE-2.0",
                "-  ~",
                "-  ~ Unless required by applicable law or agreed to in writing, software",
                "-  ~ distributed under the License is distributed on an \"AS IS\" BASIS,",
                "-  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "-  ~ See the License for the specific language governing permissions and",
                "-  ~ limitations under the License.",
                "-  -->",
                "-",
                "-<configuration scan=\"true\" scanPeriod=\"60 seconds\">",
                "-    <properties>",
                "-        <property name=\"pattern\">%d{yyyy-MM-dd HH:mm:ss.SSS} %c{1.} %t [%p] %msg%n</property>",
                "-        <property name=\"patternMetrics\">%d %-8r %m%n</property>",
                "-    </properties>",
                "-",
                "- <appender name=\"A1\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">",
                "-    <file>/var/log/storm/${logfile.name}</file>",
                "-    <rollingPolicy class=\"ch.qos.logback.core.rolling.FixedWindowRollingPolicy\">",
                "-      <fileNamePattern>/var/log/storm/${logfile.name}.%i</fileNamePattern>",
                "-      <minIndex>1</minIndex>",
                "-      <maxIndex>9</maxIndex>",
                "-    </rollingPolicy>",
                "-",
                "-    <triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\">",
                "-      <maxFileSize>100MB</maxFileSize>",
                "-    </triggeringPolicy>",
                "-",
                "-    <encoder>",
                "-            <pattern>${pattern}</pattern>",
                "-    </encoder>",
                "- </appender>",
                "-",
                "- <appender name=\"ACCESS\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">",
                "-    <file>/var/log/storm/access.log</file>",
                "-    <rollingPolicy class=\"ch.qos.logback.core.rolling.FixedWindowRollingPolicy\">",
                "-      <fileNamePattern>/var/log/storm/access.log.%i</fileNamePattern>",
                "-      <minIndex>1</minIndex>",
                "-      <maxIndex>9</maxIndex>",
                "-    </rollingPolicy>",
                "-",
                "-    <triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\">",
                "-      <maxFileSize>100MB</maxFileSize>",
                "-    </triggeringPolicy>",
                "-",
                "-    <encoder>",
                "-            <pattern>${pattern}</pattern>",
                "-    </encoder>",
                "-  </appender>",
                "-",
                "-  <appender name=\"METRICS\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\">",
                "-    <file>/var/log/storm/metrics.log</file>",
                "-    <rollingPolicy class=\"ch.qos.logback.core.rolling.FixedWindowRollingPolicy\">",
                "-      <fileNamePattern>metrics.log.%i</fileNamePattern>",
                "-      <minIndex>1</minIndex>",
                "-      <maxIndex>9</maxIndex>",
                "-    </rollingPolicy>",
                "-",
                "-    <triggeringPolicy class=\"ch.qos.logback.core.rolling.SizeBasedTriggeringPolicy\">",
                "-      <maxFileSize>2MB</maxFileSize>",
                "-    </triggeringPolicy>",
                "-",
                "-    <encoder>",
                "-            <pattern>${patternMetrics}</pattern>",
                "-    </encoder>",
                "-  </appender>",
                "-",
                "-  <root level=\"INFO\">",
                "-    <appender-ref ref=\"A1\"/>",
                "-  </root>",
                "-  ",
                "-    <logger name=\"org.apache.storm.messaging.netty\">",
                "-    <level value=\"WARN\" />",
                "-    <appender-ref ref=\"A1\" />",
                "-  </logger>",
                "-  ",
                "-    <logger name=\"org.apache.storm\">",
                "-    <level value=\"DEBUG\" />",
                "-    <appender-ref ref=\"A1\" />",
                "-  </logger>",
                "-",
                "-    <logger name=\"org.apache.storm.security.auth.authorizer\" additivity=\"false\">",
                "-    <level value=\"INFO\" />",
                "-    <appender-ref ref=\"ACCESS\" />",
                "-  </logger>",
                "-",
                "-    <logger name=\"org.apache.storm.metric.LoggingMetricsConsumer\" additivity=\"false\">",
                "-    <level value=\"INFO\"/>",
                "-    <appender-ref ref=\"METRICS\"/>",
                "-  </logger>",
                "-",
                "-</configuration>",
                "diff --git a/integration-test/config/install-storm.sh b/integration-test/config/install-storm.sh",
                "index b08e2bc3c..8316c753b 100644",
                "--- a/integration-test/config/install-storm.sh",
                "+++ b/integration-test/config/install-storm.sh",
                "@@ -33,3 +33,2 @@ rm /usr/share/storm/conf/storm.yaml",
                " cp \"${SCRIPT_DIR}/storm.yaml\" /usr/share/storm/conf/",
                "-cp \"${SCRIPT_DIR}/cluster.xml\" /usr/share/storm/logback/",
                " ln -s /usr/share/storm/conf/storm.yaml /etc/storm/conf/storm.yaml",
                "diff --git a/integration-test/config/install-zookeeper.sh b/integration-test/config/install-zookeeper.sh",
                "index 5f92f56b5..98253d7b8 100644",
                "--- a/integration-test/config/install-zookeeper.sh",
                "+++ b/integration-test/config/install-zookeeper.sh",
                "@@ -16,2 +16,3 @@",
                " #",
                "+# $1 is the Zookeeper version to install",
                " apt-get --yes install zookeeper=$1 zookeeperd=$1",
                "diff --git a/integration-test/pom.xml b/integration-test/pom.xml",
                "index 03512353f..bcf688d82 100755",
                "--- a/integration-test/pom.xml",
                "+++ b/integration-test/pom.xml",
                "@@ -89,2 +89,7 @@",
                "         </dependency>",
                "+        <dependency>",
                "+            <groupId>com.google.code.gson</groupId>",
                "+            <artifactId>gson</artifactId>",
                "+            <version>2.8.0</version>",
                "+        </dependency>",
                "         <dependency>",
                "diff --git a/integration-test/run-it.sh b/integration-test/run-it.sh",
                "index eaed79023..927c60db6 100755",
                "--- a/integration-test/run-it.sh",
                "+++ b/integration-test/run-it.sh",
                "@@ -34,3 +34,3 @@ list_storm_processes || true",
                " # increasing swap space so we can run lots of workers",
                "-sudo dd if=/dev/zero of=/swapfile.img bs=8192 count=1M",
                "+sudo dd if=/dev/zero of=/swapfile.img bs=4096 count=1M",
                " sudo mkswap /swapfile.img",
                "@@ -38,3 +38,3 @@ sudo swapon /swapfile.img",
                "-if [[ \"${USER}\" == \"vagrant\" ]]; then # install oracle jdk8",
                "+if [[ \"${USER}\" == \"ubuntu\" ]]; then # install oracle jdk8",
                "     sudo apt-get update",
                "@@ -46,2 +46,3 @@ if [[ \"${USER}\" == \"vagrant\" ]]; then # install oracle jdk8",
                "     sudo apt-get -y install maven",
                "+    sudo apt-get install unzip",
                "     java -version",
                "@@ -49,5 +50,5 @@ if [[ \"${USER}\" == \"vagrant\" ]]; then # install oracle jdk8",
                "     export MAVEN_OPTS=\"-Xmx3000m\"",
                "+    zookeeper_version=3.4.8*",
                " else",
                "     ( while true; do echo \"heartbeat\"; sleep 300; done ) & #heartbeat needed by travis ci",
                "-    (cd \"${STORM_SRC_DIR}\" && mvn clean install -DskipTests=true) || die \"maven install command failed\"",
                "     if [[ \"${USER}\" == \"travis\" ]]; then",
                "diff --git a/integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingTimeCorrectness.java b/integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingTimeCorrectness.java",
                "index 430449b4e..8e9c6c9f6 100644",
                "--- a/integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingTimeCorrectness.java",
                "+++ b/integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingTimeCorrectness.java",
                "@@ -116,3 +116,5 @@ public class SlidingTimeCorrectness implements TestableTopology {",
                "         public void nextTuple() {",
                "-            TimeUtil.sleepMilliSec(rng.nextInt(800));",
                "+            //Emitting too quickly can lead to spurious test failures because the worker log may roll right before we read it",
                "+            //Sleep a bit between emits",
                "+            TimeUtil.sleepMilliSec(rng.nextInt(100));",
                "             currentNum++;",
                "diff --git a/integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingWindowCorrectness.java b/integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingWindowCorrectness.java",
                "index 33ee00482..c05903591 100644",
                "--- a/integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingWindowCorrectness.java",
                "+++ b/integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingWindowCorrectness.java",
                "@@ -41,3 +41,2 @@ import java.util.Map;",
                " import java.util.Random;",
                "-import java.util.concurrent.TimeUnit;",
                "@@ -76,3 +75,2 @@ public class SlidingWindowCorrectness implements TestableTopology {",
                "                 new VerificationBolt()",
                "-                        .withLag(new BaseWindowedBolt.Duration(10, TimeUnit.SECONDS))",
                "                         .withWindow(new BaseWindowedBolt.Count(windowSize), new BaseWindowedBolt.Count(slideSize)),",
                "diff --git a/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java b/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java",
                "index 7cadd1869..0c9e89164 100644",
                "--- a/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java",
                "+++ b/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java",
                "@@ -113,3 +113,5 @@ public class TumblingTimeCorrectness implements TestableTopology {",
                "         public void nextTuple() {",
                "-            TimeUtil.sleepMilliSec(rng.nextInt(800));",
                "+            //Emitting too quickly can lead to spurious test failures because the worker log may roll right before we read it",
                "+            //Sleep a bit between emits",
                "+            TimeUtil.sleepMilliSec(rng.nextInt(100));",
                "             currentNum++;",
                "diff --git a/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java b/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java",
                "index adcb9ddbf..b7f5dfa3e 100644",
                "--- a/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java",
                "+++ b/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java",
                "@@ -41,3 +41,2 @@ import java.util.Map;",
                " import java.util.Random;",
                "-import java.util.concurrent.TimeUnit;",
                "@@ -74,3 +73,2 @@ public class TumblingWindowCorrectness implements TestableTopology {",
                "                 new VerificationBolt()",
                "-                        .withLag(new BaseWindowedBolt.Duration(10, TimeUnit.SECONDS))",
                "                         .withTumblingWindow(new BaseWindowedBolt.Count(tumbleSize)), 1)",
                "diff --git a/pom.xml b/pom.xml",
                "index 3b0d39b88..facb74fa9 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -269,2 +269,3 @@",
                "         <hdrhistogram.version>2.1.7</hdrhistogram.version>",
                "+        <hamcrest.version>1.3</hamcrest.version>",
                "@@ -916,2 +917,20 @@",
                "             </dependency>",
                "+            <dependency>",
                "+                <groupId>org.mockito</groupId>",
                "+                <artifactId>mockito-core</artifactId>",
                "+                <version>${mockito.version}</version>",
                "+                <scope>test</scope>",
                "+            </dependency>",
                "+            <dependency>",
                "+                <groupId>org.hamcrest</groupId>",
                "+                <artifactId>hamcrest-core</artifactId>",
                "+                <version>${hamcrest.version}</version>",
                "+                <scope>test</scope>",
                "+            </dependency>",
                "+            <dependency>",
                "+                <groupId>org.hamcrest</groupId>",
                "+                <artifactId>hamcrest-library</artifactId>",
                "+                <version>${hamcrest.version}</version>",
                "+                <scope>test</scope>",
                "+            </dependency>",
                "             <dependency>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index 3d714c0c4..7c6f310fa 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -258,3 +258,8 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "+            <artifactId>mockito-core</artifactId>",
                "+            <scope>test</scope>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>hamcrest-library</artifactId>",
                "             <scope>test</scope>",
                "diff --git a/storm-core/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java b/storm-core/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java",
                "index c9afc671d..48bcb69f4 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java",
                "@@ -201,4 +201,3 @@ public class WindowedBoltExecutor implements IRichBolt {",
                "                  slidingIntervalCount, slidingIntervalDuration);",
                "-        evictionPolicy = getEvictionPolicy(windowLengthCount, windowLengthDuration,",
                "-                                                                 manager);",
                "+        evictionPolicy = getEvictionPolicy(windowLengthCount, windowLengthDuration);",
                "         triggerPolicy = getTriggerPolicy(slidingIntervalCount, slidingIntervalDuration,",
                "@@ -253,4 +252,3 @@ public class WindowedBoltExecutor implements IRichBolt {",
                "-    private EvictionPolicy<Tuple> getEvictionPolicy(Count windowLengthCount, Duration windowLengthDuration,",
                "-                                                    WindowManager<Tuple> manager) {",
                "+    private EvictionPolicy<Tuple> getEvictionPolicy(Count windowLengthCount, Duration windowLengthDuration) {",
                "         if (windowLengthCount != null) {",
                "diff --git a/storm-core/src/jvm/org/apache/storm/windowing/EvictionContext.java b/storm-core/src/jvm/org/apache/storm/windowing/EvictionContext.java",
                "index c5a578a10..7582e2697 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/windowing/EvictionContext.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/windowing/EvictionContext.java",
                "@@ -48,3 +48,3 @@ public interface EvictionContext {",
                "     /**",
                "-     * Returns the current count of events in the queue up to the reference tim",
                "+     * Returns the current count of events in the queue up to the reference time",
                "      * based on which count based evictions can be performed.",
                "diff --git a/storm-core/src/jvm/org/apache/storm/windowing/EvictionPolicy.java b/storm-core/src/jvm/org/apache/storm/windowing/EvictionPolicy.java",
                "index 05e4d9372..e7d1e40f4 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/windowing/EvictionPolicy.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/windowing/EvictionPolicy.java",
                "@@ -29,3 +29,3 @@ public interface EvictionPolicy<T> {",
                "      */",
                "-    enum Action {",
                "+    public enum Action {",
                "         /**",
                "diff --git a/storm-core/src/jvm/org/apache/storm/windowing/TimeEvictionPolicy.java b/storm-core/src/jvm/org/apache/storm/windowing/TimeEvictionPolicy.java",
                "index 0ab28a1a9..95329ca3d 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/windowing/TimeEvictionPolicy.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/windowing/TimeEvictionPolicy.java",
                "@@ -51,3 +51,3 @@ public class TimeEvictionPolicy<T> implements EvictionPolicy<T> {",
                "     @Override",
                "-    public Action evict(Event<T> event) {",
                "+    public Action evict(Event<T> event) {      ",
                "         long now = referenceTime == null ? System.currentTimeMillis() : referenceTime;",
                "diff --git a/storm-core/src/jvm/org/apache/storm/windowing/WatermarkCountEvictionPolicy.java b/storm-core/src/jvm/org/apache/storm/windowing/WatermarkCountEvictionPolicy.java",
                "index 74240bb8c..676ccdbbc 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/windowing/WatermarkCountEvictionPolicy.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/windowing/WatermarkCountEvictionPolicy.java",
                "@@ -21,3 +21,3 @@ package org.apache.storm.windowing;",
                "  * An eviction policy that tracks count based on watermark ts and",
                "- * evicts events upto the watermark based on a threshold count.",
                "+ * evicts events up to the watermark based on a threshold count.",
                "  *",
                "@@ -32,2 +32,3 @@ public class WatermarkCountEvictionPolicy<T> extends CountEvictionPolicy<T> {",
                "     private long processed = 0L;",
                "+    private EvictionContext context;",
                "@@ -39,2 +40,9 @@ public class WatermarkCountEvictionPolicy<T> extends CountEvictionPolicy<T> {",
                "     public Action evict(Event<T> event) {",
                "+        if(context == null) {",
                "+            //It is possible to get asked about eviction before we have a context, due to WindowManager.compactWindow.",
                "+            //In this case we should hold on to all the events. When the first watermark is received, the context will be set,",
                "+            //and the events will be reevaluated for eviction",
                "+            return Action.STOP;",
                "+        }",
                "+        ",
                "         Action action;",
                "@@ -58,2 +66,3 @@ public class WatermarkCountEvictionPolicy<T> extends CountEvictionPolicy<T> {",
                "     public void setContext(EvictionContext context) {",
                "+        this.context = context;",
                "         referenceTime = context.getReferenceTime();",
                "diff --git a/storm-core/src/jvm/org/apache/storm/windowing/WatermarkTimeEvictionPolicy.java b/storm-core/src/jvm/org/apache/storm/windowing/WatermarkTimeEvictionPolicy.java",
                "index 53361d2a1..981f514d5 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/windowing/WatermarkTimeEvictionPolicy.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/windowing/WatermarkTimeEvictionPolicy.java",
                "@@ -60,3 +60,11 @@ public class WatermarkTimeEvictionPolicy<T> extends TimeEvictionPolicy<T> {",
                "     public Action evict(Event<T> event) {",
                "-        long diff = referenceTime - event.getTimestamp();",
                "+        if(evictionContext == null) {",
                "+            //It is possible to get asked about eviction before we have a context, due to WindowManager.compactWindow.",
                "+            //In this case we should hold on to all the events. When the first watermark is received, the context will be set,",
                "+            //and the events will be reevaluated for eviction",
                "+            return Action.STOP;",
                "+        }",
                "+        ",
                "+        long referenceTime = evictionContext.getReferenceTime();",
                "+        long diff =  referenceTime - event.getTimestamp();",
                "         if (diff < -lag) {",
                "diff --git a/storm-core/src/jvm/org/apache/storm/windowing/WindowManager.java b/storm-core/src/jvm/org/apache/storm/windowing/WindowManager.java",
                "index 792509e04..a7344975d 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/windowing/WindowManager.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/windowing/WindowManager.java",
                "@@ -48,2 +48,5 @@ public class WindowManager<T> implements TriggerHandler {",
                "      * keep the window size in check.",
                "+     * ",
                "+     * Note that if the eviction policy is based on watermarks, events will not be evicted until a new",
                "+     * watermark would cause them to be considered expired anyway, regardless of this limit",
                "      */"
            ],
            "changed_files": [
                "integration-test/config/Vagrantfile",
                "integration-test/config/cluster.xml",
                "integration-test/config/install-storm.sh",
                "integration-test/config/install-zookeeper.sh",
                "integration-test/pom.xml",
                "integration-test/run-it.sh",
                "integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingTimeCorrectness.java",
                "integration-test/src/main/java/org/apache/storm/st/topology/window/SlidingWindowCorrectness.java",
                "integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java",
                "integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java",
                "pom.xml",
                "storm-core/pom.xml",
                "storm-core/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java",
                "storm-core/src/jvm/org/apache/storm/windowing/EvictionContext.java",
                "storm-core/src/jvm/org/apache/storm/windowing/EvictionPolicy.java",
                "storm-core/src/jvm/org/apache/storm/windowing/TimeEvictionPolicy.java",
                "storm-core/src/jvm/org/apache/storm/windowing/WatermarkCountEvictionPolicy.java",
                "storm-core/src/jvm/org/apache/storm/windowing/WatermarkTimeEvictionPolicy.java",
                "storm-core/src/jvm/org/apache/storm/windowing/WindowManager.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2525": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "d7b7096a53edff7bedda56c85f6ece9c9d994591"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                },
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2525",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "0ecc5bf9595259c94ea52cddbed336890d281b46",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510684318,
            "hunks": 8,
            "message": "STORM-2814 Logviewer HTTP server should return 403 instead of 200 if the user is unauthorized This closes #2420",
            "diff": [
                "diff --git a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogPageHandler.java b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogPageHandler.java",
                "index 5c883d96a..0f498b375 100644",
                "--- a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogPageHandler.java",
                "+++ b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogPageHandler.java",
                "@@ -249,3 +249,3 @@ public class LogviewerLogPageHandler {",
                "             } else {",
                "-                return LogviewerResponseBuilder.buildResponseUnautohrizedUser(user);",
                "+                return LogviewerResponseBuilder.buildResponseUnauthorizedUser(user);",
                "             }",
                "diff --git a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerProfileHandler.java b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerProfileHandler.java",
                "index b6859379b..202d42174 100644",
                "--- a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerProfileHandler.java",
                "+++ b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerProfileHandler.java",
                "@@ -78,3 +78,3 @@ public class LogviewerProfileHandler {",
                "             } else {",
                "-                return LogviewerResponseBuilder.buildResponseUnautohrizedUser(user);",
                "+                return LogviewerResponseBuilder.buildResponseUnauthorizedUser(user);",
                "             }",
                "@@ -105,3 +105,3 @@ public class LogviewerProfileHandler {",
                "             } else {",
                "-                return LogviewerResponseBuilder.buildResponseUnautohrizedUser(user);",
                "+                return LogviewerResponseBuilder.buildResponseUnauthorizedUser(user);",
                "             }",
                "diff --git a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java",
                "index 7cef5494b..67b265d18 100644",
                "--- a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java",
                "+++ b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java",
                "@@ -59,3 +59,3 @@ public class LogFileDownloader {",
                "             } else {",
                "-                return LogviewerResponseBuilder.buildResponseUnautohrizedUser(user);",
                "+                return LogviewerResponseBuilder.buildResponseUnauthorizedUser(user);",
                "             }",
                "diff --git a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java",
                "index 74d70a07b..ee9443513 100644",
                "--- a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java",
                "+++ b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java",
                "@@ -22,2 +22,3 @@ import static j2html.TagCreator.body;",
                " import static j2html.TagCreator.h2;",
                "+import static javax.ws.rs.core.Response.Status.FORBIDDEN;",
                " import static javax.ws.rs.core.Response.Status.OK;",
                "@@ -91,5 +92,5 @@ public class LogviewerResponseBuilder {",
                "      */",
                "-    public static Response buildResponseUnautohrizedUser(String user) {",
                "+    public static Response buildResponseUnauthorizedUser(String user) {",
                "         String entity = buildUnauthorizedUserHtml(user);",
                "-        return Response.status(OK)",
                "+        return Response.status(FORBIDDEN)",
                "                 .entity(entity)",
                "@@ -117,3 +118,3 @@ public class LogviewerResponseBuilder {",
                "         return new JsonResponseBuilder().setData(UIHelpers.unauthorizedUserJson(user))",
                "-                .setCallback(callback).setStatus(401).build();",
                "+                .setCallback(callback).setStatus(403).build();",
                "     }"
            ],
            "changed_files": [
                "storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogPageHandler.java",
                "storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerProfileHandler.java",
                "storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogFileDownloader.java",
                "storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/LogviewerResponseBuilder.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2814": ""
            },
            "ghissue_refs": {
                "2420": "[STORM-2814] Logviewer HTTP server should return 403 if the user is unauthorized #2422"
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: user",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2814",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2420",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "9222b9b41e157ca4467a7e0dc22e2ad5adc27298",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514930079,
            "hunks": 2,
            "message": "Merge branch 'STORM-2874-1.x' of https://github.com/srdo/storm into STORM-2874-1.x STORM-2874 1.x: Minor refactoring of backpressure test code This closes #2489",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java b/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java",
                "index f3b5a66cc..3c3ae6faa 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java",
                "@@ -23,3 +23,3 @@ import org.slf4j.LoggerFactory;",
                "-public class WorkerBackpressureThread extends Thread {",
                "+public final class WorkerBackpressureThread extends Thread {",
                "@@ -68,11 +68,13 @@ public class WorkerBackpressureThread extends Thread {",
                "     }",
                "-}",
                "+   ",
                "+    private static class BackpressureUncaughtExceptionHandler implements Thread.UncaughtExceptionHandler {",
                "+",
                "+        private static final Logger LOG = LoggerFactory.getLogger(BackpressureUncaughtExceptionHandler.class);",
                "-class BackpressureUncaughtExceptionHandler implements Thread.UncaughtExceptionHandler {",
                "-    private static final Logger LOG = LoggerFactory.getLogger(BackpressureUncaughtExceptionHandler.class);",
                "-    @Override",
                "-    public void uncaughtException(Thread t, Throwable e) {",
                "-        // note that exception that happens during connecting to ZK has been ignored in the callback implementation",
                "-        LOG.error(\"Received error or exception in WorkerBackpressureThread.. terminating the worker...\", e);",
                "-        Runtime.getRuntime().exit(1);",
                "+        @Override",
                "+        public void uncaughtException(Thread t, Throwable e) {",
                "+            // note that exception that happens during connecting to ZK has been ignored in the callback implementation",
                "+            LOG.error(\"Received error or exception in WorkerBackpressureThread.. terminating the worker...\", e);",
                "+            Runtime.getRuntime().exit(1);",
                "+        }",
                "     }"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2874": ""
            },
            "ghissue_refs": {
                "2489": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: code",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2874",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2489",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5f0abb21c83c129af5b4acce2d0bb3bbb0f41887",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509397785,
            "hunks": 85,
            "message": "STORM-2792: Remove RAS EvictionPolicy and cleanup",
            "diff": [
                "diff --git a/conf/defaults.yaml b/conf/defaults.yaml",
                "index ad3405408..d931b2156 100644",
                "--- a/conf/defaults.yaml",
                "+++ b/conf/defaults.yaml",
                "@@ -275,3 +275,2 @@ topology.worker.max.heap.size.mb: 768.0",
                " topology.scheduler.strategy: \"org.apache.storm.scheduler.resource.strategies.scheduling.DefaultResourceAwareStrategy\"",
                "-resource.aware.scheduler.eviction.strategy: \"org.apache.storm.scheduler.resource.strategies.eviction.DefaultEvictionStrategy\"",
                " resource.aware.scheduler.priority.strategy: \"org.apache.storm.scheduler.resource.strategies.priority.DefaultSchedulingPriorityStrategy\"",
                "diff --git a/storm-client/pom.xml b/storm-client/pom.xml",
                "index d0cbe0e05..c4accca33 100644",
                "--- a/storm-client/pom.xml",
                "+++ b/storm-client/pom.xml",
                "@@ -195,2 +195,6 @@",
                "         </dependency>",
                "+        <dependency>",
                "+            <groupId>org.apache.curator</groupId>",
                "+            <artifactId>curator-client</artifactId>",
                "+        </dependency>",
                "     </dependencies>",
                "diff --git a/storm-server/src/main/java/org/apache/storm/DaemonConfig.java b/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "index 79b44ef07..4d2229948 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "@@ -25,3 +25,2 @@ import static org.apache.storm.validation.ConfigValidationAnnotations.isStringOr",
                " import static org.apache.storm.validation.ConfigValidationAnnotations.isPositiveNumber;",
                "-import static org.apache.storm.validation.ConfigValidationAnnotations.isType;",
                " import static org.apache.storm.validation.ConfigValidationAnnotations.NotNull;",
                "@@ -39,3 +38,2 @@ import org.apache.storm.scheduler.blacklist.reporters.IReporter;",
                " import org.apache.storm.scheduler.blacklist.strategies.IBlacklistStrategy;",
                "-import org.apache.storm.scheduler.resource.strategies.eviction.IEvictionStrategy;",
                " import org.apache.storm.scheduler.resource.strategies.priority.ISchedulingPriorityStrategy;",
                "@@ -880,9 +878,2 @@ public class DaemonConfig implements Validated {",
                "-    /**",
                "-     * The class that specifies the eviction strategy to use in ResourceAwareScheduler.",
                "-     */",
                "-    @NotNull",
                "-    @isImplementationOfClass(implementsClass = IEvictionStrategy.class)",
                "-    public static final String RESOURCE_AWARE_SCHEDULER_EVICTION_STRATEGY = \"resource.aware.scheduler.eviction.strategy\";",
                "-",
                "     /**",
                "@@ -894,2 +885,10 @@ public class DaemonConfig implements Validated {",
                "+    /**",
                "+     * The maximum number of times that the RAS will attempt to schedule a topology. The default is 5.",
                "+     */",
                "+    @isInteger",
                "+    @isPositiveNumber",
                "+    public static final String RESOURCE_AWARE_SCHEDULER_MAX_TOPOLOGY_SCHEDULING_ATTEMPTS =",
                "+        \"resource.aware.scheduler.max.topology.scheduling.attempts\";",
                "+",
                "     /**",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index d4f0ee8f5..d9020e2ad 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -136,3 +136,3 @@ import org.apache.storm.nimbus.NimbusInfo;",
                " import org.apache.storm.scheduler.Cluster;",
                "-import org.apache.storm.scheduler.Cluster.SupervisorResources;",
                "+import org.apache.storm.scheduler.SupervisorResources;",
                " import org.apache.storm.scheduler.DefaultScheduler;",
                "@@ -973,3 +973,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "         }",
                "-        ResourceUtils.checkIntialization(resourcesMap, compId, topoConf);",
                "+        ResourceUtils.checkInitialization(resourcesMap, compId, topoConf);",
                "         return resourcesMap;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "index 6cb53e4f5..1b30e8d93 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.scheduler;",
                "@@ -46,60 +47,2 @@ public class Cluster implements ISchedulingState {",
                "-    public static class SupervisorResources {",
                "-        private final double totalMem;",
                "-        private final double totalCpu;",
                "-        private final double usedMem;",
                "-        private final double usedCpu;",
                "-",
                "-        /**",
                "-         * Constructor for a Supervisor's resources.",
                "-         *",
                "-         * @param totalMem the total mem on the supervisor",
                "-         * @param totalCpu the total CPU on the supervisor",
                "-         * @param usedMem the used mem on the supervisor",
                "-         * @param usedCpu the used CPU on the supervisor",
                "-         */",
                "-        public SupervisorResources(double totalMem, double totalCpu, double usedMem, double usedCpu) {",
                "-            this.totalMem = totalMem;",
                "-            this.totalCpu = totalCpu;",
                "-            this.usedMem = usedMem;",
                "-            this.usedCpu = usedCpu;",
                "-        }",
                "-",
                "-        public double getUsedMem() {",
                "-            return usedMem;",
                "-        }",
                "-",
                "-        public double getUsedCpu() {",
                "-            return usedCpu;",
                "-        }",
                "-",
                "-        public double getTotalMem() {",
                "-            return totalMem;",
                "-        }",
                "-",
                "-        public double getTotalCpu() {",
                "-            return totalCpu;",
                "-        }",
                "-",
                "-        public double getAvailableCpu() {",
                "-            return totalCpu - usedCpu;",
                "-        }",
                "-",
                "-        public double getAvailableMem() {",
                "-            return totalMem - usedMem;",
                "-        }",
                "-",
                "-        private SupervisorResources add(WorkerResources wr) {",
                "-            return new SupervisorResources(",
                "-                totalMem,",
                "-                totalCpu,",
                "-                usedMem + wr.get_mem_off_heap() + wr.get_mem_on_heap(),",
                "-                usedCpu + wr.get_cpu());",
                "-        }",
                "-",
                "-        public SupervisorResources addMem(Double value) {",
                "-            return new SupervisorResources(totalMem, totalCpu, usedMem + value, usedCpu);",
                "-        }",
                "-    }",
                "-",
                "     /**",
                "@@ -841,4 +784,4 @@ public class Cluster implements ISchedulingState {",
                "      */",
                "-    public static Double getAssignedMemoryForSlot(final Map<String, Object> topConf) {",
                "-        Double totalWorkerMemory = 0.0;",
                "+    public static double getAssignedMemoryForSlot(final Map<String, Object> topConf) {",
                "+        double totalWorkerMemory = 0.0;",
                "         final Integer topologyWorkerDefaultMemoryAllocation = 768;",
                "@@ -889,2 +832,9 @@ public class Cluster implements ISchedulingState {",
                "+    /**",
                "+     * set scheduler status for a topology.",
                "+     */",
                "+    public void setStatus(TopologyDetails td, String statusMessage) {",
                "+        setStatus(td.getId(), statusMessage);",
                "+    }",
                "+",
                "     /**",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java b/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "index 95e0c8643..e37e4b081 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "@@ -27,3 +27,2 @@ import org.apache.storm.daemon.nimbus.TopologyResources;",
                " import org.apache.storm.generated.WorkerResources;",
                "-import org.apache.storm.scheduler.Cluster.SupervisorResources;",
                "@@ -45,3 +44,3 @@ public interface ISchedulingState {",
                "     /**",
                "-     * Does the topology need scheduling?",
                "+     * Does the topology need scheduling.",
                "      *",
                "@@ -105,2 +104,3 @@ public interface ISchedulingState {",
                "     /**",
                "+     * Get the executor to component name map for executors that need to be scheduled.",
                "      * @param topology the topology this is for",
                "@@ -111,2 +111,3 @@ public interface ISchedulingState {",
                "     /**",
                "+     * Get the component name to executor list for executors that need to be scheduled.",
                "      * @param topology the topology this is for",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/SupervisorResources.java b/storm-server/src/main/java/org/apache/storm/scheduler/SupervisorResources.java",
                "new file mode 100644",
                "index 000000000..ac2a50651",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/SupervisorResources.java",
                "@@ -0,0 +1,79 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler;",
                "+",
                "+import org.apache.storm.generated.WorkerResources;",
                "+",
                "+public class SupervisorResources {",
                "+    private final double totalMem;",
                "+    private final double totalCpu;",
                "+    private final double usedMem;",
                "+    private final double usedCpu;",
                "+",
                "+    /**",
                "+     * Constructor for a Supervisor's resources.",
                "+     *",
                "+     * @param totalMem the total mem on the supervisor",
                "+     * @param totalCpu the total CPU on the supervisor",
                "+     * @param usedMem the used mem on the supervisor",
                "+     * @param usedCpu the used CPU on the supervisor",
                "+     */",
                "+    public SupervisorResources(double totalMem, double totalCpu, double usedMem, double usedCpu) {",
                "+        this.totalMem = totalMem;",
                "+        this.totalCpu = totalCpu;",
                "+        this.usedMem = usedMem;",
                "+        this.usedCpu = usedCpu;",
                "+    }",
                "+",
                "+    public double getUsedMem() {",
                "+        return usedMem;",
                "+    }",
                "+",
                "+    public double getUsedCpu() {",
                "+        return usedCpu;",
                "+    }",
                "+",
                "+    public double getTotalMem() {",
                "+        return totalMem;",
                "+    }",
                "+",
                "+    public double getTotalCpu() {",
                "+        return totalCpu;",
                "+    }",
                "+",
                "+    public double getAvailableCpu() {",
                "+        return totalCpu - usedCpu;",
                "+    }",
                "+",
                "+    public double getAvailableMem() {",
                "+        return totalMem - usedMem;",
                "+    }",
                "+",
                "+    SupervisorResources add(WorkerResources wr) {",
                "+        return new SupervisorResources(",
                "+            totalMem,",
                "+            totalCpu,",
                "+            usedMem + wr.get_mem_off_heap() + wr.get_mem_on_heap(),",
                "+            usedCpu + wr.get_cpu());",
                "+    }",
                "+",
                "+    public SupervisorResources addMem(Double value) {",
                "+        return new SupervisorResources(totalMem, totalCpu, usedMem + value, usedCpu);",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java b/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "index 005df7d31..326440ef9 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "@@ -141,3 +141,3 @@ public class TopologyDetails {",
                "                     ResourceUtils.parseResources(bolt.getValue().get_common().get_json_conf());",
                "-                ResourceUtils.checkIntialization(topologyResources, bolt.getKey(), topologyConf);",
                "+                ResourceUtils.checkInitialization(topologyResources, bolt.getKey(), topologyConf);",
                "                 for (Map.Entry<ExecutorDetails, String> anExecutorToComponent :",
                "@@ -155,3 +155,3 @@ public class TopologyDetails {",
                "                     ResourceUtils.parseResources(spout.getValue().get_common().get_json_conf());",
                "-                ResourceUtils.checkIntialization(topologyResources, spout.getKey(), this.topologyConf);",
                "+                ResourceUtils.checkInitialization(topologyResources, spout.getKey(), this.topologyConf);",
                "                 for (Map.Entry<ExecutorDetails, String> anExecutorToComponent :",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "index 669079b68..fa7763e24 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "@@ -20,4 +20,9 @@ package org.apache.storm.scheduler.resource;",
                "+import com.google.common.collect.ImmutableList;",
                "+import java.util.ArrayList;",
                "+import java.util.Collection;",
                " import java.util.HashMap;",
                "+import java.util.List;",
                " import java.util.Map;",
                "+import java.util.stream.Collectors;",
                " import org.apache.storm.Config;",
                "@@ -26,2 +31,4 @@ import org.apache.storm.scheduler.Cluster;",
                " import org.apache.storm.scheduler.IScheduler;",
                "+import org.apache.storm.scheduler.SchedulerAssignment;",
                "+import org.apache.storm.scheduler.SchedulerAssignmentImpl;",
                " import org.apache.storm.scheduler.SingleTopologyCluster;",
                "@@ -29,3 +36,3 @@ import org.apache.storm.scheduler.Topologies;",
                " import org.apache.storm.scheduler.TopologyDetails;",
                "-import org.apache.storm.scheduler.resource.strategies.eviction.IEvictionStrategy;",
                "+import org.apache.storm.scheduler.WorkerSlot;",
                " import org.apache.storm.scheduler.resource.strategies.priority.ISchedulingPriorityStrategy;",
                "@@ -33,5 +40,6 @@ import org.apache.storm.scheduler.resource.strategies.scheduling.IStrategy;",
                " import org.apache.storm.scheduler.utils.ConfigLoaderFactoryService;",
                "-import org.apache.storm.utils.ReflectionUtils;",
                "-import org.apache.storm.utils.DisallowedStrategyException;",
                " import org.apache.storm.scheduler.utils.IConfigLoader;",
                "+import org.apache.storm.utils.DisallowedStrategyException;",
                "+import org.apache.storm.utils.ObjectReader;",
                "+import org.apache.storm.utils.ReflectionUtils;",
                " import org.apache.storm.utils.Utils;",
                "@@ -43,5 +51,5 @@ public class ResourceAwareScheduler implements IScheduler {",
                "     private Map<String, Object> conf;",
                "-    private ISchedulingPriorityStrategy schedulingPrioritystrategy;",
                "-    private IEvictionStrategy evictionStrategy;",
                "+    private ISchedulingPriorityStrategy schedulingPriorityStrategy;",
                "     private IConfigLoader configLoader;",
                "+    private int maxSchedulingAttempts;",
                "@@ -50,7 +58,7 @@ public class ResourceAwareScheduler implements IScheduler {",
                "         this.conf = conf;",
                "-        schedulingPrioritystrategy = (ISchedulingPriorityStrategy) ReflectionUtils.newInstance(",
                "+        schedulingPriorityStrategy = ReflectionUtils.newInstance(",
                "                 (String) conf.get(DaemonConfig.RESOURCE_AWARE_SCHEDULER_PRIORITY_STRATEGY));",
                "-        evictionStrategy = (IEvictionStrategy) ReflectionUtils.newInstance(",
                "-                (String) conf.get(DaemonConfig.RESOURCE_AWARE_SCHEDULER_EVICTION_STRATEGY));",
                "         configLoader = ConfigLoaderFactoryService.createConfigLoader(conf);",
                "+        maxSchedulingAttempts = ObjectReader.getInt(",
                "+            conf.get(DaemonConfig.RESOURCE_AWARE_SCHEDULER_MAX_TOPOLOGY_SCHEDULING_ATTEMPTS), 5);",
                "     }",
                "@@ -64,4 +72,6 @@ public class ResourceAwareScheduler implements IScheduler {",
                "     public void schedule(Topologies topologies, Cluster cluster) {",
                "-        //initialize data structures",
                "-        for (TopologyDetails td : cluster.getTopologies()) {",
                "+        Map<String, User> userMap = getUsers(cluster);",
                "+        List<TopologyDetails> orderedTopologies = new ArrayList<>(schedulingPriorityStrategy.getOrderedTopologies(cluster, userMap));",
                "+        LOG.info(\"Ordered list of topologies is: {}\", orderedTopologies.stream().map((t) -> t.getId()).collect(Collectors.toList()));",
                "+        for (TopologyDetails td : orderedTopologies) {",
                "             if (!cluster.needsSchedulingRas(td)) {",
                "@@ -69,25 +79,5 @@ public class ResourceAwareScheduler implements IScheduler {",
                "                 cluster.setStatusIfAbsent(td.getId(), \"Fully Scheduled\");",
                "-            }",
                "-        }",
                "-        Map<String, User> userMap = getUsers(cluster);",
                "-",
                "-        while (true) {",
                "-            TopologyDetails td;",
                "-            try {",
                "-                //Call scheduling priority strategy",
                "-                td = schedulingPrioritystrategy.getNextTopologyToSchedule(cluster, userMap);",
                "-            } catch (Exception ex) {",
                "-                LOG.error(\"Exception thrown when running priority strategy {}. No topologies will be scheduled!\",",
                "-                        schedulingPrioritystrategy.getClass().getName(), ex);",
                "-                break;",
                "-            }",
                "-            if (td == null) {",
                "-                break;",
                "-            }",
                "-            User submitter = userMap.get(td.getTopologySubmitter());",
                "-            if (cluster.needsSchedulingRas(td)) {",
                "-                scheduleTopology(td, cluster, submitter, userMap);",
                "             } else {",
                "-                LOG.warn(\"Topology {} is already fully scheduled!\", td.getName());",
                "-                cluster.setStatusIfAbsent(td.getId(), \"Fully Scheduled\");",
                "+                User submitter = userMap.get(td.getTopologySubmitter());",
                "+                scheduleTopology(td, cluster, submitter, orderedTopologies);",
                "             }",
                "@@ -96,7 +86,22 @@ public class ResourceAwareScheduler implements IScheduler {",
                "+    private static void markFailedTopology(User u, Cluster c, TopologyDetails td, String message) {",
                "+        markFailedTopology(u, c, td, message, null);",
                "+    }",
                "-    public void scheduleTopology(TopologyDetails td, Cluster cluster, final User topologySubmitter,",
                "-                                 Map<String, User> userMap) {",
                "+    private static void markFailedTopology(User u, Cluster c, TopologyDetails td, String message, Throwable t) {",
                "+        c.setStatus(td, message);",
                "+        String realMessage = td.getId() + \" \" + message;",
                "+        if (t != null) {",
                "+            LOG.error(realMessage, t);",
                "+        } else {",
                "+            LOG.error(realMessage);",
                "+        }",
                "+        u.markTopoUnsuccess(td);",
                "+    }",
                "+",
                "+    private void scheduleTopology(TopologyDetails td, Cluster cluster, final User topologySubmitter,",
                "+                                  List<TopologyDetails> orderedTopologies) {",
                "         //A copy of cluster that we can modify, but does not get committed back to cluster unless scheduling succeeds",
                "         Cluster workingState = new Cluster(cluster);",
                "+        RAS_Nodes nodes = new RAS_Nodes(workingState);",
                "         IStrategy rasStrategy = null;",
                "@@ -104,16 +109,17 @@ public class ResourceAwareScheduler implements IScheduler {",
                "         try {",
                "-            rasStrategy = (IStrategy) ReflectionUtils.newSchedulerStrategyInstance((String) td.getConf().get(Config.TOPOLOGY_SCHEDULER_STRATEGY), conf);",
                "+            rasStrategy = ReflectionUtils.newSchedulerStrategyInstance((String) td.getConf().get(Config.TOPOLOGY_SCHEDULER_STRATEGY), conf);",
                "             rasStrategy.prepare(conf);",
                "         } catch (DisallowedStrategyException e) {",
                "-            topologySubmitter.markTopoUnsuccess(td);",
                "-            cluster.setStatus(td.getId(), \"Unsuccessful in scheduling - \" + e.getAttemptedClass()",
                "-                    + \" is not an allowed strategy. Please make sure your \" + Config.TOPOLOGY_SCHEDULER_STRATEGY",
                "-                    + \" config is one of the allowed strategies: \" + e.getAllowedStrategies().toString());",
                "+            markFailedTopology(topologySubmitter, cluster, td,",
                "+                \"Unsuccessful in scheduling - \" + e.getAttemptedClass()",
                "+                    + \" is not an allowed strategy. Please make sure your \"",
                "+                    + Config.TOPOLOGY_SCHEDULER_STRATEGY",
                "+                    + \" config is one of the allowed strategies: \"",
                "+                    + e.getAllowedStrategies(), e);",
                "             return;",
                "         } catch (RuntimeException e) {",
                "-            LOG.error(\"failed to create instance of IStrategy: {} Topology {} will not be scheduled.\",",
                "-                    strategyConf, td.getName(), e);",
                "-            topologySubmitter.markTopoUnsuccess(td);",
                "-            cluster.setStatus(td.getId(), \"Unsuccessful in scheduling - failed to create instance of topology strategy \"",
                "-                    + strategyConf + \". Please check logs for details\");",
                "+            markFailedTopology(topologySubmitter, cluster, td,",
                "+                \"Unsuccessful in scheduling - failed to create instance of topology strategy \"",
                "+                    + strategyConf",
                "+                    + \". Please check logs for details\", e);",
                "             return;",
                "@@ -121,51 +127,62 @@ public class ResourceAwareScheduler implements IScheduler {",
                "-        while (true) {",
                "-            // A copy of the cluster that restricts the strategy to only modify a single topology",
                "+        for (int i = 0; i < maxSchedulingAttempts; i++) {",
                "             SingleTopologyCluster toSchedule = new SingleTopologyCluster(workingState, td.getId());",
                "-            SchedulingResult result = null;",
                "             try {",
                "-                result = rasStrategy.schedule(toSchedule, td);",
                "-            } catch (Exception ex) {",
                "-                LOG.error(\"Exception thrown when running strategy {} to schedule topology {}.\"",
                "-                        + \" Topology will not be scheduled!\", rasStrategy.getClass().getName(), td.getName(), ex);",
                "-                topologySubmitter.markTopoUnsuccess(td);",
                "-                cluster.setStatus(td.getId(), \"Unsuccessful in scheduling - Exception thrown when running strategy {}\"",
                "-                        + rasStrategy.getClass().getName() + \". Please check logs for details\");",
                "-            }",
                "-            LOG.debug(\"scheduling result: {}\", result);",
                "-            if (result != null) {",
                "-                if (result.isSuccess()) {",
                "-                    try {",
                "+                SchedulingResult result = rasStrategy.schedule(toSchedule, td);",
                "+                LOG.debug(\"scheduling result: {}\", result);",
                "+                if (result != null) {",
                "+                    if (result.isSuccess()) {",
                "                         cluster.updateFrom(toSchedule);",
                "                         cluster.setStatus(td.getId(), \"Running - \" + result.getMessage());",
                "-                    } catch (Exception ex) {",
                "-                        LOG.error(\"Unsuccessful attempting to assign executors to nodes.\", ex);",
                "-                        topologySubmitter.markTopoUnsuccess(td);",
                "-                        cluster.setStatus(td.getId(), \"Unsuccessful in scheduling - \"",
                "-                                + \"IllegalStateException thrown when attempting to assign executors to nodes. Please check\"",
                "-                                + \" log for details.\");",
                "-                    }",
                "-                    return;",
                "-                } else {",
                "-                    if (result.getStatus() == SchedulingStatus.FAIL_NOT_ENOUGH_RESOURCES) {",
                "-                        boolean madeSpace = false;",
                "+                        //DONE",
                "+                        return;",
                "+                    } else if (result.getStatus() == SchedulingStatus.FAIL_NOT_ENOUGH_RESOURCES) {",
                "+                        LOG.info(\"Not enough resources to schedule {}\", td.getName());",
                "+                        List<TopologyDetails> reversedList = ImmutableList.copyOf(orderedTopologies).reverse();",
                "                         try {",
                "-                            //need to re prepare since scheduling state might have been restored",
                "-                            madeSpace = evictionStrategy.makeSpaceForTopo(td, workingState, userMap);",
                "+                            boolean evictedSomething = false;",
                "+                            LOG.debug(\"attempting to make space for topo {} from user {}\", td.getName(), td.getTopologySubmitter());",
                "+                            int tdIndex = reversedList.indexOf(td);",
                "+                            double cpuNeeded = td.getTotalRequestedCpu();",
                "+                            double memoryNeeded = td.getTotalRequestedMemOffHeap() + td.getTotalRequestedMemOnHeap();",
                "+                            SchedulerAssignment assignment = cluster.getAssignmentById(td.getId());",
                "+                            if (assignment != null) {",
                "+                                cpuNeeded -= getCpuUsed(assignment);",
                "+                                memoryNeeded -= getMemoryUsed(assignment);",
                "+                            }",
                "+                            cluster.getTopologyResourcesMap();",
                "+                            for (int index = 0; index < tdIndex; index++) {",
                "+                                TopologyDetails topologyEvict = reversedList.get(index);",
                "+                                SchedulerAssignment evictAssignemnt = workingState.getAssignmentById(topologyEvict.getId());",
                "+                                if (evictAssignemnt != null && !evictAssignemnt.getSlots().isEmpty()) {",
                "+                                    Collection<WorkerSlot> workersToEvict = workingState.getUsedSlotsByTopologyId(topologyEvict.getId());",
                "+",
                "+                                    LOG.debug(\"Evicting Topology {} with workers: {} from user {}\", topologyEvict.getName(), workersToEvict,",
                "+                                        topologyEvict.getTopologySubmitter());",
                "+                                    cpuNeeded -= getCpuUsed(evictAssignemnt);",
                "+                                    memoryNeeded -= getMemoryUsed(evictAssignemnt);",
                "+                                    evictedSomething = true;",
                "+                                    nodes.freeSlots(workersToEvict);",
                "+                                    if (cpuNeeded <= 0 && memoryNeeded <= 0) {",
                "+                                        //We evicted enough topologies to have a hope of scheduling, so try it now, and don't evict more",
                "+                                        // than is needed",
                "+                                        break;",
                "+                                    }",
                "+                                }",
                "+                            }",
                "+",
                "+                            if (!evictedSomething) {",
                "+                                markFailedTopology(topologySubmitter, cluster, td,",
                "+                                    \"Not enough resources to schedule - \" + result.getErrorMessage());",
                "+                                return;",
                "+                            }",
                "                         } catch (Exception ex) {",
                "-                            LOG.error(\"Exception thrown when running eviction strategy {} to schedule topology {}.\"",
                "-                                            + \" No evictions will be done!\", evictionStrategy.getClass().getName(),",
                "-                                    td.getName(), ex);",
                "-                            topologySubmitter.markTopoUnsuccess(td);",
                "-                            return;",
                "-                        }",
                "-                        if (!madeSpace) {",
                "-                            LOG.debug(\"Could not make space for topo {} will move to attempted\", td);",
                "-                            topologySubmitter.markTopoUnsuccess(td);",
                "-                            cluster.setStatus(td.getId(), \"Not enough resources to schedule - \"",
                "-                                    + result.getErrorMessage());",
                "-                            return;",
                "+                            LOG.error(\"Exception thrown when running eviction to schedule topology {}.\"",
                "+                                    + \" No evictions will be done! Error: {}\",",
                "+                                td.getName(), ex.getClass().getName(), ex);",
                "                         }",
                "+                        //Only place we fall though to do the loop over again...",
                "                         continue;",
                "                     } else {",
                "+                        //The assumption is that the strategy set the status...",
                "                         topologySubmitter.markTopoUnsuccess(td, cluster);",
                "@@ -173,7 +190,9 @@ public class ResourceAwareScheduler implements IScheduler {",
                "                     }",
                "+                } else {",
                "+                    markFailedTopology(topologySubmitter, cluster, td, \"Internal scheduler error\");",
                "+                    return;",
                "                 }",
                "-            } else {",
                "-                LOG.warn(\"Scheduling results returned from topology {} is not vaild! Topology with be ignored.\",",
                "-                        td.getName());",
                "-                topologySubmitter.markTopoUnsuccess(td, cluster);",
                "+            } catch (Exception ex) {",
                "+                markFailedTopology(topologySubmitter, cluster, td,",
                "+                    \"Internal Error - Exception thrown when scheduling. Please check logs for details\", ex);",
                "                 return;",
                "@@ -181,2 +200,12 @@ public class ResourceAwareScheduler implements IScheduler {",
                "         }",
                "+        markFailedTopology(topologySubmitter, cluster, td, \"Failed to schedule within \" + maxSchedulingAttempts + \" attempts\");",
                "+    }",
                "+",
                "+    private static double getCpuUsed(SchedulerAssignment assignment) {",
                "+        return assignment.getScheduledResources().values().stream().mapToDouble((wr) -> wr.get_cpu()).sum();",
                "+    }",
                "+",
                "+    private static double getMemoryUsed(SchedulerAssignment assignment) {",
                "+        return assignment.getScheduledResources().values().stream()",
                "+            .mapToDouble((wr) -> wr.get_mem_on_heap() + wr.get_mem_off_heap()).sum();",
                "     }",
                "@@ -208,3 +237,3 @@ public class ResourceAwareScheduler implements IScheduler {",
                "     private Map<String, Map<String, Double>> convertToDouble(Map<String, Map<String, Number>> raw) {",
                "-        Map<String, Map<String, Double>> ret = new HashMap<String, Map<String, Double>>();",
                "+        Map<String, Map<String, Double>> ret = new HashMap<>();",
                "@@ -213,3 +242,3 @@ public class ResourceAwareScheduler implements IScheduler {",
                "                 String user = userPoolEntry.getKey();",
                "-                ret.put(user, new HashMap<String, Double>());",
                "+                ret.put(user, new HashMap<>());",
                "                 for (Map.Entry<String, Number> resourceEntry : userPoolEntry.getValue().entrySet()) {",
                "@@ -228,3 +257,3 @@ public class ResourceAwareScheduler implements IScheduler {",
                "      * @return a map that contains resource guarantees of every user of the following format",
                "-     * {userid->{resourceType->amountGuaranteed}}",
                "+     *     {userid->{resourceType->amountGuaranteed}}",
                "      */",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "index ab8eb14de..44403eff3 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "@@ -45,4 +45,3 @@ public class ResourceUtils {",
                "                 Map<String, Double> topologyResources = parseResources(bolt.getValue().get_common().get_json_conf());",
                "-                checkIntialization(topologyResources, bolt.getValue().toString(), topologyConf);",
                "-                LOG.warn(\"Turned {} into {}\", bolt.getValue().get_common().get_json_conf(), topologyResources);",
                "+                checkInitialization(topologyResources, bolt.getKey(), topologyConf);",
                "                 boltResources.put(bolt.getKey(), topologyResources);",
                "@@ -59,3 +58,3 @@ public class ResourceUtils {",
                "                 Map<String, Double> topologyResources = parseResources(spout.getValue().get_common().get_json_conf());",
                "-                checkIntialization(topologyResources, spout.getValue().toString(), topologyConf);",
                "+                checkInitialization(topologyResources, spout.getKey(), topologyConf);",
                "                 spoutResources.put(spout.getKey(), topologyResources);",
                "@@ -131,4 +130,4 @@ public class ResourceUtils {",
                "-    public static void checkIntialization(Map<String, Double> topologyResources, String com,",
                "-                                          Map<String, Object> topologyConf) {",
                "+    public static void checkInitialization(Map<String, Double> topologyResources, String com,",
                "+                                           Map<String, Object> topologyConf) {",
                "         checkInitMem(topologyResources, com, topologyConf);",
                "@@ -138,3 +137,3 @@ public class ResourceUtils {",
                "     private static void checkInitMem(Map<String, Double> topologyResources, String com,",
                "-                                    Map<String, Object> topologyConf) {",
                "+                                     Map<String, Object> topologyConf) {",
                "         if (!topologyResources.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB)) {",
                "@@ -144,3 +143,6 @@ public class ResourceUtils {",
                "                 topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, onHeap);",
                "-                debugMessage(\"ONHEAP\", com, topologyConf);",
                "+                LOG.debug(",
                "+                    \"Unable to extract resource requirement for Component {}\\n\"",
                "+                        + \" Resource : Memory Type : On Heap set to default {}\",",
                "+                    com, onHeap);",
                "             }",
                "@@ -152,3 +154,6 @@ public class ResourceUtils {",
                "                 topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, offHeap);",
                "-                debugMessage(\"OFFHEAP\", com, topologyConf);",
                "+                LOG.debug(",
                "+                    \"Unable to extract resource requirement for Component {}\\n\"",
                "+                        + \" Resource : Memory Type : Off Heap set to default {}\",",
                "+                    com, offHeap);",
                "             }",
                "@@ -163,3 +168,6 @@ public class ResourceUtils {",
                "                 topologyResources.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, cpu);",
                "-                debugMessage(\"CPU\", com, topologyConf);",
                "+                LOG.debug(",
                "+                    \"Unable to extract resource requirement for Component {}\\n\"",
                "+                        + \" Resource : CPU Pcore Percent set to default {}\",",
                "+                    com, cpu);",
                "             }",
                "@@ -200,21 +208,2 @@ public class ResourceUtils {",
                "-    private static void debugMessage(String memoryType, String com, Map<String, Object> topologyConf) {",
                "-        if (memoryType.equals(\"ONHEAP\")) {",
                "-            LOG.debug(",
                "-                    \"Unable to extract resource requirement for Component {}\\n\"",
                "-                        + \" Resource : Memory Type : On Heap set to default {}\",",
                "-                    com, topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB));",
                "-        } else if (memoryType.equals(\"OFFHEAP\")) {",
                "-            LOG.debug(",
                "-                    \"Unable to extract resource requirement for Component {}\\n\"",
                "-                        + \" Resource : Memory Type : Off Heap set to default {}\",",
                "-                    com, topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB));",
                "-        } else {",
                "-            LOG.debug(",
                "-                    \"Unable to extract resource requirement for Component {}\\n\"",
                "-                        + \" Resource : CPU Pcore Percent set to default {}\",",
                "-                    com, topologyConf.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT));",
                "-        }",
                "-    }",
                "-",
                "     /**",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/User.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/User.java",
                "index 4850ebb6b..908351011 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/User.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/User.java",
                "@@ -36,3 +36,3 @@ public class User {",
                "-    //Topologies that was deemed to be invalid",
                "+    //Topologies that were deemed to be invalid",
                "     private final Set<TopologyDetails> unsuccess = new HashSet<>();",
                "@@ -65,3 +65,3 @@ public class User {",
                "         TreeSet<TopologyDetails> ret =",
                "-            new TreeSet<TopologyDetails>(new PQsortByPriorityAndSubmittionTime());",
                "+            new TreeSet<>(new PQsortByPriorityAndSubmittionTime());",
                "         for (TopologyDetails td : cluster.getTopologies().getTopologiesOwnedBy(userId)) {",
                "@@ -76,3 +76,3 @@ public class User {",
                "         TreeSet<TopologyDetails> ret =",
                "-            new TreeSet<TopologyDetails>(new PQsortByPriorityAndSubmittionTime());",
                "+            new TreeSet<>(new PQsortByPriorityAndSubmittionTime());",
                "         for (TopologyDetails td : cluster.getTopologies().getTopologiesOwnedBy(userId)) {",
                "@@ -120,2 +120,20 @@ public class User {",
                "+    public double getMemoryResourceRequest(ISchedulingState cluster) {",
                "+        double sum = 0.0;",
                "+        Set<TopologyDetails> topologyDetailsSet = new HashSet<>(cluster.getTopologies().getTopologiesOwnedBy(userId));",
                "+        for (TopologyDetails topo : topologyDetailsSet) {",
                "+            sum += topo.getTotalRequestedMemOnHeap() + topo.getTotalRequestedMemOffHeap();",
                "+        }",
                "+        return sum;",
                "+    }",
                "+",
                "+    public double getCpuResourceRequest(ISchedulingState cluster) {",
                "+        double sum = 0.0;",
                "+        Set<TopologyDetails> topologyDetailsSet = new HashSet<>(cluster.getTopologies().getTopologiesOwnedBy(userId));",
                "+        for (TopologyDetails topo : topologyDetailsSet) {",
                "+            sum += topo.getTotalRequestedCpu();",
                "+        }",
                "+        return sum;",
                "+    }",
                "+",
                "     public double getCpuResourceUsedByUser(ISchedulingState cluster) {",
                "@@ -189,18 +207,2 @@ public class User {",
                "-    public String getDetailedInfo() {",
                "-        String ret = \"\\nUser: \" + userId;",
                "-        ret += \"\\n - \" + \" Resource Pool: \" + cpuGuarantee + \"% \" + memoryGuarantee + \"MB \";",
                "-        ret += \"\\n - \" + \" Unsuccess Queue: \" + unsuccess + \" size: \" + unsuccess.size();",
                "-        return ret;",
                "-    }",
                "-",
                "-    public static String getResourcePoolAverageUtilizationForUsers(",
                "-        Collection<User> users, Cluster cluster) {",
                "-        String ret = \"\";",
                "-        for (User user : users) {",
                "-            ret += user.getId() + \" - \" + user.getResourcePoolAverageUtilization(cluster) + \" \";",
                "-        }",
                "-        return ret;",
                "-    }",
                "-",
                "     /**",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/eviction/DefaultEvictionStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/eviction/DefaultEvictionStrategy.java",
                "deleted file mode 100644",
                "index 3abc65e0a..000000000",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/eviction/DefaultEvictionStrategy.java",
                "+++ /dev/null",
                "@@ -1,110 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- * <p>",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- * <p>",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-",
                "-package org.apache.storm.scheduler.resource.strategies.eviction;",
                "-",
                "-import java.util.Collection;",
                "-import java.util.Map;",
                "-",
                "-import org.apache.storm.scheduler.Cluster;",
                "-import org.apache.storm.scheduler.TopologyDetails;",
                "-import org.apache.storm.scheduler.WorkerSlot;",
                "-import org.apache.storm.scheduler.resource.RAS_Nodes;",
                "-import org.apache.storm.scheduler.resource.User;",
                "-import org.slf4j.Logger;",
                "-import org.slf4j.LoggerFactory;",
                "-",
                "-public class DefaultEvictionStrategy implements IEvictionStrategy {",
                "-    private static final Logger LOG = LoggerFactory",
                "-            .getLogger(DefaultEvictionStrategy.class);",
                "-",
                "-    private Cluster cluster;",
                "-    private Map<String, User> userMap;",
                "-    private RAS_Nodes nodes;",
                "-",
                "-    @Override",
                "-    public boolean makeSpaceForTopo(TopologyDetails td, Cluster schedulingState, Map<String, User> userMap) {",
                "-        this.cluster = schedulingState;",
                "-        this.userMap = userMap;",
                "-        this.nodes = new RAS_Nodes(schedulingState);",
                "-        LOG.debug(\"attempting to make space for topo {} from user {}\", td.getName(), td.getTopologySubmitter());",
                "-        User submitter = this.userMap.get(td.getTopologySubmitter());",
                "-        if (submitter.getCpuResourceGuaranteed() == 0.0 || submitter.getMemoryResourceGuaranteed() == 0.0) {",
                "-            return false;",
                "-        }",
                "-",
                "-        double cpuNeeded = td.getTotalRequestedCpu() / submitter.getCpuResourceGuaranteed();",
                "-        double memoryNeeded = (td.getTotalRequestedMemOffHeap() + td.getTotalRequestedMemOnHeap())",
                "-            / submitter.getMemoryResourceGuaranteed();",
                "-",
                "-        User evictUser = this.findUserWithHighestAverageResourceUtilAboveGuarantee();",
                "-        //check if user has enough resource under his or her resource guarantee to schedule topology",
                "-        if ((1.0 - submitter.getCpuResourcePoolUtilization(schedulingState)) >= cpuNeeded",
                "-            && (1.0 - submitter.getMemoryResourcePoolUtilization(schedulingState)) >= memoryNeeded) {",
                "-            if (evictUser != null) {",
                "-                TopologyDetails topologyEvict = evictUser.getRunningTopologyWithLowestPriority(schedulingState);",
                "-                evictTopology(topologyEvict);",
                "-                return true;",
                "-            }",
                "-        } else {",
                "-            if (evictUser != null) {",
                "-                if ((evictUser.getResourcePoolAverageUtilization(schedulingState) - 1.0)",
                "-                    > (((cpuNeeded + memoryNeeded) / 2)",
                "-                        + (submitter.getResourcePoolAverageUtilization(schedulingState) - 1.0))) {",
                "-                    TopologyDetails topologyEvict = evictUser.getRunningTopologyWithLowestPriority(schedulingState);",
                "-                    evictTopology(topologyEvict);",
                "-                    return true;",
                "-                }",
                "-            }",
                "-        }",
                "-        //See if there is a lower priority topology that can be evicted from the current user",
                "-        //topologies should already be sorted in order of increasing priority.",
                "-        //Thus, topology at the front of the queue has the lowest priority",
                "-        for (TopologyDetails topo : submitter.getRunningTopologies(schedulingState)) {",
                "-            //check to if there is a topology with a lower priority we can evict",
                "-            if (topo.getTopologyPriority() > td.getTopologyPriority()) {",
                "-                LOG.debug(\"POTENTIALLY Evicting Topology {} from user {} (itself) since topology {} has a lower \"",
                "-                        + \"priority than topology {}\", topo, submitter, topo, td);",
                "-                evictTopology(topo);",
                "-                return true;",
                "-            }",
                "-        }",
                "-        return false;",
                "-    }",
                "-",
                "-    private void evictTopology(TopologyDetails topologyEvict) {",
                "-        Collection<WorkerSlot> workersToEvict = this.cluster.getUsedSlotsByTopologyId(topologyEvict.getId());",
                "-",
                "-        LOG.info(\"Evicting Topology {} with workers: {} from user {}\", topologyEvict.getName(), workersToEvict,",
                "-            topologyEvict.getTopologySubmitter());",
                "-        this.nodes.freeSlots(workersToEvict);",
                "-    }",
                "-",
                "-    private User findUserWithHighestAverageResourceUtilAboveGuarantee() {",
                "-        double most = 0.0;",
                "-        User mostOverUser = null;",
                "-        for (User user : this.userMap.values()) {",
                "-            double over = user.getResourcePoolAverageUtilization(cluster) - 1.0;",
                "-            if ((over > most) && (!user.getRunningTopologies(cluster).isEmpty())) {",
                "-                most = over;",
                "-                mostOverUser = user;",
                "-            }",
                "-        }",
                "-        return mostOverUser;",
                "-    }",
                "-}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/eviction/IEvictionStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/eviction/IEvictionStrategy.java",
                "deleted file mode 100644",
                "index 2947f7ab4..000000000",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/eviction/IEvictionStrategy.java",
                "+++ /dev/null",
                "@@ -1,37 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- * <p>",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- * <p>",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-",
                "-package org.apache.storm.scheduler.resource.strategies.eviction;",
                "-",
                "-import java.util.Map;",
                "-",
                "-import org.apache.storm.scheduler.Cluster;",
                "-import org.apache.storm.scheduler.TopologyDetails;",
                "-import org.apache.storm.scheduler.resource.User;",
                "-",
                "-public interface IEvictionStrategy {",
                "-    /**",
                "-     * This method when invoked should attempt to make space on the cluster so that the topology specified can be scheduled",
                "-     * @param td the topology to make space for",
                "-     * @return return true to indicate that space has been made for topology and try schedule topology td again.",
                "-     * Return false to indicate that no space could be made for the topology on the cluster and the scheduler should give up",
                "-     * trying to schedule the topology for this round of scheduling.  This method will be invoked until the topology indicated",
                "-     * could be scheduled or the method returns false",
                "-     */",
                "-    public boolean makeSpaceForTopo(TopologyDetails td, Cluster schedulingState, Map<String, User> userMap);",
                "-}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/DefaultSchedulingPriorityStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/DefaultSchedulingPriorityStrategy.java",
                "index b6d8adad3..8b401aa9f 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/DefaultSchedulingPriorityStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/DefaultSchedulingPriorityStrategy.java",
                "@@ -20,4 +20,8 @@ package org.apache.storm.scheduler.resource.strategies.priority;",
                "+import java.util.ArrayList;",
                "+import java.util.Collections;",
                "+import java.util.Comparator;",
                "+import java.util.LinkedList;",
                "+import java.util.List;",
                " import java.util.Map;",
                "-",
                " import org.apache.storm.scheduler.ISchedulingState;",
                "@@ -29,38 +33,132 @@ import org.slf4j.LoggerFactory;",
                " public class DefaultSchedulingPriorityStrategy implements ISchedulingPriorityStrategy {",
                "-    private static final Logger LOG = LoggerFactory",
                "-            .getLogger(DefaultSchedulingPriorityStrategy.class);",
                "+    private static final Logger LOG = LoggerFactory.getLogger(DefaultSchedulingPriorityStrategy.class);",
                "+",
                "+    protected static class SimulatedUser {",
                "+        protected final LinkedList<TopologyDetails> tds = new LinkedList<>();",
                "+        private double assignedCpu = 0.0;",
                "+        private double assignedMemory = 0.0;",
                "+        public final double guaranteedCpu;",
                "+        public final double guaranteedMemory;",
                "+",
                "+        public SimulatedUser(User other, ISchedulingState cluster) {",
                "+            tds.addAll(cluster.getTopologies().getTopologiesOwnedBy(other.getId()));",
                "+            Collections.sort(tds, new TopologyByPriorityAndSubmissionTimeComparator());",
                "+            Double guaranteedCpu = other.getCpuResourceGuaranteed();",
                "+            if (guaranteedCpu == null) {",
                "+                guaranteedCpu = 0.0;",
                "+            }",
                "+            this.guaranteedCpu = guaranteedCpu;",
                "+            Double guaranteedMemory = other.getMemoryResourceGuaranteed();",
                "+            if (guaranteedMemory == null) {",
                "+                guaranteedMemory = 0.0;",
                "+            }",
                "+            this.guaranteedMemory = guaranteedMemory;",
                "+        }",
                "+",
                "+        public TopologyDetails getNextHighest() {",
                "+            return tds.peekFirst();",
                "+        }",
                "+",
                "+        public TopologyDetails simScheduleNextHighest() {",
                "+            TopologyDetails td = tds.pop();",
                "+            assignedCpu += td.getTotalRequestedCpu();",
                "+            assignedMemory += td.getTotalRequestedMemOffHeap() + td.getTotalRequestedMemOnHeap();",
                "+            return td;",
                "+        }",
                "+",
                "+        /**",
                "+         * Get a score for the simulated user.  This is used to sort the users, by their highest priority topology.",
                "+         * The only requirement is that if the user is over their guarantees, or there are no available resources the",
                "+         * returned score will be > 0.  If they are under their guarantee it must be negative.",
                "+         * @param availableCpu available CPU on the cluster.",
                "+         * @param availableMemory available memory on the cluster.",
                "+         * @param td the topology we are looking at.",
                "+         * @return the score.",
                "+         */",
                "+        protected double getScore(double availableCpu, double availableMemory, TopologyDetails td) {",
                "+            //(Requested + Assigned - Guaranteed)/Available",
                "+            if (td == null || availableCpu <= 0 || availableMemory <= 0) {",
                "+                return Double.MAX_VALUE;",
                "+            }",
                "+            double wouldBeCpu = assignedCpu + td.getTotalRequestedCpu();",
                "+            double wouldBeMem = assignedMemory + td.getTotalRequestedMemOffHeap() + td.getTotalRequestedMemOnHeap();",
                "+            double cpuScore = (wouldBeCpu - guaranteedCpu)/availableCpu;",
                "+            double memScore = (wouldBeMem - guaranteedMemory)/availableMemory;",
                "+            return Math.max(cpuScore, memScore);",
                "+        }",
                "+",
                "+        public double getScore(double availableCpu, double availableMemory) {",
                "+            TopologyDetails td = getNextHighest();",
                "+            return getScore(availableCpu, availableMemory, td);",
                "+        }",
                "+",
                "+    }",
                "+",
                "+    protected SimulatedUser getSimulatedUserFor(User u, ISchedulingState cluster) {",
                "+        return new SimulatedUser(u, cluster);",
                "+    }",
                "     @Override",
                "-    public TopologyDetails getNextTopologyToSchedule(ISchedulingState schedulingState, Map<String, User> userMap) {",
                "-        User nextUser = getNextUser(schedulingState, userMap);",
                "-        if (nextUser == null) {",
                "-            return null;",
                "+    public List<TopologyDetails> getOrderedTopologies(ISchedulingState cluster, Map<String, User> userMap) {",
                "+        double cpuAvail = cluster.getClusterTotalCpuResource();",
                "+        double memAvail = cluster.getClusterTotalMemoryResource();",
                "+",
                "+        List<TopologyDetails> allUserTopologies = new ArrayList<>();",
                "+        List<SimulatedUser> users = new ArrayList<>();",
                "+        for (User u : userMap.values()) {",
                "+            users.add(getSimulatedUserFor(u, cluster));",
                "+        }",
                "+        while (!users.isEmpty()) {",
                "+            Collections.sort(users, new SimulatedUserComparator(cpuAvail, memAvail));",
                "+            SimulatedUser u = users.get(0);",
                "+            TopologyDetails td = u.getNextHighest();",
                "+            if (td == null) {",
                "+                users.remove(0);",
                "+            } else {",
                "+                double score = u.getScore(cpuAvail, memAvail);",
                "+                td = u.simScheduleNextHighest();",
                "+                LOG.info(\"SIM Scheduling {} with score of {}\", td.getId(), score);",
                "+                cpuAvail -= td.getTotalRequestedCpu();",
                "+                memAvail -= (td.getTotalRequestedMemOffHeap() + td.getTotalRequestedMemOnHeap());",
                "+                allUserTopologies.add(td);",
                "+            }",
                "         }",
                "-        return nextUser.getNextTopologyToSchedule(schedulingState);",
                "+        return allUserTopologies;",
                "     }",
                "-    public User getNextUser(ISchedulingState cluster, Map<String, User> userMap) {",
                "-        double least = Double.POSITIVE_INFINITY;",
                "-        User ret = null;",
                "-        final double totalCpu = cluster.getClusterTotalCpuResource();",
                "-        final double totalMem = cluster.getClusterTotalMemoryResource();",
                "-        for (User user : userMap.values()) {",
                "-            if (user.hasTopologyNeedSchedule(cluster)) {",
                "-                double userResourcePoolAverageUtilization = user.getResourcePoolAverageUtilization(cluster);",
                "-                if (least > userResourcePoolAverageUtilization) {",
                "-                    ret = user;",
                "-                    least = userResourcePoolAverageUtilization;",
                "-                } else if (Math.abs(least - userResourcePoolAverageUtilization) < 0.0001) {",
                "-                    // if ResourcePoolAverageUtilization is equal to the user that is being compared",
                "-                    double currentCpuPercentage = ret.getCpuResourceGuaranteed() / totalCpu;",
                "-                    double currentMemoryPercentage = ret.getMemoryResourceGuaranteed() / totalMem;",
                "-                    double currentAvgPercentage = (currentCpuPercentage + currentMemoryPercentage) / 2.0;",
                "-",
                "-                    double userCpuPercentage = user.getCpuResourceGuaranteed() / totalCpu;",
                "-                    double userMemoryPercentage = user.getMemoryResourceGuaranteed() / totalMem;",
                "-                    double userAvgPercentage = (userCpuPercentage + userMemoryPercentage) / 2.0;",
                "-                    if (userAvgPercentage > currentAvgPercentage) {",
                "-                        ret = user;",
                "-                        least = userResourcePoolAverageUtilization;",
                "-                    }",
                "+    private static class SimulatedUserComparator implements Comparator<SimulatedUser> {",
                "+",
                "+        private final double cpuAvail;",
                "+        private final double memAvail;",
                "+",
                "+        private SimulatedUserComparator(double cpuAvail, double memAvail) {",
                "+            this.cpuAvail = cpuAvail;",
                "+            this.memAvail = memAvail;",
                "+        }",
                "+",
                "+        @Override",
                "+        public int compare(SimulatedUser o1, SimulatedUser o2) {",
                "+            return Double.compare(o1.getScore(cpuAvail, memAvail), o2.getScore(cpuAvail, memAvail));",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Comparator that sorts topologies by priority and then by submission time",
                "+     * First sort by Topology Priority, if there is a tie for topology priority, topology uptime is used to sort",
                "+     */",
                "+    private static class TopologyByPriorityAndSubmissionTimeComparator implements Comparator<TopologyDetails> {",
                "+",
                "+        @Override",
                "+        public int compare(TopologyDetails topo1, TopologyDetails topo2) {",
                "+            if (topo1.getTopologyPriority() > topo2.getTopologyPriority()) {",
                "+                return 1;",
                "+            } else if (topo1.getTopologyPriority() < topo2.getTopologyPriority()) {",
                "+                return -1;",
                "+            } else {",
                "+                if (topo1.getUpTime() > topo2.getUpTime()) {",
                "+                    return -1;",
                "+                } else if (topo1.getUpTime() < topo2.getUpTime()) {",
                "+                    return 1;",
                "+                } else {",
                "+                    return topo1.getId().compareTo(topo2.getId());",
                "                 }",
                "@@ -68,4 +166,3 @@ public class DefaultSchedulingPriorityStrategy implements ISchedulingPriorityStr",
                "         }",
                "-        return ret;",
                "     }",
                "-}",
                "+}",
                "\\ No newline at end of file",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/FIFOSchedulingPriorityStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/FIFOSchedulingPriorityStrategy.java",
                "new file mode 100644",
                "index 000000000..1c76b8d6d",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/FIFOSchedulingPriorityStrategy.java",
                "@@ -0,0 +1,79 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * <p>",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ * <p>",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource.strategies.priority;",
                "+",
                "+import java.util.Collections;",
                "+import java.util.Comparator;",
                "+import org.apache.storm.scheduler.ISchedulingState;",
                "+import org.apache.storm.scheduler.TopologyDetails;",
                "+import org.apache.storm.scheduler.resource.User;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+public class FIFOSchedulingPriorityStrategy extends DefaultSchedulingPriorityStrategy {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(FIFOSchedulingPriorityStrategy.class);",
                "+",
                "+    protected static class FIFOSimulatedUser extends SimulatedUser {",
                "+",
                "+        public FIFOSimulatedUser(User other, ISchedulingState cluster) {",
                "+            super(other, cluster);",
                "+        }",
                "+",
                "+        @Override",
                "+        public double getScore(double availableCpu, double availableMemory) {",
                "+            TopologyDetails td = getNextHighest();",
                "+            double origScore = getScore(availableCpu, availableMemory, td);",
                "+            if (origScore < 0) {",
                "+                return origScore;",
                "+            }",
                "+            //Not enough guaranteed use the age of the topology instead.",
                "+            //TODO need a good way to only do this once...",
                "+            Collections.sort(tds, new TopologyBySubmissionTimeComparator());",
                "+            td = getNextHighest();",
                "+            if (td != null) {",
                "+                LOG.info(\"SCORE FOR {} is {}\", td.getId(), td.getUpTime());",
                "+                return td.getUpTime();",
                "+            }",
                "+            return Double.MAX_VALUE;",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    protected SimulatedUser getSimulatedUserFor(User u, ISchedulingState cluster) {",
                "+        return new FIFOSimulatedUser(u, cluster);",
                "+    }",
                "+",
                "+    /**",
                "+     * Comparator that sorts topologies by priority and then by submission time",
                "+     * First sort by Topology Priority, if there is a tie for topology priority, topology uptime is used to sort",
                "+     */",
                "+    private static class TopologyBySubmissionTimeComparator implements Comparator<TopologyDetails> {",
                "+",
                "+        @Override",
                "+        public int compare(TopologyDetails topo1, TopologyDetails topo2) {",
                "+            if (topo1.getUpTime() > topo2.getUpTime()) {",
                "+                return 1;",
                "+            } else if (topo1.getUpTime() < topo2.getUpTime()) {",
                "+                return -1;",
                "+            } else {",
                "+                return topo1.getId().compareTo(topo2.getId());",
                "+            }",
                "+        }",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/ISchedulingPriorityStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/ISchedulingPriorityStrategy.java",
                "index 8654bb379..77ce0d903 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/ISchedulingPriorityStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/ISchedulingPriorityStrategy.java",
                "@@ -20,2 +20,3 @@ package org.apache.storm.scheduler.resource.strategies.priority;",
                "+import java.util.List;",
                " import java.util.Map;",
                "@@ -27,7 +28,8 @@ import org.apache.storm.scheduler.resource.User;",
                " public interface ISchedulingPriorityStrategy {",
                "+",
                "     /**",
                "-     * Gets the next topology to schedule",
                "-     * @return return the next topology to schedule.  If there is no topologies left to schedule, return null",
                "+     * Prioritize the list of all topologies in the cluster.",
                "+     * @return ordered list of topologies to schedule.",
                "      */",
                "-    public TopologyDetails getNextTopologyToSchedule(ISchedulingState schedulingState, Map<String, User> userMap);",
                "+    List<TopologyDetails> getOrderedTopologies(ISchedulingState schedulingState, Map<String, User> userMap);",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "index 9f644c92e..cb324a300 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "@@ -198,3 +198,3 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "         } else {",
                "-            LOG.error(\"Not Enough Resources to schedule Task {}\", exec);",
                "+            LOG.error(\"Not Enough Resources to schedule Task {} - {}\", td.getName(), exec);",
                "         }",
                "@@ -203,3 +203,3 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "     /**",
                "-     * Find a worker to schedule executor exec on",
                "+     * Find a worker to schedule executor exec on.",
                "      *",
                "@@ -475,3 +475,3 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "         TreeSet<ObjectResources> sortedObjectResources =",
                "-            new TreeSet<>(( o1, o2) -> {",
                "+            new TreeSet<>((o1, o2) -> {",
                "                 int execsScheduled1 = existingScheduleFunc.getNumExistingSchedule(o1.id);",
                "@@ -613,3 +613,4 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "      * For each component sort the neighbors of that component by how many connections it will have to make with that component.",
                "-     * Add an executor from this component and then from each neighboring component in sorted order.  Do this until there is nothing left to schedule",
                "+     * Add an executor from this component and then from each neighboring component in sorted order.  Do this until there is nothing",
                "+     * left to schedule",
                "      *"
            ],
            "changed_files": [
                "conf/defaults.yaml",
                "storm-client/pom.xml",
                "storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/SupervisorResources.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/User.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/eviction/DefaultEvictionStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/eviction/IEvictionStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/DefaultSchedulingPriorityStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/FIFOSchedulingPriorityStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/priority/ISchedulingPriorityStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2792": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster, user",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2792",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1d42d8fb59f5870a671a8cb5c02e1cf46f08a22f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515783934,
            "hunks": 2,
            "message": "STORM-2153: add taskId to user-defined metrics names",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "index e0023fd7e..cfeb711c1 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "@@ -164,2 +164,4 @@ public class StormMetricRegistry {",
                "     public static String metricName(String name, TopologyContext context){",
                "+",
                "+",
                "         StringBuilder sb = new StringBuilder(\"storm.topology.\");",
                "@@ -171,2 +173,4 @@ public class StormMetricRegistry {",
                "         sb.append(\".\");",
                "+        sb.append(context.getThisTaskId());",
                "+        sb.append(\".\");",
                "         sb.append(context.getThisWorkerPort());"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: user",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7947a0755c6717de16a7755ec7f1f2dc83388d11",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515729382,
            "hunks": 6,
            "message": "STORM-2153: fix a missing spot, extract string to constant, modify accessors",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java b/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java",
                "index 05c62daf5..239c1a034 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java",
                "@@ -28,6 +28,11 @@ import java.util.concurrent.ConcurrentMap;",
                " public class TaskMetrics {",
                "-    ConcurrentMap<String, Counter> ackedByStream = new ConcurrentHashMap<>();",
                "-    ConcurrentMap<String, Counter> failedByStream = new ConcurrentHashMap<>();",
                "-    ConcurrentMap<String, Counter> emittedByStream = new ConcurrentHashMap<>();",
                "-    ConcurrentMap<String, Counter> transferredByStream = new ConcurrentHashMap<>();",
                "+    private static final String METRIC_NAME_ACKED = \"acked\";",
                "+    private static final String METRIC_NAME_FAILED = \"failed\";",
                "+    private static final String METRIC_NAME_EMITTED = \"emitted\";",
                "+    private static final String METRIC_NAME_TRANSFERRED = \"transferred\";",
                "+",
                "+    private ConcurrentMap<String, Counter> ackedByStream = new ConcurrentHashMap<>();",
                "+    private ConcurrentMap<String, Counter> failedByStream = new ConcurrentHashMap<>();",
                "+    private ConcurrentMap<String, Counter> emittedByStream = new ConcurrentHashMap<>();",
                "+    private ConcurrentMap<String, Counter> transferredByStream = new ConcurrentHashMap<>();",
                "@@ -48,3 +53,3 @@ public class TaskMetrics {",
                "         if (c == null) {",
                "-            c = StormMetricRegistry.counter(\"acked\", this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "+            c = StormMetricRegistry.counter(METRIC_NAME_ACKED, this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "             this.ackedByStream.put(streamId, c);",
                "@@ -55,5 +60,5 @@ public class TaskMetrics {",
                "     public Counter getFailed(String streamId) {",
                "-        Counter c = this.ackedByStream.get(streamId);",
                "+        Counter c = this.failedByStream.get(streamId);",
                "         if (c == null) {",
                "-            c = StormMetricRegistry.counter(\"failed\", this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "+            c = StormMetricRegistry.counter(METRIC_NAME_FAILED, this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "             this.failedByStream.put(streamId, c);",
                "@@ -66,3 +71,3 @@ public class TaskMetrics {",
                "         if (c == null) {",
                "-            c = StormMetricRegistry.counter(\"emitted\", this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "+            c = StormMetricRegistry.counter(METRIC_NAME_EMITTED, this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "             this.emittedByStream.put(streamId, c);",
                "@@ -75,3 +80,3 @@ public class TaskMetrics {",
                "         if (c == null) {",
                "-            c = StormMetricRegistry.counter(\"transferred\", this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "+            c = StormMetricRegistry.counter(METRIC_NAME_TRANSFERRED, this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "             this.transferredByStream.put(streamId, c);"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: access",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8a9141c24d2732865061765a43cd8e1871709095",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1504810205,
            "hunks": 264,
            "message": "STORM-2725: Generic Resource Aware Scheduling STORM-2725: Generic Resource Scheduling - initial config changes and TopologyBuilder API STORM-2725: Generic Resource Aware Scheduling(GRAS) - metadata, scheduling strategies and configuration enhancements STORM-2725: Generic Resource Aware Scheduling(GRAS) - metadata, scheduling strategies and configuration enhancements STORM-2725: Checkstyle stuff STORM-2725: GRAS tests STORM-2725: Feedback STORM-2725: Merging with revans2 scheduler cleanups STORM-2725: Leftover feedback",
            "diff": [
                "diff --git a/examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/CaptureLoad.java b/examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/CaptureLoad.java",
                "index 11d64a1b6..5c1ef4e85 100644",
                "--- a/examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/CaptureLoad.java",
                "+++ b/examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/CaptureLoad.java",
                "@@ -27,3 +27,2 @@ import java.util.function.Function;",
                " import java.util.stream.Collectors;",
                "-import java.util.stream.Stream;",
                " import org.apache.commons.cli.CommandLine;",
                "@@ -110,3 +109,3 @@ public class CaptureLoad {",
                "         if (savedTopoConf.containsKey(Config.TOPOLOGY_WORKERS)) {",
                "-            numWorkers = Math.max(numWorkers, ((Number)savedTopoConf.get(Config.TOPOLOGY_WORKERS)).intValue());",
                "+            numWorkers = Math.max(numWorkers, ((Number) savedTopoConf.get(Config.TOPOLOGY_WORKERS)).intValue());",
                "         }",
                "@@ -374,4 +373,3 @@ public class CaptureLoad {",
                "     // So we have copied and pasted some of the needed methods here. (with a few changes to logging)",
                "-    static Map<String, Map<String, Double>> getBoltsResources(StormTopology topology,",
                "-                                                                     Map<String, Object> topologyConf) {",
                "+    static Map<String, Map<String, Double>> getBoltsResources(StormTopology topology, Map<String, Object> topologyConf) {",
                "         Map<String, Map<String, Double>> boltResources = new HashMap<>();",
                "@@ -380,3 +378,3 @@ public class CaptureLoad {",
                "                 Map<String, Double> topologyResources = parseResources(bolt.getValue().get_common().get_json_conf());",
                "-                checkIntialization(topologyResources, bolt.getValue().toString(), topologyConf);",
                "+                checkInitialization(topologyResources, bolt.getValue().toString(), topologyConf);",
                "                 boltResources.put(bolt.getKey(), topologyResources);",
                "@@ -387,4 +385,3 @@ public class CaptureLoad {",
                "-    static Map<String, Map<String, Double>> getSpoutsResources(StormTopology topology,",
                "-                                                                      Map<String, Object> topologyConf) {",
                "+    static Map<String, Map<String, Double>> getSpoutsResources(StormTopology topology, Map<String, Object> topologyConf) {",
                "         Map<String, Map<String, Double>> spoutResources = new HashMap<>();",
                "@@ -393,3 +390,3 @@ public class CaptureLoad {",
                "                 Map<String, Double> topologyResources = parseResources(spout.getValue().get_common().get_json_conf());",
                "-                checkIntialization(topologyResources, spout.getValue().toString(), topologyConf);",
                "+                checkInitialization(topologyResources, spout.getValue().toString(), topologyConf);",
                "                 spoutResources.put(spout.getKey(), topologyResources);",
                "@@ -432,23 +429,22 @@ public class CaptureLoad {",
                "-    static void checkIntialization(Map<String, Double> topologyResources, String com,",
                "-                                          Map<String, Object> topologyConf) {",
                "-        checkInitMem(topologyResources, com, topologyConf);",
                "-        checkInitCpu(topologyResources, com, topologyConf);",
                "-    }",
                "+    /**",
                "+     * Checks if the topology's resource requirements are initialized.",
                "+     * Will modify topologyResources by adding the appropriate defaults",
                "+     * @param topologyResources map of resouces requirements",
                "+     * @param componentId component for which initialization is being conducted",
                "+     * @param topologyConf topology configuration",
                "+     * @throws Exception on any error",
                "+     */",
                "+    public static void checkInitialization(Map<String, Double> topologyResources, String componentId, Map topologyConf) {",
                "+        StringBuilder msgBuilder = new StringBuilder();",
                "-    static void checkInitMem(Map<String, Double> topologyResources, String com,",
                "-                                     Map<String, Object> topologyConf) {",
                "-        if (!topologyResources.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB)) {",
                "-            Double onHeap = ObjectReader.getDouble(",
                "-                topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB), null);",
                "-            if (onHeap != null) {",
                "-                topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, onHeap);",
                "-            }",
                "+        for (String resourceName : topologyResources.keySet()) {",
                "+            msgBuilder.append(checkInitResource(topologyResources, topologyConf, resourceName));",
                "         }",
                "-        if (!topologyResources.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB)) {",
                "-            Double offHeap = ObjectReader.getDouble(",
                "-                topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB), null);",
                "-            if (offHeap != null) {",
                "-                topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, offHeap);",
                "-            }",
                "+",
                "+        if (msgBuilder.length() > 0) {",
                "+            String resourceDefaults = msgBuilder.toString();",
                "+            LOG.debug(",
                "+                    \"Unable to extract resource requirement for Component {} \\n Resources : {}\",",
                "+                    componentId, resourceDefaults);",
                "         }",
                "@@ -456,11 +452,15 @@ public class CaptureLoad {",
                "-    static void checkInitCpu(Map<String, Double> topologyResources, String com,",
                "-                                     Map<String, Object> topologyConf) {",
                "-        if (!topologyResources.containsKey(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT)) {",
                "-            Double cpu = ObjectReader.getDouble(topologyConf.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT), null);",
                "-            if (cpu != null) {",
                "-                topologyResources.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, cpu);",
                "+    private static String checkInitResource(Map<String, Double> topologyResources, Map topologyConf, String resourceName) {",
                "+        StringBuilder msgBuilder = new StringBuilder();",
                "+        if (topologyResources.containsKey(resourceName)) {",
                "+            Double resourceValue = (Double) topologyConf.getOrDefault(resourceName, null);",
                "+            if (resourceValue != null) {",
                "+                topologyResources.put(resourceName, resourceValue);",
                "+                msgBuilder.append(resourceName.substring(resourceName.lastIndexOf(\".\")) + \" has been set to \" + resourceValue);",
                "             }",
                "         }",
                "+",
                "+        return msgBuilder.toString();",
                "     }",
                "+",
                " }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/Config.java b/storm-client/src/jvm/org/apache/storm/Config.java",
                "index 42a4b4498..cc47e14dd 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/Config.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/Config.java",
                "@@ -242,2 +242,8 @@ public class Config extends HashMap<String, Object> {",
                "+    /**",
                "+     * A map of resources used by each component e.g {\"cpu.pcore.percent\" : 200.0. \"onheap.memory.mb\": 256.0, \"gpu.count\" : 2 }",
                "+     */",
                "+    @isMapEntryType(keyType = String.class, valueType = Number.class)",
                "+    public static final String TOPOLOGY_COMPONENT_RESOURCES_MAP = \"topology.component.resources.map\";",
                "+",
                "     /**",
                "@@ -1289,2 +1295,8 @@ public class Config extends HashMap<String, Object> {",
                "+    /**",
                "+     * A map of resources the Supervisor has e.g {\"cpu.pcore.percent\" : 200.0. \"onheap.memory.mb\": 256.0, \"gpu.count\" : 2.0 }",
                "+     */",
                "+    @isMapEntryType(keyType = String.class, valueType = Number.class)",
                "+    public static final String SUPERVISOR_RESOURCES_MAP = \"supervisor.resources.map\";",
                "+",
                "     /**",
                "diff --git a/storm-client/src/jvm/org/apache/storm/Constants.java b/storm-client/src/jvm/org/apache/storm/Constants.java",
                "index 7375c702e..c8d3b0966 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/Constants.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/Constants.java",
                "@@ -17,8 +17,11 @@",
                "  */",
                "-package org.apache.storm;",
                "-import org.apache.storm.coordination.CoordinatedBolt;",
                "+package org.apache.storm;",
                " import java.util.Arrays;",
                "+import java.util.Collections;",
                "+import java.util.HashMap;",
                " import java.util.List;",
                "+import java.util.Map;",
                "+import org.apache.storm.coordination.CoordinatedBolt;",
                "@@ -58,2 +61,19 @@ public class Constants {",
                "     public static final Object LOAD_MAPPING = \"load-mapping\";",
                "+",
                "+    public static final String COMMON_CPU_RESOURCE_NAME = \"cpu.pcore.percent\";",
                "+    public static final String COMMON_ONHEAP_MEMORY_RESOURCE_NAME = \"onheap.memory.mb\";",
                "+    public static final String COMMON_OFFHEAP_MEMORY_RESOURCE_NAME = \"offheap.memory.mb\";",
                "+    public static final String COMMON_TOTAL_MEMORY_RESOURCE_NAME = \"memory.mb\";",
                "+",
                "+    public static final Map<String, String> resourceNameMapping;",
                "+",
                "+    static {",
                "+        Map<String, String> tmp = new HashMap<>();",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, COMMON_CPU_RESOURCE_NAME);",
                "+        tmp.put(Config.SUPERVISOR_CPU_CAPACITY, COMMON_CPU_RESOURCE_NAME);",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "+        tmp.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, COMMON_TOTAL_MEMORY_RESOURCE_NAME);",
                "+        resourceNameMapping = Collections.unmodifiableMap(tmp);",
                "+    }",
                " }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "index 142e5f916..9318aa822 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "@@ -17,4 +17,13 @@",
                "  */",
                "+",
                " package org.apache.storm.coordination;",
                "+import java.util.ArrayList;",
                "+import java.util.HashMap;",
                "+import java.util.HashSet;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import java.util.Set;",
                "+",
                "+import org.apache.storm.Config;",
                " import org.apache.storm.Constants;",
                "@@ -34,8 +43,3 @@ import org.apache.storm.topology.TopologyBuilder;",
                " import org.apache.storm.tuple.Fields;",
                "-import java.util.ArrayList;",
                "-import java.util.HashMap;",
                "-import java.util.HashSet;",
                "-import java.util.List;",
                "-import java.util.Map;",
                "-import java.util.Set;",
                "+",
                "@@ -78,3 +82,5 @@ public class BatchSubtopologyBuilder {",
                "         Integer p = null;",
                "-        if(parallelism!=null) p = parallelism.intValue();",
                "+        if (parallelism != null) {",
                "+            p = parallelism.intValue();",
                "+        }",
                "         Component component = new Component(bolt, p);",
                "@@ -86,14 +92,14 @@ public class BatchSubtopologyBuilder {",
                "         BoltDeclarer declarer = builder.setBolt(_masterId, new CoordinatedBolt(_masterBolt.bolt), _masterBolt.parallelism);",
                "-        for(InputDeclaration decl: _masterBolt.declarations) {",
                "+        for (InputDeclaration decl: _masterBolt.declarations) {",
                "             decl.declare(declarer);",
                "         }",
                "-        for(Map<String, Object> conf: _masterBolt.componentConfs) {",
                "+        for (Map<String, Object> conf: _masterBolt.componentConfs) {",
                "             declarer.addConfigurations(conf);",
                "         }",
                "-        for(String id: _bolts.keySet()) {",
                "+        for (String id: _bolts.keySet()) {",
                "             Component component = _bolts.get(id);",
                "             Map<String, SourceArgs> coordinatedArgs = new HashMap<String, SourceArgs>();",
                "-            for(String c: componentBoltSubscriptions(component)) {",
                "+            for (String c: componentBoltSubscriptions(component)) {",
                "                 SourceArgs source;",
                "-                if(c.equals(_masterId)) {",
                "+                if (c.equals(_masterId)) {",
                "                     source = SourceArgs.single();",
                "@@ -114,9 +120,9 @@ public class BatchSubtopologyBuilder {",
                "             }",
                "-            for(Map<String, Object> conf: component.componentConfs) {",
                "+            for (Map<String, Object> conf: component.componentConfs) {",
                "                 input.addConfigurations(conf);",
                "             }",
                "-            for(String c: componentBoltSubscriptions(component)) {",
                "+            for (String c: componentBoltSubscriptions(component)) {",
                "                 input.directGrouping(c, Constants.COORDINATED_STREAM_ID);",
                "             }",
                "-            for(InputDeclaration d: component.declarations) {",
                "+            for (InputDeclaration d: component.declarations) {",
                "                 d.declare(input);",
                "@@ -128,3 +134,3 @@ public class BatchSubtopologyBuilder {",
                "         Set<String> ret = new HashSet<String>();",
                "-        for(InputDeclaration d: component.declarations) {",
                "+        for (InputDeclaration d: component.declarations) {",
                "             ret.add(d.getComponent());",
                "@@ -451,2 +457,15 @@ public class BatchSubtopologyBuilder {",
                "+        @Override",
                "+        public Map getRASConfiguration() {",
                "+            for (Map<String, Object> conf : _component.componentConfs) {",
                "+                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "+                    return conf;",
                "+                }",
                "+            }",
                "+            Map<String, Object> newConf = new HashMap<>();",
                "+            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "+            _component.componentConfs.add(newConf);",
                "+            return newConf;",
                "+        }",
                "+",
                "         @Override",
                "@@ -456,2 +475,13 @@ public class BatchSubtopologyBuilder {",
                "         }",
                "+",
                "+        @SuppressWarnings(\"unchecked\")",
                "+        @Override",
                "+        public BoltDeclarer addResource(String resourceName, Number resourceValue) {",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+",
                "+            resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "+",
                "+            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "+            return this;",
                "+        }",
                "     }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "index 8fe85f34f..928a5faab 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "@@ -17,4 +17,13 @@",
                "  */",
                "+",
                " package org.apache.storm.drpc;",
                "+import java.util.ArrayList;",
                "+import java.util.HashMap;",
                "+import java.util.HashSet;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import java.util.Set;",
                "+",
                "+import org.apache.storm.Config;",
                " import org.apache.storm.Constants;",
                "@@ -41,8 +50,2 @@ import org.apache.storm.topology.TopologyBuilder;",
                " import org.apache.storm.tuple.Fields;",
                "-import java.util.ArrayList;",
                "-import java.util.HashMap;",
                "-import java.util.HashSet;",
                "-import java.util.List;",
                "-import java.util.Map;",
                "-import java.util.Set;",
                "@@ -70,3 +73,5 @@ public class LinearDRPCTopologyBuilder {",
                "     public LinearDRPCInputDeclarer addBolt(IRichBolt bolt, Number parallelism) {",
                "-        if(parallelism==null) parallelism = 1; ",
                "+        if (parallelism == null) {",
                "+            parallelism = 1;",
                "+        } ",
                "         Component component = new Component(bolt, parallelism.intValue());",
                "@@ -106,4 +111,4 @@ public class LinearDRPCTopologyBuilder {",
                "                 .noneGrouping(SPOUT_ID);",
                "-        int i=0;",
                "-        for(; i<_components.size();i++) {",
                "+        int i = 0;",
                "+        for (; i < _components.size();i++) {",
                "             Component component = _components.get(i);",
                "@@ -111,9 +116,9 @@ public class LinearDRPCTopologyBuilder {",
                "             Map<String, SourceArgs> source = new HashMap<String, SourceArgs>();",
                "-            if (i==1) {",
                "-                source.put(boltId(i-1), SourceArgs.single());",
                "-            } else if (i>=2) {",
                "-                source.put(boltId(i-1), SourceArgs.all());",
                "+            if (i == 1) {",
                "+                source.put(boltId(i - 1), SourceArgs.single());",
                "+            } else if (i >= 2) {",
                "+                source.put(boltId(i - 1), SourceArgs.all());",
                "             }",
                "             IdStreamSpec idSpec = null;",
                "-            if(i==_components.size()-1 && component.bolt instanceof FinishedCallback) {",
                "+            if (i == _components.size() - 1 && component.bolt instanceof FinishedCallback) {",
                "                 idSpec = IdStreamSpec.makeDetectSpec(PREPARE_ID, PrepareRequest.ID_STREAM);",
                "@@ -129,3 +134,3 @@ public class LinearDRPCTopologyBuilder {",
                "-            for(Map<String, Object> conf: component.componentConfs) {",
                "+            for (Map<String, Object> conf: component.componentConfs) {",
                "                 declarer.addConfigurations(conf);",
                "@@ -133,6 +138,6 @@ public class LinearDRPCTopologyBuilder {",
                "-            if(idSpec!=null) {",
                "+            if (idSpec != null) {",
                "                 declarer.fieldsGrouping(idSpec.getGlobalStreamId().get_componentId(), PrepareRequest.ID_STREAM, new Fields(\"request\"));",
                "             }",
                "-            if(i==0 && component.declarations.isEmpty()) {",
                "+            if (i == 0 && component.declarations.isEmpty()) {",
                "                 declarer.noneGrouping(PREPARE_ID, PrepareRequest.ARGS_STREAM);",
                "@@ -140,8 +145,8 @@ public class LinearDRPCTopologyBuilder {",
                "                 String prevId;",
                "-                if(i==0) {",
                "+                if (i == 0) {",
                "                     prevId = PREPARE_ID;",
                "                 } else {",
                "-                    prevId = boltId(i-1);",
                "+                    prevId = boltId(i - 1);",
                "                 }",
                "-                for(InputDeclaration declaration: component.declarations) {",
                "+                for (InputDeclaration declaration: component.declarations) {",
                "                     declaration.declare(prevId, declarer);",
                "@@ -149,4 +154,4 @@ public class LinearDRPCTopologyBuilder {",
                "             }",
                "-            if(i>0) {",
                "-                declarer.directGrouping(boltId(i-1), Constants.COORDINATED_STREAM_ID); ",
                "+            if (i > 0) {",
                "+                declarer.directGrouping(boltId(i - 1), Constants.COORDINATED_STREAM_ID); ",
                "             }",
                "@@ -154,3 +159,3 @@ public class LinearDRPCTopologyBuilder {",
                "-        IRichBolt lastBolt = _components.get(_components.size()-1).bolt;",
                "+        IRichBolt lastBolt = _components.get(_components.size() - 1).bolt;",
                "         OutputFieldsGetter getter = new OutputFieldsGetter();",
                "@@ -158,3 +163,3 @@ public class LinearDRPCTopologyBuilder {",
                "         Map<String, StreamInfo> streams = getter.getFieldsDeclaration();",
                "-        if(streams.size()!=1) {",
                "+        if (streams.size() != 1) {",
                "             throw new RuntimeException(\"Must declare exactly one stream from last bolt in LinearDRPCTopology\");",
                "@@ -163,4 +168,6 @@ public class LinearDRPCTopologyBuilder {",
                "         List<String> fields = streams.get(outputStream).get_output_fields();",
                "-        if(fields.size()!=2) {",
                "-            throw new RuntimeException(\"Output stream of last component in LinearDRPCTopology must contain exactly two fields. The first should be the request id, and the second should be the result.\");",
                "+        if (fields.size() != 2) {",
                "+            throw new RuntimeException(",
                "+                    \"Output stream of last component in LinearDRPCTopology must contain exactly two fields. \" ",
                "+                            + \"The first should be the request id, and the second should be the result.\");",
                "         }",
                "@@ -168,3 +175,3 @@ public class LinearDRPCTopologyBuilder {",
                "         builder.setBolt(boltId(i), new JoinResult(PREPARE_ID))",
                "-                .fieldsGrouping(boltId(i-1), outputStream, new Fields(fields.get(0)))",
                "+                .fieldsGrouping(boltId(i - 1), outputStream, new Fields(fields.get(0)))",
                "                 .fieldsGrouping(PREPARE_ID, PrepareRequest.RETURN_STREAM, new Fields(\"request\"));",
                "@@ -172,3 +179,3 @@ public class LinearDRPCTopologyBuilder {",
                "         builder.setBolt(boltId(i), new ReturnResults())",
                "-                .noneGrouping(boltId(i-1));",
                "+                .noneGrouping(boltId(i - 1));",
                "         return builder.createTopology();",
                "@@ -400,2 +407,26 @@ public class LinearDRPCTopologyBuilder {",
                "+        @SuppressWarnings(\"unchecked\")",
                "+        @Override",
                "+        public LinearDRPCInputDeclarer addResource(String resourceName, Number resourceValue) {",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+",
                "+            resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "+",
                "+            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "+            return this;",
                "+        }",
                "+",
                "+        @Override",
                "+        public Map getRASConfiguration() {",
                "+            for (Map<String, Object> conf : _component.componentConfs) {",
                "+                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "+                    return conf;",
                "+                }",
                "+            }",
                "+            Map<String, Object> newConf = new HashMap<>();",
                "+            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "+            _component.componentConfs.add(newConf);",
                "+            return newConf;",
                "+        }",
                "+",
                "         @Override",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/WorkerResources.java b/storm-client/src/jvm/org/apache/storm/generated/WorkerResources.java",
                "index 16c0a210d..56e23480f 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/WorkerResources.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/WorkerResources.java",
                "@@ -62,2 +62,4 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "   private static final org.apache.thrift.protocol.TField SHARED_MEM_OFF_HEAP_FIELD_DESC = new org.apache.thrift.protocol.TField(\"shared_mem_off_heap\", org.apache.thrift.protocol.TType.DOUBLE, (short)5);",
                "+  private static final org.apache.thrift.protocol.TField RESOURCES_FIELD_DESC = new org.apache.thrift.protocol.TField(\"resources\", org.apache.thrift.protocol.TType.MAP, (short)6);",
                "+  private static final org.apache.thrift.protocol.TField SHARED_RESOURCES_FIELD_DESC = new org.apache.thrift.protocol.TField(\"shared_resources\", org.apache.thrift.protocol.TType.MAP, (short)7);",
                "@@ -74,2 +76,4 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "   private double shared_mem_off_heap; // optional",
                "+  private Map<String,Double> resources; // optional",
                "+  private Map<String,Double> shared_resources; // optional",
                "@@ -81,3 +85,5 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "     SHARED_MEM_ON_HEAP((short)4, \"shared_mem_on_heap\"),",
                "-    SHARED_MEM_OFF_HEAP((short)5, \"shared_mem_off_heap\");",
                "+    SHARED_MEM_OFF_HEAP((short)5, \"shared_mem_off_heap\"),",
                "+    RESOURCES((short)6, \"resources\"),",
                "+    SHARED_RESOURCES((short)7, \"shared_resources\");",
                "@@ -106,2 +112,6 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "           return SHARED_MEM_OFF_HEAP;",
                "+        case 6: // RESOURCES",
                "+          return RESOURCES;",
                "+        case 7: // SHARED_RESOURCES",
                "+          return SHARED_RESOURCES;",
                "         default:",
                "@@ -152,3 +162,3 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "   private byte __isset_bitfield = 0;",
                "-  private static final _Fields optionals[] = {_Fields.MEM_ON_HEAP,_Fields.MEM_OFF_HEAP,_Fields.CPU,_Fields.SHARED_MEM_ON_HEAP,_Fields.SHARED_MEM_OFF_HEAP};",
                "+  private static final _Fields optionals[] = {_Fields.MEM_ON_HEAP,_Fields.MEM_OFF_HEAP,_Fields.CPU,_Fields.SHARED_MEM_ON_HEAP,_Fields.SHARED_MEM_OFF_HEAP,_Fields.RESOURCES,_Fields.SHARED_RESOURCES};",
                "   public static final Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;",
                "@@ -166,2 +176,10 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "         new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.DOUBLE)));",
                "+    tmpMap.put(_Fields.RESOURCES, new org.apache.thrift.meta_data.FieldMetaData(\"resources\", org.apache.thrift.TFieldRequirementType.OPTIONAL, ",
                "+        new org.apache.thrift.meta_data.MapMetaData(org.apache.thrift.protocol.TType.MAP, ",
                "+            new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING), ",
                "+            new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.DOUBLE))));",
                "+    tmpMap.put(_Fields.SHARED_RESOURCES, new org.apache.thrift.meta_data.FieldMetaData(\"shared_resources\", org.apache.thrift.TFieldRequirementType.OPTIONAL, ",
                "+        new org.apache.thrift.meta_data.MapMetaData(org.apache.thrift.protocol.TType.MAP, ",
                "+            new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING), ",
                "+            new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.DOUBLE))));",
                "     metaDataMap = Collections.unmodifiableMap(tmpMap);",
                "@@ -183,2 +201,10 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "     this.shared_mem_off_heap = other.shared_mem_off_heap;",
                "+    if (other.is_set_resources()) {",
                "+      Map<String,Double> __this__resources = new HashMap<String,Double>(other.resources);",
                "+      this.resources = __this__resources;",
                "+    }",
                "+    if (other.is_set_shared_resources()) {",
                "+      Map<String,Double> __this__shared_resources = new HashMap<String,Double>(other.shared_resources);",
                "+      this.shared_resources = __this__shared_resources;",
                "+    }",
                "   }",
                "@@ -201,2 +227,4 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "     this.shared_mem_off_heap = 0.0;",
                "+    this.resources = null;",
                "+    this.shared_resources = null;",
                "   }",
                "@@ -313,2 +341,70 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "+  public int get_resources_size() {",
                "+    return (this.resources == null) ? 0 : this.resources.size();",
                "+  }",
                "+",
                "+  public void put_to_resources(String key, double val) {",
                "+    if (this.resources == null) {",
                "+      this.resources = new HashMap<String,Double>();",
                "+    }",
                "+    this.resources.put(key, val);",
                "+  }",
                "+",
                "+  public Map<String,Double> get_resources() {",
                "+    return this.resources;",
                "+  }",
                "+",
                "+  public void set_resources(Map<String,Double> resources) {",
                "+    this.resources = resources;",
                "+  }",
                "+",
                "+  public void unset_resources() {",
                "+    this.resources = null;",
                "+  }",
                "+",
                "+  /** Returns true if field resources is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_resources() {",
                "+    return this.resources != null;",
                "+  }",
                "+",
                "+  public void set_resources_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.resources = null;",
                "+    }",
                "+  }",
                "+",
                "+  public int get_shared_resources_size() {",
                "+    return (this.shared_resources == null) ? 0 : this.shared_resources.size();",
                "+  }",
                "+",
                "+  public void put_to_shared_resources(String key, double val) {",
                "+    if (this.shared_resources == null) {",
                "+      this.shared_resources = new HashMap<String,Double>();",
                "+    }",
                "+    this.shared_resources.put(key, val);",
                "+  }",
                "+",
                "+  public Map<String,Double> get_shared_resources() {",
                "+    return this.shared_resources;",
                "+  }",
                "+",
                "+  public void set_shared_resources(Map<String,Double> shared_resources) {",
                "+    this.shared_resources = shared_resources;",
                "+  }",
                "+",
                "+  public void unset_shared_resources() {",
                "+    this.shared_resources = null;",
                "+  }",
                "+",
                "+  /** Returns true if field shared_resources is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_shared_resources() {",
                "+    return this.shared_resources != null;",
                "+  }",
                "+",
                "+  public void set_shared_resources_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.shared_resources = null;",
                "+    }",
                "+  }",
                "+",
                "   public void setFieldValue(_Fields field, Object value) {",
                "@@ -355,2 +451,18 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "+    case RESOURCES:",
                "+      if (value == null) {",
                "+        unset_resources();",
                "+      } else {",
                "+        set_resources((Map<String,Double>)value);",
                "+      }",
                "+      break;",
                "+",
                "+    case SHARED_RESOURCES:",
                "+      if (value == null) {",
                "+        unset_shared_resources();",
                "+      } else {",
                "+        set_shared_resources((Map<String,Double>)value);",
                "+      }",
                "+      break;",
                "+",
                "     }",
                "@@ -375,2 +487,8 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "+    case RESOURCES:",
                "+      return get_resources();",
                "+",
                "+    case SHARED_RESOURCES:",
                "+      return get_shared_resources();",
                "+",
                "     }",
                "@@ -396,2 +514,6 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "       return is_set_shared_mem_off_heap();",
                "+    case RESOURCES:",
                "+      return is_set_resources();",
                "+    case SHARED_RESOURCES:",
                "+      return is_set_shared_resources();",
                "     }",
                "@@ -458,2 +580,20 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "+    boolean this_present_resources = true && this.is_set_resources();",
                "+    boolean that_present_resources = true && that.is_set_resources();",
                "+    if (this_present_resources || that_present_resources) {",
                "+      if (!(this_present_resources && that_present_resources))",
                "+        return false;",
                "+      if (!this.resources.equals(that.resources))",
                "+        return false;",
                "+    }",
                "+",
                "+    boolean this_present_shared_resources = true && this.is_set_shared_resources();",
                "+    boolean that_present_shared_resources = true && that.is_set_shared_resources();",
                "+    if (this_present_shared_resources || that_present_shared_resources) {",
                "+      if (!(this_present_shared_resources && that_present_shared_resources))",
                "+        return false;",
                "+      if (!this.shared_resources.equals(that.shared_resources))",
                "+        return false;",
                "+    }",
                "+",
                "     return true;",
                "@@ -490,2 +630,12 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "+    boolean present_resources = true && (is_set_resources());",
                "+    list.add(present_resources);",
                "+    if (present_resources)",
                "+      list.add(resources);",
                "+",
                "+    boolean present_shared_resources = true && (is_set_shared_resources());",
                "+    list.add(present_shared_resources);",
                "+    if (present_shared_resources)",
                "+      list.add(shared_resources);",
                "+",
                "     return list.hashCode();",
                "@@ -551,2 +701,22 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "     }",
                "+    lastComparison = Boolean.valueOf(is_set_resources()).compareTo(other.is_set_resources());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_resources()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.resources, other.resources);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    lastComparison = Boolean.valueOf(is_set_shared_resources()).compareTo(other.is_set_shared_resources());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_shared_resources()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.shared_resources, other.shared_resources);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "     return 0;",
                "@@ -600,2 +770,22 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "     }",
                "+    if (is_set_resources()) {",
                "+      if (!first) sb.append(\", \");",
                "+      sb.append(\"resources:\");",
                "+      if (this.resources == null) {",
                "+        sb.append(\"null\");",
                "+      } else {",
                "+        sb.append(this.resources);",
                "+      }",
                "+      first = false;",
                "+    }",
                "+    if (is_set_shared_resources()) {",
                "+      if (!first) sb.append(\", \");",
                "+      sb.append(\"shared_resources:\");",
                "+      if (this.shared_resources == null) {",
                "+        sb.append(\"null\");",
                "+      } else {",
                "+        sb.append(this.shared_resources);",
                "+      }",
                "+      first = false;",
                "+    }",
                "     sb.append(\")\");",
                "@@ -685,2 +875,42 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "             break;",
                "+          case 6: // RESOURCES",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.MAP) {",
                "+              {",
                "+                org.apache.thrift.protocol.TMap _map646 = iprot.readMapBegin();",
                "+                struct.resources = new HashMap<String,Double>(2*_map646.size);",
                "+                String _key647;",
                "+                double _val648;",
                "+                for (int _i649 = 0; _i649 < _map646.size; ++_i649)",
                "+                {",
                "+                  _key647 = iprot.readString();",
                "+                  _val648 = iprot.readDouble();",
                "+                  struct.resources.put(_key647, _val648);",
                "+                }",
                "+                iprot.readMapEnd();",
                "+              }",
                "+              struct.set_resources_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          case 7: // SHARED_RESOURCES",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.MAP) {",
                "+              {",
                "+                org.apache.thrift.protocol.TMap _map650 = iprot.readMapBegin();",
                "+                struct.shared_resources = new HashMap<String,Double>(2*_map650.size);",
                "+                String _key651;",
                "+                double _val652;",
                "+                for (int _i653 = 0; _i653 < _map650.size; ++_i653)",
                "+                {",
                "+                  _key651 = iprot.readString();",
                "+                  _val652 = iprot.readDouble();",
                "+                  struct.shared_resources.put(_key651, _val652);",
                "+                }",
                "+                iprot.readMapEnd();",
                "+              }",
                "+              struct.set_shared_resources_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "           default:",
                "@@ -723,2 +953,32 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "       }",
                "+      if (struct.resources != null) {",
                "+        if (struct.is_set_resources()) {",
                "+          oprot.writeFieldBegin(RESOURCES_FIELD_DESC);",
                "+          {",
                "+            oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, struct.resources.size()));",
                "+            for (Map.Entry<String, Double> _iter654 : struct.resources.entrySet())",
                "+            {",
                "+              oprot.writeString(_iter654.getKey());",
                "+              oprot.writeDouble(_iter654.getValue());",
                "+            }",
                "+            oprot.writeMapEnd();",
                "+          }",
                "+          oprot.writeFieldEnd();",
                "+        }",
                "+      }",
                "+      if (struct.shared_resources != null) {",
                "+        if (struct.is_set_shared_resources()) {",
                "+          oprot.writeFieldBegin(SHARED_RESOURCES_FIELD_DESC);",
                "+          {",
                "+            oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, struct.shared_resources.size()));",
                "+            for (Map.Entry<String, Double> _iter655 : struct.shared_resources.entrySet())",
                "+            {",
                "+              oprot.writeString(_iter655.getKey());",
                "+              oprot.writeDouble(_iter655.getValue());",
                "+            }",
                "+            oprot.writeMapEnd();",
                "+          }",
                "+          oprot.writeFieldEnd();",
                "+        }",
                "+      }",
                "       oprot.writeFieldStop();",
                "@@ -756,3 +1016,9 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "       }",
                "-      oprot.writeBitSet(optionals, 5);",
                "+      if (struct.is_set_resources()) {",
                "+        optionals.set(5);",
                "+      }",
                "+      if (struct.is_set_shared_resources()) {",
                "+        optionals.set(6);",
                "+      }",
                "+      oprot.writeBitSet(optionals, 7);",
                "       if (struct.is_set_mem_on_heap()) {",
                "@@ -772,2 +1038,22 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "       }",
                "+      if (struct.is_set_resources()) {",
                "+        {",
                "+          oprot.writeI32(struct.resources.size());",
                "+          for (Map.Entry<String, Double> _iter656 : struct.resources.entrySet())",
                "+          {",
                "+            oprot.writeString(_iter656.getKey());",
                "+            oprot.writeDouble(_iter656.getValue());",
                "+          }",
                "+        }",
                "+      }",
                "+      if (struct.is_set_shared_resources()) {",
                "+        {",
                "+          oprot.writeI32(struct.shared_resources.size());",
                "+          for (Map.Entry<String, Double> _iter657 : struct.shared_resources.entrySet())",
                "+          {",
                "+            oprot.writeString(_iter657.getKey());",
                "+            oprot.writeDouble(_iter657.getValue());",
                "+          }",
                "+        }",
                "+      }",
                "     }",
                "@@ -777,3 +1063,3 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "       TTupleProtocol iprot = (TTupleProtocol) prot;",
                "-      BitSet incoming = iprot.readBitSet(5);",
                "+      BitSet incoming = iprot.readBitSet(7);",
                "       if (incoming.get(0)) {",
                "@@ -798,2 +1084,32 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "       }",
                "+      if (incoming.get(5)) {",
                "+        {",
                "+          org.apache.thrift.protocol.TMap _map658 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "+          struct.resources = new HashMap<String,Double>(2*_map658.size);",
                "+          String _key659;",
                "+          double _val660;",
                "+          for (int _i661 = 0; _i661 < _map658.size; ++_i661)",
                "+          {",
                "+            _key659 = iprot.readString();",
                "+            _val660 = iprot.readDouble();",
                "+            struct.resources.put(_key659, _val660);",
                "+          }",
                "+        }",
                "+        struct.set_resources_isSet(true);",
                "+      }",
                "+      if (incoming.get(6)) {",
                "+        {",
                "+          org.apache.thrift.protocol.TMap _map662 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "+          struct.shared_resources = new HashMap<String,Double>(2*_map662.size);",
                "+          String _key663;",
                "+          double _val664;",
                "+          for (int _i665 = 0; _i665 < _map662.size; ++_i665)",
                "+          {",
                "+            _key663 = iprot.readString();",
                "+            _val664 = iprot.readDouble();",
                "+            struct.shared_resources.put(_key663, _val664);",
                "+          }",
                "+        }",
                "+        struct.set_shared_resources_isSet(true);",
                "+      }",
                "     }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/scheduler/SupervisorDetails.java b/storm-client/src/jvm/org/apache/storm/scheduler/SupervisorDetails.java",
                "index a0c94a883..0df8bdd36 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/scheduler/SupervisorDetails.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/scheduler/SupervisorDetails.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.scheduler;",
                "@@ -21,6 +22,7 @@ import java.util.Collection;",
                " import java.util.HashSet;",
                "-import java.util.Set;",
                " import java.util.Map;",
                "+import java.util.Set;",
                "-import org.apache.storm.Config;",
                "+",
                "+import org.apache.storm.Constants;",
                " import org.slf4j.Logger;",
                "@@ -33,3 +35,3 @@ public class SupervisorDetails {",
                "     /**",
                "-     * hostname of this supervisor",
                "+     * hostname of this supervisor.",
                "      */",
                "@@ -38,3 +40,3 @@ public class SupervisorDetails {",
                "     /**",
                "-     * meta data configured for this supervisor",
                "+     * meta data configured for this supervisor.",
                "      */",
                "@@ -42,3 +44,3 @@ public class SupervisorDetails {",
                "     /**",
                "-     * all the ports of the supervisor",
                "+     * all the ports of the supervisor.",
                "      */",
                "@@ -46,3 +48,3 @@ public class SupervisorDetails {",
                "     /**",
                "-     * Map containing a manifest of resources for the node the supervisor resides",
                "+     * Map containing a manifest of resources for the node the supervisor resides.",
                "      */",
                "@@ -51,3 +53,3 @@ public class SupervisorDetails {",
                "     public SupervisorDetails(String id, String host, Object meta, Object schedulerMeta,",
                "-                             Collection<? extends Number> allPorts, Map<String, Double> total_resources){",
                "+                             Collection<? extends Number> allPorts, Map<String, Double> total_resources) {",
                "@@ -57,3 +59,3 @@ public class SupervisorDetails {",
                "         this.schedulerMeta = schedulerMeta;",
                "-        if(allPorts!=null) {",
                "+        if (allPorts != null) {",
                "             setAllPorts(allPorts);",
                "@@ -74,3 +76,3 @@ public class SupervisorDetails {",
                "-    public SupervisorDetails(String id, Object meta, Collection<? extends Number> allPorts){",
                "+    public SupervisorDetails(String id, Object meta, Collection<? extends Number> allPorts) {",
                "         this(id, null, meta, null, allPorts, null);",
                "@@ -89,4 +91,4 @@ public class SupervisorDetails {",
                "     public String toString() {",
                "-        return getClass().getSimpleName() + \" ID: \" + id + \" HOST: \" + host + \" META: \" + meta +",
                "-                \" SCHED_META: \" + schedulerMeta + \" PORTS: \" + allPorts;",
                "+        return getClass().getSimpleName() + \" ID: \" + id + \" HOST: \" + host + \" META: \" + meta",
                "+                + \" SCHED_META: \" + schedulerMeta + \" PORTS: \" + allPorts;",
                "     }",
                "@@ -95,3 +97,3 @@ public class SupervisorDetails {",
                "         this.allPorts = new HashSet<>();",
                "-        if (allPorts!=null) {",
                "+        if (allPorts != null) {",
                "             for (Number n: allPorts) {",
                "@@ -127,3 +129,3 @@ public class SupervisorDetails {",
                "     public double getTotalMemory() {",
                "-        Double totalMemory = getTotalResource(Config.SUPERVISOR_MEMORY_CAPACITY_MB);",
                "+        Double totalMemory = getTotalResource(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME);",
                "         assert totalMemory != null;",
                "@@ -133,3 +135,3 @@ public class SupervisorDetails {",
                "     public double getTotalCPU() {",
                "-        Double totalCPU = getTotalResource(Config.SUPERVISOR_CPU_CAPACITY);",
                "+        Double totalCPU = getTotalResource(Constants.COMMON_CPU_RESOURCE_NAME);",
                "         assert totalCPU != null;",
                "@@ -137,2 +139,9 @@ public class SupervisorDetails {",
                "     }",
                "+",
                "+    /**",
                "+     * get all resources for this Supervisor.",
                "+     */",
                "+    public Map<String, Double> getTotalResources() {",
                "+        return _total_resources;",
                "+    }",
                " }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java b/storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java",
                "index a0bb7ba38..eac23bbb6 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java",
                "@@ -59,2 +59,3 @@ public abstract class BaseConfigurationDeclarer<T extends ComponentConfiguration",
                "             onHeap = onHeap.doubleValue();",
                "+            addResource(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, onHeap);",
                "             return addConfiguration(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, onHeap);",
                "@@ -72,2 +73,3 @@ public abstract class BaseConfigurationDeclarer<T extends ComponentConfiguration",
                "             offHeap = offHeap.doubleValue();",
                "+            addResource(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, offHeap);",
                "             ret = addConfiguration(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, offHeap);",
                "@@ -80,3 +82,4 @@ public abstract class BaseConfigurationDeclarer<T extends ComponentConfiguration",
                "     public T setCPULoad(Number amount) {",
                "-        if(amount != null) {",
                "+        if (amount != null) {",
                "+            addResource(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, amount);",
                "             return addConfiguration(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, amount);",
                "@@ -85,2 +88,11 @@ public abstract class BaseConfigurationDeclarer<T extends ComponentConfiguration",
                "     }",
                "+",
                "+    @SuppressWarnings(\"unchecked\")",
                "+    @Override",
                "+    public T addResources(Map<String, Double> resources) {",
                "+        if (resources != null) {",
                "+            return addConfiguration(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resources);",
                "+        }",
                "+        return (T) this;",
                "+    }",
                " }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java b/storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java",
                "index 19cbdab52..80980298b 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.topology;",
                "@@ -23,3 +24,3 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration",
                "     /**",
                "-     * add in several configs to the component",
                "+     * add in several configs to the component.",
                "      * @param conf the configs to add",
                "@@ -30,3 +31,10 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration",
                "     /**",
                "-     * Add in a single config",
                "+     * return the configuration.",
                "+     *",
                "+     * @return the current configuration.",
                "+     */",
                "+    Map<String, Object> getRASConfiguration();",
                "+",
                "+    /**",
                "+     * Add in a single config.",
                "      * @param config the key for the config",
                "@@ -38,3 +46,3 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration",
                "     /**",
                "-     * Turn on/off debug for this component ",
                "+     * Turn on/off debug for this component.",
                "      * @param debug true for debug on false for debug off",
                "@@ -45,3 +53,3 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration",
                "     /**",
                "-     * Set the max task parallelism for this component",
                "+     * Set the max task parallelism for this component.",
                "      * @param val the maximum parallelism",
                "@@ -52,3 +60,3 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration",
                "     /**",
                "-     * Set the max spout pending config for this component",
                "+     * Set the max spout pending config for this component.",
                "      * @param val the value of max spout pending.",
                "@@ -59,3 +67,3 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration",
                "     /**",
                "-     * Set the number of tasks for this component",
                "+     * Set the number of tasks for this component.",
                "      * @param val the number of tasks",
                "@@ -64,2 +72,12 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration",
                "     T setNumTasks(Number val);",
                "+",
                "+    /**",
                "+     * Add generic resources for this component.",
                "+     */",
                "+    T addResources(Map<String, Double> resources);",
                "+",
                "+    /**",
                "+     * Add generic resource for this component.",
                "+     */",
                "+    T addResource(String resourceName, Number resourceValue);",
                " }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "index 971e5f349..0ebdae6ac 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "@@ -52,2 +52,4 @@ import java.nio.ByteBuffer;",
                " import java.util.ArrayList;",
                "+import java.util.Collection;",
                "+import java.util.Collections;",
                " import java.util.HashMap;",
                "@@ -60,2 +62,3 @@ import static org.apache.storm.spout.CheckpointSpout.CHECKPOINT_COMPONENT_ID;",
                " import static org.apache.storm.spout.CheckpointSpout.CHECKPOINT_STREAM_ID;",
                "+import static org.apache.storm.utils.Utils.parseJson;",
                "@@ -578,6 +581,24 @@ public class TopologyBuilder {",
                "             String currConf = _commons.get(_id).get_json_conf();",
                "-            _commons.get(_id).set_json_conf(mergeIntoJson(Utils.parseJson(currConf), conf));",
                "+            _commons.get(_id).set_json_conf(mergeIntoJson(parseJson(currConf), conf));",
                "             return (T) this;",
                "         }",
                "-        ",
                "+",
                "+        @SuppressWarnings(\"unchecked\")",
                "+        @Override",
                "+        public T addResource(String resourceName, Number resourceValue) {",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().getOrDefault(",
                "+                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "+",
                "+            resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "+",
                "+            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "+            return (T) this;",
                "+        }",
                "+",
                "+        @SuppressWarnings(\"unchecked\")",
                "+        @Override",
                "+        public Map getRASConfiguration() {",
                "+            return parseJson(_commons.get(_id).get_json_conf());",
                "+        }",
                "+",
                "         @SuppressWarnings(\"unchecked\")",
                "diff --git a/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "index 07d35249b..f2deca879 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "@@ -17,5 +17,5 @@",
                "  */",
                "+",
                " package org.apache.storm.transactional;",
                "-import org.apache.storm.coordination.IBatchBolt;",
                " import org.apache.storm.coordination.BatchBoltExecutor;",
                "@@ -36,4 +36,5 @@ import org.apache.storm.topology.BoltDeclarer;",
                " import org.apache.storm.topology.IBasicBolt;",
                "-import org.apache.storm.topology.IRichBolt;",
                "+import org.apache.storm.coordination.IBatchBolt;",
                " import org.apache.storm.topology.InputDeclarer;",
                "+import org.apache.storm.topology.IRichBolt;",
                " import org.apache.storm.topology.SpoutDeclarer;",
                "@@ -229,2 +230,15 @@ public class TransactionalTopologyBuilder {",
                "+        @Override",
                "+        public Map getRASConfiguration() {",
                "+            for(Map<String, Object> conf : _spoutConfs) {",
                "+                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "+                    return conf;",
                "+                }",
                "+            }",
                "+            Map<String, Object> newConf = new HashMap<>();",
                "+            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "+            _spoutConfs.add(newConf);",
                "+            return newConf;",
                "+        }",
                "+",
                "         @Override",
                "@@ -233,3 +247,17 @@ public class TransactionalTopologyBuilder {",
                "             return this;",
                "-        }        ",
                "+        }",
                "+",
                "+        @SuppressWarnings(\"unchecked\")",
                "+        @Override",
                "+        public SpoutDeclarer addResource(String resourceName, Number resourceValue) {",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+",
                "+            if (resourcesMap == null) {",
                "+                resourcesMap = new HashMap<>();",
                "+            }",
                "+            resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "+",
                "+            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "+            return this;",
                "+        }",
                "     }",
                "@@ -535,2 +563,15 @@ public class TransactionalTopologyBuilder {",
                "+        @Override",
                "+        public Map getRASConfiguration() {",
                "+            for(Map<String, Object> conf : _component.componentConfs) {",
                "+                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "+                    return conf;",
                "+                }",
                "+            }",
                "+            Map<String, Object> newConf = new HashMap<>();",
                "+            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "+            _component.componentConfs.add(newConf);",
                "+            return newConf;",
                "+        }",
                "+",
                "         @Override",
                "@@ -540,2 +581,13 @@ public class TransactionalTopologyBuilder {",
                "         }",
                "+",
                "+        @SuppressWarnings(\"unchecked\")",
                "+        @Override",
                "+        public BoltDeclarer addResource(String resourceName, Number resourceValue) {",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+",
                "+            resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "+",
                "+            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "+            return this;",
                "+        }",
                "     }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java",
                "index 3873117a0..be36b4ee5 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java",
                "@@ -373,2 +373,26 @@ public class TridentTopologyBuilder {",
                "+        @SuppressWarnings(\"unchecked\")",
                "+        @Override",
                "+        public SpoutDeclarer addResource(String resourceName, Number resourceValue) {",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+",
                "+            resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "+",
                "+            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "+            return this;",
                "+        }",
                "+",
                "+        @Override",
                "+        public Map getRASConfiguration() {",
                "+            for(Map<String, Object> conf : _component.componentConfs) {",
                "+                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "+                    return conf;",
                "+                }",
                "+            }",
                "+            Map<String, Object> newConf = new HashMap<>();",
                "+            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "+            _component.componentConfs.add(newConf);",
                "+            return newConf;",
                "+        }",
                "+",
                "         @Override",
                "@@ -764,2 +788,15 @@ public class TridentTopologyBuilder {",
                "+        @Override",
                "+        public Map getRASConfiguration() {",
                "+            for(Map<String, Object> conf : _component.componentConfs) {",
                "+                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "+                    return conf;",
                "+                }",
                "+            }",
                "+            Map<String, Object> newConf = new HashMap<>();",
                "+            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "+            _component.componentConfs.add(newConf);",
                "+            return newConf;",
                "+        }",
                "+",
                "         @Override",
                "@@ -769,2 +806,13 @@ public class TridentTopologyBuilder {",
                "         }",
                "+",
                "+        @SuppressWarnings(\"unchecked\")",
                "+        @Override",
                "+        public BoltDeclarer addResource(String resourceName, Number resourceValue) {",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+",
                "+            resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "+",
                "+            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "+            return this;",
                "+        }",
                "     }    ",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java b/storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java",
                "index 4a54ac7c1..1c8015a46 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java",
                "@@ -20,8 +20,2 @@ package org.apache.storm.utils;",
                "-import org.apache.commons.io.FileUtils;",
                "-import org.apache.storm.Config;",
                "-import org.apache.storm.daemon.supervisor.AdvancedFSOps;",
                "-import org.apache.storm.generated.StormTopology;",
                "-import org.apache.storm.validation.ConfigValidation;",
                "-",
                " import java.io.File;",
                "@@ -32,2 +26,3 @@ import java.util.Collection;",
                " import java.util.HashMap;",
                "+import java.util.HashSet;",
                " import java.util.List;",
                "@@ -35,3 +30,2 @@ import java.util.Map;",
                " import java.util.Random;",
                "-import java.util.HashSet;",
                " import java.util.concurrent.Callable;",
                "@@ -39,2 +33,9 @@ import java.util.stream.Collectors;",
                "+import org.apache.commons.io.FileUtils;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.daemon.supervisor.AdvancedFSOps;",
                "+import org.apache.storm.generated.StormTopology;",
                "+import org.apache.storm.validation.ConfigValidation;",
                "+",
                "+",
                " public class ConfigUtils {",
                "@@ -81,3 +82,3 @@ public class ConfigUtils {",
                "         Collection<File> ret = readDirFiles(dir);",
                "-        return ret.stream().map( car -> car.getName() ).collect( Collectors.toList() );",
                "+        return ret.stream().map(car -> car.getName()).collect(Collectors.toList());",
                "     }",
                "@@ -435,3 +436,2 @@ public class ConfigUtils {",
                "     }",
                "-",
                " }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/ObjectReader.java b/storm-client/src/jvm/org/apache/storm/utils/ObjectReader.java",
                "index c6e340a0f..07fece1ec 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/utils/ObjectReader.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/ObjectReader.java",
                "@@ -93,3 +93,3 @@ public class ObjectReader {",
                "         } else {",
                "-            throw new IllegalArgumentException(\"Don't know how to convert \" + o + \" to double\");",
                "+            throw new IllegalArgumentException(\"Don't know how to convert (\" + o + \") to double\");",
                "         }",
                "diff --git a/storm-client/src/storm.thrift b/storm-client/src/storm.thrift",
                "index a875e3821..aff7507c1 100644",
                "--- a/storm-client/src/storm.thrift",
                "+++ b/storm-client/src/storm.thrift",
                "@@ -498,2 +498,4 @@ struct WorkerResources {",
                "     5: optional double shared_mem_off_heap; //This is just for accounting mem_off_heap should be used for enforcement",
                "+    6: optional map<string, double> resources; // Generic resources Map",
                "+    7: optional map<string, double> shared_resources; // Shared Generic resources Map",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/LocalCluster.java b/storm-server/src/main/java/org/apache/storm/LocalCluster.java",
                "index fa49d126f..de3053e5b 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/LocalCluster.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/LocalCluster.java",
                "@@ -63,2 +63,4 @@ import org.apache.storm.generated.ListBlobsResult;",
                " import org.apache.storm.generated.LogConfig;",
                "+import org.apache.storm.generated.Nimbus.Iface;",
                "+import org.apache.storm.generated.Nimbus.Processor;",
                " import org.apache.storm.generated.NimbusSummary;",
                "@@ -77,4 +79,2 @@ import org.apache.storm.generated.TopologyInfo;",
                " import org.apache.storm.generated.TopologyPageInfo;",
                "-import org.apache.storm.generated.Nimbus.Iface;",
                "-import org.apache.storm.generated.Nimbus.Processor;",
                " import org.apache.storm.messaging.IContext;",
                "@@ -95,3 +95,2 @@ import org.apache.storm.utils.DRPCClient;",
                " import org.apache.storm.utils.NimbusClient;",
                "-import org.apache.storm.utils.Utils;",
                " import org.apache.storm.utils.ObjectReader;",
                "@@ -101,2 +100,3 @@ import org.apache.storm.utils.Time;",
                " import org.apache.storm.utils.Time.SimulatedTime;",
                "+import org.apache.storm.utils.Utils;",
                " import org.apache.thrift.TException;",
                "@@ -111,6 +111,6 @@ import org.slf4j.LoggerFactory;",
                "  * Apache Storm itself and for people building storm topologies.",
                "- * ",
                "+ *<p>",
                "  * LocalCluster is an AutoCloseable so if you are using it in tests you can use",
                "  * a try block to be sure it is shut down.",
                "- * ",
                "+ * </p>",
                "  * try (LocalCluster cluster = new LocalCluster()) {",
                "@@ -161,3 +161,3 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "         /**",
                "-         * Set the number of slots/ports each supervisor should have",
                "+         * Set the number of slots/ports each supervisor should have.",
                "          */",
                "@@ -182,3 +182,3 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "         /**",
                "-         * Add an single key/value config to the daemon conf",
                "+         * Add an single key/value config to the daemon conf.",
                "          */",
                "@@ -332,2 +332,3 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "         /**",
                "+         * Builds a new LocalCluster.",
                "          * @return the LocalCluster",
                "@@ -342,2 +343,3 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "     private static class TrackedStormCommon extends StormCommon {",
                "+",
                "         private final String id;",
                "@@ -369,3 +371,3 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "     /**",
                "-     * Create a default LocalCluster ",
                "+     * Create a default LocalCluster.",
                "      * @throws Exception on any error",
                "@@ -377,3 +379,3 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "     /**",
                "-     * Create a LocalCluster that connects to an existing Zookeeper instance",
                "+     * Create a LocalCluster that connects to an existing Zookeeper instance.",
                "      * @param zkHost the host for ZK",
                "@@ -482,3 +484,9 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "     }",
                "-    ",
                "+",
                "+    /**",
                "+     * Checks if Nimbuses have elected a leader.",
                "+     * @return boolean",
                "+     * @throws AuthorizationException",
                "+     * @throws TException",
                "+     */",
                "     private boolean hasLeader() throws AuthorizationException, TException {",
                "@@ -510,2 +518,3 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "     public static final KillOptions KILL_NOW = new KillOptions();",
                "+",
                "     static {",
                "@@ -922,3 +931,3 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "     @Override",
                "-    public List<ProfileRequest> getComponentPendingProfileActions(String id, String component_id, ProfileAction action)",
                "+    public List<ProfileRequest> getComponentPendingProfileActions(String id, String componentId, ProfileAction action)",
                "             throws TException {",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index d9020e2ad..63a143d81 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -56,2 +56,3 @@ import javax.security.auth.Subject;",
                " import org.apache.storm.Config;",
                "+import org.apache.storm.Constants;",
                " import org.apache.storm.DaemonConfig;",
                "@@ -2567,4 +2568,4 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "             double memoryRequirement =",
                "-                entry.getOrDefault(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, 0.0)",
                "-                    + entry.getOrDefault(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, 0.0);",
                "+                entry.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "+                    + entry.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "             if (memoryRequirement > largestMemoryOperator) {",
                "@@ -2576,4 +2577,4 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "             double memoryRequirement =",
                "-                entry.getOrDefault(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, 0.0)",
                "-                    + entry.getOrDefault(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, 0.0);",
                "+                entry.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "+                    + entry.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "             if (memoryRequirement > largestMemoryOperator) {",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "index c49f50bd5..26fb72923 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "@@ -28,2 +28,4 @@ import org.apache.storm.utils.Time;",
                " import java.util.ArrayList;",
                "+import java.util.Collection;",
                "+import java.util.Collections;",
                " import java.util.HashMap;",
                "@@ -32,2 +34,4 @@ import java.util.Map;",
                "+import static org.apache.storm.scheduler.resource.ResourceUtils.normalizedResourceMap;",
                "+",
                " public class SupervisorHeartbeat implements Runnable {",
                "@@ -75,2 +79,3 @@ public class SupervisorHeartbeat implements Runnable {",
                "         Map<String, Double> ret = new HashMap<String, Double>();",
                "+        // Put in legacy values",
                "         Double mem = ObjectReader.getDouble(conf.get(Config.SUPERVISOR_MEMORY_CAPACITY_MB), 4096.0);",
                "@@ -79,3 +84,14 @@ public class SupervisorHeartbeat implements Runnable {",
                "         ret.put(Config.SUPERVISOR_CPU_CAPACITY, cpu);",
                "-        return ret;",
                "+",
                "+",
                "+        // If configs are present in Generic map and legacy - the legacy values will be overwritten",
                "+        Map<String, Number> rawResourcesMap = (Map<String,Number>) conf.getOrDefault(",
                "+                Config.SUPERVISOR_RESOURCES_MAP, Collections.emptyMap()",
                "+        );",
                "+",
                "+        for (Map.Entry<String, Number> stringNumberEntry : rawResourcesMap.entrySet()) {",
                "+            ret.put(stringNumberEntry.getKey(), stringNumberEntry.getValue().doubleValue());",
                "+        }",
                "+",
                "+        return normalizedResourceMap(ret);",
                "     }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/logging/ThriftAccessLogger.java b/storm-server/src/main/java/org/apache/storm/logging/ThriftAccessLogger.java",
                "index 84540a8ae..e326174d7 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/logging/ThriftAccessLogger.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/logging/ThriftAccessLogger.java",
                "@@ -17,3 +17,5 @@",
                "  */",
                "+",
                " package org.apache.storm.logging;",
                "+",
                " import java.net.InetAddress;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "index 1b30e8d93..93e4dfaba 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "@@ -33,2 +33,3 @@ import java.util.Set;",
                " import org.apache.storm.Config;",
                "+import org.apache.storm.Constants;",
                " import org.apache.storm.daemon.nimbus.TopologyResources;",
                "@@ -44,4 +45,7 @@ import org.slf4j.LoggerFactory;",
                "+import static org.apache.storm.scheduler.resource.ResourceUtils.normalizedResourceMap;",
                "+",
                " public class Cluster implements ISchedulingState {",
                "     private static final Logger LOG = LoggerFactory.getLogger(Cluster.class);",
                "+    private SchedulerAssignmentImpl assignment;",
                "@@ -405,37 +409,74 @@ public class Cluster implements ISchedulingState {",
                "+    private void addResource(Map<String, Double> resourceMap, String resourceName, Double valueToBeAdded) {",
                "+        if (!resourceMap.containsKey(resourceName)) {",
                "+            resourceMap.put(resourceName, 0.0);",
                "+        }",
                "+        Double currentPresent = resourceMap.get(resourceName);",
                "+        resourceMap.put(resourceName, currentPresent + valueToBeAdded);",
                "+    }",
                "+",
                "     private WorkerResources calculateWorkerResources(",
                "         TopologyDetails td, Collection<ExecutorDetails> executors) {",
                "-        double onHeapMem = 0.0;",
                "-        double offHeapMem = 0.0;",
                "-        double cpu = 0.0;",
                "-        double sharedOn = 0.0;",
                "-        double sharedOff = 0.0;",
                "+        Map<String, Double> totalResources = new HashMap<>();",
                "+        Map<String, Double> sharedTotalResources = new HashMap<>();",
                "         for (ExecutorDetails exec : executors) {",
                "-            Double onHeapMemForExec = td.getOnHeapMemoryRequirement(exec);",
                "-            if (onHeapMemForExec != null) {",
                "-                onHeapMem += onHeapMemForExec;",
                "-            }",
                "-            Double offHeapMemForExec = td.getOffHeapMemoryRequirement(exec);",
                "-            if (offHeapMemForExec != null) {",
                "-                offHeapMem += offHeapMemForExec;",
                "+            Map<String, Double> allResources = td.getTotalResources(exec);",
                "+            if (allResources == null) {",
                "+                continue;",
                "             }",
                "-            Double cpuForExec = td.getTotalCpuReqTask(exec);",
                "-            if (cpuForExec != null) {",
                "-                cpu += cpuForExec;",
                "+            for (Entry<String, Double> resource : allResources.entrySet()) {",
                "+",
                "+                if (!totalResources.containsKey(resource.getKey())) {",
                "+                    totalResources.put(resource.getKey(), 0.0);",
                "+                }",
                "+                totalResources.put(",
                "+                        resource.getKey(),",
                "+                        totalResources.get(resource.getKey()) + resource.getValue());",
                "             }",
                "         }",
                "-",
                "+        totalResources = normalizedResourceMap(totalResources);",
                "         for (SharedMemory shared : td.getSharedMemoryRequests(executors)) {",
                "-            onHeapMem += shared.get_on_heap();",
                "-            sharedOn += shared.get_on_heap();",
                "-            offHeapMem += shared.get_off_heap_worker();",
                "-            sharedOff += shared.get_off_heap_worker();",
                "+            addResource(",
                "+                    totalResources,",
                "+                    Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, shared.get_off_heap_worker()",
                "+            );",
                "+            addResource(",
                "+                    totalResources,",
                "+                    Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, shared.get_on_heap()",
                "+            );",
                "+",
                "+            addResource(",
                "+                    sharedTotalResources,",
                "+                    Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, shared.get_off_heap_worker()",
                "+            );",
                "+            addResource(",
                "+                    sharedTotalResources,",
                "+                    Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, shared.get_on_heap()",
                "+            );",
                "         }",
                "-",
                "+        sharedTotalResources = normalizedResourceMap(sharedTotalResources);",
                "         WorkerResources ret = new WorkerResources();",
                "-        ret.set_cpu(cpu);",
                "-        ret.set_mem_on_heap(onHeapMem);",
                "-        ret.set_mem_off_heap(offHeapMem);",
                "-        ret.set_shared_mem_on_heap(sharedOn);",
                "-        ret.set_shared_mem_off_heap(sharedOff);",
                "+        ret.set_resources(totalResources);",
                "+        ret.set_shared_resources(sharedTotalResources);",
                "+",
                "+        ret.set_cpu(",
                "+                totalResources.getOrDefault(",
                "+                        Constants.COMMON_CPU_RESOURCE_NAME, 0.0)",
                "+        );",
                "+        ret.set_mem_off_heap(",
                "+                totalResources.getOrDefault(",
                "+                        Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "+        );",
                "+        ret.set_mem_on_heap(",
                "+                totalResources.getOrDefault(",
                "+                        Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "+        );",
                "+        ret.set_shared_mem_off_heap(",
                "+                sharedTotalResources.getOrDefault(",
                "+                        Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "+        );",
                "+        ret.set_shared_mem_on_heap(",
                "+                sharedTotalResources.getOrDefault(",
                "+                        Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "+        );",
                "         return ret;",
                "@@ -448,31 +489,28 @@ public class Cluster implements ISchedulingState {",
                "         TopologyDetails td,",
                "-        double maxHeap,",
                "-        double memoryAvailable,",
                "-        double cpuAvailable) {",
                "-        //NOTE this is called lots and lots by schedulers, so anything we can do to make it faster is going to help a lot.",
                "-        //CPU is simplest because it does not have odd interactions.",
                "-        double cpuNeeded = td.getTotalCpuReqTask(exec);",
                "-        if (cpuNeeded > cpuAvailable) {",
                "-            if (LOG.isTraceEnabled()) {",
                "-                LOG.trace(\"Could not schedule {}:{} on {} not enough CPU {} > {}\",",
                "-                    td.getName(),",
                "-                    exec,",
                "-                    ws,",
                "-                    cpuNeeded,",
                "-                    cpuAvailable);",
                "-            }",
                "-            //Not enough CPU no need to try any more",
                "-            return false;",
                "-        }",
                "+        Map<String, Double> resourcesAvailable,",
                "+        double maxHeap) {",
                "-        //Lets see if we can make the Memory one fast too, at least in the failure case.",
                "-        //The totalMemReq is not really that accurate because it does not include shared memory, but if it does not fit we know",
                "-        // Even with shared it will not work",
                "-        double minMemNeeded = td.getTotalMemReqTask(exec);",
                "-        if (minMemNeeded > memoryAvailable) {",
                "-            if (LOG.isTraceEnabled()) {",
                "-                LOG.trace(\"Could not schedule {}:{} on {} not enough Mem {} > {}\", td.getName(), exec, ws, minMemNeeded, memoryAvailable);",
                "+        Map<String, Double> requestedResources = td.getTotalResources(exec);",
                "+",
                "+        for (Entry resourceNeededEntry : requestedResources.entrySet()) {",
                "+            String resourceName = resourceNeededEntry.getKey().toString();",
                "+            if (resourceName.equals(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME) ||",
                "+                    resourceName.equals(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME)) {",
                "+                continue;",
                "+            }",
                "+            Double resourceNeeded = ObjectReader.getDouble(resourceNeededEntry.getValue());",
                "+            Double resourceAvailable = ObjectReader.getDouble(resourcesAvailable.get(resourceName), 0.0);",
                "+            if (resourceNeeded > resourceAvailable) {",
                "+                if (LOG.isTraceEnabled()) {",
                "+                    LOG.trace(\"Could not schedule {}:{} on {} not enough {} {} > {}\",",
                "+                            td.getName(),",
                "+                            exec,",
                "+                            ws,",
                "+                            resourceName,",
                "+                            resourceNeeded,",
                "+                            resourceAvailable);",
                "+                }",
                "+                //Not enough resources - stop trying",
                "+                return false;",
                "             }",
                "-            //Not enough minimum MEM no need to try any more",
                "-            return false;",
                "         }",
                "@@ -482,2 +520,3 @@ public class Cluster implements ISchedulingState {",
                "         double afterOnHeap = 0.0;",
                "+",
                "         Set<ExecutorDetails> wouldBeAssigned = new HashSet<>();",
                "@@ -485,2 +524,3 @@ public class Cluster implements ISchedulingState {",
                "         SchedulerAssignmentImpl assignment = assignments.get(td.getId());",
                "+",
                "         if (assignment != null) {",
                "@@ -501,2 +541,5 @@ public class Cluster implements ISchedulingState {",
                "         double memoryAdded = afterTotal - currentTotal;",
                "+        double memoryAvailable = ObjectReader.getDouble(resourcesAvailable.get(",
                "+                Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME), 0.0);",
                "+",
                "         if (memoryAdded > memoryAvailable) {",
                "@@ -609,2 +652,3 @@ public class Cluster implements ISchedulingState {",
                "         String nodeId, SchedulerAssignmentImpl assignment, ExecutorDetails extra) {",
                "+        double memorySharedWithinNode = 0.0;",
                "         TopologyDetails td = topologies.getById(assignment.getTopologyId());",
                "@@ -620,3 +664,2 @@ public class Cluster implements ISchedulingState {",
                "         }",
                "-        double memorySharedWithinNode = 0.0;",
                "         //Now check for overlap on the node",
                "@@ -944,2 +987,29 @@ public class Cluster implements ISchedulingState {",
                "+    @Override",
                "+    public Map<String, Double> getAllScheduledResourcesForNode(String nodeId) {",
                "+        Map<String, Double> totalScheduledResources = new HashMap<>();",
                "+        for (SchedulerAssignmentImpl assignment : assignments.values()) {",
                "+            for (Entry<WorkerSlot, WorkerResources> entry :",
                "+                    assignment.getScheduledResources().entrySet()) {",
                "+                if (nodeId.equals(entry.getKey().getNodeId())) {",
                "+                    WorkerResources resources = entry.getValue();",
                "+                    for (Map.Entry<String, Double> resourceEntry : resources.get_resources().entrySet()) {",
                "+                        Double currentResourceValue = totalScheduledResources.getOrDefault(resourceEntry.getKey(), 0.0);",
                "+                        totalScheduledResources.put(",
                "+                                resourceEntry.getKey(),",
                "+                                currentResourceValue + ObjectReader.getDouble(resourceEntry.getValue()));",
                "+                    }",
                "+",
                "+                }",
                "+            }",
                "+            Double sharedOffHeap = assignment.getNodeIdToTotalSharedOffHeapMemory().get(nodeId);",
                "+            if (sharedOffHeap != null) {",
                "+                String resourceName = Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME;",
                "+                Double currentResourceValue = totalScheduledResources.getOrDefault(resourceName, 0.0);",
                "+                totalScheduledResources.put(resourceName, currentResourceValue + sharedOffHeap);",
                "+            }",
                "+        }",
                "+        return totalScheduledResources;",
                "+    }",
                "+",
                "     @Override",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java b/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "index e37e4b081..b539b1f93 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "@@ -173,5 +173,4 @@ public interface ISchedulingState {",
                "      * @param td the topology detains for this executor",
                "+     * @param resourcesAvailable all the available resources",
                "      * @param maxHeap the maximum heap size for ws",
                "-     * @param memoryAvailable the amount of memory available",
                "-     * @param cpuAvailable the amount of CPU available",
                "      * @return true it fits else false",
                "@@ -182,5 +181,4 @@ public interface ISchedulingState {",
                "         TopologyDetails td,",
                "-        double maxHeap,",
                "-        double memoryAvailable,",
                "-        double cpuAvailable);",
                "+        Map<String, Double> resourcesAvailable,",
                "+        double maxHeap);",
                "@@ -209,2 +207,5 @@ public interface ISchedulingState {",
                "+    /** Get all scheduled resources for node **/",
                "+    Map<String, Double> getAllScheduledResourcesForNode(String nodeId);",
                "+",
                "     /** Get the total amount of CPU resources in cluster. */",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java b/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "index 326440ef9..69c875678 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "@@ -39,2 +39,3 @@ import org.apache.storm.generated.StormTopology;",
                " import org.apache.storm.scheduler.resource.ResourceUtils;",
                "+import org.apache.storm.utils.ConfigUtils;",
                " import org.apache.storm.utils.ObjectReader;",
                "@@ -45,2 +46,4 @@ import org.slf4j.LoggerFactory;",
                "+import static org.apache.storm.scheduler.resource.ResourceUtils.normalizedResourceMap;",
                "+",
                " public class TopologyDetails {",
                "@@ -135,3 +138,3 @@ public class TopologyDetails {",
                "         this.resourceList = new HashMap<>();",
                "-        // Extract bolt memory info",
                "+        // Extract bolt resource info",
                "         if (topology.get_bolts() != null) {",
                "@@ -141,6 +144,6 @@ public class TopologyDetails {",
                "                     ResourceUtils.parseResources(bolt.getValue().get_common().get_json_conf());",
                "-                ResourceUtils.checkInitialization(topologyResources, bolt.getKey(), topologyConf);",
                "+                ResourceUtils.checkInitialization(topologyResources, bolt.getKey(), this.topologyConf);",
                "                 for (Map.Entry<ExecutorDetails, String> anExecutorToComponent :",
                "                     executorToComponent.entrySet()) {",
                "-                    if (bolt.getKey().equals(anExecutorToComponent.getValue())) {",
                "+                    if (bolt.getKey().equals(anExecutorToComponent.getValue()) && !topologyResources.isEmpty()) {",
                "                         resourceList.put(anExecutorToComponent.getKey(), topologyResources);",
                "@@ -150,3 +153,3 @@ public class TopologyDetails {",
                "         }",
                "-        // Extract spout memory info",
                "+        // Extract spout resource info",
                "         if (topology.get_spouts() != null) {",
                "@@ -158,3 +161,3 @@ public class TopologyDetails {",
                "                     executorToComponent.entrySet()) {",
                "-                    if (spout.getKey().equals(anExecutorToComponent.getValue())) {",
                "+                    if (spout.getKey().equals(anExecutorToComponent.getValue()) && !topologyResources.isEmpty()) {",
                "                         resourceList.put(anExecutorToComponent.getKey(), topologyResources);",
                "@@ -170,2 +173,9 @@ public class TopologyDetails {",
                "             if (!resourceList.containsKey(exec)) {",
                "+                LOG.debug(",
                "+                    \"Scheduling {} {} with resource requirement as {}\",",
                "+                    getExecutorToComponent().get(exec),",
                "+                    exec,",
                "+                    topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB),",
                "+                        resourceList.get(exec)",
                "+                    );",
                "                 addDefaultResforExec(exec);",
                "@@ -256,3 +266,3 @@ public class TopologyDetails {",
                "                     .get(exec)",
                "-                    .get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB);",
                "+                    .get(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "         }",
                "@@ -271,3 +281,3 @@ public class TopologyDetails {",
                "                     .get(exec)",
                "-                    .get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB);",
                "+                    .get(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "         }",
                "@@ -291,13 +301,9 @@ public class TopologyDetails {",
                "      * Gets the total memory resource list for a set of tasks that is part of a topology.",
                "-     * @return Map<ExecutorDetails, Double> a map of the total memory requirement for all tasks in topology topoId.",
                "+     * @param executors all executors for a topology",
                "+     * @return Map<ExecutorDetails, Double> ,",
                "+     * a map of the total memory requirement for all tasks in topology topoId.",
                "      */",
                "-    public Map<ExecutorDetails, Double> getTotalMemoryResourceList() {",
                "-        Map<ExecutorDetails, Double> ret = new HashMap<>();",
                "-        for (ExecutorDetails exec : resourceList.keySet()) {",
                "-            ret.put(exec, getTotalMemReqTask(exec));",
                "-        }",
                "-        return ret;",
                "-    }",
                "-",
                "-    public Set<SharedMemory> getSharedMemoryRequests(Collection<ExecutorDetails> executors) {",
                "+    public Set<SharedMemory> getSharedMemoryRequests(",
                "+            Collection<ExecutorDetails> executors",
                "+    ) {",
                "         Set<String> components = new HashSet<>();",
                "@@ -329,5 +335,18 @@ public class TopologyDetails {",
                "     /**",
                "-     * Get the total CPU requirement for executor",
                "+     * Get the total resource requirement for an executor.",
                "+     * @param exec",
                "      * @return Double the total about of cpu requirement for executor",
                "      */",
                "+    public Map<String, Double> getTotalResources(ExecutorDetails exec) {",
                "+        if (hasExecInTopo(exec)) {",
                "+            return this.resourceList.get(exec);",
                "+        }",
                "+        return null;",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the total CPU requirement for executor.",
                "+     * @param exec",
                "+     * @return Map<String, Double> generic resource mapping requirement for the executor",
                "+     */",
                "     public Double getTotalCpuReqTask(ExecutorDetails exec) {",
                "@@ -336,3 +355,3 @@ public class TopologyDetails {",
                "                     .get(exec)",
                "-                    .get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT);",
                "+                    .get(Constants.COMMON_CPU_RESOURCE_NAME);",
                "         }",
                "@@ -421,3 +440,3 @@ public class TopologyDetails {",
                "     /**",
                "-     * get the resources requirements for a executor",
                "+     * get the resources requirements for a executor.",
                "      * @param exec",
                "@@ -433,3 +452,3 @@ public class TopologyDetails {",
                "     /**",
                "-     * Checks if a executor is part of this topology",
                "+     * Checks if a executor is part of this topology.",
                "      * @return Boolean whether or not a certain ExecutorDetail is included in the resourceList.",
                "@@ -441,3 +460,3 @@ public class TopologyDetails {",
                "     /**",
                "-     * add resource requirements for a executor",
                "+     * add resource requirements for a executor.",
                "      */",
                "@@ -452,3 +471,3 @@ public class TopologyDetails {",
                "     /**",
                "-     * Add default resource requirements for a executor",
                "+     * Add default resource requirements for a executor.",
                "      */",
                "@@ -481,2 +500,11 @@ public class TopologyDetails {",
                "+        Map<String,Double> topologyComponentResourcesMap = (",
                "+                Map<String, Double>) this.topologyConf.getOrDefault(",
                "+                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<>()",
                "+        );",
                "+",
                "+        topologyComponentResourcesMap = normalizedResourceMap(topologyComponentResourcesMap);",
                "+",
                "+        LOG.info(\"Scheduling Executor: {} with resource requirement as {}\",",
                "+                exec, topologyComponentResourcesMap);",
                "         LOG.debug(",
                "@@ -489,4 +517,3 @@ public class TopologyDetails {",
                "             topologyConf.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT));",
                "-",
                "-        addResourcesForExec(exec, defaultResourceList);",
                "+        addResourcesForExec(exec, normalizedResourceMap(defaultResourceList));",
                "     }",
                "@@ -528,3 +555,3 @@ public class TopologyDetails {",
                "     /**",
                "-     * initializes member variables",
                "+     * initializes member variables.",
                "      */",
                "@@ -542,3 +569,3 @@ public class TopologyDetails {",
                "     /**",
                "-     * Get the max heap size for a worker used by this topology",
                "+     * Get the max heap size for a worker used by this topology.",
                "      * @return the worker max heap size",
                "@@ -550,3 +577,3 @@ public class TopologyDetails {",
                "     /**",
                "-     * Get the user that submitted this topology",
                "+     * Get the user that submitted this topology.",
                "      */",
                "@@ -557,3 +584,3 @@ public class TopologyDetails {",
                "     /**",
                "-     * get the priority of this topology",
                "+     * get the priority of this topology.",
                "      */",
                "@@ -564,3 +591,3 @@ public class TopologyDetails {",
                "     /**",
                "-     * Get the timestamp of when this topology was launched",
                "+     * Get the timestamp of when this topology was launched.",
                "      */",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "index 56035416b..633fb5c16 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "@@ -28,2 +28,4 @@ import java.util.Set;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.Constants;",
                " import org.apache.storm.scheduler.Cluster;",
                "@@ -33,2 +35,3 @@ import org.apache.storm.scheduler.TopologyDetails;",
                " import org.apache.storm.scheduler.WorkerSlot;",
                "+import org.apache.storm.utils.ObjectReader;",
                " import org.slf4j.Logger;",
                "@@ -48,4 +51,2 @@ public class RAS_Node {",
                "-    private final double totalMemory;",
                "-    private final double totalCpu;",
                "     private final String nodeId;",
                "@@ -56,2 +57,3 @@ public class RAS_Node {",
                "     private final Set<WorkerSlot> originallyFreeSlots;",
                "+    private final Map<String, Double> totalResources;",
                "@@ -89,4 +91,8 @@ public class RAS_Node {",
                "-        totalMemory = isAlive ? getTotalMemoryResources() : 0.0;",
                "-        totalCpu = isAlive ? getTotalCpuResources() : 0.0;",
                "+        if (isAlive) {",
                "+            totalResources = getTotalResources();",
                "+        } else {",
                "+            totalResources = new HashMap<>();",
                "+        }",
                "+",
                "         HashSet<String> freeById = new HashSet<>(slots.keySet());",
                "@@ -363,5 +369,5 @@ public class RAS_Node {",
                "             td,",
                "-            td.getTopologyWorkerMaxHeapSize(),",
                "-            getAvailableMemoryResources(),",
                "-            getAvailableCpuResources());",
                "+            this.getTotalAvailableResources(),",
                "+            td.getTopologyWorkerMaxHeapSize()",
                "+            );",
                "     }",
                "@@ -426,4 +432,48 @@ public class RAS_Node {",
                "     public Double getAvailableMemoryResources() {",
                "-        double used = cluster.getScheduledMemoryForNode(nodeId);",
                "-        return totalMemory - used;",
                "+        Map<String, Double> allAvailableResources = getTotalAvailableResources();",
                "+        return allAvailableResources.getOrDefault(",
                "+                Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, 0.0);",
                "+    }",
                "+",
                "+    /**",
                "+     * Gets total resources for this node.",
                "+     *",
                "+     * @return Map<String, Double> of all resources",
                "+     */",
                "+    public Map<String, Double> getTotalResources() {",
                "+        if (sup != null) {",
                "+            return sup.getTotalResources();",
                "+        } else {",
                "+            return new HashMap<>();",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Gets all available resources for this node.",
                "+     *",
                "+     * @return Map<String, Double> of all resources",
                "+     */",
                "+    public Map<String, Double> getTotalAvailableResources() {",
                "+        if (sup != null) {",
                "+            Map<String, Double> totalResources = sup.getTotalResources();",
                "+            Map<String, Double> scheduledResources = cluster.getAllScheduledResourcesForNode(sup.getId());",
                "+            Map<String, Double> availableResources = new HashMap<>();",
                "+            for (Entry resource : totalResources.entrySet()) {",
                "+                if(resource.getKey() == Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME) {",
                "+                    availableResources.put(resource.getKey().toString(),",
                "+                            ObjectReader.getDouble(resource.getValue())",
                "+                                    - (scheduledResources.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "+                                    + scheduledResources.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0))",
                "+                    );",
                "+                    continue;",
                "+                }",
                "+                availableResources.put(resource.getKey().toString(),",
                "+                        ObjectReader.getDouble(resource.getValue())",
                "+                                - scheduledResources.getOrDefault(resource.getKey(), 0.0));",
                "+",
                "+            }",
                "+            return availableResources;",
                "+        } else {",
                "+            return new HashMap<>();",
                "+        }",
                "     }",
                "@@ -449,3 +499,3 @@ public class RAS_Node {",
                "     public double getAvailableCpuResources() {",
                "-        return totalCpu - cluster.getScheduledCpuForNode(nodeId);",
                "+        return getTotalAvailableResources().getOrDefault(Constants.COMMON_CPU_RESOURCE_NAME, 0.0);",
                "     }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "index 44403eff3..c13dce68e 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "@@ -21,6 +21,10 @@ package org.apache.storm.scheduler.resource;",
                " import java.util.Collection;",
                "+import java.util.Collections;",
                " import java.util.HashMap;",
                "+import java.util.HashSet;",
                " import java.util.Map;",
                "+import java.util.Set;",
                " import org.apache.storm.Config;",
                "+import org.apache.storm.Constants;",
                " import org.apache.storm.generated.Bolt;",
                "@@ -29,2 +33,3 @@ import org.apache.storm.generated.SpoutSpec;",
                " import org.apache.storm.generated.StormTopology;",
                "+import org.apache.storm.utils.ConfigUtils;",
                " import org.apache.storm.utils.ObjectReader;",
                "@@ -36,2 +41,4 @@ import org.slf4j.LoggerFactory;",
                "+import static org.apache.storm.Constants.resourceNameMapping;",
                "+",
                " public class ResourceUtils {",
                "@@ -45,3 +52,6 @@ public class ResourceUtils {",
                "                 Map<String, Double> topologyResources = parseResources(bolt.getValue().get_common().get_json_conf());",
                "-                checkInitialization(topologyResources, bolt.getKey(), topologyConf);",
                "+                checkInitialization(topologyResources, bolt.getValue().toString(), topologyConf);",
                "+                if (LOG.isTraceEnabled()) {",
                "+                    LOG.trace(\"Turned {} into {}\", bolt.getValue().get_common().get_json_conf(), topologyResources);",
                "+                }",
                "                 boltResources.put(bolt.getKey(), topologyResources);",
                "@@ -58,3 +68,6 @@ public class ResourceUtils {",
                "                 Map<String, Double> topologyResources = parseResources(spout.getValue().get_common().get_json_conf());",
                "-                checkInitialization(topologyResources, spout.getKey(), topologyConf);",
                "+                checkInitialization(topologyResources, spout.getValue().toString(), topologyConf);",
                "+                if (LOG.isTraceEnabled()) {",
                "+                    LOG.trace(\"Turned {} into {}\", spout.getValue().get_common().get_json_conf(), topologyResources);",
                "+                }",
                "                 spoutResources.put(spout.getKey(), topologyResources);",
                "@@ -130,31 +143,38 @@ public class ResourceUtils {",
                "-    public static void checkInitialization(Map<String, Double> topologyResources, String com,",
                "-                                           Map<String, Object> topologyConf) {",
                "-        checkInitMem(topologyResources, com, topologyConf);",
                "-        checkInitCpu(topologyResources, com, topologyConf);",
                "-    }",
                "+    public static void checkInitialization(Map<String, Double> topologyResources,",
                "+                                           String componentId, Map<String, Object> topologyConf) {",
                "+        StringBuilder msgBuilder = new StringBuilder();",
                "-    private static void checkInitMem(Map<String, Double> topologyResources, String com,",
                "-                                     Map<String, Object> topologyConf) {",
                "-        if (!topologyResources.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB)) {",
                "-            Double onHeap = ObjectReader.getDouble(",
                "-                topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB), null);",
                "-            if (onHeap != null) {",
                "-                topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, onHeap);",
                "-                LOG.debug(",
                "-                    \"Unable to extract resource requirement for Component {}\\n\"",
                "-                        + \" Resource : Memory Type : On Heap set to default {}\",",
                "-                    com, onHeap);",
                "-            }",
                "+        Set<String> resourceNameSet = new HashSet<>();",
                "+",
                "+        resourceNameSet.add(",
                "+                Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT",
                "+        );",
                "+        resourceNameSet.add(",
                "+                Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB",
                "+        );",
                "+        resourceNameSet.add(",
                "+                Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB",
                "+        );",
                "+",
                "+        Map<String, Double> topologyComponentResourcesMap =",
                "+                (Map<String, Double>) topologyConf.getOrDefault(",
                "+                        Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, Collections.emptyMap());",
                "+",
                "+        resourceNameSet.addAll(topologyResources.keySet());",
                "+        resourceNameSet.addAll(topologyComponentResourcesMap.keySet());",
                "+",
                "+        for (String resourceName : resourceNameSet) {",
                "+            msgBuilder.append(checkInitResource(topologyResources, topologyConf, topologyComponentResourcesMap, resourceName));",
                "         }",
                "-        if (!topologyResources.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB)) {",
                "-            Double offHeap = ObjectReader.getDouble(",
                "-                topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB), null);",
                "-            if (offHeap != null) {",
                "-                topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, offHeap);",
                "-                LOG.debug(",
                "-                    \"Unable to extract resource requirement for Component {}\\n\"",
                "-                        + \" Resource : Memory Type : Off Heap set to default {}\",",
                "-                    com, offHeap);",
                "-            }",
                "+",
                "+        Map<String, Double> normalizedTopologyResources = normalizedResourceMap(topologyResources);",
                "+        topologyResources.clear();",
                "+        topologyResources.putAll(normalizedTopologyResources);",
                "+",
                "+        if (msgBuilder.length() > 0) {",
                "+            String resourceDefaults = msgBuilder.toString();",
                "+            LOG.debug(",
                "+                    \"Unable to extract resource requirement for Component {} \\n Resources : {}\",",
                "+                    componentId, resourceDefaults);",
                "         }",
                "@@ -162,14 +182,23 @@ public class ResourceUtils {",
                "-    private static void checkInitCpu(Map<String, Double> topologyResources, String com,",
                "-                                     Map<String, Object> topologyConf) {",
                "-        if (!topologyResources.containsKey(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT)) {",
                "-            Double cpu = ObjectReader.getDouble(topologyConf.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT), null);",
                "-            if (cpu != null) {",
                "-                topologyResources.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, cpu);",
                "-                LOG.debug(",
                "-                    \"Unable to extract resource requirement for Component {}\\n\"",
                "-                        + \" Resource : CPU Pcore Percent set to default {}\",",
                "-                    com, cpu);",
                "+    private static String checkInitResource(Map<String, Double> topologyResources, Map topologyConf,",
                "+                                            Map<String, Double> topologyComponentResourcesMap, String resourceName) {",
                "+        StringBuilder msgBuilder = new StringBuilder();",
                "+        String normalizedResourceName = resourceNameMapping.getOrDefault(resourceName, resourceName);",
                "+        if (!topologyResources.containsKey(normalizedResourceName)) {",
                "+            if (topologyConf.containsKey(resourceName)) {",
                "+                Double resourceValue = ObjectReader.getDouble(topologyConf.get(resourceName));",
                "+                if (resourceValue != null) {",
                "+                    topologyResources.put(normalizedResourceName, resourceValue);",
                "+                }",
                "+            }",
                "+",
                "+            if (topologyComponentResourcesMap.containsKey(normalizedResourceName)) {",
                "+                Double resourceValue = ObjectReader.getDouble(topologyComponentResourcesMap.get(resourceName));",
                "+                if (resourceValue != null) {",
                "+                    topologyResources.put(normalizedResourceName, resourceValue);",
                "+                }",
                "             }",
                "         }",
                "+",
                "+        return msgBuilder.toString();",
                "     }",
                "@@ -184,2 +213,4 @@ public class ResourceUtils {",
                "                 JSONObject jsonObject = (JSONObject) obj;",
                "+",
                "+                // Legacy resource parsing",
                "                 if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB)) {",
                "@@ -199,3 +230,16 @@ public class ResourceUtils {",
                "                 }",
                "-                LOG.debug(\"Topology Resources {}\", topologyResources);",
                "+",
                "+                // If resource is also present in resources map will overwrite the above",
                "+                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "+                    Map<String, Number> rawResourcesMap =",
                "+                            (Map<String, Number>) jsonObject.getOrDefault(",
                "+                                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, Collections.emptyMap());",
                "+",
                "+                    for (Map.Entry<String, Number> stringNumberEntry : rawResourcesMap.entrySet()) {",
                "+                        topologyResources.put(",
                "+                                stringNumberEntry.getKey(), stringNumberEntry.getValue().doubleValue());",
                "+                    }",
                "+",
                "+",
                "+                }",
                "             }",
                "@@ -205,3 +249,3 @@ public class ResourceUtils {",
                "         }",
                "-        return topologyResources;",
                "+        return normalizedResourceMap(topologyResources);",
                "     }",
                "@@ -229,2 +273,55 @@ public class ResourceUtils {",
                "     }",
                "+",
                "+    /**",
                "+     * Normalizes a supervisor resource map or topology details map's keys to universal resource names.",
                "+     * @param resourceMap resource map of either Supervisor or Topology",
                "+     * @return the resource map with common resource names",
                "+     */",
                "+    public static Map<String, Double> normalizedResourceMap(Map<String, Double> resourceMap) {",
                "+        Map<String, Double> result = new HashMap();",
                "+",
                "+        result.putAll(resourceMap);",
                "+        for (Map.Entry entry: resourceMap.entrySet()) {",
                "+            if (resourceNameMapping.containsKey(entry.getKey())) {",
                "+                result.put(resourceNameMapping.get(entry.getKey()), ObjectReader.getDouble(entry.getValue(), 0.0));",
                "+                result.remove(entry.getKey());",
                "+            }",
                "+        }",
                "+        return result;",
                "+    }",
                "+",
                "+    public static Map<String, Double> addResources(Map<String, Double> resourceMap1, Map<String, Double> resourceMap2) {",
                "+        Map<String, Double> result = new HashMap();",
                "+",
                "+        result.putAll(resourceMap1);",
                "+",
                "+        for (Map.Entry<String, Double> entry: resourceMap2.entrySet()) {",
                "+            if (result.containsKey(entry.getKey())) {",
                "+                result.put(entry.getKey(), ObjectReader.getDouble(entry.getValue(),",
                "+                        0.0) + ObjectReader.getDouble(resourceMap1.get(entry.getKey()), 0.0));",
                "+            } else {",
                "+                result.put(entry.getKey(), entry.getValue());",
                "+            }",
                "+        }",
                "+        return result;",
                "+",
                "+    }",
                "+",
                "+    public static Double getMinValuePresentInResourceMap(Map<String, Double> resourceMap) {",
                "+        return Collections.min(resourceMap.values());",
                "+    }",
                "+",
                "+    public static Map<String, Double> getPercentageOfTotalResourceMap(Map<String, Double> resourceMap, Map<String, Double> totalResourceMap) {",
                "+        Map<String, Double> result = new HashMap();",
                "+",
                "+        for(Map.Entry<String, Double> entry: totalResourceMap.entrySet()) {",
                "+            if (resourceMap.containsKey(entry.getKey())) {",
                "+                result.put(entry.getKey(),  (ObjectReader.getDouble(resourceMap.get(entry.getKey()))/ entry.getValue()) * 100.0) ;",
                "+            } else {",
                "+                result.put(entry.getKey(), 0.0);",
                "+            }",
                "+        }",
                "+        return result;",
                "+",
                "+    }",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "new file mode 100644",
                "index 000000000..07c908e95",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "@@ -0,0 +1,610 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource.strategies.scheduling;",
                "+",
                "+import com.google.common.annotations.VisibleForTesting;",
                "+",
                "+import java.util.ArrayList;",
                "+import java.util.Collection;",
                "+import java.util.Collections;",
                "+import java.util.HashMap;",
                "+import java.util.HashSet;",
                "+import java.util.LinkedList;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import java.util.Queue;",
                "+import java.util.Set;",
                "+import java.util.TreeSet;",
                "+",
                "+import org.apache.storm.executor.Executor;",
                "+import org.apache.storm.generated.ComponentType;",
                "+import org.apache.storm.scheduler.Cluster;",
                "+import org.apache.storm.scheduler.Component;",
                "+import org.apache.storm.scheduler.ExecutorDetails;",
                "+import org.apache.storm.scheduler.TopologyDetails;",
                "+import org.apache.storm.scheduler.WorkerSlot;",
                "+import org.apache.storm.scheduler.resource.RAS_Node;",
                "+import org.apache.storm.scheduler.resource.RAS_Nodes;",
                "+import org.apache.storm.scheduler.resource.ResourceUtils;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(BaseResourceAwareStrategy.class);",
                "+    protected Cluster cluster;",
                "+    private Map<String, List<String>> networkTopography;",
                "+    protected RAS_Nodes nodes;",
                "+",
                "+    @VisibleForTesting",
                "+    void prepare(Cluster cluster) {",
                "+        this.cluster = cluster;",
                "+        nodes = new RAS_Nodes(cluster);",
                "+        networkTopography = cluster.getNetworkTopography();",
                "+        logClusterInfo();",
                "+    }",
                "+",
                "+    @Override",
                "+    public void prepare(Map<String, Object> config) {",
                "+        //NOOP",
                "+    }",
                "+",
                "+    /**",
                "+     * Schedule executor exec from topology td.",
                "+     *",
                "+     * @param exec the executor to schedule",
                "+     * @param td the topology executor exec is a part of",
                "+     * @param scheduledTasks executors that have been scheduled",
                "+     */",
                "+    protected void scheduleExecutor(",
                "+            ExecutorDetails exec, TopologyDetails td, Collection<ExecutorDetails> scheduledTasks, List<ObjectResources> sortedNodes) {",
                "+        WorkerSlot targetSlot = findWorkerForExec(exec, td, sortedNodes);",
                "+        if (targetSlot != null) {",
                "+            RAS_Node targetNode = idToNode(targetSlot.getNodeId());",
                "+            targetNode.assignSingleExecutor(targetSlot, exec, td);",
                "+            scheduledTasks.add(exec);",
                "+            LOG.debug(",
                "+                    \"TASK {} assigned to Node: {} avail [ mem: {} cpu: {} ] total [ mem: {} cpu: {} ] on \"",
                "+                            + \"slot: {} on Rack: {}\",",
                "+                    exec,",
                "+                    targetNode.getHostname(),",
                "+                    targetNode.getAvailableMemoryResources(),",
                "+                    targetNode.getAvailableCpuResources(),",
                "+                    targetNode.getTotalMemoryResources(),",
                "+                    targetNode.getTotalCpuResources(),",
                "+                    targetSlot,",
                "+                    nodeToRack(targetNode));",
                "+        } else {",
                "+            LOG.error(\"Not Enough Resources to schedule Task {}\", exec);",
                "+        }",
                "+    }",
                "+",
                "+    protected abstract TreeSet<ObjectResources> sortObjectResources(",
                "+            final AllResources allResources, ExecutorDetails exec, TopologyDetails topologyDetails,",
                "+            final ExistingScheduleFunc existingScheduleFunc",
                "+    );",
                "+    /**",
                "+     * Find a worker to schedule executor exec on.",
                "+     *",
                "+     * @param exec the executor to schedule",
                "+     * @param td the topology that the executor is a part of",
                "+     * @return a worker to assign exec on. Returns null if a worker cannot be successfully found in cluster",
                "+     */",
                "+    protected WorkerSlot findWorkerForExec(ExecutorDetails exec, TopologyDetails td, List<ObjectResources> sortedNodes) {",
                "+        for (ObjectResources nodeResources : sortedNodes) {",
                "+            RAS_Node node = nodes.getNodeById(nodeResources.id);",
                "+            for (WorkerSlot ws : node.getSlotsAvailbleTo(td)) {",
                "+                if (node.wouldFit(ws, exec, td)) {",
                "+                    return ws;",
                "+                }",
                "+            }",
                "+        }",
                "+        return null;",
                "+    }",
                "+",
                "+    /**",
                "+     * interface for calculating the number of existing executors scheduled on a object (rack or",
                "+     * node).",
                "+     */",
                "+    protected interface ExistingScheduleFunc {",
                "+        int getNumExistingSchedule(String objectId);",
                "+    }",
                "+",
                "+    /**",
                "+     * a class to contain individual object resources as well as cumulative stats.",
                "+     */",
                "+    static class AllResources {",
                "+        List<ObjectResources> objectResources = new LinkedList<ObjectResources>();",
                "+        Map<String, Double> availableResourcesOverall = new HashMap<>();",
                "+        Map<String, Double> totalResourcesOverall = new HashMap<>();",
                "+        String identifier;",
                "+",
                "+        public AllResources(String identifier) {",
                "+            this.identifier = identifier;",
                "+        }",
                "+",
                "+        public AllResources(AllResources other) {",
                "+            this(   null,",
                "+                    new HashMap<>(other.availableResourcesOverall),",
                "+                    new HashMap(other.totalResourcesOverall),",
                "+                    other.identifier);",
                "+            List<ObjectResources> objectResourcesList = new ArrayList<>();",
                "+            for (ObjectResources objectResource : other.objectResources) {",
                "+                objectResourcesList.add(new ObjectResources(objectResource));",
                "+            }",
                "+            this.objectResources = objectResourcesList;",
                "+",
                "+        }",
                "+",
                "+        public AllResources(List<ObjectResources> objectResources, Map<String, Double> availableResourcesOverall, Map<String, Double> totalResourcesOverall, String identifier) {",
                "+            this.objectResources = objectResources;",
                "+            this.availableResourcesOverall = availableResourcesOverall;",
                "+            this.totalResourcesOverall = totalResourcesOverall;",
                "+            this.identifier = identifier;",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * class to keep track of resources on a rack or node.",
                "+     */",
                "+    static class ObjectResources {",
                "+        String id;",
                "+        Map<String, Double> availableResources = new HashMap<>();",
                "+        Map<String, Double> totalResources = new HashMap<>();",
                "+        double effectiveResources = 0.0;",
                "+",
                "+        public ObjectResources(String id) {",
                "+            this.id = id;",
                "+        }",
                "+",
                "+        public ObjectResources(ObjectResources other) {",
                "+            this(other.id, other.availableResources, other.totalResources, other.effectiveResources);",
                "+        }",
                "+",
                "+        public ObjectResources(String id, Map<String, Double> availableResources, Map<String, Double> totalResources, double effectiveResources) {",
                "+            this.id = id;",
                "+            this.availableResources = availableResources;",
                "+            this.totalResources = totalResources;",
                "+            this.effectiveResources = effectiveResources;",
                "+        }",
                "+",
                "+        @Override",
                "+        public String toString() {",
                "+            return this.id;",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Nodes are sorted by two criteria.",
                "+     *",
                "+     * <p>1) the number executors of the topology that needs to be scheduled is already on the node in",
                "+     * descending order. The reasoning to sort based on criterion 1 is so we schedule the rest of a",
                "+     * topology on the same node as the existing executors of the topology.",
                "+     *",
                "+     * <p>2) the subordinate/subservient resource availability percentage of a node in descending",
                "+     * order We calculate the resource availability percentage by dividing the resource availability",
                "+     * that have exhausted or little of one of the resources mentioned above will be ranked after",
                "+     * on the node by the resource availability of the entire rack By doing this calculation, nodes",
                "+     * nodes that have more balanced resource availability. So we will be less likely to pick a node",
                "+     * that have a lot of one resource but a low amount of another.",
                "+     *",
                "+     * @param availNodes a list of all the nodes we want to sort",
                "+     * @param rackId the rack id availNodes are a part of",
                "+     * @return a sorted list of nodes.",
                "+     */",
                "+    protected TreeSet<ObjectResources> sortNodes(",
                "+            List<RAS_Node> availNodes, ExecutorDetails exec, TopologyDetails topologyDetails, String rackId) {",
                "+        AllResources allResources = new AllResources(\"RACK\");",
                "+        List<ObjectResources> nodes = allResources.objectResources;",
                "+",
                "+        for (RAS_Node rasNode : availNodes) {",
                "+            String nodeId = rasNode.getId();",
                "+            ObjectResources node = new ObjectResources(nodeId);",
                "+",
                "+            node.availableResources = rasNode.getTotalAvailableResources();",
                "+            node.totalResources = rasNode.getTotalResources();",
                "+",
                "+            nodes.add(node);",
                "+            allResources.availableResourcesOverall = ResourceUtils.addResources(",
                "+                    allResources.availableResourcesOverall, node.availableResources);",
                "+            allResources.totalResourcesOverall = ResourceUtils.addResources(",
                "+                    allResources.totalResourcesOverall, node.totalResources);",
                "+",
                "+        }",
                "+",
                "+        LOG.debug(",
                "+            \"Rack {}: Overall Avail [ {} ] Total [ {} ]\",",
                "+            rackId,",
                "+            allResources.availableResourcesOverall,",
                "+            allResources.totalResourcesOverall);",
                "+",
                "+        String topoId = topologyDetails.getId();",
                "+        return this.sortObjectResources(",
                "+            allResources,",
                "+            exec,",
                "+            topologyDetails,",
                "+            new ExistingScheduleFunc() {",
                "+                @Override",
                "+                public int getNumExistingSchedule(String objectId) {",
                "+",
                "+                    //Get execs already assigned in rack",
                "+                    Collection<ExecutorDetails> execs = new LinkedList<ExecutorDetails>();",
                "+                    if (cluster.getAssignmentById(topoId) != null) {",
                "+                        for (Map.Entry<ExecutorDetails, WorkerSlot> entry :",
                "+                            cluster.getAssignmentById(topoId).getExecutorToSlot().entrySet()) {",
                "+                            WorkerSlot workerSlot = entry.getValue();",
                "+                            ExecutorDetails exec = entry.getKey();",
                "+                            if (workerSlot.getNodeId().equals(objectId)) {",
                "+                                execs.add(exec);",
                "+                            }",
                "+                        }",
                "+                    }",
                "+                    return execs.size();",
                "+                }",
                "+            });",
                "+    }",
                "+",
                "+    protected List<ObjectResources> sortAllNodes(TopologyDetails td, ExecutorDetails exec,",
                "+                                                 List<String> favoredNodes, List<String> unFavoredNodes) {",
                "+        TreeSet<ObjectResources> sortedRacks = sortRacks(exec, td);",
                "+        ArrayList<ObjectResources> totallySortedNodes = new ArrayList<>();",
                "+        for (ObjectResources rack : sortedRacks) {",
                "+            final String rackId = rack.id;",
                "+            TreeSet<ObjectResources> sortedNodes = sortNodes(",
                "+                    getAvailableNodesFromRack(rackId), exec, td, rackId);",
                "+            totallySortedNodes.addAll(sortedNodes);",
                "+        }",
                "+        //Now do some post processing to add make some nodes preferred over others.",
                "+        if (favoredNodes != null || unFavoredNodes != null) {",
                "+            HashMap<String, Integer> hostOrder = new HashMap<>();",
                "+            if (favoredNodes != null) {",
                "+                int size = favoredNodes.size();",
                "+                for (int i = 0; i < size; i++) {",
                "+                    //First in the list is the most desired so gets the Lowest possible value",
                "+                    hostOrder.put(favoredNodes.get(i), -(size - i));",
                "+                }",
                "+            }",
                "+            if (unFavoredNodes != null) {",
                "+                int size = unFavoredNodes.size();",
                "+                for (int i = 0; i < size; i++) {",
                "+                    //First in the list is the least desired so gets the highest value",
                "+                    hostOrder.put(unFavoredNodes.get(i), size - i);",
                "+                }",
                "+            }",
                "+            //java guarantees a stable sort so we can just return 0 for values we don't want to move.",
                "+            Collections.sort(totallySortedNodes, (o1, o2) -> {",
                "+                RAS_Node n1 = this.nodes.getNodeById(o1.id);",
                "+                String host1 = n1.getHostname();",
                "+                int h1Value = hostOrder.getOrDefault(host1, 0);",
                "+",
                "+                RAS_Node n2 = this.nodes.getNodeById(o2.id);",
                "+                String host2 = n2.getHostname();",
                "+                int h2Value = hostOrder.getOrDefault(host2, 0);",
                "+",
                "+                return Integer.compare(h1Value, h2Value);",
                "+            });",
                "+        }",
                "+        return totallySortedNodes;",
                "+    }",
                "+",
                "+    /**",
                "+     * Racks are sorted by two criteria.",
                "+     *",
                "+     * <p>1) the number executors of the topology that needs to be scheduled is already on the rack in descending order.",
                "+     * The reasoning to sort based on criterion 1 is so we schedule the rest of a topology on the same rack as the",
                "+     * existing executors of the topology.",
                "+     *",
                "+     * <p>2) the subordinate/subservient resource availability percentage of a rack in descending order We calculate",
                "+     * the resource availability percentage by dividing the resource availability on the rack by the resource",
                "+     * availability of the  entire cluster By doing this calculation, racks that have exhausted or little of one of",
                "+     * the resources mentioned above will be ranked after racks that have more balanced resource availability. So we",
                "+     * will be less likely to pick a rack that have a lot of one resource but a low amount of another.",
                "+     *",
                "+     * @return a sorted list of racks",
                "+     */",
                "+    @VisibleForTesting",
                "+    TreeSet<ObjectResources> sortRacks(ExecutorDetails exec, TopologyDetails topologyDetails) {",
                "+        AllResources allResources = new AllResources(\"Cluster\");",
                "+        List<ObjectResources> racks = allResources.objectResources;",
                "+",
                "+        final Map<String, String> nodeIdToRackId = new HashMap<String, String>();",
                "+",
                "+        for (Map.Entry<String, List<String>> entry : networkTopography.entrySet()) {",
                "+            String rackId = entry.getKey();",
                "+            List<String> nodeIds = entry.getValue();",
                "+            ObjectResources rack = new ObjectResources(rackId);",
                "+            racks.add(rack);",
                "+            for (String nodeId : nodeIds) {",
                "+                RAS_Node node = nodes.getNodeById(nodeHostnameToId(nodeId));",
                "+                rack.availableResources = ResourceUtils.addResources(rack.availableResources, node.getTotalAvailableResources());",
                "+                rack.totalResources = ResourceUtils.addResources(rack.totalResources, node.getTotalResources());",
                "+",
                "+                nodeIdToRackId.put(nodeId, rack.id);",
                "+",
                "+                allResources.totalResourcesOverall = ResourceUtils.addResources(allResources.totalResourcesOverall, rack.totalResources);",
                "+                allResources.availableResourcesOverall = ResourceUtils.addResources(allResources.availableResourcesOverall, rack.availableResources);",
                "+",
                "+            }",
                "+        }",
                "+        LOG.debug(",
                "+            \"Cluster Overall Avail [ {} ] Total [ {} ]\",",
                "+            allResources.availableResourcesOverall,",
                "+            allResources.totalResourcesOverall);",
                "+",
                "+        String topoId = topologyDetails.getId();",
                "+        return this.sortObjectResources(",
                "+            allResources,",
                "+            exec,",
                "+            topologyDetails,",
                "+            new ExistingScheduleFunc() {",
                "+                @Override",
                "+                public int getNumExistingSchedule(String objectId) {",
                "+                    String rackId = objectId;",
                "+                    //Get execs already assigned in rack",
                "+                    Collection<ExecutorDetails> execs = new LinkedList<ExecutorDetails>();",
                "+                    if (cluster.getAssignmentById(topoId) != null) {",
                "+                        for (Map.Entry<ExecutorDetails, WorkerSlot> entry :",
                "+                            cluster.getAssignmentById(topoId).getExecutorToSlot().entrySet()) {",
                "+                            String nodeId = entry.getValue().getNodeId();",
                "+                            String hostname = idToNode(nodeId).getHostname();",
                "+                            ExecutorDetails exec = entry.getKey();",
                "+                            if (nodeIdToRackId.get(hostname) != null",
                "+                                && nodeIdToRackId.get(hostname).equals(rackId)) {",
                "+                                execs.add(exec);",
                "+                            }",
                "+                        }",
                "+                    }",
                "+                    return execs.size();",
                "+                }",
                "+            });",
                "+    }",
                "+",
                "+",
                "+    /**",
                "+     * Get the rack on which a node is a part of.",
                "+     *",
                "+     * @param node the node to find out which rack its on",
                "+     * @return the rack id",
                "+     */",
                "+    protected String nodeToRack(RAS_Node node) {",
                "+        for (Map.Entry<String, List<String>> entry : networkTopography.entrySet()) {",
                "+            if (entry.getValue().contains(node.getHostname())) {",
                "+                return entry.getKey();",
                "+            }",
                "+        }",
                "+        LOG.error(\"Node: {} not found in any racks\", node.getHostname());",
                "+        return null;",
                "+    }",
                "+",
                "+    /**",
                "+     * get a list nodes from a rack.",
                "+     *",
                "+     * @param rackId the rack id of the rack to get nodes from",
                "+     * @return a list of nodes",
                "+     */",
                "+    protected List<RAS_Node> getAvailableNodesFromRack(String rackId) {",
                "+        List<RAS_Node> retList = new ArrayList<>();",
                "+        for (String nodeId : networkTopography.get(rackId)) {",
                "+            retList.add(nodes.getNodeById(this.nodeHostnameToId(nodeId)));",
                "+        }",
                "+        return retList;",
                "+    }",
                "+",
                "+    /**",
                "+     * sort components by the number of in and out connections that need to be made, in descending order.",
                "+     *",
                "+     * @param componentMap The components that need to be sorted",
                "+     * @return a sorted set of components",
                "+     */",
                "+    private Set<Component> sortComponents(final Map<String, Component> componentMap) {",
                "+        Set<Component> sortedComponents =",
                "+            new TreeSet<>((o1, o2) -> {",
                "+                int connections1 = 0;",
                "+                int connections2 = 0;",
                "+",
                "+                for (String childId : union(o1.getChildren(), o1.getParents())) {",
                "+                    connections1 +=",
                "+                        (componentMap.get(childId).getExecs().size() * o1.getExecs().size());",
                "+                }",
                "+",
                "+                for (String childId : union(o2.getChildren(), o2.getParents())) {",
                "+                    connections2 +=",
                "+                        (componentMap.get(childId).getExecs().size() * o2.getExecs().size());",
                "+                }",
                "+",
                "+                if (connections1 > connections2) {",
                "+                    return -1;",
                "+                } else if (connections1 < connections2) {",
                "+                    return 1;",
                "+                } else {",
                "+                    return o1.getId().compareTo(o2.getId());",
                "+                }",
                "+            });",
                "+        sortedComponents.addAll(componentMap.values());",
                "+        return sortedComponents;",
                "+    }",
                "+",
                "+    private static <T> Set<T> union(Set<T> a, Set<T> b) {",
                "+        HashSet<T> ret = new HashSet<>(a);",
                "+        ret.addAll(b);",
                "+        return ret;",
                "+    }",
                "+",
                "+    /**",
                "+     * Sort a component's neighbors by the number of connections it needs to make with this component.",
                "+     *",
                "+     * @param thisComp the component that we need to sort its neighbors",
                "+     * @param componentMap all the components to sort",
                "+     * @return a sorted set of components",
                "+     */",
                "+    private Set<Component> sortNeighbors(",
                "+        final Component thisComp, final Map<String, Component> componentMap) {",
                "+        Set<Component> sortedComponents =",
                "+            new TreeSet<>((o1, o2) -> {",
                "+                int connections1 = o1.getExecs().size() * thisComp.getExecs().size();",
                "+                int connections2 = o2.getExecs().size() * thisComp.getExecs().size();",
                "+                if (connections1 < connections2) {",
                "+                    return -1;",
                "+                } else if (connections1 > connections2) {",
                "+                    return 1;",
                "+                } else {",
                "+                    return o1.getId().compareTo(o2.getId());",
                "+                }",
                "+            });",
                "+        sortedComponents.addAll(componentMap.values());",
                "+        return sortedComponents;",
                "+    }",
                "+",
                "+    /**",
                "+     * Order executors based on how many in and out connections it will potentially need to make, in descending order.",
                "+     * First order components by the number of in and out connections it will have.  Then iterate through the sorted list of components.",
                "+     * For each component sort the neighbors of that component by how many connections it will have to make with that component.",
                "+     * Add an executor from this component and then from each neighboring component in sorted order.",
                "+     * Do this until there is nothing left to schedule.",
                "+     *",
                "+     * @param td The topology the executors belong to",
                "+     * @param unassignedExecutors a collection of unassigned executors that need to be unassigned. Should only try to",
                "+     *     assign executors from this list",
                "+     * @return a list of executors in sorted order",
                "+     */",
                "+    protected List<ExecutorDetails> orderExecutors(",
                "+        TopologyDetails td, Collection<ExecutorDetails> unassignedExecutors) {",
                "+        Map<String, Component> componentMap = td.getComponents();",
                "+        List<ExecutorDetails> execsScheduled = new LinkedList<>();",
                "+",
                "+        Map<String, Queue<ExecutorDetails>> compToExecsToSchedule = new HashMap<>();",
                "+        for (Component component : componentMap.values()) {",
                "+            compToExecsToSchedule.put(component.getId(), new LinkedList<ExecutorDetails>());",
                "+            for (ExecutorDetails exec : component.getExecs()) {",
                "+                if (unassignedExecutors.contains(exec)) {",
                "+                    compToExecsToSchedule.get(component.getId()).add(exec);",
                "+                }",
                "+            }",
                "+        }",
                "+",
                "+        Set<Component> sortedComponents = sortComponents(componentMap);",
                "+        sortedComponents.addAll(componentMap.values());",
                "+",
                "+        for (Component currComp : sortedComponents) {",
                "+            Map<String, Component> neighbors = new HashMap<String, Component>();",
                "+            for (String compId : union(currComp.getChildren(), currComp.getParents())) {",
                "+                neighbors.put(compId, componentMap.get(compId));",
                "+            }",
                "+            Set<Component> sortedNeighbors = sortNeighbors(currComp, neighbors);",
                "+            Queue<ExecutorDetails> currCompExesToSched = compToExecsToSchedule.get(currComp.getId());",
                "+",
                "+            boolean flag = false;",
                "+            do {",
                "+                flag = false;",
                "+                if (!currCompExesToSched.isEmpty()) {",
                "+                    execsScheduled.add(currCompExesToSched.poll());",
                "+                    flag = true;",
                "+                }",
                "+",
                "+                for (Component neighborComp : sortedNeighbors) {",
                "+                    Queue<ExecutorDetails> neighborCompExesToSched =",
                "+                        compToExecsToSchedule.get(neighborComp.getId());",
                "+                    if (!neighborCompExesToSched.isEmpty()) {",
                "+                        execsScheduled.add(neighborCompExesToSched.poll());",
                "+                        flag = true;",
                "+                    }",
                "+                }",
                "+            } while (flag);",
                "+        }",
                "+        return execsScheduled;",
                "+    }",
                "+",
                "+    /**",
                "+     * Get a list of all the spouts in the topology.",
                "+     *",
                "+     * @param td topology to get spouts from",
                "+     * @return a list of spouts",
                "+     */",
                "+    protected List<Component> getSpouts(TopologyDetails td) {",
                "+        List<Component> spouts = new ArrayList<>();",
                "+",
                "+        for (Component c : td.getComponents().values()) {",
                "+            if (c.getType() == ComponentType.SPOUT) {",
                "+                spouts.add(c);",
                "+            }",
                "+        }",
                "+        return spouts;",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the amount of resources available and total for each node.",
                "+     *",
                "+     * @return a String with cluster resource info for debug",
                "+     */",
                "+    private void logClusterInfo() {",
                "+        if (LOG.isDebugEnabled()) {",
                "+            LOG.debug(\"Cluster:\");",
                "+            for (Map.Entry<String, List<String>> clusterEntry : networkTopography.entrySet()) {",
                "+                String rackId = clusterEntry.getKey();",
                "+                LOG.debug(\"Rack: {}\", rackId);",
                "+                for (String nodeHostname : clusterEntry.getValue()) {",
                "+                    RAS_Node node = idToNode(this.nodeHostnameToId(nodeHostname));",
                "+                    LOG.debug(\"-> Node: {} {}\", node.getHostname(), node.getId());",
                "+                    LOG.debug(",
                "+                        \"--> Avail Resources: {Mem {}, CPU {} Slots: {}}\",",
                "+                        node.getAvailableMemoryResources(),",
                "+                        node.getAvailableCpuResources(),",
                "+                        node.totalSlotsFree());",
                "+                    LOG.debug(",
                "+                        \"--> Total Resources: {Mem {}, CPU {} Slots: {}}\",",
                "+                        node.getTotalMemoryResources(),",
                "+                        node.getTotalCpuResources(),",
                "+                        node.totalSlots());",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * hostname to Id.",
                "+     *",
                "+     * @param hostname the hostname to convert to node id",
                "+     * @return the id of a node",
                "+     */",
                "+    public String nodeHostnameToId(String hostname) {",
                "+        for (RAS_Node n : nodes.getNodes()) {",
                "+            if (n.getHostname() == null) {",
                "+                continue;",
                "+            }",
                "+            if (n.getHostname().equals(hostname)) {",
                "+                return n.getId();",
                "+            }",
                "+        }",
                "+        LOG.error(\"Cannot find Node with hostname {}\", hostname);",
                "+        return null;",
                "+    }",
                "+",
                "+    /**",
                "+     * Find RAS_Node for specified node id.",
                "+     *",
                "+     * @param id the node/supervisor id to lookup",
                "+     * @return a RAS_Node object",
                "+     */",
                "+    public RAS_Node idToNode(String id) {",
                "+        RAS_Node ret = nodes.getNodeById(id);",
                "+        if (ret == null) {",
                "+            LOG.error(\"Cannot find Node with Id: {}\", id);",
                "+        }",
                "+        return ret;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "index cb324a300..adf05f542 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "@@ -20,4 +20,2 @@ package org.apache.storm.scheduler.resource.strategies.scheduling;",
                "-import com.google.common.annotations.VisibleForTesting;",
                "-",
                " import java.util.ArrayList;",
                "@@ -25,3 +23,2 @@ import java.util.Collection;",
                " import java.util.Collections;",
                "-import java.util.HashMap;",
                " import java.util.HashSet;",
                "@@ -30,4 +27,2 @@ import java.util.List;",
                " import java.util.Map;",
                "-import java.util.Queue;",
                "-import java.util.Set;",
                " import java.util.TreeSet;",
                "@@ -35,3 +30,2 @@ import java.util.TreeSet;",
                " import org.apache.storm.Config;",
                "-import org.apache.storm.generated.ComponentType;",
                " import org.apache.storm.scheduler.Cluster;",
                "@@ -40,5 +34,3 @@ import org.apache.storm.scheduler.ExecutorDetails;",
                " import org.apache.storm.scheduler.TopologyDetails;",
                "-import org.apache.storm.scheduler.WorkerSlot;",
                "-import org.apache.storm.scheduler.resource.RAS_Node;",
                "-import org.apache.storm.scheduler.resource.RAS_Nodes;",
                "+",
                " import org.apache.storm.scheduler.resource.ResourceUtils;",
                "@@ -49,20 +41,5 @@ import org.slf4j.LoggerFactory;",
                "-public class DefaultResourceAwareStrategy implements IStrategy {",
                "-    private static final Logger LOG = LoggerFactory.getLogger(DefaultResourceAwareStrategy.class);",
                "-    private Cluster cluster;",
                "-    private Map<String, List<String>> networkTopography;",
                "-    private RAS_Nodes nodes;",
                "-",
                "-    @VisibleForTesting",
                "-    void prepare(Cluster cluster) {",
                "-        this.cluster = cluster;",
                "-        nodes = new RAS_Nodes(cluster);",
                "-        networkTopography = cluster.getNetworkTopography();",
                "-        logClusterInfo();",
                "-    }",
                "+public class DefaultResourceAwareStrategy extends BaseResourceAwareStrategy implements IStrategy {",
                "-    @Override",
                "-    public void prepare(Map<String, Object> config) {",
                "-        //NOOP",
                "-    }",
                "+    private static final Logger LOG = LoggerFactory.getLogger(BaseResourceAwareStrategy.class);",
                "@@ -74,6 +51,6 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "             return SchedulingResult.failure(",
                "-                SchedulingStatus.FAIL_NOT_ENOUGH_RESOURCES, \"No available nodes to schedule tasks on!\");",
                "+                    SchedulingStatus.FAIL_NOT_ENOUGH_RESOURCES, \"No available nodes to schedule tasks on!\");",
                "         }",
                "         Collection<ExecutorDetails> unassignedExecutors =",
                "-            new HashSet<>(this.cluster.getUnassignedExecutors(td));",
                "+                new HashSet<>(this.cluster.getUnassignedExecutors(td));",
                "         LOG.debug(\"ExecutorsNeedScheduling: {}\", unassignedExecutors);",
                "@@ -85,3 +62,3 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "             return SchedulingResult.failure(",
                "-                SchedulingStatus.FAIL_INVALID_TOPOLOGY, \"Cannot find a Spout!\");",
                "+                    SchedulingStatus.FAIL_INVALID_TOPOLOGY, \"Cannot find a Spout!\");",
                "         }",
                "@@ -89,4 +66,3 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "         //order executors to be scheduled",
                "-        List<ExecutorDetails> orderedExecutors = orderExecutors(td, unassignedExecutors);",
                "-",
                "+        List<ExecutorDetails> orderedExecutors = this.orderExecutors(td, unassignedExecutors);",
                "         Collection<ExecutorDetails> executorsNotScheduled = new HashSet<>(unassignedExecutors);",
                "@@ -94,3 +70,3 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "         List<String> unFavoredNodes = (List<String>) td.getConf().get(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES);",
                "-        final List<ObjectResources> sortedNodes = sortAllNodes(td, favoredNodes, unFavoredNodes);",
                "+        final List<ObjectResources> sortedNodes = this.sortAllNodes(td, null, favoredNodes, unFavoredNodes);",
                "@@ -98,6 +74,6 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "             LOG.debug(",
                "-                \"Attempting to schedule: {} of component {}[ REQ {} ]\",",
                "-                exec,",
                "-                td.getExecutorToComponent().get(exec),",
                "-                td.getTaskResourceReqList(exec));",
                "+                    \"Attempting to schedule: {} of component {}[ REQ {} ]\",",
                "+                    exec,",
                "+                    td.getExecutorToComponent().get(exec),",
                "+                    td.getTaskResourceReqList(exec));",
                "             scheduleExecutor(exec, td, scheduledTasks, sortedNodes);",
                "@@ -117,11 +93,11 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "             result =",
                "-                SchedulingResult.failure(",
                "-                    SchedulingStatus.FAIL_NOT_ENOUGH_RESOURCES,",
                "-                    (td.getExecutors().size() - unassignedExecutors.size())",
                "-                        + \"/\"",
                "-                        + td.getExecutors().size()",
                "-                        + \" executors scheduled\");",
                "+                    SchedulingResult.failure(",
                "+                            SchedulingStatus.FAIL_NOT_ENOUGH_RESOURCES,",
                "+                            (td.getExecutors().size() - unassignedExecutors.size())",
                "+                                    + \"/\"",
                "+                                    + td.getExecutors().size()",
                "+                                    + \" executors scheduled\");",
                "         } else {",
                "             LOG.debug(\"All resources successfully scheduled!\");",
                "-            result = SchedulingResult.success(\"Fully Scheduled by DefaultResourceAwareStrategy\");",
                "+            result = SchedulingResult.success(\"Fully Scheduled by \" + this.getClass().getSimpleName());",
                "         }",
                "@@ -130,295 +106,2 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "-    private List<ObjectResources> sortAllNodes(TopologyDetails td, List<String> favoredNodes, List<String> unFavoredNodes) {",
                "-        TreeSet<ObjectResources> sortedRacks = sortRacks(td.getId());",
                "-        ArrayList<ObjectResources> totallySortedNodes = new ArrayList<>();",
                "-        for (ObjectResources rack : sortedRacks) {",
                "-            final String rackId = rack.id;",
                "-            TreeSet<ObjectResources> sortedNodes =",
                "-                sortNodes(getAvailableNodesFromRack(rackId), rackId, td.getId());",
                "-            totallySortedNodes.addAll(sortedNodes);",
                "-        }",
                "-        //Now do some post processing to add make some nodes preferred over others.",
                "-        if (favoredNodes != null || unFavoredNodes != null) {",
                "-            HashMap<String, Integer> hostOrder = new HashMap<>();",
                "-            if (favoredNodes != null) {",
                "-                int size = favoredNodes.size();",
                "-                for (int i = 0; i < size; i++) {",
                "-                    //First in the list is the most desired so gets the Lowest possible value",
                "-                    hostOrder.put(favoredNodes.get(i), -(size - i));",
                "-                }",
                "-            }",
                "-            if (unFavoredNodes != null) {",
                "-                int size = unFavoredNodes.size();",
                "-                for (int i = 0; i < size; i++) {",
                "-                    //First in the list is the least desired so gets the highest value",
                "-                    hostOrder.put(unFavoredNodes.get(i), size - i);",
                "-                }",
                "-            }",
                "-            //java guarantees a stable sort so we can just return 0 for values we don't want to move.",
                "-            Collections.sort(totallySortedNodes, (o1, o2) -> {",
                "-                RAS_Node n1 = nodes.getNodeById(o1.id);",
                "-                String host1 = n1.getHostname();",
                "-                int h1Value = hostOrder.getOrDefault(host1, 0);",
                "-",
                "-                RAS_Node n2 = nodes.getNodeById(o2.id);",
                "-                String host2 = n2.getHostname();",
                "-                int h2Value = hostOrder.getOrDefault(host2, 0);",
                "-",
                "-                return Integer.compare(h1Value, h2Value);",
                "-            });",
                "-        }",
                "-        return totallySortedNodes;",
                "-    }",
                "-",
                "-    /**",
                "-     * Schedule executor exec from topology td.",
                "-     *",
                "-     * @param exec the executor to schedule",
                "-     * @param td the topology executor exec is a part of",
                "-     * @param scheduledTasks executors that have been scheduled",
                "-     */",
                "-    private void scheduleExecutor(",
                "-        ExecutorDetails exec, TopologyDetails td, Collection<ExecutorDetails> scheduledTasks, List<ObjectResources> sortedNodes) {",
                "-        WorkerSlot targetSlot = findWorkerForExec(exec, td, sortedNodes);",
                "-        if (targetSlot != null) {",
                "-            RAS_Node targetNode = idToNode(targetSlot.getNodeId());",
                "-            targetNode.assignSingleExecutor(targetSlot, exec, td);",
                "-            scheduledTasks.add(exec);",
                "-            LOG.debug(",
                "-                \"TASK {} assigned to Node: {} avail [ mem: {} cpu: {} ] total [ mem: {} cpu: {} ] on \"",
                "-                    + \"slot: {} on Rack: {}\",",
                "-                exec,",
                "-                targetNode.getHostname(),",
                "-                targetNode.getAvailableMemoryResources(),",
                "-                targetNode.getAvailableCpuResources(),",
                "-                targetNode.getTotalMemoryResources(),",
                "-                targetNode.getTotalCpuResources(),",
                "-                targetSlot,",
                "-                nodeToRack(targetNode));",
                "-        } else {",
                "-            LOG.error(\"Not Enough Resources to schedule Task {} - {}\", td.getName(), exec);",
                "-        }",
                "-    }",
                "-",
                "-    /**",
                "-     * Find a worker to schedule executor exec on.",
                "-     *",
                "-     * @param exec the executor to schedule",
                "-     * @param td the topology that the executor is a part of",
                "-     * @return a worker to assign exec on. Returns null if a worker cannot be successfully found in cluster",
                "-     */",
                "-    private WorkerSlot findWorkerForExec(ExecutorDetails exec, TopologyDetails td, List<ObjectResources> sortedNodes) {",
                "-        for (ObjectResources nodeResources : sortedNodes) {",
                "-            RAS_Node node = nodes.getNodeById(nodeResources.id);",
                "-            for (WorkerSlot ws : node.getSlotsAvailbleTo(td)) {",
                "-                if (node.wouldFit(ws, exec, td)) {",
                "-                    return ws;",
                "-                }",
                "-            }",
                "-        }",
                "-        return null;",
                "-    }",
                "-",
                "-    /**",
                "-     * interface for calculating the number of existing executors scheduled on a object (rack or",
                "-     * node).",
                "-     */",
                "-    private interface ExistingScheduleFunc {",
                "-        int getNumExistingSchedule(String objectId);",
                "-    }",
                "-",
                "-    /**",
                "-     * a class to contain individual object resources as well as cumulative stats.",
                "-     */",
                "-    static class AllResources {",
                "-        List<ObjectResources> objectResources = new LinkedList<ObjectResources>();",
                "-        double availMemResourcesOverall = 0.0;",
                "-        double totalMemResourcesOverall = 0.0;",
                "-        double availCpuResourcesOverall = 0.0;",
                "-        double totalCpuResourcesOverall = 0.0;",
                "-        String identifier;",
                "-",
                "-        public AllResources(String identifier) {",
                "-            this.identifier = identifier;",
                "-        }",
                "-    }",
                "-",
                "-    /**",
                "-     * class to keep track of resources on a rack or node.",
                "-     */",
                "-    static class ObjectResources {",
                "-        String id;",
                "-        double availMem = 0.0;",
                "-        double totalMem = 0.0;",
                "-        double availCpu = 0.0;",
                "-        double totalCpu = 0.0;",
                "-        double effectiveResources = 0.0;",
                "-",
                "-        public ObjectResources(String id) {",
                "-            this.id = id;",
                "-        }",
                "-",
                "-        @Override",
                "-        public String toString() {",
                "-            return this.id;",
                "-        }",
                "-    }",
                "-",
                "-    /**",
                "-     * Nodes are sorted by two criteria.",
                "-     *",
                "-     * <p>1) the number executors of the topology that needs to be scheduled is already on the node in",
                "-     * descending order. The reasoning to sort based on criterion 1 is so we schedule the rest of a",
                "-     * topology on the same node as the existing executors of the topology.",
                "-     *",
                "-     * <p>2) the subordinate/subservient resource availability percentage of a node in descending",
                "-     * order We calculate the resource availability percentage by dividing the resource availability",
                "-     * that have exhausted or little of one of the resources mentioned above will be ranked after",
                "-     * on the node by the resource availability of the entire rack By doing this calculation, nodes",
                "-     * nodes that have more balanced resource availability. So we will be less likely to pick a node",
                "-     * that have a lot of one resource but a low amount of another.",
                "-     *",
                "-     * @param availNodes a list of all the nodes we want to sort",
                "-     * @param rackId the rack id availNodes are a part of",
                "-     * @param topoId the topology that we are trying to schedule",
                "-     * @return a sorted list of nodes.",
                "-     */",
                "-    private TreeSet<ObjectResources> sortNodes(",
                "-        List<RAS_Node> availNodes, String rackId, final String topoId) {",
                "-        AllResources allResources = new AllResources(\"RACK\");",
                "-        List<ObjectResources> nodes = allResources.objectResources;",
                "-",
                "-        for (RAS_Node rasNode : availNodes) {",
                "-            String nodeId = rasNode.getId();",
                "-            ObjectResources node = new ObjectResources(nodeId);",
                "-",
                "-            double availMem = rasNode.getAvailableMemoryResources();",
                "-            node.availMem = availMem;",
                "-            double availCpu = rasNode.getAvailableCpuResources();",
                "-            node.availCpu = availCpu;",
                "-            double totalMem = rasNode.getTotalMemoryResources();",
                "-            node.totalMem = totalMem;",
                "-            double totalCpu = rasNode.getTotalCpuResources();",
                "-            node.totalCpu = totalCpu;",
                "-",
                "-            nodes.add(node);",
                "-",
                "-            allResources.availMemResourcesOverall += availMem;",
                "-            allResources.availCpuResourcesOverall += availCpu;",
                "-",
                "-            allResources.totalMemResourcesOverall += totalMem;",
                "-            allResources.totalCpuResourcesOverall += totalCpu;",
                "-        }",
                "-",
                "-        LOG.debug(",
                "-            \"Rack {}: Overall Avail [ CPU {} MEM {} ] Total [ CPU {} MEM {} ]\",",
                "-            rackId,",
                "-            allResources.availCpuResourcesOverall,",
                "-            allResources.availMemResourcesOverall,",
                "-            allResources.totalCpuResourcesOverall,",
                "-            allResources.totalMemResourcesOverall);",
                "-",
                "-        return sortObjectResources(",
                "-            allResources,",
                "-            new ExistingScheduleFunc() {",
                "-                @Override",
                "-                public int getNumExistingSchedule(String objectId) {",
                "-",
                "-                    //Get execs already assigned in rack",
                "-                    Collection<ExecutorDetails> execs = new LinkedList<ExecutorDetails>();",
                "-                    if (cluster.getAssignmentById(topoId) != null) {",
                "-                        for (Map.Entry<ExecutorDetails, WorkerSlot> entry :",
                "-                            cluster.getAssignmentById(topoId).getExecutorToSlot().entrySet()) {",
                "-                            WorkerSlot workerSlot = entry.getValue();",
                "-                            ExecutorDetails exec = entry.getKey();",
                "-                            if (workerSlot.getNodeId().equals(objectId)) {",
                "-                                execs.add(exec);",
                "-                            }",
                "-                        }",
                "-                    }",
                "-                    return execs.size();",
                "-                }",
                "-            });",
                "-    }",
                "-",
                "-    /**",
                "-     * Racks are sorted by two criteria.",
                "-     *",
                "-     * <p>1) the number executors of the topology that needs to be scheduled is already on the rack in descending order.",
                "-     * The reasoning to sort based on criterion 1 is so we schedule the rest of a topology on the same rack as the",
                "-     * existing executors of the topology.",
                "-     *",
                "-     * <p>2) the subordinate/subservient resource availability percentage of a rack in descending order We calculate",
                "-     * the resource availability percentage by dividing the resource availability on the rack by the resource",
                "-     * availability of the  entire cluster By doing this calculation, racks that have exhausted or little of one of",
                "-     * the resources mentioned above will be ranked after racks that have more balanced resource availability. So we",
                "-     * will be less likely to pick a rack that have a lot of one resource but a low amount of another.",
                "-     *",
                "-     * @param topoId topology id",
                "-     * @return a sorted list of racks",
                "-     */",
                "-    @VisibleForTesting",
                "-    TreeSet<ObjectResources> sortRacks(final String topoId) {",
                "-        AllResources allResources = new AllResources(\"Cluster\");",
                "-        List<ObjectResources> racks = allResources.objectResources;",
                "-",
                "-        final Map<String, String> nodeIdToRackId = new HashMap<String, String>();",
                "-",
                "-        for (Map.Entry<String, List<String>> entry : networkTopography.entrySet()) {",
                "-            String rackId = entry.getKey();",
                "-            List<String> nodeIds = entry.getValue();",
                "-            ObjectResources rack = new ObjectResources(rackId);",
                "-            racks.add(rack);",
                "-            for (String nodeId : nodeIds) {",
                "-                RAS_Node node = nodes.getNodeById(nodeHostnameToId(nodeId));",
                "-                double availMem = node.getAvailableMemoryResources();",
                "-                rack.availMem += availMem;",
                "-                double availCpu = node.getAvailableCpuResources();",
                "-                rack.availCpu += availCpu;",
                "-                double totalMem = node.getTotalMemoryResources();",
                "-                rack.totalMem += totalMem;",
                "-                double totalCpu = node.getTotalCpuResources();",
                "-                rack.totalCpu += totalCpu;",
                "-",
                "-                nodeIdToRackId.put(nodeId, rack.id);",
                "-",
                "-                allResources.availMemResourcesOverall += availMem;",
                "-                allResources.availCpuResourcesOverall += availCpu;",
                "-",
                "-                allResources.totalMemResourcesOverall += totalMem;",
                "-                allResources.totalCpuResourcesOverall += totalCpu;",
                "-            }",
                "-        }",
                "-        LOG.debug(",
                "-            \"Cluster Overall Avail [ CPU {} MEM {} ] Total [ CPU {} MEM {} ]\",",
                "-            allResources.availCpuResourcesOverall,",
                "-            allResources.availMemResourcesOverall,",
                "-            allResources.totalCpuResourcesOverall,",
                "-            allResources.totalMemResourcesOverall);",
                "-",
                "-        return sortObjectResources(",
                "-            allResources,",
                "-            new ExistingScheduleFunc() {",
                "-                @Override",
                "-                public int getNumExistingSchedule(String objectId) {",
                "-                    String rackId = objectId;",
                "-                    //Get execs already assigned in rack",
                "-                    Collection<ExecutorDetails> execs = new LinkedList<ExecutorDetails>();",
                "-                    if (cluster.getAssignmentById(topoId) != null) {",
                "-                        for (Map.Entry<ExecutorDetails, WorkerSlot> entry :",
                "-                            cluster.getAssignmentById(topoId).getExecutorToSlot().entrySet()) {",
                "-                            String nodeId = entry.getValue().getNodeId();",
                "-                            String hostname = idToNode(nodeId).getHostname();",
                "-                            ExecutorDetails exec = entry.getKey();",
                "-                            if (nodeIdToRackId.get(hostname) != null",
                "-                                && nodeIdToRackId.get(hostname).equals(rackId)) {",
                "-                                execs.add(exec);",
                "-                            }",
                "-                        }",
                "-                    }",
                "-                    return execs.size();",
                "-                }",
                "-            });",
                "-    }",
                "-",
                "     /**",
                "@@ -440,4 +123,6 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "      */",
                "-    private TreeSet<ObjectResources> sortObjectResources(",
                "-        final AllResources allResources, final ExistingScheduleFunc existingScheduleFunc) {",
                "+    @Override",
                "+    protected TreeSet<ObjectResources> sortObjectResources(",
                "+            final AllResources allResources, ExecutorDetails exec, TopologyDetails topologyDetails,",
                "+            final ExistingScheduleFunc existingScheduleFunc) {",
                "@@ -445,4 +130,3 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "             StringBuilder sb = new StringBuilder();",
                "-            if (allResources.availCpuResourcesOverall <= 0.0",
                "-                || allResources.availMemResourcesOverall <= 0.0) {",
                "+            if (ResourceUtils.getMinValuePresentInResourceMap(objectResources.availableResources) <= 0) {",
                "                 objectResources.effectiveResources = 0.0;",
                "@@ -451,13 +135,13 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "-                //add cpu",
                "-                double cpuPercent =",
                "-                    (objectResources.availCpu / allResources.availCpuResourcesOverall) * 100.0;",
                "-                values.add(cpuPercent);",
                "-                sb.append(String.format(\"CPU %f(%f%%) \", objectResources.availCpu, cpuPercent));",
                "+                Map<String, Double> percentageTotal = ResourceUtils.getPercentageOfTotalResourceMap(",
                "+                        objectResources.availableResources, allResources.availableResourcesOverall",
                "+                );",
                "+                for(Map.Entry<String, Double> percentageEntry : percentageTotal.entrySet()) {",
                "+                    values.add(percentageEntry.getValue());",
                "+                    sb.append(String.format(\"%s %f(%f%%) \", percentageEntry.getKey(),",
                "+                            objectResources.availableResources.get(percentageEntry.getKey()),",
                "+                            percentageEntry.getValue())",
                "+                    );",
                "-                //add memory",
                "-                double memoryPercent =",
                "-                    (objectResources.availMem / allResources.availMemResourcesOverall) * 100.0;",
                "-                values.add(memoryPercent);",
                "-                sb.append(String.format(\"MEM %f(%f%%) \", objectResources.availMem, memoryPercent));",
                "+                }",
                "@@ -466,8 +150,7 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "             LOG.debug(",
                "-                \"{}: Avail [ {} ] Total [ CPU {} MEM {}] effective resources: {}\",",
                "-                objectResources.id,",
                "-                sb.toString(),",
                "-                objectResources.totalCpu,",
                "-                objectResources.totalMem,",
                "-                objectResources.effectiveResources);",
                "+                    \"{}: Avail [ {} ] Total [ {} ] effective resources: {}\",",
                "+                    objectResources.id,",
                "+                    sb.toString(),",
                "+                    objectResources.totalResources,",
                "+                    objectResources.effectiveResources);",
                "         }",
                "@@ -475,36 +158,33 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "         TreeSet<ObjectResources> sortedObjectResources =",
                "-            new TreeSet<>((o1, o2) -> {",
                "-                int execsScheduled1 = existingScheduleFunc.getNumExistingSchedule(o1.id);",
                "-                int execsScheduled2 = existingScheduleFunc.getNumExistingSchedule(o2.id);",
                "-                if (execsScheduled1 > execsScheduled2) {",
                "-                    return -1;",
                "-                } else if (execsScheduled1 < execsScheduled2) {",
                "-                    return 1;",
                "-                } else {",
                "-                    if (o1.effectiveResources > o2.effectiveResources) {",
                "+                new TreeSet<>((o1, o2) -> {",
                "+                    int execsScheduled1 = existingScheduleFunc.getNumExistingSchedule(o1.id);",
                "+                    int execsScheduled2 = existingScheduleFunc.getNumExistingSchedule(o2.id);",
                "+                    if (execsScheduled1 > execsScheduled2) {",
                "                         return -1;",
                "-                    } else if (o1.effectiveResources < o2.effectiveResources) {",
                "+                    } else if (execsScheduled1 < execsScheduled2) {",
                "                         return 1;",
                "                     } else {",
                "-                        List<Double> o1Values = new LinkedList<Double>();",
                "-                        List<Double> o2Values = new LinkedList<Double>();",
                "-                        o1Values.add((o1.availCpu / allResources.availCpuResourcesOverall) * 100.0);",
                "-                        o2Values.add((o2.availCpu / allResources.availCpuResourcesOverall) * 100.0);",
                "-",
                "-                        o1Values.add((o1.availMem / allResources.availMemResourcesOverall) * 100.0);",
                "-                        o2Values.add((o2.availMem / allResources.availMemResourcesOverall) * 100.0);",
                "-",
                "-                        double o1Avg = ResourceUtils.avg(o1Values);",
                "-                        double o2Avg = ResourceUtils.avg(o2Values);",
                "-",
                "-                        if (o1Avg > o2Avg) {",
                "+                        if (o1.effectiveResources > o2.effectiveResources) {",
                "                             return -1;",
                "-                        } else if (o1Avg < o2Avg) {",
                "+                        } else if (o1.effectiveResources < o2.effectiveResources) {",
                "                             return 1;",
                "                         } else {",
                "-                            return o1.id.compareTo(o2.id);",
                "+                            Collection<Double> o1Values = ResourceUtils.getPercentageOfTotalResourceMap(",
                "+                                    o1.availableResources, allResources.availableResourcesOverall).values();",
                "+                            Collection<Double> o2Values = ResourceUtils.getPercentageOfTotalResourceMap(",
                "+                                    o2.availableResources, allResources.availableResourcesOverall).values();",
                "+",
                "+                            double o1Avg = ResourceUtils.avg(o1Values);",
                "+                            double o2Avg = ResourceUtils.avg(o2Values);",
                "+",
                "+                            if (o1Avg > o2Avg) {",
                "+                                return -1;",
                "+                            } else if (o1Avg < o2Avg) {",
                "+                                return 1;",
                "+                            } else {",
                "+                                return o1.id.compareTo(o2.id);",
                "+                            }",
                "                         }",
                "                     }",
                "-                }",
                "-            });",
                "+                });",
                "         sortedObjectResources.addAll(allResources.objectResources);",
                "@@ -513,235 +193,2 @@ public class DefaultResourceAwareStrategy implements IStrategy {",
                "     }",
                "-",
                "-    /**",
                "-     * Get the rack on which a node is a part of.",
                "-     *",
                "-     * @param node the node to find out which rack its on",
                "-     * @return the rack id",
                "-     */",
                "-    private String nodeToRack(RAS_Node node) {",
                "-        for (Map.Entry<String, List<String>> entry : networkTopography.entrySet()) {",
                "-            if (entry.getValue().contains(node.getHostname())) {",
                "-                return entry.getKey();",
                "-            }",
                "-        }",
                "-        LOG.error(\"Node: {} not found in any racks\", node.getHostname());",
                "-        return null;",
                "-    }",
                "-",
                "-    /**",
                "-     * get a list nodes from a rack.",
                "-     *",
                "-     * @param rackId the rack id of the rack to get nodes from",
                "-     * @return a list of nodes",
                "-     */",
                "-    private List<RAS_Node> getAvailableNodesFromRack(String rackId) {",
                "-        List<RAS_Node> retList = new ArrayList<>();",
                "-        for (String nodeId : networkTopography.get(rackId)) {",
                "-            retList.add(nodes.getNodeById(this.nodeHostnameToId(nodeId)));",
                "-        }",
                "-        return retList;",
                "-    }",
                "-",
                "-    /**",
                "-     * sort components by the number of in and out connections that need to be made, in descending order",
                "-     *",
                "-     * @param componentMap The components that need to be sorted",
                "-     * @return a sorted set of components",
                "-     */",
                "-    private Set<Component> sortComponents(final Map<String, Component> componentMap) {",
                "-        Set<Component> sortedComponents =",
                "-            new TreeSet<>((o1, o2) -> {",
                "-                int connections1 = 0;",
                "-                int connections2 = 0;",
                "-",
                "-                for (String childId : union(o1.getChildren(), o1.getParents())) {",
                "-                    connections1 +=",
                "-                        (componentMap.get(childId).getExecs().size() * o1.getExecs().size());",
                "-                }",
                "-",
                "-                for (String childId : union(o2.getChildren(), o2.getParents())) {",
                "-                    connections2 +=",
                "-                        (componentMap.get(childId).getExecs().size() * o2.getExecs().size());",
                "-                }",
                "-",
                "-                if (connections1 > connections2) {",
                "-                    return -1;",
                "-                } else if (connections1 < connections2) {",
                "-                    return 1;",
                "-                } else {",
                "-                    return o1.getId().compareTo(o2.getId());",
                "-                }",
                "-            });",
                "-        sortedComponents.addAll(componentMap.values());",
                "-        return sortedComponents;",
                "-    }",
                "-",
                "-    private static <T> Set<T> union(Set<T> a, Set<T> b) {",
                "-        HashSet<T> ret = new HashSet<>(a);",
                "-        ret.addAll(b);",
                "-        return ret;",
                "-    }",
                "-",
                "-    /**",
                "-     * Sort a component's neighbors by the number of connections it needs to make with this component.",
                "-     *",
                "-     * @param thisComp the component that we need to sort its neighbors",
                "-     * @param componentMap all the components to sort",
                "-     * @return a sorted set of components",
                "-     */",
                "-    private Set<Component> sortNeighbors(",
                "-        final Component thisComp, final Map<String, Component> componentMap) {",
                "-        Set<Component> sortedComponents =",
                "-            new TreeSet<>((o1, o2) -> {",
                "-                int connections1 = o1.getExecs().size() * thisComp.getExecs().size();",
                "-                int connections2 = o2.getExecs().size() * thisComp.getExecs().size();",
                "-                if (connections1 < connections2) {",
                "-                    return -1;",
                "-                } else if (connections1 > connections2) {",
                "-                    return 1;",
                "-                } else {",
                "-                    return o1.getId().compareTo(o2.getId());",
                "-                }",
                "-            });",
                "-        sortedComponents.addAll(componentMap.values());",
                "-        return sortedComponents;",
                "-    }",
                "-",
                "-    /**",
                "-     * Order executors based on how many in and out connections it will potentially need to make, in descending order.",
                "-     * First order components by the number of in and out connections it will have.  Then iterate through the sorted list of components.",
                "-     * For each component sort the neighbors of that component by how many connections it will have to make with that component.",
                "-     * Add an executor from this component and then from each neighboring component in sorted order.  Do this until there is nothing",
                "-     * left to schedule",
                "-     *",
                "-     * @param td The topology the executors belong to",
                "-     * @param unassignedExecutors a collection of unassigned executors that need to be unassigned. Should only try to",
                "-     *     assign executors from this list",
                "-     * @return a list of executors in sorted order",
                "-     */",
                "-    private List<ExecutorDetails> orderExecutors(",
                "-        TopologyDetails td, Collection<ExecutorDetails> unassignedExecutors) {",
                "-        Map<String, Component> componentMap = td.getComponents();",
                "-        List<ExecutorDetails> execsScheduled = new LinkedList<>();",
                "-",
                "-        Map<String, Queue<ExecutorDetails>> compToExecsToSchedule = new HashMap<>();",
                "-        for (Component component : componentMap.values()) {",
                "-            compToExecsToSchedule.put(component.getId(), new LinkedList<ExecutorDetails>());",
                "-            for (ExecutorDetails exec : component.getExecs()) {",
                "-                if (unassignedExecutors.contains(exec)) {",
                "-                    compToExecsToSchedule.get(component.getId()).add(exec);",
                "-                }",
                "-            }",
                "-        }",
                "-",
                "-        Set<Component> sortedComponents = sortComponents(componentMap);",
                "-        sortedComponents.addAll(componentMap.values());",
                "-",
                "-        for (Component currComp : sortedComponents) {",
                "-            Map<String, Component> neighbors = new HashMap<String, Component>();",
                "-            for (String compId : union(currComp.getChildren(), currComp.getParents())) {",
                "-                neighbors.put(compId, componentMap.get(compId));",
                "-            }",
                "-            Set<Component> sortedNeighbors = sortNeighbors(currComp, neighbors);",
                "-            Queue<ExecutorDetails> currCompExesToSched = compToExecsToSchedule.get(currComp.getId());",
                "-",
                "-            boolean flag = false;",
                "-            do {",
                "-                flag = false;",
                "-                if (!currCompExesToSched.isEmpty()) {",
                "-                    execsScheduled.add(currCompExesToSched.poll());",
                "-                    flag = true;",
                "-                }",
                "-",
                "-                for (Component neighborComp : sortedNeighbors) {",
                "-                    Queue<ExecutorDetails> neighborCompExesToSched =",
                "-                        compToExecsToSchedule.get(neighborComp.getId());",
                "-                    if (!neighborCompExesToSched.isEmpty()) {",
                "-                        execsScheduled.add(neighborCompExesToSched.poll());",
                "-                        flag = true;",
                "-                    }",
                "-                }",
                "-            } while (flag);",
                "-        }",
                "-        return execsScheduled;",
                "-    }",
                "-",
                "-    /**",
                "-     * Get a list of all the spouts in the topology.",
                "-     *",
                "-     * @param td topology to get spouts from",
                "-     * @return a list of spouts",
                "-     */",
                "-    private List<Component> getSpouts(TopologyDetails td) {",
                "-        List<Component> spouts = new ArrayList<>();",
                "-",
                "-        for (Component c : td.getComponents().values()) {",
                "-            if (c.getType() == ComponentType.SPOUT) {",
                "-                spouts.add(c);",
                "-            }",
                "-        }",
                "-        return spouts;",
                "-    }",
                "-",
                "-    /**",
                "-     * Get the amount of resources available and total for each node.",
                "-     *",
                "-     * @return a String with cluster resource info for debug",
                "-     */",
                "-    private void logClusterInfo() {",
                "-        if (LOG.isDebugEnabled()) {",
                "-            LOG.debug(\"Cluster:\");",
                "-            for (Map.Entry<String, List<String>> clusterEntry : networkTopography.entrySet()) {",
                "-                String rackId = clusterEntry.getKey();",
                "-                LOG.debug(\"Rack: {}\", rackId);",
                "-                for (String nodeHostname : clusterEntry.getValue()) {",
                "-                    RAS_Node node = idToNode(this.nodeHostnameToId(nodeHostname));",
                "-                    LOG.debug(\"-> Node: {} {}\", node.getHostname(), node.getId());",
                "-                    LOG.debug(",
                "-                        \"--> Avail Resources: {Mem {}, CPU {} Slots: {}}\",",
                "-                        node.getAvailableMemoryResources(),",
                "-                        node.getAvailableCpuResources(),",
                "-                        node.totalSlotsFree());",
                "-                    LOG.debug(",
                "-                        \"--> Total Resources: {Mem {}, CPU {} Slots: {}}\",",
                "-                        node.getTotalMemoryResources(),",
                "-                        node.getTotalCpuResources(),",
                "-                        node.totalSlots());",
                "-                }",
                "-            }",
                "-        }",
                "-    }",
                "-",
                "-    /**",
                "-     * hostname to Id.",
                "-     *",
                "-     * @param hostname the hostname to convert to node id",
                "-     * @return the id of a node",
                "-     */",
                "-    public String nodeHostnameToId(String hostname) {",
                "-        for (RAS_Node n : nodes.getNodes()) {",
                "-            if (n.getHostname() == null) {",
                "-                continue;",
                "-            }",
                "-            if (n.getHostname().equals(hostname)) {",
                "-                return n.getId();",
                "-            }",
                "-        }",
                "-        LOG.error(\"Cannot find Node with hostname {}\", hostname);",
                "-        return null;",
                "-    }",
                "-",
                "-    /**",
                "-     * Find RAS_Node for specified node id.",
                "-     *",
                "-     * @param id the node/supervisor id to lookup",
                "-     * @return a RAS_Node object",
                "-     */",
                "-    public RAS_Node idToNode(String id) {",
                "-        RAS_Node ret = nodes.getNodeById(id);",
                "-        if (ret == null) {",
                "-            LOG.error(\"Cannot find Node with Id: {}\", id);",
                "-        }",
                "-        return ret;",
                "-    }",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "new file mode 100644",
                "index 000000000..a4de7c29d",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "@@ -0,0 +1,193 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource.strategies.scheduling;",
                "+",
                "+import com.google.common.annotations.VisibleForTesting;",
                "+",
                "+import java.util.ArrayList;",
                "+import java.util.Collection;",
                "+import java.util.HashSet;",
                "+import java.util.LinkedList;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import java.util.TreeSet;",
                "+",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.scheduler.Cluster;",
                "+import org.apache.storm.scheduler.Component;",
                "+import org.apache.storm.scheduler.ExecutorDetails;",
                "+import org.apache.storm.scheduler.TopologyDetails;",
                "+import org.apache.storm.scheduler.resource.ResourceUtils;",
                "+import org.apache.storm.scheduler.resource.SchedulingResult;",
                "+import org.apache.storm.scheduler.resource.SchedulingStatus;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+public class GenericResourceAwareStrategy extends BaseResourceAwareStrategy implements IStrategy {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(GenericResourceAwareStrategy.class);",
                "+",
                "+    @Override",
                "+    public SchedulingResult schedule(Cluster cluster, TopologyDetails td) {",
                "+        prepare(cluster);",
                "+        if (nodes.getNodes().size() <= 0) {",
                "+            LOG.warn(\"No available nodes to schedule tasks on!\");",
                "+            return SchedulingResult.failure(",
                "+                    SchedulingStatus.FAIL_NOT_ENOUGH_RESOURCES, \"No available nodes to schedule tasks on!\");",
                "+        }",
                "+        Collection<ExecutorDetails> unassignedExecutors =",
                "+                new HashSet<>(this.cluster.getUnassignedExecutors(td));",
                "+        LOG.info(\"ExecutorsNeedScheduling: {}\", unassignedExecutors);",
                "+        Collection<ExecutorDetails> scheduledTasks = new ArrayList<>();",
                "+        List<Component> spouts = this.getSpouts(td);",
                "+",
                "+        if (spouts.size() == 0) {",
                "+            LOG.error(\"Cannot find a Spout!\");",
                "+            return SchedulingResult.failure(",
                "+                    SchedulingStatus.FAIL_INVALID_TOPOLOGY, \"Cannot find a Spout!\");",
                "+        }",
                "+",
                "+        //order executors to be scheduled",
                "+        List<ExecutorDetails> orderedExecutors = this.orderExecutors(td, unassignedExecutors);",
                "+        Collection<ExecutorDetails> executorsNotScheduled = new HashSet<>(unassignedExecutors);",
                "+        List<String> favoredNodes = (List<String>) td.getConf().get(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES);",
                "+        List<String> unFavoredNodes = (List<String>) td.getConf().get(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES);",
                "+",
                "+        for (ExecutorDetails exec : orderedExecutors) {",
                "+            LOG.debug(",
                "+                    \"Attempting to schedule: {} of component {}[ REQ {} ]\",",
                "+                    exec,",
                "+                    td.getExecutorToComponent().get(exec),",
                "+                    td.getTaskResourceReqList(exec));",
                "+            final List<ObjectResources> sortedNodes = this.sortAllNodes(td, exec, favoredNodes, unFavoredNodes);",
                "+",
                "+            scheduleExecutor(exec, td, scheduledTasks, sortedNodes);",
                "+        }",
                "+",
                "+        executorsNotScheduled.removeAll(scheduledTasks);",
                "+        LOG.error(\"/* Scheduling left over task (most likely sys tasks) */\");",
                "+        // schedule left over system tasks",
                "+        for (ExecutorDetails exec : executorsNotScheduled) {",
                "+            final List<ObjectResources> sortedNodes = this.sortAllNodes(td, exec, favoredNodes, unFavoredNodes);",
                "+            scheduleExecutor(exec, td, scheduledTasks, sortedNodes);",
                "+        }",
                "+",
                "+        SchedulingResult result;",
                "+        executorsNotScheduled.removeAll(scheduledTasks);",
                "+        if (executorsNotScheduled.size() > 0) {",
                "+            LOG.error(\"Not all executors successfully scheduled: {}\", executorsNotScheduled);",
                "+            result =",
                "+                    SchedulingResult.failure(",
                "+                            SchedulingStatus.FAIL_NOT_ENOUGH_RESOURCES,",
                "+                            (td.getExecutors().size() - unassignedExecutors.size())",
                "+                                    + \"/\"",
                "+                                    + td.getExecutors().size()",
                "+                                    + \" executors scheduled\");",
                "+        } else {",
                "+            LOG.debug(\"All resources successfully scheduled!\");",
                "+            result = SchedulingResult.success(\"Fully Scheduled by \" + this.getClass().getSimpleName());",
                "+        }",
                "+        return result;",
                "+    }",
                "+",
                "+",
                "+    /**",
                "+     * Sort objects by the following two criteria. 1) the number executors of the topology that needs",
                "+     * to be scheduled is already on the object (node or rack) in descending order. The reasoning to",
                "+     * sort based on criterion 1 is so we schedule the rest of a topology on the same object (node or",
                "+     * rack) as the existing executors of the topology. 2) the subordinate/subservient resource",
                "+     * availability percentage of a rack in descending order We calculate the resource availability",
                "+     * percentage by dividing the resource availability of the object (node or rack) by the resource",
                "+     * availability of the entire rack or cluster depending on if object references a node or a rack.",
                "+     * How this differs from the DefaultResourceAwareStrategy is that the percentage boosts the node or rack",
                "+     * if it is requested by the executor that the sorting is being done for and pulls it down if it is not.",
                "+     * By doing this calculation, objects (node or rack) that have exhausted or little of one of the",
                "+     * resources mentioned above will be ranked after racks that have more balanced resource",
                "+     * availability and nodes or racks that have resources that are not requested will be ranked below",
                "+     * . So we will be less likely to pick a rack that have a lot of one resource but a",
                "+     * low amount of another and have a lot of resources that are not requested by the executor.",
                "+     *",
                "+     * @param allResources contains all individual ObjectResources as well as cumulative stats",
                "+     * @param exec executor for which the sorting is done",
                "+     * @param topologyDetails topologyDetails for the above executor",
                "+     * @param existingScheduleFunc a function to get existing executors already scheduled on this object",
                "+     * @return a sorted list of ObjectResources",
                "+     */",
                "+    @Override",
                "+    protected TreeSet<ObjectResources> sortObjectResources(",
                "+            final AllResources allResources, ExecutorDetails exec, TopologyDetails topologyDetails,",
                "+            final ExistingScheduleFunc existingScheduleFunc) {",
                "+",
                "+        Map<String, Double> requestedResources = topologyDetails.getTotalResources(exec);",
                "+        AllResources affinityBasedAllResources = new AllResources(allResources);",
                "+        for (ObjectResources objectResources : affinityBasedAllResources.objectResources) {",
                "+            StringBuilder sb = new StringBuilder();",
                "+            List<Double> values = new LinkedList<>();",
                "+",
                "+            for (Map.Entry<String, Double> availableResourcesEntry : objectResources.availableResources.entrySet()) {",
                "+                if (!requestedResources.containsKey(availableResourcesEntry.getKey())) {",
                "+                    objectResources.availableResources.put(availableResourcesEntry.getKey(), -1.0 * availableResourcesEntry.getValue());",
                "+                }",
                "+            }",
                "+",
                "+            Map<String, Double> percentageTotal = ResourceUtils.getPercentageOfTotalResourceMap(",
                "+                    objectResources.availableResources, allResources.availableResourcesOverall",
                "+            );",
                "+            for(Map.Entry<String, Double> percentageEntry : percentageTotal.entrySet()) {",
                "+                values.add(percentageEntry.getValue());",
                "+                sb.append(String.format(\"%s %f(%f%%) \", percentageEntry.getKey(),",
                "+                        objectResources.availableResources.get(percentageEntry.getKey()),",
                "+                        percentageEntry.getValue())",
                "+                );",
                "+",
                "+            }",
                "+",
                "+        }",
                "+",
                "+        TreeSet<ObjectResources> sortedObjectResources =",
                "+                new TreeSet<>((o1, o2) -> {",
                "+                    int execsScheduled1 = existingScheduleFunc.getNumExistingSchedule(o1.id);",
                "+                    int execsScheduled2 = existingScheduleFunc.getNumExistingSchedule(o2.id);",
                "+                    if (execsScheduled1 > execsScheduled2) {",
                "+                        return -1;",
                "+                    } else if (execsScheduled1 < execsScheduled2) {",
                "+                        return 1;",
                "+                    } else {",
                "+                        Collection<Double> o1Values = ResourceUtils.getPercentageOfTotalResourceMap(",
                "+                                o1.availableResources, allResources.availableResourcesOverall).values();",
                "+                        Collection<Double> o2Values = ResourceUtils.getPercentageOfTotalResourceMap(",
                "+                                o2.availableResources, allResources.availableResourcesOverall).values();",
                "+",
                "+                        double o1Avg = ResourceUtils.avg(o1Values);",
                "+                        double o2Avg = ResourceUtils.avg(o2Values);",
                "+",
                "+                        if (o1Avg > o2Avg) {",
                "+                            return -1;",
                "+                        } else if (o1Avg < o2Avg) {",
                "+                            return 1;",
                "+                        } else {",
                "+                            return o1.id.compareTo(o2.id);",
                "+                        }",
                "+",
                "+                    }",
                "+                });",
                "+        sortedObjectResources.addAll(affinityBasedAllResources.objectResources);",
                "+        LOG.debug(\"Sorted Object Resources: {}\", sortedObjectResources);",
                "+        return sortedObjectResources;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/utils/BufferInputStream.java b/storm-server/src/main/java/org/apache/storm/utils/BufferInputStream.java",
                "index 40dd3b7ae..cd230c31e 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/utils/BufferInputStream.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/utils/BufferInputStream.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.utils;",
                "@@ -34,3 +35,3 @@ public class BufferInputStream implements AutoCloseable {",
                "     public BufferInputStream(InputStream stream) {",
                "-        this(stream, 15*1024);",
                "+        this(stream, 15 * 1024);",
                "     }",
                "@@ -39,6 +40,6 @@ public class BufferInputStream implements AutoCloseable {",
                "         int length = stream.read(buffer);",
                "-        if(length==-1) {",
                "+        if (length == -1) {",
                "             close();",
                "             return new byte[0];",
                "-        } else if(length==buffer.length) {",
                "+        } else if (length == buffer.length) {",
                "             return buffer;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/utils/ServerConfigUtils.java b/storm-server/src/main/java/org/apache/storm/utils/ServerConfigUtils.java",
                "index 480d523a7..dcf3d0910 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/utils/ServerConfigUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/utils/ServerConfigUtils.java",
                "@@ -20,6 +20,2 @@ package org.apache.storm.utils;",
                "-import org.apache.commons.io.FileUtils;",
                "-import org.apache.storm.Config;",
                "-import org.apache.storm.DaemonConfig;",
                "-",
                " import java.io.File;",
                "@@ -34,6 +30,12 @@ import java.util.regex.Pattern;",
                "+import org.apache.commons.io.FileUtils;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.DaemonConfig;",
                "+",
                "+",
                "+",
                " public class ServerConfigUtils {",
                "     public static final String FILE_SEPARATOR = File.separator;",
                "-    public final static String NIMBUS_DO_NOT_REASSIGN = \"NIMBUS-DO-NOT-REASSIGN\";",
                "-    public final static String RESOURCES_SUBDIR = \"resources\";",
                "+    public static final String NIMBUS_DO_NOT_REASSIGN = \"NIMBUS-DO-NOT-REASSIGN\";",
                "+    public static final String RESOURCES_SUBDIR = \"resources\";",
                "diff --git a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "index 340db1cba..bd4c4feba 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "@@ -20,3 +20,31 @@ package org.apache.storm.utils;",
                "+import java.io.BufferedInputStream;",
                "+import java.io.BufferedOutputStream;",
                "+import java.io.BufferedWriter;",
                "+import java.io.File;",
                "+import java.io.FileInputStream;",
                "+import java.io.FileOutputStream;",
                "+import java.io.FileWriter;",
                "+import java.io.InputStream;",
                "+import java.io.IOException;",
                "+import java.io.OutputStream;",
                "+import java.io.PrintStream;",
                "+import java.io.RandomAccessFile;",
                "+import java.nio.file.Files;",
                "+import java.nio.file.FileSystems;",
                "+import java.nio.file.Paths;",
                "+import java.util.ArrayList;",
                "+import java.util.Arrays;",
                "+import java.util.Collections;",
                "+import java.util.Enumeration;",
                "+import java.util.HashMap;",
                "+import java.util.jar.JarEntry;",
                "+import java.util.jar.JarFile;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import java.util.zip.GZIPInputStream;",
                "+import java.util.zip.ZipEntry;",
                "+import java.util.zip.ZipFile;",
                " import javax.security.auth.Subject;",
                "+",
                " import org.apache.commons.compress.archivers.tar.TarArchiveEntry;",
                "@@ -28,4 +56,2 @@ import org.apache.commons.io.IOUtils;",
                " import org.apache.commons.lang.StringUtils;",
                "-import org.apache.storm.Config;",
                "-import org.apache.storm.DaemonConfig;",
                " import org.apache.storm.blobstore.BlobStore;",
                "@@ -36,3 +62,6 @@ import org.apache.storm.blobstore.LocalFsBlobStore;",
                " import org.apache.storm.blobstore.LocalModeClientBlobStore;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.Constants;",
                " import org.apache.storm.daemon.StormCommon;",
                "+import org.apache.storm.DaemonConfig;",
                " import org.apache.storm.generated.AccessControl;",
                "@@ -52,30 +81,2 @@ import org.slf4j.LoggerFactory;",
                "-import java.io.BufferedInputStream;",
                "-import java.io.BufferedOutputStream;",
                "-import java.io.BufferedWriter;",
                "-import java.io.File;",
                "-import java.io.FileInputStream;",
                "-import java.io.FileOutputStream;",
                "-import java.io.FileWriter;",
                "-import java.io.IOException;",
                "-import java.io.InputStream;",
                "-import java.io.OutputStream;",
                "-import java.io.PrintStream;",
                "-import java.io.RandomAccessFile;",
                "-import java.nio.file.FileSystems;",
                "-import java.nio.file.Files;",
                "-import java.nio.file.Paths;",
                "-import java.util.ArrayList;",
                "-import java.util.Arrays;",
                "-import java.util.Collections;",
                "-import java.util.Enumeration;",
                "-import java.util.HashMap;",
                "-import java.util.List;",
                "-import java.util.Map;",
                "-import java.util.jar.JarEntry;",
                "-import java.util.jar.JarFile;",
                "-import java.util.zip.GZIPInputStream;",
                "-import java.util.zip.ZipEntry;",
                "-import java.util.zip.ZipFile;",
                "-",
                " public class ServerUtils {",
                "@@ -727,3 +728,3 @@ public class ServerUtils {",
                "             int parallelism = componentParallelism.getOrDefault(entry.getKey(), 1);",
                "-            double memoryRequirement = entry.getValue().get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB);",
                "+            double memoryRequirement = entry.getValue().get(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "             totalMemoryRequired += memoryRequirement * parallelism;",
                "@@ -733,3 +734,3 @@ public class ServerUtils {",
                "             int parallelism = componentParallelism.getOrDefault(entry.getKey(), 1);",
                "-            double memoryRequirement = entry.getValue().get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB);",
                "+            double memoryRequirement = entry.getValue().get(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "             totalMemoryRequired += memoryRequirement * parallelism;"
            ],
            "changed_files": [
                "examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/CaptureLoad.java",
                "storm-client/src/jvm/org/apache/storm/Config.java",
                "storm-client/src/jvm/org/apache/storm/Constants.java",
                "storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/generated/WorkerResources.java",
                "storm-client/src/jvm/org/apache/storm/scheduler/SupervisorDetails.java",
                "storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java",
                "storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java",
                "storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java",
                "storm-client/src/jvm/org/apache/storm/utils/ObjectReader.java",
                "storm-client/src/storm.thrift",
                "storm-server/src/main/java/org/apache/storm/LocalCluster.java",
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "storm-server/src/main/java/org/apache/storm/logging/ThriftAccessLogger.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "storm-server/src/main/java/org/apache/storm/utils/BufferInputStream.java",
                "storm-server/src/main/java/org/apache/storm/utils/ServerConfigUtils.java",
                "storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2725": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: access, cluster",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2725",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e94b735a1381193fbf406657430284f381f9d8a7",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1506009324,
            "hunks": 528,
            "message": "STORM-2438: added in rebalance changes to support RAS",
            "diff": [
                "diff --git a/bin/storm.py b/bin/storm.py",
                "index e9d4bee95..798cf99bd 100755",
                "--- a/bin/storm.py",
                "+++ b/bin/storm.py",
                "@@ -563,6 +563,6 @@ def deactivate(*args):",
                " def rebalance(*args):",
                "-    \"\"\"Syntax: [storm rebalance topology-name [-w wait-time-secs] [-n new-num-workers] [-e component=parallelism]*]",
                "+    \"\"\"Syntax: [storm rebalance topology-name [-w wait-time-secs] [-n new-num-workers] [-e component=parallelism]*  [-r '{\"component1\": {\"resource1\": new_amount, \"resource2\": new_amount, ... }*}'] [-t '{\"conf1\": newValue, *}']]",
                "-    Sometimes you may wish to spread out where the workers for a topology",
                "-    are running. For example, let's say you have a 10 node cluster running",
                "+    Sometimes you may wish to spread out the workers for a running topology.",
                "+    For example, let's say you have a 10 node cluster running",
                "     4 workers per node, and then let's say you add another 10 nodes to",
                "@@ -574,10 +574,11 @@ def rebalance(*args):",
                "     Rebalance will first deactivate the topology for the duration of the",
                "-    message timeout (overridable with the -w flag) and then redistribute",
                "-    the workers evenly around the cluster. The topology will then return to",
                "-    its previous state of activation (so a deactivated topology will still",
                "-    be deactivated and an activated topology will go back to being activated).",
                "-",
                "-    The rebalance command can also be used to change the parallelism of a running topology.",
                "-    Use the -n and -e switches to change the number of workers or number of executors of a component",
                "-    respectively.",
                "+    message timeout (overridable with the -w flag) make requested adjustments to the topology",
                "+    and let the scheduler try to find a better scheduling based off of the",
                "+    new situation. The topology will then return to its previous state of activation",
                "+    (so a deactivated topology will still be deactivated and an activated",
                "+    topology will go back to being activated).",
                "+",
                "+    Some of what you can change about a topology includes the number of requested workers (-n flag)",
                "+    The number of executors for a given component (-e flag) the resources each component is",
                "+    requesting as used by the resource aware scheduler (-r flag) and configs (-t flag).",
                "     \"\"\"",
                "diff --git a/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsClientBlobStore.java b/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsClientBlobStore.java",
                "index 389dc71b3..e68d5e3e5 100644",
                "--- a/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsClientBlobStore.java",
                "+++ b/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsClientBlobStore.java",
                "@@ -119,2 +119,7 @@ public class HdfsClientBlobStore extends ClientBlobStore {",
                "     public void shutdown() {",
                "+        close();",
                "+    }",
                "+",
                "+    @Override",
                "+    public void close() {",
                "         if(client != null) {",
                "diff --git a/storm-client/src/jvm/org/apache/storm/blobstore/ClientBlobStore.java b/storm-client/src/jvm/org/apache/storm/blobstore/ClientBlobStore.java",
                "index 3291eb77d..d84f88bbf 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/blobstore/ClientBlobStore.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/blobstore/ClientBlobStore.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.blobstore;",
                "@@ -47,7 +48,4 @@ import java.util.Map;",
                "  * @see org.apache.storm.blobstore.NimbusBlobStore",
                "- * @see org.apache.storm.blobstore.LocalFsBlobStore",
                "- * @see org.apache.storm.hdfs.blobstore.HdfsClientBlobStore",
                "- * @see org.apache.storm.hdfs.blobstore.HdfsBlobStore",
                "  */",
                "-public abstract class ClientBlobStore implements Shutdownable {",
                "+public abstract class ClientBlobStore implements Shutdownable, AutoCloseable {",
                "     protected Map<String, Object> conf;",
                "@@ -60,8 +58,4 @@ public abstract class ClientBlobStore implements Shutdownable {",
                "         Map<String, Object> conf = ConfigUtils.readStormConfig();",
                "-        ClientBlobStore blobStore = Utils.getClientBlobStore(conf);",
                "-",
                "-        try {",
                "+        try (ClientBlobStore blobStore = Utils.getClientBlobStore(conf)) {",
                "             withBlobstore.run(blobStore);",
                "-        } finally {",
                "-            blobStore.shutdown();",
                "         }",
                "@@ -170,2 +164,4 @@ public abstract class ClientBlobStore implements Shutdownable {",
                "+    public abstract void close();",
                "+",
                "     /**",
                "diff --git a/storm-client/src/jvm/org/apache/storm/blobstore/LocalModeClientBlobStore.java b/storm-client/src/jvm/org/apache/storm/blobstore/LocalModeClientBlobStore.java",
                "new file mode 100644",
                "index 000000000..1ca11efd9",
                "--- /dev/null",
                "+++ b/storm-client/src/jvm/org/apache/storm/blobstore/LocalModeClientBlobStore.java",
                "@@ -0,0 +1,121 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.blobstore;",
                "+",
                "+import java.util.Iterator;",
                "+import java.util.Map;",
                "+import org.apache.storm.generated.AuthorizationException;",
                "+import org.apache.storm.generated.KeyAlreadyExistsException;",
                "+import org.apache.storm.generated.KeyNotFoundException;",
                "+import org.apache.storm.generated.ReadableBlobMeta;",
                "+import org.apache.storm.generated.SettableBlobMeta;",
                "+import org.apache.storm.utils.NimbusClient;",
                "+",
                "+/**",
                "+ * A Client blob store for LocalMode.",
                "+ */",
                "+public class LocalModeClientBlobStore extends ClientBlobStore {",
                "+    private final BlobStore wrapped;",
                "+",
                "+    public LocalModeClientBlobStore(BlobStore wrapped) {",
                "+        this.wrapped = wrapped;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void shutdown() {",
                "+        wrapped.shutdown();",
                "+    }",
                "+",
                "+    @Override",
                "+    public void prepare(Map<String, Object> conf) {",
                "+        //NOOP prepare should have already been called",
                "+    }",
                "+",
                "+    @Override",
                "+    protected AtomicOutputStream createBlobToExtend(String key, SettableBlobMeta meta) throws AuthorizationException, KeyAlreadyExistsException {",
                "+        return wrapped.createBlob(key, meta, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public AtomicOutputStream updateBlob(String key) throws AuthorizationException, KeyNotFoundException {",
                "+        return wrapped.updateBlob(key, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public ReadableBlobMeta getBlobMeta(String key) throws AuthorizationException, KeyNotFoundException {",
                "+        return wrapped.getBlobMeta(key, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    protected void setBlobMetaToExtend(String key, SettableBlobMeta meta) throws AuthorizationException, KeyNotFoundException {",
                "+        wrapped.setBlobMeta(key, meta, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public void deleteBlob(String key) throws AuthorizationException, KeyNotFoundException {",
                "+        wrapped.deleteBlob(key, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public InputStreamWithMeta getBlob(String key) throws AuthorizationException, KeyNotFoundException {",
                "+        return wrapped.getBlob(key, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public Iterator<String> listKeys() {",
                "+        return wrapped.listKeys();",
                "+    }",
                "+",
                "+    @Override",
                "+    public int getBlobReplication(String key) throws AuthorizationException, KeyNotFoundException {",
                "+        try {",
                "+            return wrapped.getBlobReplication(key, null);",
                "+        } catch (AuthorizationException | KeyNotFoundException rethrow) {",
                "+            throw rethrow;",
                "+        } catch (Exception e) {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public int updateBlobReplication(String key, int replication) throws AuthorizationException, KeyNotFoundException {",
                "+        try {",
                "+            return wrapped.updateBlobReplication(key, replication, null);",
                "+        } catch (AuthorizationException | KeyNotFoundException rethrow) {",
                "+            throw rethrow;",
                "+        } catch (Exception e) {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean setClient(Map<String, Object> conf, NimbusClient client) {",
                "+        return true;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void createStateInZookeeper(String key) {",
                "+        //NOOP",
                "+    }",
                "+",
                "+    @Override",
                "+    public void close() {",
                "+        wrapped.shutdown();",
                "+    }",
                "+}",
                "\\ No newline at end of file",
                "diff --git a/storm-client/src/jvm/org/apache/storm/daemon/supervisor/AdvancedFSOps.java b/storm-client/src/jvm/org/apache/storm/daemon/supervisor/AdvancedFSOps.java",
                "index bee45883a..f1740f253 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/daemon/supervisor/AdvancedFSOps.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/daemon/supervisor/AdvancedFSOps.java",
                "@@ -26,2 +26,3 @@ import java.io.OutputStream;",
                " import java.io.Writer;",
                "+import java.nio.file.DirectoryStream;",
                " import java.nio.file.Files;",
                "@@ -281,2 +282,21 @@ public class AdvancedFSOps implements IAdvancedFSOps {",
                "+    /**",
                "+     * Makes a directory, including any necessary but nonexistent parent",
                "+     * directories.",
                "+     *",
                "+     * @param path the directory to create",
                "+     * @throws IOException on any error",
                "+     */",
                "+    public void forceMkdir(Path path) throws IOException {",
                "+        Files.createDirectories(path);",
                "+    }",
                "+",
                "+    public DirectoryStream<Path> newDirectoryStream(Path dir, DirectoryStream.Filter<? super Path> filter) throws IOException {",
                "+        return Files.newDirectoryStream(dir, filter);",
                "+    }",
                "+",
                "+    public DirectoryStream<Path> newDirectoryStream(Path dir) throws IOException {",
                "+        return Files.newDirectoryStream(dir);",
                "+    }",
                "+",
                "     /**",
                "@@ -291,2 +311,12 @@ public class AdvancedFSOps implements IAdvancedFSOps {",
                "+    /**",
                "+     * Check if a file exists or not",
                "+     * @param path the path to check",
                "+     * @return true if it exists else false",
                "+     * @throws IOException on any error.",
                "+     */",
                "+    public boolean fileExists(Path path) throws IOException {",
                "+        return Files.exists(path);",
                "+    }",
                "+",
                "     /**",
                "diff --git a/storm-client/src/jvm/org/apache/storm/daemon/supervisor/IAdvancedFSOps.java b/storm-client/src/jvm/org/apache/storm/daemon/supervisor/IAdvancedFSOps.java",
                "index 5f23774ae..2b09d3901 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/daemon/supervisor/IAdvancedFSOps.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/daemon/supervisor/IAdvancedFSOps.java",
                "@@ -25,2 +25,4 @@ import java.io.OutputStream;",
                " import java.io.Writer;",
                "+import java.nio.file.DirectoryStream;",
                "+import java.nio.file.Path;",
                " import java.util.Map;",
                "@@ -117,2 +119,28 @@ public interface IAdvancedFSOps {",
                "+    /**",
                "+     * Makes a directory, including any necessary but nonexistent parent",
                "+     * directories.",
                "+     *",
                "+     * @param path the directory to create",
                "+     * @throws IOException on any error",
                "+     */",
                "+    void forceMkdir(Path path) throws IOException;",
                "+",
                "+    /**",
                "+     * List the contents of a directory.",
                "+     * @param dir the driectory to list the contents of",
                "+     * @param filter a filter to decide if it should be included or not",
                "+     * @return A stream of directory entries",
                "+     * @throws IOException on any error",
                "+     */",
                "+    DirectoryStream<Path> newDirectoryStream(Path dir, DirectoryStream.Filter<? super Path> filter) throws IOException;",
                "+",
                "+    /**",
                "+     * List the contents of a directory.",
                "+     * @param dir the driectory to list the contents of",
                "+     * @return A stream of directory entries",
                "+     * @throws IOException on any error",
                "+     */",
                "+    DirectoryStream<Path> newDirectoryStream(Path dir) throws IOException;",
                "+",
                "     /**",
                "@@ -125,2 +153,10 @@ public interface IAdvancedFSOps {",
                "+    /**",
                "+     * Check if a file exists or not",
                "+     * @param path the path to check",
                "+     * @return true if it exists else false",
                "+     * @throws IOException on any error.",
                "+     */",
                "+    boolean fileExists(Path path) throws IOException;",
                "+",
                "     /**",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/Assignment.java b/storm-client/src/jvm/org/apache/storm/generated/Assignment.java",
                "index ccae34b02..c3436d52f 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/Assignment.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/Assignment.java",
                "@@ -968,11 +968,11 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map646 = iprot.readMapBegin();",
                "-                struct.node_host = new HashMap<String,String>(2*_map646.size);",
                "-                String _key647;",
                "-                String _val648;",
                "-                for (int _i649 = 0; _i649 < _map646.size; ++_i649)",
                "+                org.apache.thrift.protocol.TMap _map666 = iprot.readMapBegin();",
                "+                struct.node_host = new HashMap<String,String>(2*_map666.size);",
                "+                String _key667;",
                "+                String _val668;",
                "+                for (int _i669 = 0; _i669 < _map666.size; ++_i669)",
                "                 {",
                "-                  _key647 = iprot.readString();",
                "-                  _val648 = iprot.readString();",
                "-                  struct.node_host.put(_key647, _val648);",
                "+                  _key667 = iprot.readString();",
                "+                  _val668 = iprot.readString();",
                "+                  struct.node_host.put(_key667, _val668);",
                "                 }",
                "@@ -988,16 +988,16 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map650 = iprot.readMapBegin();",
                "-                struct.executor_node_port = new HashMap<List<Long>,NodeInfo>(2*_map650.size);",
                "-                List<Long> _key651;",
                "-                NodeInfo _val652;",
                "-                for (int _i653 = 0; _i653 < _map650.size; ++_i653)",
                "+                org.apache.thrift.protocol.TMap _map670 = iprot.readMapBegin();",
                "+                struct.executor_node_port = new HashMap<List<Long>,NodeInfo>(2*_map670.size);",
                "+                List<Long> _key671;",
                "+                NodeInfo _val672;",
                "+                for (int _i673 = 0; _i673 < _map670.size; ++_i673)",
                "                 {",
                "                   {",
                "-                    org.apache.thrift.protocol.TList _list654 = iprot.readListBegin();",
                "-                    _key651 = new ArrayList<Long>(_list654.size);",
                "-                    long _elem655;",
                "-                    for (int _i656 = 0; _i656 < _list654.size; ++_i656)",
                "+                    org.apache.thrift.protocol.TList _list674 = iprot.readListBegin();",
                "+                    _key671 = new ArrayList<Long>(_list674.size);",
                "+                    long _elem675;",
                "+                    for (int _i676 = 0; _i676 < _list674.size; ++_i676)",
                "                     {",
                "-                      _elem655 = iprot.readI64();",
                "-                      _key651.add(_elem655);",
                "+                      _elem675 = iprot.readI64();",
                "+                      _key671.add(_elem675);",
                "                     }",
                "@@ -1005,5 +1005,5 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "                   }",
                "-                  _val652 = new NodeInfo();",
                "-                  _val652.read(iprot);",
                "-                  struct.executor_node_port.put(_key651, _val652);",
                "+                  _val672 = new NodeInfo();",
                "+                  _val672.read(iprot);",
                "+                  struct.executor_node_port.put(_key671, _val672);",
                "                 }",
                "@@ -1019,16 +1019,16 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map657 = iprot.readMapBegin();",
                "-                struct.executor_start_time_secs = new HashMap<List<Long>,Long>(2*_map657.size);",
                "-                List<Long> _key658;",
                "-                long _val659;",
                "-                for (int _i660 = 0; _i660 < _map657.size; ++_i660)",
                "+                org.apache.thrift.protocol.TMap _map677 = iprot.readMapBegin();",
                "+                struct.executor_start_time_secs = new HashMap<List<Long>,Long>(2*_map677.size);",
                "+                List<Long> _key678;",
                "+                long _val679;",
                "+                for (int _i680 = 0; _i680 < _map677.size; ++_i680)",
                "                 {",
                "                   {",
                "-                    org.apache.thrift.protocol.TList _list661 = iprot.readListBegin();",
                "-                    _key658 = new ArrayList<Long>(_list661.size);",
                "-                    long _elem662;",
                "-                    for (int _i663 = 0; _i663 < _list661.size; ++_i663)",
                "+                    org.apache.thrift.protocol.TList _list681 = iprot.readListBegin();",
                "+                    _key678 = new ArrayList<Long>(_list681.size);",
                "+                    long _elem682;",
                "+                    for (int _i683 = 0; _i683 < _list681.size; ++_i683)",
                "                     {",
                "-                      _elem662 = iprot.readI64();",
                "-                      _key658.add(_elem662);",
                "+                      _elem682 = iprot.readI64();",
                "+                      _key678.add(_elem682);",
                "                     }",
                "@@ -1036,4 +1036,4 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "                   }",
                "-                  _val659 = iprot.readI64();",
                "-                  struct.executor_start_time_secs.put(_key658, _val659);",
                "+                  _val679 = iprot.readI64();",
                "+                  struct.executor_start_time_secs.put(_key678, _val679);",
                "                 }",
                "@@ -1049,13 +1049,13 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map664 = iprot.readMapBegin();",
                "-                struct.worker_resources = new HashMap<NodeInfo,WorkerResources>(2*_map664.size);",
                "-                NodeInfo _key665;",
                "-                WorkerResources _val666;",
                "-                for (int _i667 = 0; _i667 < _map664.size; ++_i667)",
                "+                org.apache.thrift.protocol.TMap _map684 = iprot.readMapBegin();",
                "+                struct.worker_resources = new HashMap<NodeInfo,WorkerResources>(2*_map684.size);",
                "+                NodeInfo _key685;",
                "+                WorkerResources _val686;",
                "+                for (int _i687 = 0; _i687 < _map684.size; ++_i687)",
                "                 {",
                "-                  _key665 = new NodeInfo();",
                "-                  _key665.read(iprot);",
                "-                  _val666 = new WorkerResources();",
                "-                  _val666.read(iprot);",
                "-                  struct.worker_resources.put(_key665, _val666);",
                "+                  _key685 = new NodeInfo();",
                "+                  _key685.read(iprot);",
                "+                  _val686 = new WorkerResources();",
                "+                  _val686.read(iprot);",
                "+                  struct.worker_resources.put(_key685, _val686);",
                "                 }",
                "@@ -1071,11 +1071,11 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map668 = iprot.readMapBegin();",
                "-                struct.total_shared_off_heap = new HashMap<String,Double>(2*_map668.size);",
                "-                String _key669;",
                "-                double _val670;",
                "-                for (int _i671 = 0; _i671 < _map668.size; ++_i671)",
                "+                org.apache.thrift.protocol.TMap _map688 = iprot.readMapBegin();",
                "+                struct.total_shared_off_heap = new HashMap<String,Double>(2*_map688.size);",
                "+                String _key689;",
                "+                double _val690;",
                "+                for (int _i691 = 0; _i691 < _map688.size; ++_i691)",
                "                 {",
                "-                  _key669 = iprot.readString();",
                "-                  _val670 = iprot.readDouble();",
                "-                  struct.total_shared_off_heap.put(_key669, _val670);",
                "+                  _key689 = iprot.readString();",
                "+                  _val690 = iprot.readDouble();",
                "+                  struct.total_shared_off_heap.put(_key689, _val690);",
                "                 }",
                "@@ -1119,6 +1119,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, struct.node_host.size()));",
                "-            for (Map.Entry<String, String> _iter672 : struct.node_host.entrySet())",
                "+            for (Map.Entry<String, String> _iter692 : struct.node_host.entrySet())",
                "             {",
                "-              oprot.writeString(_iter672.getKey());",
                "-              oprot.writeString(_iter672.getValue());",
                "+              oprot.writeString(_iter692.getKey());",
                "+              oprot.writeString(_iter692.getValue());",
                "             }",
                "@@ -1134,9 +1134,9 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.STRUCT, struct.executor_node_port.size()));",
                "-            for (Map.Entry<List<Long>, NodeInfo> _iter673 : struct.executor_node_port.entrySet())",
                "+            for (Map.Entry<List<Long>, NodeInfo> _iter693 : struct.executor_node_port.entrySet())",
                "             {",
                "               {",
                "-                oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, _iter673.getKey().size()));",
                "-                for (long _iter674 : _iter673.getKey())",
                "+                oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, _iter693.getKey().size()));",
                "+                for (long _iter694 : _iter693.getKey())",
                "                 {",
                "-                  oprot.writeI64(_iter674);",
                "+                  oprot.writeI64(_iter694);",
                "                 }",
                "@@ -1144,3 +1144,3 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               }",
                "-              _iter673.getValue().write(oprot);",
                "+              _iter693.getValue().write(oprot);",
                "             }",
                "@@ -1156,9 +1156,9 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.I64, struct.executor_start_time_secs.size()));",
                "-            for (Map.Entry<List<Long>, Long> _iter675 : struct.executor_start_time_secs.entrySet())",
                "+            for (Map.Entry<List<Long>, Long> _iter695 : struct.executor_start_time_secs.entrySet())",
                "             {",
                "               {",
                "-                oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, _iter675.getKey().size()));",
                "-                for (long _iter676 : _iter675.getKey())",
                "+                oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, _iter695.getKey().size()));",
                "+                for (long _iter696 : _iter695.getKey())",
                "                 {",
                "-                  oprot.writeI64(_iter676);",
                "+                  oprot.writeI64(_iter696);",
                "                 }",
                "@@ -1166,3 +1166,3 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               }",
                "-              oprot.writeI64(_iter675.getValue());",
                "+              oprot.writeI64(_iter695.getValue());",
                "             }",
                "@@ -1178,6 +1178,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, struct.worker_resources.size()));",
                "-            for (Map.Entry<NodeInfo, WorkerResources> _iter677 : struct.worker_resources.entrySet())",
                "+            for (Map.Entry<NodeInfo, WorkerResources> _iter697 : struct.worker_resources.entrySet())",
                "             {",
                "-              _iter677.getKey().write(oprot);",
                "-              _iter677.getValue().write(oprot);",
                "+              _iter697.getKey().write(oprot);",
                "+              _iter697.getValue().write(oprot);",
                "             }",
                "@@ -1193,6 +1193,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, struct.total_shared_off_heap.size()));",
                "-            for (Map.Entry<String, Double> _iter678 : struct.total_shared_off_heap.entrySet())",
                "+            for (Map.Entry<String, Double> _iter698 : struct.total_shared_off_heap.entrySet())",
                "             {",
                "-              oprot.writeString(_iter678.getKey());",
                "-              oprot.writeDouble(_iter678.getValue());",
                "+              oprot.writeString(_iter698.getKey());",
                "+              oprot.writeDouble(_iter698.getValue());",
                "             }",
                "@@ -1251,6 +1251,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "           oprot.writeI32(struct.node_host.size());",
                "-          for (Map.Entry<String, String> _iter679 : struct.node_host.entrySet())",
                "+          for (Map.Entry<String, String> _iter699 : struct.node_host.entrySet())",
                "           {",
                "-            oprot.writeString(_iter679.getKey());",
                "-            oprot.writeString(_iter679.getValue());",
                "+            oprot.writeString(_iter699.getKey());",
                "+            oprot.writeString(_iter699.getValue());",
                "           }",
                "@@ -1261,12 +1261,12 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "           oprot.writeI32(struct.executor_node_port.size());",
                "-          for (Map.Entry<List<Long>, NodeInfo> _iter680 : struct.executor_node_port.entrySet())",
                "+          for (Map.Entry<List<Long>, NodeInfo> _iter700 : struct.executor_node_port.entrySet())",
                "           {",
                "             {",
                "-              oprot.writeI32(_iter680.getKey().size());",
                "-              for (long _iter681 : _iter680.getKey())",
                "+              oprot.writeI32(_iter700.getKey().size());",
                "+              for (long _iter701 : _iter700.getKey())",
                "               {",
                "-                oprot.writeI64(_iter681);",
                "+                oprot.writeI64(_iter701);",
                "               }",
                "             }",
                "-            _iter680.getValue().write(oprot);",
                "+            _iter700.getValue().write(oprot);",
                "           }",
                "@@ -1277,12 +1277,12 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "           oprot.writeI32(struct.executor_start_time_secs.size());",
                "-          for (Map.Entry<List<Long>, Long> _iter682 : struct.executor_start_time_secs.entrySet())",
                "+          for (Map.Entry<List<Long>, Long> _iter702 : struct.executor_start_time_secs.entrySet())",
                "           {",
                "             {",
                "-              oprot.writeI32(_iter682.getKey().size());",
                "-              for (long _iter683 : _iter682.getKey())",
                "+              oprot.writeI32(_iter702.getKey().size());",
                "+              for (long _iter703 : _iter702.getKey())",
                "               {",
                "-                oprot.writeI64(_iter683);",
                "+                oprot.writeI64(_iter703);",
                "               }",
                "             }",
                "-            oprot.writeI64(_iter682.getValue());",
                "+            oprot.writeI64(_iter702.getValue());",
                "           }",
                "@@ -1293,6 +1293,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "           oprot.writeI32(struct.worker_resources.size());",
                "-          for (Map.Entry<NodeInfo, WorkerResources> _iter684 : struct.worker_resources.entrySet())",
                "+          for (Map.Entry<NodeInfo, WorkerResources> _iter704 : struct.worker_resources.entrySet())",
                "           {",
                "-            _iter684.getKey().write(oprot);",
                "-            _iter684.getValue().write(oprot);",
                "+            _iter704.getKey().write(oprot);",
                "+            _iter704.getValue().write(oprot);",
                "           }",
                "@@ -1303,6 +1303,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "           oprot.writeI32(struct.total_shared_off_heap.size());",
                "-          for (Map.Entry<String, Double> _iter685 : struct.total_shared_off_heap.entrySet())",
                "+          for (Map.Entry<String, Double> _iter705 : struct.total_shared_off_heap.entrySet())",
                "           {",
                "-            oprot.writeString(_iter685.getKey());",
                "-            oprot.writeDouble(_iter685.getValue());",
                "+            oprot.writeString(_iter705.getKey());",
                "+            oprot.writeDouble(_iter705.getValue());",
                "           }",
                "@@ -1323,11 +1323,11 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map686 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-          struct.node_host = new HashMap<String,String>(2*_map686.size);",
                "-          String _key687;",
                "-          String _val688;",
                "-          for (int _i689 = 0; _i689 < _map686.size; ++_i689)",
                "+          org.apache.thrift.protocol.TMap _map706 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+          struct.node_host = new HashMap<String,String>(2*_map706.size);",
                "+          String _key707;",
                "+          String _val708;",
                "+          for (int _i709 = 0; _i709 < _map706.size; ++_i709)",
                "           {",
                "-            _key687 = iprot.readString();",
                "-            _val688 = iprot.readString();",
                "-            struct.node_host.put(_key687, _val688);",
                "+            _key707 = iprot.readString();",
                "+            _val708 = iprot.readString();",
                "+            struct.node_host.put(_key707, _val708);",
                "           }",
                "@@ -1338,21 +1338,21 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map690 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-          struct.executor_node_port = new HashMap<List<Long>,NodeInfo>(2*_map690.size);",
                "-          List<Long> _key691;",
                "-          NodeInfo _val692;",
                "-          for (int _i693 = 0; _i693 < _map690.size; ++_i693)",
                "+          org.apache.thrift.protocol.TMap _map710 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+          struct.executor_node_port = new HashMap<List<Long>,NodeInfo>(2*_map710.size);",
                "+          List<Long> _key711;",
                "+          NodeInfo _val712;",
                "+          for (int _i713 = 0; _i713 < _map710.size; ++_i713)",
                "           {",
                "             {",
                "-              org.apache.thrift.protocol.TList _list694 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "-              _key691 = new ArrayList<Long>(_list694.size);",
                "-              long _elem695;",
                "-              for (int _i696 = 0; _i696 < _list694.size; ++_i696)",
                "+              org.apache.thrift.protocol.TList _list714 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "+              _key711 = new ArrayList<Long>(_list714.size);",
                "+              long _elem715;",
                "+              for (int _i716 = 0; _i716 < _list714.size; ++_i716)",
                "               {",
                "-                _elem695 = iprot.readI64();",
                "-                _key691.add(_elem695);",
                "+                _elem715 = iprot.readI64();",
                "+                _key711.add(_elem715);",
                "               }",
                "             }",
                "-            _val692 = new NodeInfo();",
                "-            _val692.read(iprot);",
                "-            struct.executor_node_port.put(_key691, _val692);",
                "+            _val712 = new NodeInfo();",
                "+            _val712.read(iprot);",
                "+            struct.executor_node_port.put(_key711, _val712);",
                "           }",
                "@@ -1363,20 +1363,20 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map697 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "-          struct.executor_start_time_secs = new HashMap<List<Long>,Long>(2*_map697.size);",
                "-          List<Long> _key698;",
                "-          long _val699;",
                "-          for (int _i700 = 0; _i700 < _map697.size; ++_i700)",
                "+          org.apache.thrift.protocol.TMap _map717 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "+          struct.executor_start_time_secs = new HashMap<List<Long>,Long>(2*_map717.size);",
                "+          List<Long> _key718;",
                "+          long _val719;",
                "+          for (int _i720 = 0; _i720 < _map717.size; ++_i720)",
                "           {",
                "             {",
                "-              org.apache.thrift.protocol.TList _list701 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "-              _key698 = new ArrayList<Long>(_list701.size);",
                "-              long _elem702;",
                "-              for (int _i703 = 0; _i703 < _list701.size; ++_i703)",
                "+              org.apache.thrift.protocol.TList _list721 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "+              _key718 = new ArrayList<Long>(_list721.size);",
                "+              long _elem722;",
                "+              for (int _i723 = 0; _i723 < _list721.size; ++_i723)",
                "               {",
                "-                _elem702 = iprot.readI64();",
                "-                _key698.add(_elem702);",
                "+                _elem722 = iprot.readI64();",
                "+                _key718.add(_elem722);",
                "               }",
                "             }",
                "-            _val699 = iprot.readI64();",
                "-            struct.executor_start_time_secs.put(_key698, _val699);",
                "+            _val719 = iprot.readI64();",
                "+            struct.executor_start_time_secs.put(_key718, _val719);",
                "           }",
                "@@ -1387,13 +1387,13 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map704 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-          struct.worker_resources = new HashMap<NodeInfo,WorkerResources>(2*_map704.size);",
                "-          NodeInfo _key705;",
                "-          WorkerResources _val706;",
                "-          for (int _i707 = 0; _i707 < _map704.size; ++_i707)",
                "+          org.apache.thrift.protocol.TMap _map724 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+          struct.worker_resources = new HashMap<NodeInfo,WorkerResources>(2*_map724.size);",
                "+          NodeInfo _key725;",
                "+          WorkerResources _val726;",
                "+          for (int _i727 = 0; _i727 < _map724.size; ++_i727)",
                "           {",
                "-            _key705 = new NodeInfo();",
                "-            _key705.read(iprot);",
                "-            _val706 = new WorkerResources();",
                "-            _val706.read(iprot);",
                "-            struct.worker_resources.put(_key705, _val706);",
                "+            _key725 = new NodeInfo();",
                "+            _key725.read(iprot);",
                "+            _val726 = new WorkerResources();",
                "+            _val726.read(iprot);",
                "+            struct.worker_resources.put(_key725, _val726);",
                "           }",
                "@@ -1404,11 +1404,11 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map708 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "-          struct.total_shared_off_heap = new HashMap<String,Double>(2*_map708.size);",
                "-          String _key709;",
                "-          double _val710;",
                "-          for (int _i711 = 0; _i711 < _map708.size; ++_i711)",
                "+          org.apache.thrift.protocol.TMap _map728 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "+          struct.total_shared_off_heap = new HashMap<String,Double>(2*_map728.size);",
                "+          String _key729;",
                "+          double _val730;",
                "+          for (int _i731 = 0; _i731 < _map728.size; ++_i731)",
                "           {",
                "-            _key709 = iprot.readString();",
                "-            _val710 = iprot.readDouble();",
                "-            struct.total_shared_off_heap.put(_key709, _val710);",
                "+            _key729 = iprot.readString();",
                "+            _val730 = iprot.readDouble();",
                "+            struct.total_shared_off_heap.put(_key729, _val730);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/ClusterWorkerHeartbeat.java b/storm-client/src/jvm/org/apache/storm/generated/ClusterWorkerHeartbeat.java",
                "index f5a8acc21..1613778f7 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/ClusterWorkerHeartbeat.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/ClusterWorkerHeartbeat.java",
                "@@ -637,13 +637,13 @@ public class ClusterWorkerHeartbeat implements org.apache.thrift.TBase<ClusterWo",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map732 = iprot.readMapBegin();",
                "-                struct.executor_stats = new HashMap<ExecutorInfo,ExecutorStats>(2*_map732.size);",
                "-                ExecutorInfo _key733;",
                "-                ExecutorStats _val734;",
                "-                for (int _i735 = 0; _i735 < _map732.size; ++_i735)",
                "+                org.apache.thrift.protocol.TMap _map752 = iprot.readMapBegin();",
                "+                struct.executor_stats = new HashMap<ExecutorInfo,ExecutorStats>(2*_map752.size);",
                "+                ExecutorInfo _key753;",
                "+                ExecutorStats _val754;",
                "+                for (int _i755 = 0; _i755 < _map752.size; ++_i755)",
                "                 {",
                "-                  _key733 = new ExecutorInfo();",
                "-                  _key733.read(iprot);",
                "-                  _val734 = new ExecutorStats();",
                "-                  _val734.read(iprot);",
                "-                  struct.executor_stats.put(_key733, _val734);",
                "+                  _key753 = new ExecutorInfo();",
                "+                  _key753.read(iprot);",
                "+                  _val754 = new ExecutorStats();",
                "+                  _val754.read(iprot);",
                "+                  struct.executor_stats.put(_key753, _val754);",
                "                 }",
                "@@ -694,6 +694,6 @@ public class ClusterWorkerHeartbeat implements org.apache.thrift.TBase<ClusterWo",
                "           oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, struct.executor_stats.size()));",
                "-          for (Map.Entry<ExecutorInfo, ExecutorStats> _iter736 : struct.executor_stats.entrySet())",
                "+          for (Map.Entry<ExecutorInfo, ExecutorStats> _iter756 : struct.executor_stats.entrySet())",
                "           {",
                "-            _iter736.getKey().write(oprot);",
                "-            _iter736.getValue().write(oprot);",
                "+            _iter756.getKey().write(oprot);",
                "+            _iter756.getValue().write(oprot);",
                "           }",
                "@@ -729,6 +729,6 @@ public class ClusterWorkerHeartbeat implements org.apache.thrift.TBase<ClusterWo",
                "         oprot.writeI32(struct.executor_stats.size());",
                "-        for (Map.Entry<ExecutorInfo, ExecutorStats> _iter737 : struct.executor_stats.entrySet())",
                "+        for (Map.Entry<ExecutorInfo, ExecutorStats> _iter757 : struct.executor_stats.entrySet())",
                "         {",
                "-          _iter737.getKey().write(oprot);",
                "-          _iter737.getValue().write(oprot);",
                "+          _iter757.getKey().write(oprot);",
                "+          _iter757.getValue().write(oprot);",
                "         }",
                "@@ -745,13 +745,13 @@ public class ClusterWorkerHeartbeat implements org.apache.thrift.TBase<ClusterWo",
                "       {",
                "-        org.apache.thrift.protocol.TMap _map738 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.executor_stats = new HashMap<ExecutorInfo,ExecutorStats>(2*_map738.size);",
                "-        ExecutorInfo _key739;",
                "-        ExecutorStats _val740;",
                "-        for (int _i741 = 0; _i741 < _map738.size; ++_i741)",
                "+        org.apache.thrift.protocol.TMap _map758 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.executor_stats = new HashMap<ExecutorInfo,ExecutorStats>(2*_map758.size);",
                "+        ExecutorInfo _key759;",
                "+        ExecutorStats _val760;",
                "+        for (int _i761 = 0; _i761 < _map758.size; ++_i761)",
                "         {",
                "-          _key739 = new ExecutorInfo();",
                "-          _key739.read(iprot);",
                "-          _val740 = new ExecutorStats();",
                "-          _val740.read(iprot);",
                "-          struct.executor_stats.put(_key739, _val740);",
                "+          _key759 = new ExecutorInfo();",
                "+          _key759.read(iprot);",
                "+          _val760 = new ExecutorStats();",
                "+          _val760.read(iprot);",
                "+          struct.executor_stats.put(_key759, _val760);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/Credentials.java b/storm-client/src/jvm/org/apache/storm/generated/Credentials.java",
                "index 4d7b6940e..84a8fb904 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/Credentials.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/Credentials.java",
                "@@ -367,11 +367,11 @@ public class Credentials implements org.apache.thrift.TBase<Credentials, Credent",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map576 = iprot.readMapBegin();",
                "-                struct.creds = new HashMap<String,String>(2*_map576.size);",
                "-                String _key577;",
                "-                String _val578;",
                "-                for (int _i579 = 0; _i579 < _map576.size; ++_i579)",
                "+                org.apache.thrift.protocol.TMap _map596 = iprot.readMapBegin();",
                "+                struct.creds = new HashMap<String,String>(2*_map596.size);",
                "+                String _key597;",
                "+                String _val598;",
                "+                for (int _i599 = 0; _i599 < _map596.size; ++_i599)",
                "                 {",
                "-                  _key577 = iprot.readString();",
                "-                  _val578 = iprot.readString();",
                "-                  struct.creds.put(_key577, _val578);",
                "+                  _key597 = iprot.readString();",
                "+                  _val598 = iprot.readString();",
                "+                  struct.creds.put(_key597, _val598);",
                "                 }",
                "@@ -401,6 +401,6 @@ public class Credentials implements org.apache.thrift.TBase<Credentials, Credent",
                "           oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, struct.creds.size()));",
                "-          for (Map.Entry<String, String> _iter580 : struct.creds.entrySet())",
                "+          for (Map.Entry<String, String> _iter600 : struct.creds.entrySet())",
                "           {",
                "-            oprot.writeString(_iter580.getKey());",
                "-            oprot.writeString(_iter580.getValue());",
                "+            oprot.writeString(_iter600.getKey());",
                "+            oprot.writeString(_iter600.getValue());",
                "           }",
                "@@ -429,6 +429,6 @@ public class Credentials implements org.apache.thrift.TBase<Credentials, Credent",
                "         oprot.writeI32(struct.creds.size());",
                "-        for (Map.Entry<String, String> _iter581 : struct.creds.entrySet())",
                "+        for (Map.Entry<String, String> _iter601 : struct.creds.entrySet())",
                "         {",
                "-          oprot.writeString(_iter581.getKey());",
                "-          oprot.writeString(_iter581.getValue());",
                "+          oprot.writeString(_iter601.getKey());",
                "+          oprot.writeString(_iter601.getValue());",
                "         }",
                "@@ -441,11 +441,11 @@ public class Credentials implements org.apache.thrift.TBase<Credentials, Credent",
                "       {",
                "-        org.apache.thrift.protocol.TMap _map582 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-        struct.creds = new HashMap<String,String>(2*_map582.size);",
                "-        String _key583;",
                "-        String _val584;",
                "-        for (int _i585 = 0; _i585 < _map582.size; ++_i585)",
                "+        org.apache.thrift.protocol.TMap _map602 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+        struct.creds = new HashMap<String,String>(2*_map602.size);",
                "+        String _key603;",
                "+        String _val604;",
                "+        for (int _i605 = 0; _i605 < _map602.size; ++_i605)",
                "         {",
                "-          _key583 = iprot.readString();",
                "-          _val584 = iprot.readString();",
                "-          struct.creds.put(_key583, _val584);",
                "+          _key603 = iprot.readString();",
                "+          _val604 = iprot.readString();",
                "+          struct.creds.put(_key603, _val604);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/HBNodes.java b/storm-client/src/jvm/org/apache/storm/generated/HBNodes.java",
                "index bdec1afe9..75c5c6d4a 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/HBNodes.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/HBNodes.java",
                "@@ -366,9 +366,9 @@ public class HBNodes implements org.apache.thrift.TBase<HBNodes, HBNodes._Fields",
                "               {",
                "-                org.apache.thrift.protocol.TList _list838 = iprot.readListBegin();",
                "-                struct.pulseIds = new ArrayList<String>(_list838.size);",
                "-                String _elem839;",
                "-                for (int _i840 = 0; _i840 < _list838.size; ++_i840)",
                "+                org.apache.thrift.protocol.TList _list858 = iprot.readListBegin();",
                "+                struct.pulseIds = new ArrayList<String>(_list858.size);",
                "+                String _elem859;",
                "+                for (int _i860 = 0; _i860 < _list858.size; ++_i860)",
                "                 {",
                "-                  _elem839 = iprot.readString();",
                "-                  struct.pulseIds.add(_elem839);",
                "+                  _elem859 = iprot.readString();",
                "+                  struct.pulseIds.add(_elem859);",
                "                 }",
                "@@ -398,5 +398,5 @@ public class HBNodes implements org.apache.thrift.TBase<HBNodes, HBNodes._Fields",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, struct.pulseIds.size()));",
                "-          for (String _iter841 : struct.pulseIds)",
                "+          for (String _iter861 : struct.pulseIds)",
                "           {",
                "-            oprot.writeString(_iter841);",
                "+            oprot.writeString(_iter861);",
                "           }",
                "@@ -431,5 +431,5 @@ public class HBNodes implements org.apache.thrift.TBase<HBNodes, HBNodes._Fields",
                "           oprot.writeI32(struct.pulseIds.size());",
                "-          for (String _iter842 : struct.pulseIds)",
                "+          for (String _iter862 : struct.pulseIds)",
                "           {",
                "-            oprot.writeString(_iter842);",
                "+            oprot.writeString(_iter862);",
                "           }",
                "@@ -445,9 +445,9 @@ public class HBNodes implements org.apache.thrift.TBase<HBNodes, HBNodes._Fields",
                "         {",
                "-          org.apache.thrift.protocol.TList _list843 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-          struct.pulseIds = new ArrayList<String>(_list843.size);",
                "-          String _elem844;",
                "-          for (int _i845 = 0; _i845 < _list843.size; ++_i845)",
                "+          org.apache.thrift.protocol.TList _list863 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+          struct.pulseIds = new ArrayList<String>(_list863.size);",
                "+          String _elem864;",
                "+          for (int _i865 = 0; _i865 < _list863.size; ++_i865)",
                "           {",
                "-            _elem844 = iprot.readString();",
                "-            struct.pulseIds.add(_elem844);",
                "+            _elem864 = iprot.readString();",
                "+            struct.pulseIds.add(_elem864);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/HBRecords.java b/storm-client/src/jvm/org/apache/storm/generated/HBRecords.java",
                "index 83d49bfc5..f726e5cf3 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/HBRecords.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/HBRecords.java",
                "@@ -369,10 +369,10 @@ public class HBRecords implements org.apache.thrift.TBase<HBRecords, HBRecords._",
                "               {",
                "-                org.apache.thrift.protocol.TList _list830 = iprot.readListBegin();",
                "-                struct.pulses = new ArrayList<HBPulse>(_list830.size);",
                "-                HBPulse _elem831;",
                "-                for (int _i832 = 0; _i832 < _list830.size; ++_i832)",
                "+                org.apache.thrift.protocol.TList _list850 = iprot.readListBegin();",
                "+                struct.pulses = new ArrayList<HBPulse>(_list850.size);",
                "+                HBPulse _elem851;",
                "+                for (int _i852 = 0; _i852 < _list850.size; ++_i852)",
                "                 {",
                "-                  _elem831 = new HBPulse();",
                "-                  _elem831.read(iprot);",
                "-                  struct.pulses.add(_elem831);",
                "+                  _elem851 = new HBPulse();",
                "+                  _elem851.read(iprot);",
                "+                  struct.pulses.add(_elem851);",
                "                 }",
                "@@ -402,5 +402,5 @@ public class HBRecords implements org.apache.thrift.TBase<HBRecords, HBRecords._",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.pulses.size()));",
                "-          for (HBPulse _iter833 : struct.pulses)",
                "+          for (HBPulse _iter853 : struct.pulses)",
                "           {",
                "-            _iter833.write(oprot);",
                "+            _iter853.write(oprot);",
                "           }",
                "@@ -435,5 +435,5 @@ public class HBRecords implements org.apache.thrift.TBase<HBRecords, HBRecords._",
                "           oprot.writeI32(struct.pulses.size());",
                "-          for (HBPulse _iter834 : struct.pulses)",
                "+          for (HBPulse _iter854 : struct.pulses)",
                "           {",
                "-            _iter834.write(oprot);",
                "+            _iter854.write(oprot);",
                "           }",
                "@@ -449,10 +449,10 @@ public class HBRecords implements org.apache.thrift.TBase<HBRecords, HBRecords._",
                "         {",
                "-          org.apache.thrift.protocol.TList _list835 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-          struct.pulses = new ArrayList<HBPulse>(_list835.size);",
                "-          HBPulse _elem836;",
                "-          for (int _i837 = 0; _i837 < _list835.size; ++_i837)",
                "+          org.apache.thrift.protocol.TList _list855 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+          struct.pulses = new ArrayList<HBPulse>(_list855.size);",
                "+          HBPulse _elem856;",
                "+          for (int _i857 = 0; _i857 < _list855.size; ++_i857)",
                "           {",
                "-            _elem836 = new HBPulse();",
                "-            _elem836.read(iprot);",
                "-            struct.pulses.add(_elem836);",
                "+            _elem856 = new HBPulse();",
                "+            _elem856.read(iprot);",
                "+            struct.pulses.add(_elem856);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LSApprovedWorkers.java b/storm-client/src/jvm/org/apache/storm/generated/LSApprovedWorkers.java",
                "index fc7fe5a28..9e8e5cf7d 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LSApprovedWorkers.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LSApprovedWorkers.java",
                "@@ -367,11 +367,11 @@ public class LSApprovedWorkers implements org.apache.thrift.TBase<LSApprovedWork",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map760 = iprot.readMapBegin();",
                "-                struct.approved_workers = new HashMap<String,Integer>(2*_map760.size);",
                "-                String _key761;",
                "-                int _val762;",
                "-                for (int _i763 = 0; _i763 < _map760.size; ++_i763)",
                "+                org.apache.thrift.protocol.TMap _map780 = iprot.readMapBegin();",
                "+                struct.approved_workers = new HashMap<String,Integer>(2*_map780.size);",
                "+                String _key781;",
                "+                int _val782;",
                "+                for (int _i783 = 0; _i783 < _map780.size; ++_i783)",
                "                 {",
                "-                  _key761 = iprot.readString();",
                "-                  _val762 = iprot.readI32();",
                "-                  struct.approved_workers.put(_key761, _val762);",
                "+                  _key781 = iprot.readString();",
                "+                  _val782 = iprot.readI32();",
                "+                  struct.approved_workers.put(_key781, _val782);",
                "                 }",
                "@@ -401,6 +401,6 @@ public class LSApprovedWorkers implements org.apache.thrift.TBase<LSApprovedWork",
                "           oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, struct.approved_workers.size()));",
                "-          for (Map.Entry<String, Integer> _iter764 : struct.approved_workers.entrySet())",
                "+          for (Map.Entry<String, Integer> _iter784 : struct.approved_workers.entrySet())",
                "           {",
                "-            oprot.writeString(_iter764.getKey());",
                "-            oprot.writeI32(_iter764.getValue());",
                "+            oprot.writeString(_iter784.getKey());",
                "+            oprot.writeI32(_iter784.getValue());",
                "           }",
                "@@ -429,6 +429,6 @@ public class LSApprovedWorkers implements org.apache.thrift.TBase<LSApprovedWork",
                "         oprot.writeI32(struct.approved_workers.size());",
                "-        for (Map.Entry<String, Integer> _iter765 : struct.approved_workers.entrySet())",
                "+        for (Map.Entry<String, Integer> _iter785 : struct.approved_workers.entrySet())",
                "         {",
                "-          oprot.writeString(_iter765.getKey());",
                "-          oprot.writeI32(_iter765.getValue());",
                "+          oprot.writeString(_iter785.getKey());",
                "+          oprot.writeI32(_iter785.getValue());",
                "         }",
                "@@ -441,11 +441,11 @@ public class LSApprovedWorkers implements org.apache.thrift.TBase<LSApprovedWork",
                "       {",
                "-        org.apache.thrift.protocol.TMap _map766 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, iprot.readI32());",
                "-        struct.approved_workers = new HashMap<String,Integer>(2*_map766.size);",
                "-        String _key767;",
                "-        int _val768;",
                "-        for (int _i769 = 0; _i769 < _map766.size; ++_i769)",
                "+        org.apache.thrift.protocol.TMap _map786 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, iprot.readI32());",
                "+        struct.approved_workers = new HashMap<String,Integer>(2*_map786.size);",
                "+        String _key787;",
                "+        int _val788;",
                "+        for (int _i789 = 0; _i789 < _map786.size; ++_i789)",
                "         {",
                "-          _key767 = iprot.readString();",
                "-          _val768 = iprot.readI32();",
                "-          struct.approved_workers.put(_key767, _val768);",
                "+          _key787 = iprot.readString();",
                "+          _val788 = iprot.readI32();",
                "+          struct.approved_workers.put(_key787, _val788);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LSSupervisorAssignments.java b/storm-client/src/jvm/org/apache/storm/generated/LSSupervisorAssignments.java",
                "index ddf645646..64c36e27e 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LSSupervisorAssignments.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LSSupervisorAssignments.java",
                "@@ -378,12 +378,12 @@ public class LSSupervisorAssignments implements org.apache.thrift.TBase<LSSuperv",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map770 = iprot.readMapBegin();",
                "-                struct.assignments = new HashMap<Integer,LocalAssignment>(2*_map770.size);",
                "-                int _key771;",
                "-                LocalAssignment _val772;",
                "-                for (int _i773 = 0; _i773 < _map770.size; ++_i773)",
                "+                org.apache.thrift.protocol.TMap _map790 = iprot.readMapBegin();",
                "+                struct.assignments = new HashMap<Integer,LocalAssignment>(2*_map790.size);",
                "+                int _key791;",
                "+                LocalAssignment _val792;",
                "+                for (int _i793 = 0; _i793 < _map790.size; ++_i793)",
                "                 {",
                "-                  _key771 = iprot.readI32();",
                "-                  _val772 = new LocalAssignment();",
                "-                  _val772.read(iprot);",
                "-                  struct.assignments.put(_key771, _val772);",
                "+                  _key791 = iprot.readI32();",
                "+                  _val792 = new LocalAssignment();",
                "+                  _val792.read(iprot);",
                "+                  struct.assignments.put(_key791, _val792);",
                "                 }",
                "@@ -413,6 +413,6 @@ public class LSSupervisorAssignments implements org.apache.thrift.TBase<LSSuperv",
                "           oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.I32, org.apache.thrift.protocol.TType.STRUCT, struct.assignments.size()));",
                "-          for (Map.Entry<Integer, LocalAssignment> _iter774 : struct.assignments.entrySet())",
                "+          for (Map.Entry<Integer, LocalAssignment> _iter794 : struct.assignments.entrySet())",
                "           {",
                "-            oprot.writeI32(_iter774.getKey());",
                "-            _iter774.getValue().write(oprot);",
                "+            oprot.writeI32(_iter794.getKey());",
                "+            _iter794.getValue().write(oprot);",
                "           }",
                "@@ -441,6 +441,6 @@ public class LSSupervisorAssignments implements org.apache.thrift.TBase<LSSuperv",
                "         oprot.writeI32(struct.assignments.size());",
                "-        for (Map.Entry<Integer, LocalAssignment> _iter775 : struct.assignments.entrySet())",
                "+        for (Map.Entry<Integer, LocalAssignment> _iter795 : struct.assignments.entrySet())",
                "         {",
                "-          oprot.writeI32(_iter775.getKey());",
                "-          _iter775.getValue().write(oprot);",
                "+          oprot.writeI32(_iter795.getKey());",
                "+          _iter795.getValue().write(oprot);",
                "         }",
                "@@ -453,12 +453,12 @@ public class LSSupervisorAssignments implements org.apache.thrift.TBase<LSSuperv",
                "       {",
                "-        org.apache.thrift.protocol.TMap _map776 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.I32, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.assignments = new HashMap<Integer,LocalAssignment>(2*_map776.size);",
                "-        int _key777;",
                "-        LocalAssignment _val778;",
                "-        for (int _i779 = 0; _i779 < _map776.size; ++_i779)",
                "+        org.apache.thrift.protocol.TMap _map796 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.I32, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.assignments = new HashMap<Integer,LocalAssignment>(2*_map796.size);",
                "+        int _key797;",
                "+        LocalAssignment _val798;",
                "+        for (int _i799 = 0; _i799 < _map796.size; ++_i799)",
                "         {",
                "-          _key777 = iprot.readI32();",
                "-          _val778 = new LocalAssignment();",
                "-          _val778.read(iprot);",
                "-          struct.assignments.put(_key777, _val778);",
                "+          _key797 = iprot.readI32();",
                "+          _val798 = new LocalAssignment();",
                "+          _val798.read(iprot);",
                "+          struct.assignments.put(_key797, _val798);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistory.java b/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistory.java",
                "index 8896a0dc5..6de53df9b 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistory.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistory.java",
                "@@ -658,9 +658,9 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "               {",
                "-                org.apache.thrift.protocol.TList _list788 = iprot.readListBegin();",
                "-                struct.users = new ArrayList<String>(_list788.size);",
                "-                String _elem789;",
                "-                for (int _i790 = 0; _i790 < _list788.size; ++_i790)",
                "+                org.apache.thrift.protocol.TList _list808 = iprot.readListBegin();",
                "+                struct.users = new ArrayList<String>(_list808.size);",
                "+                String _elem809;",
                "+                for (int _i810 = 0; _i810 < _list808.size; ++_i810)",
                "                 {",
                "-                  _elem789 = iprot.readString();",
                "-                  struct.users.add(_elem789);",
                "+                  _elem809 = iprot.readString();",
                "+                  struct.users.add(_elem809);",
                "                 }",
                "@@ -676,9 +676,9 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "               {",
                "-                org.apache.thrift.protocol.TList _list791 = iprot.readListBegin();",
                "-                struct.groups = new ArrayList<String>(_list791.size);",
                "-                String _elem792;",
                "-                for (int _i793 = 0; _i793 < _list791.size; ++_i793)",
                "+                org.apache.thrift.protocol.TList _list811 = iprot.readListBegin();",
                "+                struct.groups = new ArrayList<String>(_list811.size);",
                "+                String _elem812;",
                "+                for (int _i813 = 0; _i813 < _list811.size; ++_i813)",
                "                 {",
                "-                  _elem792 = iprot.readString();",
                "-                  struct.groups.add(_elem792);",
                "+                  _elem812 = iprot.readString();",
                "+                  struct.groups.add(_elem812);",
                "                 }",
                "@@ -716,5 +716,5 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, struct.users.size()));",
                "-          for (String _iter794 : struct.users)",
                "+          for (String _iter814 : struct.users)",
                "           {",
                "-            oprot.writeString(_iter794);",
                "+            oprot.writeString(_iter814);",
                "           }",
                "@@ -728,5 +728,5 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, struct.groups.size()));",
                "-          for (String _iter795 : struct.groups)",
                "+          for (String _iter815 : struct.groups)",
                "           {",
                "-            oprot.writeString(_iter795);",
                "+            oprot.writeString(_iter815);",
                "           }",
                "@@ -757,5 +757,5 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "         oprot.writeI32(struct.users.size());",
                "-        for (String _iter796 : struct.users)",
                "+        for (String _iter816 : struct.users)",
                "         {",
                "-          oprot.writeString(_iter796);",
                "+          oprot.writeString(_iter816);",
                "         }",
                "@@ -764,5 +764,5 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "         oprot.writeI32(struct.groups.size());",
                "-        for (String _iter797 : struct.groups)",
                "+        for (String _iter817 : struct.groups)",
                "         {",
                "-          oprot.writeString(_iter797);",
                "+          oprot.writeString(_iter817);",
                "         }",
                "@@ -779,9 +779,9 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "       {",
                "-        org.apache.thrift.protocol.TList _list798 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-        struct.users = new ArrayList<String>(_list798.size);",
                "-        String _elem799;",
                "-        for (int _i800 = 0; _i800 < _list798.size; ++_i800)",
                "+        org.apache.thrift.protocol.TList _list818 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+        struct.users = new ArrayList<String>(_list818.size);",
                "+        String _elem819;",
                "+        for (int _i820 = 0; _i820 < _list818.size; ++_i820)",
                "         {",
                "-          _elem799 = iprot.readString();",
                "-          struct.users.add(_elem799);",
                "+          _elem819 = iprot.readString();",
                "+          struct.users.add(_elem819);",
                "         }",
                "@@ -790,9 +790,9 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "       {",
                "-        org.apache.thrift.protocol.TList _list801 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-        struct.groups = new ArrayList<String>(_list801.size);",
                "-        String _elem802;",
                "-        for (int _i803 = 0; _i803 < _list801.size; ++_i803)",
                "+        org.apache.thrift.protocol.TList _list821 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+        struct.groups = new ArrayList<String>(_list821.size);",
                "+        String _elem822;",
                "+        for (int _i823 = 0; _i823 < _list821.size; ++_i823)",
                "         {",
                "-          _elem802 = iprot.readString();",
                "-          struct.groups.add(_elem802);",
                "+          _elem822 = iprot.readString();",
                "+          struct.groups.add(_elem822);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistoryList.java b/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistoryList.java",
                "index 5d8f526e1..790a6fba3 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistoryList.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistoryList.java",
                "@@ -373,10 +373,10 @@ public class LSTopoHistoryList implements org.apache.thrift.TBase<LSTopoHistoryL",
                "               {",
                "-                org.apache.thrift.protocol.TList _list804 = iprot.readListBegin();",
                "-                struct.topo_history = new ArrayList<LSTopoHistory>(_list804.size);",
                "-                LSTopoHistory _elem805;",
                "-                for (int _i806 = 0; _i806 < _list804.size; ++_i806)",
                "+                org.apache.thrift.protocol.TList _list824 = iprot.readListBegin();",
                "+                struct.topo_history = new ArrayList<LSTopoHistory>(_list824.size);",
                "+                LSTopoHistory _elem825;",
                "+                for (int _i826 = 0; _i826 < _list824.size; ++_i826)",
                "                 {",
                "-                  _elem805 = new LSTopoHistory();",
                "-                  _elem805.read(iprot);",
                "-                  struct.topo_history.add(_elem805);",
                "+                  _elem825 = new LSTopoHistory();",
                "+                  _elem825.read(iprot);",
                "+                  struct.topo_history.add(_elem825);",
                "                 }",
                "@@ -406,5 +406,5 @@ public class LSTopoHistoryList implements org.apache.thrift.TBase<LSTopoHistoryL",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.topo_history.size()));",
                "-          for (LSTopoHistory _iter807 : struct.topo_history)",
                "+          for (LSTopoHistory _iter827 : struct.topo_history)",
                "           {",
                "-            _iter807.write(oprot);",
                "+            _iter827.write(oprot);",
                "           }",
                "@@ -433,5 +433,5 @@ public class LSTopoHistoryList implements org.apache.thrift.TBase<LSTopoHistoryL",
                "         oprot.writeI32(struct.topo_history.size());",
                "-        for (LSTopoHistory _iter808 : struct.topo_history)",
                "+        for (LSTopoHistory _iter828 : struct.topo_history)",
                "         {",
                "-          _iter808.write(oprot);",
                "+          _iter828.write(oprot);",
                "         }",
                "@@ -444,10 +444,10 @@ public class LSTopoHistoryList implements org.apache.thrift.TBase<LSTopoHistoryL",
                "       {",
                "-        org.apache.thrift.protocol.TList _list809 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.topo_history = new ArrayList<LSTopoHistory>(_list809.size);",
                "-        LSTopoHistory _elem810;",
                "-        for (int _i811 = 0; _i811 < _list809.size; ++_i811)",
                "+        org.apache.thrift.protocol.TList _list829 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.topo_history = new ArrayList<LSTopoHistory>(_list829.size);",
                "+        LSTopoHistory _elem830;",
                "+        for (int _i831 = 0; _i831 < _list829.size; ++_i831)",
                "         {",
                "-          _elem810 = new LSTopoHistory();",
                "-          _elem810.read(iprot);",
                "-          struct.topo_history.add(_elem810);",
                "+          _elem830 = new LSTopoHistory();",
                "+          _elem830.read(iprot);",
                "+          struct.topo_history.add(_elem830);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LSWorkerHeartbeat.java b/storm-client/src/jvm/org/apache/storm/generated/LSWorkerHeartbeat.java",
                "index 6db829815..6cf386f10 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LSWorkerHeartbeat.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LSWorkerHeartbeat.java",
                "@@ -640,10 +640,10 @@ public class LSWorkerHeartbeat implements org.apache.thrift.TBase<LSWorkerHeartb",
                "               {",
                "-                org.apache.thrift.protocol.TList _list780 = iprot.readListBegin();",
                "-                struct.executors = new ArrayList<ExecutorInfo>(_list780.size);",
                "-                ExecutorInfo _elem781;",
                "-                for (int _i782 = 0; _i782 < _list780.size; ++_i782)",
                "+                org.apache.thrift.protocol.TList _list800 = iprot.readListBegin();",
                "+                struct.executors = new ArrayList<ExecutorInfo>(_list800.size);",
                "+                ExecutorInfo _elem801;",
                "+                for (int _i802 = 0; _i802 < _list800.size; ++_i802)",
                "                 {",
                "-                  _elem781 = new ExecutorInfo();",
                "-                  _elem781.read(iprot);",
                "-                  struct.executors.add(_elem781);",
                "+                  _elem801 = new ExecutorInfo();",
                "+                  _elem801.read(iprot);",
                "+                  struct.executors.add(_elem801);",
                "                 }",
                "@@ -689,5 +689,5 @@ public class LSWorkerHeartbeat implements org.apache.thrift.TBase<LSWorkerHeartb",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.executors.size()));",
                "-          for (ExecutorInfo _iter783 : struct.executors)",
                "+          for (ExecutorInfo _iter803 : struct.executors)",
                "           {",
                "-            _iter783.write(oprot);",
                "+            _iter803.write(oprot);",
                "           }",
                "@@ -721,5 +721,5 @@ public class LSWorkerHeartbeat implements org.apache.thrift.TBase<LSWorkerHeartb",
                "         oprot.writeI32(struct.executors.size());",
                "-        for (ExecutorInfo _iter784 : struct.executors)",
                "+        for (ExecutorInfo _iter804 : struct.executors)",
                "         {",
                "-          _iter784.write(oprot);",
                "+          _iter804.write(oprot);",
                "         }",
                "@@ -737,10 +737,10 @@ public class LSWorkerHeartbeat implements org.apache.thrift.TBase<LSWorkerHeartb",
                "       {",
                "-        org.apache.thrift.protocol.TList _list785 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.executors = new ArrayList<ExecutorInfo>(_list785.size);",
                "-        ExecutorInfo _elem786;",
                "-        for (int _i787 = 0; _i787 < _list785.size; ++_i787)",
                "+        org.apache.thrift.protocol.TList _list805 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.executors = new ArrayList<ExecutorInfo>(_list805.size);",
                "+        ExecutorInfo _elem806;",
                "+        for (int _i807 = 0; _i807 < _list805.size; ++_i807)",
                "         {",
                "-          _elem786 = new ExecutorInfo();",
                "-          _elem786.read(iprot);",
                "-          struct.executors.add(_elem786);",
                "+          _elem806 = new ExecutorInfo();",
                "+          _elem806.read(iprot);",
                "+          struct.executors.add(_elem806);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/ListBlobsResult.java b/storm-client/src/jvm/org/apache/storm/generated/ListBlobsResult.java",
                "index ef24599b6..21b3e5ba8 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/ListBlobsResult.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/ListBlobsResult.java",
                "@@ -455,9 +455,9 @@ public class ListBlobsResult implements org.apache.thrift.TBase<ListBlobsResult,",
                "               {",
                "-                org.apache.thrift.protocol.TList _list594 = iprot.readListBegin();",
                "-                struct.keys = new ArrayList<String>(_list594.size);",
                "-                String _elem595;",
                "-                for (int _i596 = 0; _i596 < _list594.size; ++_i596)",
                "+                org.apache.thrift.protocol.TList _list614 = iprot.readListBegin();",
                "+                struct.keys = new ArrayList<String>(_list614.size);",
                "+                String _elem615;",
                "+                for (int _i616 = 0; _i616 < _list614.size; ++_i616)",
                "                 {",
                "-                  _elem595 = iprot.readString();",
                "-                  struct.keys.add(_elem595);",
                "+                  _elem615 = iprot.readString();",
                "+                  struct.keys.add(_elem615);",
                "                 }",
                "@@ -495,5 +495,5 @@ public class ListBlobsResult implements org.apache.thrift.TBase<ListBlobsResult,",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, struct.keys.size()));",
                "-          for (String _iter597 : struct.keys)",
                "+          for (String _iter617 : struct.keys)",
                "           {",
                "-            oprot.writeString(_iter597);",
                "+            oprot.writeString(_iter617);",
                "           }",
                "@@ -527,5 +527,5 @@ public class ListBlobsResult implements org.apache.thrift.TBase<ListBlobsResult,",
                "         oprot.writeI32(struct.keys.size());",
                "-        for (String _iter598 : struct.keys)",
                "+        for (String _iter618 : struct.keys)",
                "         {",
                "-          oprot.writeString(_iter598);",
                "+          oprot.writeString(_iter618);",
                "         }",
                "@@ -539,9 +539,9 @@ public class ListBlobsResult implements org.apache.thrift.TBase<ListBlobsResult,",
                "       {",
                "-        org.apache.thrift.protocol.TList _list599 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-        struct.keys = new ArrayList<String>(_list599.size);",
                "-        String _elem600;",
                "-        for (int _i601 = 0; _i601 < _list599.size; ++_i601)",
                "+        org.apache.thrift.protocol.TList _list619 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+        struct.keys = new ArrayList<String>(_list619.size);",
                "+        String _elem620;",
                "+        for (int _i621 = 0; _i621 < _list619.size; ++_i621)",
                "         {",
                "-          _elem600 = iprot.readString();",
                "-          struct.keys.add(_elem600);",
                "+          _elem620 = iprot.readString();",
                "+          struct.keys.add(_elem620);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LocalAssignment.java b/storm-client/src/jvm/org/apache/storm/generated/LocalAssignment.java",
                "index 88ce03987..e4d83aafe 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LocalAssignment.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LocalAssignment.java",
                "@@ -712,10 +712,10 @@ public class LocalAssignment implements org.apache.thrift.TBase<LocalAssignment,",
                "               {",
                "-                org.apache.thrift.protocol.TList _list752 = iprot.readListBegin();",
                "-                struct.executors = new ArrayList<ExecutorInfo>(_list752.size);",
                "-                ExecutorInfo _elem753;",
                "-                for (int _i754 = 0; _i754 < _list752.size; ++_i754)",
                "+                org.apache.thrift.protocol.TList _list772 = iprot.readListBegin();",
                "+                struct.executors = new ArrayList<ExecutorInfo>(_list772.size);",
                "+                ExecutorInfo _elem773;",
                "+                for (int _i774 = 0; _i774 < _list772.size; ++_i774)",
                "                 {",
                "-                  _elem753 = new ExecutorInfo();",
                "-                  _elem753.read(iprot);",
                "-                  struct.executors.add(_elem753);",
                "+                  _elem773 = new ExecutorInfo();",
                "+                  _elem773.read(iprot);",
                "+                  struct.executors.add(_elem773);",
                "                 }",
                "@@ -775,5 +775,5 @@ public class LocalAssignment implements org.apache.thrift.TBase<LocalAssignment,",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.executors.size()));",
                "-          for (ExecutorInfo _iter755 : struct.executors)",
                "+          for (ExecutorInfo _iter775 : struct.executors)",
                "           {",
                "-            _iter755.write(oprot);",
                "+            _iter775.write(oprot);",
                "           }",
                "@@ -822,5 +822,5 @@ public class LocalAssignment implements org.apache.thrift.TBase<LocalAssignment,",
                "         oprot.writeI32(struct.executors.size());",
                "-        for (ExecutorInfo _iter756 : struct.executors)",
                "+        for (ExecutorInfo _iter776 : struct.executors)",
                "         {",
                "-          _iter756.write(oprot);",
                "+          _iter776.write(oprot);",
                "         }",
                "@@ -855,10 +855,10 @@ public class LocalAssignment implements org.apache.thrift.TBase<LocalAssignment,",
                "       {",
                "-        org.apache.thrift.protocol.TList _list757 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.executors = new ArrayList<ExecutorInfo>(_list757.size);",
                "-        ExecutorInfo _elem758;",
                "-        for (int _i759 = 0; _i759 < _list757.size; ++_i759)",
                "+        org.apache.thrift.protocol.TList _list777 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.executors = new ArrayList<ExecutorInfo>(_list777.size);",
                "+        ExecutorInfo _elem778;",
                "+        for (int _i779 = 0; _i779 < _list777.size; ++_i779)",
                "         {",
                "-          _elem758 = new ExecutorInfo();",
                "-          _elem758.read(iprot);",
                "-          struct.executors.add(_elem758);",
                "+          _elem778 = new ExecutorInfo();",
                "+          _elem778.read(iprot);",
                "+          struct.executors.add(_elem778);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LocalStateData.java b/storm-client/src/jvm/org/apache/storm/generated/LocalStateData.java",
                "index d644e7330..3536c0b17 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LocalStateData.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LocalStateData.java",
                "@@ -378,12 +378,12 @@ public class LocalStateData implements org.apache.thrift.TBase<LocalStateData, L",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map742 = iprot.readMapBegin();",
                "-                struct.serialized_parts = new HashMap<String,ThriftSerializedObject>(2*_map742.size);",
                "-                String _key743;",
                "-                ThriftSerializedObject _val744;",
                "-                for (int _i745 = 0; _i745 < _map742.size; ++_i745)",
                "+                org.apache.thrift.protocol.TMap _map762 = iprot.readMapBegin();",
                "+                struct.serialized_parts = new HashMap<String,ThriftSerializedObject>(2*_map762.size);",
                "+                String _key763;",
                "+                ThriftSerializedObject _val764;",
                "+                for (int _i765 = 0; _i765 < _map762.size; ++_i765)",
                "                 {",
                "-                  _key743 = iprot.readString();",
                "-                  _val744 = new ThriftSerializedObject();",
                "-                  _val744.read(iprot);",
                "-                  struct.serialized_parts.put(_key743, _val744);",
                "+                  _key763 = iprot.readString();",
                "+                  _val764 = new ThriftSerializedObject();",
                "+                  _val764.read(iprot);",
                "+                  struct.serialized_parts.put(_key763, _val764);",
                "                 }",
                "@@ -413,6 +413,6 @@ public class LocalStateData implements org.apache.thrift.TBase<LocalStateData, L",
                "           oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, struct.serialized_parts.size()));",
                "-          for (Map.Entry<String, ThriftSerializedObject> _iter746 : struct.serialized_parts.entrySet())",
                "+          for (Map.Entry<String, ThriftSerializedObject> _iter766 : struct.serialized_parts.entrySet())",
                "           {",
                "-            oprot.writeString(_iter746.getKey());",
                "-            _iter746.getValue().write(oprot);",
                "+            oprot.writeString(_iter766.getKey());",
                "+            _iter766.getValue().write(oprot);",
                "           }",
                "@@ -441,6 +441,6 @@ public class LocalStateData implements org.apache.thrift.TBase<LocalStateData, L",
                "         oprot.writeI32(struct.serialized_parts.size());",
                "-        for (Map.Entry<String, ThriftSerializedObject> _iter747 : struct.serialized_parts.entrySet())",
                "+        for (Map.Entry<String, ThriftSerializedObject> _iter767 : struct.serialized_parts.entrySet())",
                "         {",
                "-          oprot.writeString(_iter747.getKey());",
                "-          _iter747.getValue().write(oprot);",
                "+          oprot.writeString(_iter767.getKey());",
                "+          _iter767.getValue().write(oprot);",
                "         }",
                "@@ -453,12 +453,12 @@ public class LocalStateData implements org.apache.thrift.TBase<LocalStateData, L",
                "       {",
                "-        org.apache.thrift.protocol.TMap _map748 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.serialized_parts = new HashMap<String,ThriftSerializedObject>(2*_map748.size);",
                "-        String _key749;",
                "-        ThriftSerializedObject _val750;",
                "-        for (int _i751 = 0; _i751 < _map748.size; ++_i751)",
                "+        org.apache.thrift.protocol.TMap _map768 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.serialized_parts = new HashMap<String,ThriftSerializedObject>(2*_map768.size);",
                "+        String _key769;",
                "+        ThriftSerializedObject _val770;",
                "+        for (int _i771 = 0; _i771 < _map768.size; ++_i771)",
                "         {",
                "-          _key749 = iprot.readString();",
                "-          _val750 = new ThriftSerializedObject();",
                "-          _val750.read(iprot);",
                "-          struct.serialized_parts.put(_key749, _val750);",
                "+          _key769 = iprot.readString();",
                "+          _val770 = new ThriftSerializedObject();",
                "+          _val770.read(iprot);",
                "+          struct.serialized_parts.put(_key769, _val770);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LogConfig.java b/storm-client/src/jvm/org/apache/storm/generated/LogConfig.java",
                "index 58086a176..e783e6c45 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LogConfig.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LogConfig.java",
                "@@ -370,12 +370,12 @@ public class LogConfig implements org.apache.thrift.TBase<LogConfig, LogConfig._",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map812 = iprot.readMapBegin();",
                "-                struct.named_logger_level = new HashMap<String,LogLevel>(2*_map812.size);",
                "-                String _key813;",
                "-                LogLevel _val814;",
                "-                for (int _i815 = 0; _i815 < _map812.size; ++_i815)",
                "+                org.apache.thrift.protocol.TMap _map832 = iprot.readMapBegin();",
                "+                struct.named_logger_level = new HashMap<String,LogLevel>(2*_map832.size);",
                "+                String _key833;",
                "+                LogLevel _val834;",
                "+                for (int _i835 = 0; _i835 < _map832.size; ++_i835)",
                "                 {",
                "-                  _key813 = iprot.readString();",
                "-                  _val814 = new LogLevel();",
                "-                  _val814.read(iprot);",
                "-                  struct.named_logger_level.put(_key813, _val814);",
                "+                  _key833 = iprot.readString();",
                "+                  _val834 = new LogLevel();",
                "+                  _val834.read(iprot);",
                "+                  struct.named_logger_level.put(_key833, _val834);",
                "                 }",
                "@@ -406,6 +406,6 @@ public class LogConfig implements org.apache.thrift.TBase<LogConfig, LogConfig._",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, struct.named_logger_level.size()));",
                "-            for (Map.Entry<String, LogLevel> _iter816 : struct.named_logger_level.entrySet())",
                "+            for (Map.Entry<String, LogLevel> _iter836 : struct.named_logger_level.entrySet())",
                "             {",
                "-              oprot.writeString(_iter816.getKey());",
                "-              _iter816.getValue().write(oprot);",
                "+              oprot.writeString(_iter836.getKey());",
                "+              _iter836.getValue().write(oprot);",
                "             }",
                "@@ -441,6 +441,6 @@ public class LogConfig implements org.apache.thrift.TBase<LogConfig, LogConfig._",
                "           oprot.writeI32(struct.named_logger_level.size());",
                "-          for (Map.Entry<String, LogLevel> _iter817 : struct.named_logger_level.entrySet())",
                "+          for (Map.Entry<String, LogLevel> _iter837 : struct.named_logger_level.entrySet())",
                "           {",
                "-            oprot.writeString(_iter817.getKey());",
                "-            _iter817.getValue().write(oprot);",
                "+            oprot.writeString(_iter837.getKey());",
                "+            _iter837.getValue().write(oprot);",
                "           }",
                "@@ -456,12 +456,12 @@ public class LogConfig implements org.apache.thrift.TBase<LogConfig, LogConfig._",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map818 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-          struct.named_logger_level = new HashMap<String,LogLevel>(2*_map818.size);",
                "-          String _key819;",
                "-          LogLevel _val820;",
                "-          for (int _i821 = 0; _i821 < _map818.size; ++_i821)",
                "+          org.apache.thrift.protocol.TMap _map838 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+          struct.named_logger_level = new HashMap<String,LogLevel>(2*_map838.size);",
                "+          String _key839;",
                "+          LogLevel _val840;",
                "+          for (int _i841 = 0; _i841 < _map838.size; ++_i841)",
                "           {",
                "-            _key819 = iprot.readString();",
                "-            _val820 = new LogLevel();",
                "-            _val820.read(iprot);",
                "-            struct.named_logger_level.put(_key819, _val820);",
                "+            _key839 = iprot.readString();",
                "+            _val840 = new LogLevel();",
                "+            _val840.read(iprot);",
                "+            struct.named_logger_level.put(_key839, _val840);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/Nimbus.java b/storm-client/src/jvm/org/apache/storm/generated/Nimbus.java",
                "index f14ddcf9e..ea4dcc90b 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/Nimbus.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/Nimbus.java",
                "@@ -18289,10 +18289,10 @@ public class Nimbus {",
                "                 {",
                "-                  org.apache.thrift.protocol.TList _list846 = iprot.readListBegin();",
                "-                  struct.success = new ArrayList<ProfileRequest>(_list846.size);",
                "-                  ProfileRequest _elem847;",
                "-                  for (int _i848 = 0; _i848 < _list846.size; ++_i848)",
                "+                  org.apache.thrift.protocol.TList _list866 = iprot.readListBegin();",
                "+                  struct.success = new ArrayList<ProfileRequest>(_list866.size);",
                "+                  ProfileRequest _elem867;",
                "+                  for (int _i868 = 0; _i868 < _list866.size; ++_i868)",
                "                   {",
                "-                    _elem847 = new ProfileRequest();",
                "-                    _elem847.read(iprot);",
                "-                    struct.success.add(_elem847);",
                "+                    _elem867 = new ProfileRequest();",
                "+                    _elem867.read(iprot);",
                "+                    struct.success.add(_elem867);",
                "                   }",
                "@@ -18322,5 +18322,5 @@ public class Nimbus {",
                "             oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.success.size()));",
                "-            for (ProfileRequest _iter849 : struct.success)",
                "+            for (ProfileRequest _iter869 : struct.success)",
                "             {",
                "-              _iter849.write(oprot);",
                "+              _iter869.write(oprot);",
                "             }",
                "@@ -18355,5 +18355,5 @@ public class Nimbus {",
                "             oprot.writeI32(struct.success.size());",
                "-            for (ProfileRequest _iter850 : struct.success)",
                "+            for (ProfileRequest _iter870 : struct.success)",
                "             {",
                "-              _iter850.write(oprot);",
                "+              _iter870.write(oprot);",
                "             }",
                "@@ -18369,10 +18369,10 @@ public class Nimbus {",
                "           {",
                "-            org.apache.thrift.protocol.TList _list851 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-            struct.success = new ArrayList<ProfileRequest>(_list851.size);",
                "-            ProfileRequest _elem852;",
                "-            for (int _i853 = 0; _i853 < _list851.size; ++_i853)",
                "+            org.apache.thrift.protocol.TList _list871 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+            struct.success = new ArrayList<ProfileRequest>(_list871.size);",
                "+            ProfileRequest _elem872;",
                "+            for (int _i873 = 0; _i873 < _list871.size; ++_i873)",
                "             {",
                "-              _elem852 = new ProfileRequest();",
                "-              _elem852.read(iprot);",
                "-              struct.success.add(_elem852);",
                "+              _elem872 = new ProfileRequest();",
                "+              _elem872.read(iprot);",
                "+              struct.success.add(_elem872);",
                "             }",
                "@@ -48139,10 +48139,10 @@ public class Nimbus {",
                "                 {",
                "-                  org.apache.thrift.protocol.TList _list854 = iprot.readListBegin();",
                "-                  struct.success = new ArrayList<OwnerResourceSummary>(_list854.size);",
                "-                  OwnerResourceSummary _elem855;",
                "-                  for (int _i856 = 0; _i856 < _list854.size; ++_i856)",
                "+                  org.apache.thrift.protocol.TList _list874 = iprot.readListBegin();",
                "+                  struct.success = new ArrayList<OwnerResourceSummary>(_list874.size);",
                "+                  OwnerResourceSummary _elem875;",
                "+                  for (int _i876 = 0; _i876 < _list874.size; ++_i876)",
                "                   {",
                "-                    _elem855 = new OwnerResourceSummary();",
                "-                    _elem855.read(iprot);",
                "-                    struct.success.add(_elem855);",
                "+                    _elem875 = new OwnerResourceSummary();",
                "+                    _elem875.read(iprot);",
                "+                    struct.success.add(_elem875);",
                "                   }",
                "@@ -48181,5 +48181,5 @@ public class Nimbus {",
                "             oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.success.size()));",
                "-            for (OwnerResourceSummary _iter857 : struct.success)",
                "+            for (OwnerResourceSummary _iter877 : struct.success)",
                "             {",
                "-              _iter857.write(oprot);",
                "+              _iter877.write(oprot);",
                "             }",
                "@@ -48222,5 +48222,5 @@ public class Nimbus {",
                "             oprot.writeI32(struct.success.size());",
                "-            for (OwnerResourceSummary _iter858 : struct.success)",
                "+            for (OwnerResourceSummary _iter878 : struct.success)",
                "             {",
                "-              _iter858.write(oprot);",
                "+              _iter878.write(oprot);",
                "             }",
                "@@ -48239,10 +48239,10 @@ public class Nimbus {",
                "           {",
                "-            org.apache.thrift.protocol.TList _list859 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-            struct.success = new ArrayList<OwnerResourceSummary>(_list859.size);",
                "-            OwnerResourceSummary _elem860;",
                "-            for (int _i861 = 0; _i861 < _list859.size; ++_i861)",
                "+            org.apache.thrift.protocol.TList _list879 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+            struct.success = new ArrayList<OwnerResourceSummary>(_list879.size);",
                "+            OwnerResourceSummary _elem880;",
                "+            for (int _i881 = 0; _i881 < _list879.size; ++_i881)",
                "             {",
                "-              _elem860 = new OwnerResourceSummary();",
                "-              _elem860.read(iprot);",
                "-              struct.success.add(_elem860);",
                "+              _elem880 = new OwnerResourceSummary();",
                "+              _elem880.read(iprot);",
                "+              struct.success.add(_elem880);",
                "             }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/NodeInfo.java b/storm-client/src/jvm/org/apache/storm/generated/NodeInfo.java",
                "index 403892c42..6bb9c1f23 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/NodeInfo.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/NodeInfo.java",
                "@@ -463,9 +463,9 @@ public class NodeInfo implements org.apache.thrift.TBase<NodeInfo, NodeInfo._Fie",
                "               {",
                "-                org.apache.thrift.protocol.TSet _set638 = iprot.readSetBegin();",
                "-                struct.port = new HashSet<Long>(2*_set638.size);",
                "-                long _elem639;",
                "-                for (int _i640 = 0; _i640 < _set638.size; ++_i640)",
                "+                org.apache.thrift.protocol.TSet _set658 = iprot.readSetBegin();",
                "+                struct.port = new HashSet<Long>(2*_set658.size);",
                "+                long _elem659;",
                "+                for (int _i660 = 0; _i660 < _set658.size; ++_i660)",
                "                 {",
                "-                  _elem639 = iprot.readI64();",
                "-                  struct.port.add(_elem639);",
                "+                  _elem659 = iprot.readI64();",
                "+                  struct.port.add(_elem659);",
                "                 }",
                "@@ -500,5 +500,5 @@ public class NodeInfo implements org.apache.thrift.TBase<NodeInfo, NodeInfo._Fie",
                "           oprot.writeSetBegin(new org.apache.thrift.protocol.TSet(org.apache.thrift.protocol.TType.I64, struct.port.size()));",
                "-          for (long _iter641 : struct.port)",
                "+          for (long _iter661 : struct.port)",
                "           {",
                "-            oprot.writeI64(_iter641);",
                "+            oprot.writeI64(_iter661);",
                "           }",
                "@@ -528,5 +528,5 @@ public class NodeInfo implements org.apache.thrift.TBase<NodeInfo, NodeInfo._Fie",
                "         oprot.writeI32(struct.port.size());",
                "-        for (long _iter642 : struct.port)",
                "+        for (long _iter662 : struct.port)",
                "         {",
                "-          oprot.writeI64(_iter642);",
                "+          oprot.writeI64(_iter662);",
                "         }",
                "@@ -541,9 +541,9 @@ public class NodeInfo implements org.apache.thrift.TBase<NodeInfo, NodeInfo._Fie",
                "       {",
                "-        org.apache.thrift.protocol.TSet _set643 = new org.apache.thrift.protocol.TSet(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "-        struct.port = new HashSet<Long>(2*_set643.size);",
                "-        long _elem644;",
                "-        for (int _i645 = 0; _i645 < _set643.size; ++_i645)",
                "+        org.apache.thrift.protocol.TSet _set663 = new org.apache.thrift.protocol.TSet(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "+        struct.port = new HashSet<Long>(2*_set663.size);",
                "+        long _elem664;",
                "+        for (int _i665 = 0; _i665 < _set663.size; ++_i665)",
                "         {",
                "-          _elem644 = iprot.readI64();",
                "-          struct.port.add(_elem644);",
                "+          _elem664 = iprot.readI64();",
                "+          struct.port.add(_elem664);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/RebalanceOptions.java b/storm-client/src/jvm/org/apache/storm/generated/RebalanceOptions.java",
                "index ea7c25625..9cd8a2e95 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/RebalanceOptions.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/RebalanceOptions.java",
                "@@ -60,2 +60,5 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "   private static final org.apache.thrift.protocol.TField NUM_EXECUTORS_FIELD_DESC = new org.apache.thrift.protocol.TField(\"num_executors\", org.apache.thrift.protocol.TType.MAP, (short)3);",
                "+  private static final org.apache.thrift.protocol.TField TOPOLOGY_RESOURCES_OVERRIDES_FIELD_DESC = new org.apache.thrift.protocol.TField(\"topology_resources_overrides\", org.apache.thrift.protocol.TType.MAP, (short)4);",
                "+  private static final org.apache.thrift.protocol.TField TOPOLOGY_CONF_OVERRIDES_FIELD_DESC = new org.apache.thrift.protocol.TField(\"topology_conf_overrides\", org.apache.thrift.protocol.TType.STRING, (short)5);",
                "+  private static final org.apache.thrift.protocol.TField PRINCIPAL_FIELD_DESC = new org.apache.thrift.protocol.TField(\"principal\", org.apache.thrift.protocol.TType.STRING, (short)6);",
                "@@ -70,2 +73,5 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "   private Map<String,Integer> num_executors; // optional",
                "+  private Map<String,Map<String,Double>> topology_resources_overrides; // optional",
                "+  private String topology_conf_overrides; // optional",
                "+  private String principal; // optional",
                "@@ -75,3 +81,6 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "     NUM_WORKERS((short)2, \"num_workers\"),",
                "-    NUM_EXECUTORS((short)3, \"num_executors\");",
                "+    NUM_EXECUTORS((short)3, \"num_executors\"),",
                "+    TOPOLOGY_RESOURCES_OVERRIDES((short)4, \"topology_resources_overrides\"),",
                "+    TOPOLOGY_CONF_OVERRIDES((short)5, \"topology_conf_overrides\"),",
                "+    PRINCIPAL((short)6, \"principal\");",
                "@@ -96,2 +105,8 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "           return NUM_EXECUTORS;",
                "+        case 4: // TOPOLOGY_RESOURCES_OVERRIDES",
                "+          return TOPOLOGY_RESOURCES_OVERRIDES;",
                "+        case 5: // TOPOLOGY_CONF_OVERRIDES",
                "+          return TOPOLOGY_CONF_OVERRIDES;",
                "+        case 6: // PRINCIPAL",
                "+          return PRINCIPAL;",
                "         default:",
                "@@ -139,3 +154,3 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "   private byte __isset_bitfield = 0;",
                "-  private static final _Fields optionals[] = {_Fields.WAIT_SECS,_Fields.NUM_WORKERS,_Fields.NUM_EXECUTORS};",
                "+  private static final _Fields optionals[] = {_Fields.WAIT_SECS,_Fields.NUM_WORKERS,_Fields.NUM_EXECUTORS,_Fields.TOPOLOGY_RESOURCES_OVERRIDES,_Fields.TOPOLOGY_CONF_OVERRIDES,_Fields.PRINCIPAL};",
                "   public static final Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;",
                "@@ -151,2 +166,12 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "             new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I32))));",
                "+    tmpMap.put(_Fields.TOPOLOGY_RESOURCES_OVERRIDES, new org.apache.thrift.meta_data.FieldMetaData(\"topology_resources_overrides\", org.apache.thrift.TFieldRequirementType.OPTIONAL, ",
                "+        new org.apache.thrift.meta_data.MapMetaData(org.apache.thrift.protocol.TType.MAP, ",
                "+            new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING), ",
                "+            new org.apache.thrift.meta_data.MapMetaData(org.apache.thrift.protocol.TType.MAP, ",
                "+                new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING), ",
                "+                new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.DOUBLE)))));",
                "+    tmpMap.put(_Fields.TOPOLOGY_CONF_OVERRIDES, new org.apache.thrift.meta_data.FieldMetaData(\"topology_conf_overrides\", org.apache.thrift.TFieldRequirementType.OPTIONAL, ",
                "+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING)));",
                "+    tmpMap.put(_Fields.PRINCIPAL, new org.apache.thrift.meta_data.FieldMetaData(\"principal\", org.apache.thrift.TFieldRequirementType.OPTIONAL, ",
                "+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING)));",
                "     metaDataMap = Collections.unmodifiableMap(tmpMap);",
                "@@ -169,2 +194,23 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "     }",
                "+    if (other.is_set_topology_resources_overrides()) {",
                "+      Map<String,Map<String,Double>> __this__topology_resources_overrides = new HashMap<String,Map<String,Double>>(other.topology_resources_overrides.size());",
                "+      for (Map.Entry<String, Map<String,Double>> other_element : other.topology_resources_overrides.entrySet()) {",
                "+",
                "+        String other_element_key = other_element.getKey();",
                "+        Map<String,Double> other_element_value = other_element.getValue();",
                "+",
                "+        String __this__topology_resources_overrides_copy_key = other_element_key;",
                "+",
                "+        Map<String,Double> __this__topology_resources_overrides_copy_value = new HashMap<String,Double>(other_element_value);",
                "+",
                "+        __this__topology_resources_overrides.put(__this__topology_resources_overrides_copy_key, __this__topology_resources_overrides_copy_value);",
                "+      }",
                "+      this.topology_resources_overrides = __this__topology_resources_overrides;",
                "+    }",
                "+    if (other.is_set_topology_conf_overrides()) {",
                "+      this.topology_conf_overrides = other.topology_conf_overrides;",
                "+    }",
                "+    if (other.is_set_principal()) {",
                "+      this.principal = other.principal;",
                "+    }",
                "   }",
                "@@ -182,2 +228,5 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "     this.num_executors = null;",
                "+    this.topology_resources_overrides = null;",
                "+    this.topology_conf_overrides = null;",
                "+    this.principal = null;",
                "   }",
                "@@ -262,2 +311,82 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "+  public int get_topology_resources_overrides_size() {",
                "+    return (this.topology_resources_overrides == null) ? 0 : this.topology_resources_overrides.size();",
                "+  }",
                "+",
                "+  public void put_to_topology_resources_overrides(String key, Map<String,Double> val) {",
                "+    if (this.topology_resources_overrides == null) {",
                "+      this.topology_resources_overrides = new HashMap<String,Map<String,Double>>();",
                "+    }",
                "+    this.topology_resources_overrides.put(key, val);",
                "+  }",
                "+",
                "+  public Map<String,Map<String,Double>> get_topology_resources_overrides() {",
                "+    return this.topology_resources_overrides;",
                "+  }",
                "+",
                "+  public void set_topology_resources_overrides(Map<String,Map<String,Double>> topology_resources_overrides) {",
                "+    this.topology_resources_overrides = topology_resources_overrides;",
                "+  }",
                "+",
                "+  public void unset_topology_resources_overrides() {",
                "+    this.topology_resources_overrides = null;",
                "+  }",
                "+",
                "+  /** Returns true if field topology_resources_overrides is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_topology_resources_overrides() {",
                "+    return this.topology_resources_overrides != null;",
                "+  }",
                "+",
                "+  public void set_topology_resources_overrides_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.topology_resources_overrides = null;",
                "+    }",
                "+  }",
                "+",
                "+  public String get_topology_conf_overrides() {",
                "+    return this.topology_conf_overrides;",
                "+  }",
                "+",
                "+  public void set_topology_conf_overrides(String topology_conf_overrides) {",
                "+    this.topology_conf_overrides = topology_conf_overrides;",
                "+  }",
                "+",
                "+  public void unset_topology_conf_overrides() {",
                "+    this.topology_conf_overrides = null;",
                "+  }",
                "+",
                "+  /** Returns true if field topology_conf_overrides is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_topology_conf_overrides() {",
                "+    return this.topology_conf_overrides != null;",
                "+  }",
                "+",
                "+  public void set_topology_conf_overrides_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.topology_conf_overrides = null;",
                "+    }",
                "+  }",
                "+",
                "+  public String get_principal() {",
                "+    return this.principal;",
                "+  }",
                "+",
                "+  public void set_principal(String principal) {",
                "+    this.principal = principal;",
                "+  }",
                "+",
                "+  public void unset_principal() {",
                "+    this.principal = null;",
                "+  }",
                "+",
                "+  /** Returns true if field principal is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_principal() {",
                "+    return this.principal != null;",
                "+  }",
                "+",
                "+  public void set_principal_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.principal = null;",
                "+    }",
                "+  }",
                "+",
                "   public void setFieldValue(_Fields field, Object value) {",
                "@@ -288,2 +417,26 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "+    case TOPOLOGY_RESOURCES_OVERRIDES:",
                "+      if (value == null) {",
                "+        unset_topology_resources_overrides();",
                "+      } else {",
                "+        set_topology_resources_overrides((Map<String,Map<String,Double>>)value);",
                "+      }",
                "+      break;",
                "+",
                "+    case TOPOLOGY_CONF_OVERRIDES:",
                "+      if (value == null) {",
                "+        unset_topology_conf_overrides();",
                "+      } else {",
                "+        set_topology_conf_overrides((String)value);",
                "+      }",
                "+      break;",
                "+",
                "+    case PRINCIPAL:",
                "+      if (value == null) {",
                "+        unset_principal();",
                "+      } else {",
                "+        set_principal((String)value);",
                "+      }",
                "+      break;",
                "+",
                "     }",
                "@@ -302,2 +455,11 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "+    case TOPOLOGY_RESOURCES_OVERRIDES:",
                "+      return get_topology_resources_overrides();",
                "+",
                "+    case TOPOLOGY_CONF_OVERRIDES:",
                "+      return get_topology_conf_overrides();",
                "+",
                "+    case PRINCIPAL:",
                "+      return get_principal();",
                "+",
                "     }",
                "@@ -319,2 +481,8 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "       return is_set_num_executors();",
                "+    case TOPOLOGY_RESOURCES_OVERRIDES:",
                "+      return is_set_topology_resources_overrides();",
                "+    case TOPOLOGY_CONF_OVERRIDES:",
                "+      return is_set_topology_conf_overrides();",
                "+    case PRINCIPAL:",
                "+      return is_set_principal();",
                "     }",
                "@@ -363,2 +531,29 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "+    boolean this_present_topology_resources_overrides = true && this.is_set_topology_resources_overrides();",
                "+    boolean that_present_topology_resources_overrides = true && that.is_set_topology_resources_overrides();",
                "+    if (this_present_topology_resources_overrides || that_present_topology_resources_overrides) {",
                "+      if (!(this_present_topology_resources_overrides && that_present_topology_resources_overrides))",
                "+        return false;",
                "+      if (!this.topology_resources_overrides.equals(that.topology_resources_overrides))",
                "+        return false;",
                "+    }",
                "+",
                "+    boolean this_present_topology_conf_overrides = true && this.is_set_topology_conf_overrides();",
                "+    boolean that_present_topology_conf_overrides = true && that.is_set_topology_conf_overrides();",
                "+    if (this_present_topology_conf_overrides || that_present_topology_conf_overrides) {",
                "+      if (!(this_present_topology_conf_overrides && that_present_topology_conf_overrides))",
                "+        return false;",
                "+      if (!this.topology_conf_overrides.equals(that.topology_conf_overrides))",
                "+        return false;",
                "+    }",
                "+",
                "+    boolean this_present_principal = true && this.is_set_principal();",
                "+    boolean that_present_principal = true && that.is_set_principal();",
                "+    if (this_present_principal || that_present_principal) {",
                "+      if (!(this_present_principal && that_present_principal))",
                "+        return false;",
                "+      if (!this.principal.equals(that.principal))",
                "+        return false;",
                "+    }",
                "+",
                "     return true;",
                "@@ -385,2 +580,17 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "+    boolean present_topology_resources_overrides = true && (is_set_topology_resources_overrides());",
                "+    list.add(present_topology_resources_overrides);",
                "+    if (present_topology_resources_overrides)",
                "+      list.add(topology_resources_overrides);",
                "+",
                "+    boolean present_topology_conf_overrides = true && (is_set_topology_conf_overrides());",
                "+    list.add(present_topology_conf_overrides);",
                "+    if (present_topology_conf_overrides)",
                "+      list.add(topology_conf_overrides);",
                "+",
                "+    boolean present_principal = true && (is_set_principal());",
                "+    list.add(present_principal);",
                "+    if (present_principal)",
                "+      list.add(principal);",
                "+",
                "     return list.hashCode();",
                "@@ -426,2 +636,32 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "     }",
                "+    lastComparison = Boolean.valueOf(is_set_topology_resources_overrides()).compareTo(other.is_set_topology_resources_overrides());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_topology_resources_overrides()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.topology_resources_overrides, other.topology_resources_overrides);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    lastComparison = Boolean.valueOf(is_set_topology_conf_overrides()).compareTo(other.is_set_topology_conf_overrides());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_topology_conf_overrides()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.topology_conf_overrides, other.topology_conf_overrides);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    lastComparison = Boolean.valueOf(is_set_principal()).compareTo(other.is_set_principal());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_principal()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.principal, other.principal);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "     return 0;",
                "@@ -467,2 +707,32 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "     }",
                "+    if (is_set_topology_resources_overrides()) {",
                "+      if (!first) sb.append(\", \");",
                "+      sb.append(\"topology_resources_overrides:\");",
                "+      if (this.topology_resources_overrides == null) {",
                "+        sb.append(\"null\");",
                "+      } else {",
                "+        sb.append(this.topology_resources_overrides);",
                "+      }",
                "+      first = false;",
                "+    }",
                "+    if (is_set_topology_conf_overrides()) {",
                "+      if (!first) sb.append(\", \");",
                "+      sb.append(\"topology_conf_overrides:\");",
                "+      if (this.topology_conf_overrides == null) {",
                "+        sb.append(\"null\");",
                "+      } else {",
                "+        sb.append(this.topology_conf_overrides);",
                "+      }",
                "+      first = false;",
                "+    }",
                "+    if (is_set_principal()) {",
                "+      if (!first) sb.append(\", \");",
                "+      sb.append(\"principal:\");",
                "+      if (this.principal == null) {",
                "+        sb.append(\"null\");",
                "+      } else {",
                "+        sb.append(this.principal);",
                "+      }",
                "+      first = false;",
                "+    }",
                "     sb.append(\")\");",
                "@@ -548,2 +818,50 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "             break;",
                "+          case 4: // TOPOLOGY_RESOURCES_OVERRIDES",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.MAP) {",
                "+              {",
                "+                org.apache.thrift.protocol.TMap _map570 = iprot.readMapBegin();",
                "+                struct.topology_resources_overrides = new HashMap<String,Map<String,Double>>(2*_map570.size);",
                "+                String _key571;",
                "+                Map<String,Double> _val572;",
                "+                for (int _i573 = 0; _i573 < _map570.size; ++_i573)",
                "+                {",
                "+                  _key571 = iprot.readString();",
                "+                  {",
                "+                    org.apache.thrift.protocol.TMap _map574 = iprot.readMapBegin();",
                "+                    _val572 = new HashMap<String,Double>(2*_map574.size);",
                "+                    String _key575;",
                "+                    double _val576;",
                "+                    for (int _i577 = 0; _i577 < _map574.size; ++_i577)",
                "+                    {",
                "+                      _key575 = iprot.readString();",
                "+                      _val576 = iprot.readDouble();",
                "+                      _val572.put(_key575, _val576);",
                "+                    }",
                "+                    iprot.readMapEnd();",
                "+                  }",
                "+                  struct.topology_resources_overrides.put(_key571, _val572);",
                "+                }",
                "+                iprot.readMapEnd();",
                "+              }",
                "+              struct.set_topology_resources_overrides_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          case 5: // TOPOLOGY_CONF_OVERRIDES",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {",
                "+              struct.topology_conf_overrides = iprot.readString();",
                "+              struct.set_topology_conf_overrides_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          case 6: // PRINCIPAL",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {",
                "+              struct.principal = iprot.readString();",
                "+              struct.set_principal_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "           default:",
                "@@ -576,6 +894,6 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, struct.num_executors.size()));",
                "-            for (Map.Entry<String, Integer> _iter570 : struct.num_executors.entrySet())",
                "+            for (Map.Entry<String, Integer> _iter578 : struct.num_executors.entrySet())",
                "             {",
                "-              oprot.writeString(_iter570.getKey());",
                "-              oprot.writeI32(_iter570.getValue());",
                "+              oprot.writeString(_iter578.getKey());",
                "+              oprot.writeI32(_iter578.getValue());",
                "             }",
                "@@ -586,2 +904,39 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "       }",
                "+      if (struct.topology_resources_overrides != null) {",
                "+        if (struct.is_set_topology_resources_overrides()) {",
                "+          oprot.writeFieldBegin(TOPOLOGY_RESOURCES_OVERRIDES_FIELD_DESC);",
                "+          {",
                "+            oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.MAP, struct.topology_resources_overrides.size()));",
                "+            for (Map.Entry<String, Map<String,Double>> _iter579 : struct.topology_resources_overrides.entrySet())",
                "+            {",
                "+              oprot.writeString(_iter579.getKey());",
                "+              {",
                "+                oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, _iter579.getValue().size()));",
                "+                for (Map.Entry<String, Double> _iter580 : _iter579.getValue().entrySet())",
                "+                {",
                "+                  oprot.writeString(_iter580.getKey());",
                "+                  oprot.writeDouble(_iter580.getValue());",
                "+                }",
                "+                oprot.writeMapEnd();",
                "+              }",
                "+            }",
                "+            oprot.writeMapEnd();",
                "+          }",
                "+          oprot.writeFieldEnd();",
                "+        }",
                "+      }",
                "+      if (struct.topology_conf_overrides != null) {",
                "+        if (struct.is_set_topology_conf_overrides()) {",
                "+          oprot.writeFieldBegin(TOPOLOGY_CONF_OVERRIDES_FIELD_DESC);",
                "+          oprot.writeString(struct.topology_conf_overrides);",
                "+          oprot.writeFieldEnd();",
                "+        }",
                "+      }",
                "+      if (struct.principal != null) {",
                "+        if (struct.is_set_principal()) {",
                "+          oprot.writeFieldBegin(PRINCIPAL_FIELD_DESC);",
                "+          oprot.writeString(struct.principal);",
                "+          oprot.writeFieldEnd();",
                "+        }",
                "+      }",
                "       oprot.writeFieldStop();",
                "@@ -613,3 +968,12 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "       }",
                "-      oprot.writeBitSet(optionals, 3);",
                "+      if (struct.is_set_topology_resources_overrides()) {",
                "+        optionals.set(3);",
                "+      }",
                "+      if (struct.is_set_topology_conf_overrides()) {",
                "+        optionals.set(4);",
                "+      }",
                "+      if (struct.is_set_principal()) {",
                "+        optionals.set(5);",
                "+      }",
                "+      oprot.writeBitSet(optionals, 6);",
                "       if (struct.is_set_wait_secs()) {",
                "@@ -623,6 +987,23 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "           oprot.writeI32(struct.num_executors.size());",
                "-          for (Map.Entry<String, Integer> _iter571 : struct.num_executors.entrySet())",
                "+          for (Map.Entry<String, Integer> _iter581 : struct.num_executors.entrySet())",
                "+          {",
                "+            oprot.writeString(_iter581.getKey());",
                "+            oprot.writeI32(_iter581.getValue());",
                "+          }",
                "+        }",
                "+      }",
                "+      if (struct.is_set_topology_resources_overrides()) {",
                "+        {",
                "+          oprot.writeI32(struct.topology_resources_overrides.size());",
                "+          for (Map.Entry<String, Map<String,Double>> _iter582 : struct.topology_resources_overrides.entrySet())",
                "           {",
                "-            oprot.writeString(_iter571.getKey());",
                "-            oprot.writeI32(_iter571.getValue());",
                "+            oprot.writeString(_iter582.getKey());",
                "+            {",
                "+              oprot.writeI32(_iter582.getValue().size());",
                "+              for (Map.Entry<String, Double> _iter583 : _iter582.getValue().entrySet())",
                "+              {",
                "+                oprot.writeString(_iter583.getKey());",
                "+                oprot.writeDouble(_iter583.getValue());",
                "+              }",
                "+            }",
                "           }",
                "@@ -630,2 +1011,8 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "       }",
                "+      if (struct.is_set_topology_conf_overrides()) {",
                "+        oprot.writeString(struct.topology_conf_overrides);",
                "+      }",
                "+      if (struct.is_set_principal()) {",
                "+        oprot.writeString(struct.principal);",
                "+      }",
                "     }",
                "@@ -635,3 +1022,3 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "       TTupleProtocol iprot = (TTupleProtocol) prot;",
                "-      BitSet incoming = iprot.readBitSet(3);",
                "+      BitSet incoming = iprot.readBitSet(6);",
                "       if (incoming.get(0)) {",
                "@@ -646,11 +1033,11 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map572 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, iprot.readI32());",
                "-          struct.num_executors = new HashMap<String,Integer>(2*_map572.size);",
                "-          String _key573;",
                "-          int _val574;",
                "-          for (int _i575 = 0; _i575 < _map572.size; ++_i575)",
                "+          org.apache.thrift.protocol.TMap _map584 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, iprot.readI32());",
                "+          struct.num_executors = new HashMap<String,Integer>(2*_map584.size);",
                "+          String _key585;",
                "+          int _val586;",
                "+          for (int _i587 = 0; _i587 < _map584.size; ++_i587)",
                "           {",
                "-            _key573 = iprot.readString();",
                "-            _val574 = iprot.readI32();",
                "-            struct.num_executors.put(_key573, _val574);",
                "+            _key585 = iprot.readString();",
                "+            _val586 = iprot.readI32();",
                "+            struct.num_executors.put(_key585, _val586);",
                "           }",
                "@@ -659,2 +1046,36 @@ public class RebalanceOptions implements org.apache.thrift.TBase<RebalanceOption",
                "       }",
                "+      if (incoming.get(3)) {",
                "+        {",
                "+          org.apache.thrift.protocol.TMap _map588 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.MAP, iprot.readI32());",
                "+          struct.topology_resources_overrides = new HashMap<String,Map<String,Double>>(2*_map588.size);",
                "+          String _key589;",
                "+          Map<String,Double> _val590;",
                "+          for (int _i591 = 0; _i591 < _map588.size; ++_i591)",
                "+          {",
                "+            _key589 = iprot.readString();",
                "+            {",
                "+              org.apache.thrift.protocol.TMap _map592 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "+              _val590 = new HashMap<String,Double>(2*_map592.size);",
                "+              String _key593;",
                "+              double _val594;",
                "+              for (int _i595 = 0; _i595 < _map592.size; ++_i595)",
                "+              {",
                "+                _key593 = iprot.readString();",
                "+                _val594 = iprot.readDouble();",
                "+                _val590.put(_key593, _val594);",
                "+              }",
                "+            }",
                "+            struct.topology_resources_overrides.put(_key589, _val590);",
                "+          }",
                "+        }",
                "+        struct.set_topology_resources_overrides_isSet(true);",
                "+      }",
                "+      if (incoming.get(4)) {",
                "+        struct.topology_conf_overrides = iprot.readString();",
                "+        struct.set_topology_conf_overrides_isSet(true);",
                "+      }",
                "+      if (incoming.get(5)) {",
                "+        struct.principal = iprot.readString();",
                "+        struct.set_principal_isSet(true);",
                "+      }",
                "     }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/SettableBlobMeta.java b/storm-client/src/jvm/org/apache/storm/generated/SettableBlobMeta.java",
                "index 48e1fd490..604e44600 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/SettableBlobMeta.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/SettableBlobMeta.java",
                "@@ -454,10 +454,10 @@ public class SettableBlobMeta implements org.apache.thrift.TBase<SettableBlobMet",
                "               {",
                "-                org.apache.thrift.protocol.TList _list586 = iprot.readListBegin();",
                "-                struct.acl = new ArrayList<AccessControl>(_list586.size);",
                "-                AccessControl _elem587;",
                "-                for (int _i588 = 0; _i588 < _list586.size; ++_i588)",
                "+                org.apache.thrift.protocol.TList _list606 = iprot.readListBegin();",
                "+                struct.acl = new ArrayList<AccessControl>(_list606.size);",
                "+                AccessControl _elem607;",
                "+                for (int _i608 = 0; _i608 < _list606.size; ++_i608)",
                "                 {",
                "-                  _elem587 = new AccessControl();",
                "-                  _elem587.read(iprot);",
                "-                  struct.acl.add(_elem587);",
                "+                  _elem607 = new AccessControl();",
                "+                  _elem607.read(iprot);",
                "+                  struct.acl.add(_elem607);",
                "                 }",
                "@@ -495,5 +495,5 @@ public class SettableBlobMeta implements org.apache.thrift.TBase<SettableBlobMet",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.acl.size()));",
                "-          for (AccessControl _iter589 : struct.acl)",
                "+          for (AccessControl _iter609 : struct.acl)",
                "           {",
                "-            _iter589.write(oprot);",
                "+            _iter609.write(oprot);",
                "           }",
                "@@ -527,5 +527,5 @@ public class SettableBlobMeta implements org.apache.thrift.TBase<SettableBlobMet",
                "         oprot.writeI32(struct.acl.size());",
                "-        for (AccessControl _iter590 : struct.acl)",
                "+        for (AccessControl _iter610 : struct.acl)",
                "         {",
                "-          _iter590.write(oprot);",
                "+          _iter610.write(oprot);",
                "         }",
                "@@ -546,10 +546,10 @@ public class SettableBlobMeta implements org.apache.thrift.TBase<SettableBlobMet",
                "       {",
                "-        org.apache.thrift.protocol.TList _list591 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.acl = new ArrayList<AccessControl>(_list591.size);",
                "-        AccessControl _elem592;",
                "-        for (int _i593 = 0; _i593 < _list591.size; ++_i593)",
                "+        org.apache.thrift.protocol.TList _list611 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.acl = new ArrayList<AccessControl>(_list611.size);",
                "+        AccessControl _elem612;",
                "+        for (int _i613 = 0; _i613 < _list611.size; ++_i613)",
                "         {",
                "-          _elem592 = new AccessControl();",
                "-          _elem592.read(iprot);",
                "-          struct.acl.add(_elem592);",
                "+          _elem612 = new AccessControl();",
                "+          _elem612.read(iprot);",
                "+          struct.acl.add(_elem612);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/StormBase.java b/storm-client/src/jvm/org/apache/storm/generated/StormBase.java",
                "index 84051f136..b1b205c1f 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/StormBase.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/StormBase.java",
                "@@ -1254,11 +1254,11 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map712 = iprot.readMapBegin();",
                "-                struct.component_executors = new HashMap<String,Integer>(2*_map712.size);",
                "-                String _key713;",
                "-                int _val714;",
                "-                for (int _i715 = 0; _i715 < _map712.size; ++_i715)",
                "+                org.apache.thrift.protocol.TMap _map732 = iprot.readMapBegin();",
                "+                struct.component_executors = new HashMap<String,Integer>(2*_map732.size);",
                "+                String _key733;",
                "+                int _val734;",
                "+                for (int _i735 = 0; _i735 < _map732.size; ++_i735)",
                "                 {",
                "-                  _key713 = iprot.readString();",
                "-                  _val714 = iprot.readI32();",
                "-                  struct.component_executors.put(_key713, _val714);",
                "+                  _key733 = iprot.readString();",
                "+                  _val734 = iprot.readI32();",
                "+                  struct.component_executors.put(_key733, _val734);",
                "                 }",
                "@@ -1307,12 +1307,12 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map716 = iprot.readMapBegin();",
                "-                struct.component_debug = new HashMap<String,DebugOptions>(2*_map716.size);",
                "-                String _key717;",
                "-                DebugOptions _val718;",
                "-                for (int _i719 = 0; _i719 < _map716.size; ++_i719)",
                "+                org.apache.thrift.protocol.TMap _map736 = iprot.readMapBegin();",
                "+                struct.component_debug = new HashMap<String,DebugOptions>(2*_map736.size);",
                "+                String _key737;",
                "+                DebugOptions _val738;",
                "+                for (int _i739 = 0; _i739 < _map736.size; ++_i739)",
                "                 {",
                "-                  _key717 = iprot.readString();",
                "-                  _val718 = new DebugOptions();",
                "-                  _val718.read(iprot);",
                "-                  struct.component_debug.put(_key717, _val718);",
                "+                  _key737 = iprot.readString();",
                "+                  _val738 = new DebugOptions();",
                "+                  _val738.read(iprot);",
                "+                  struct.component_debug.put(_key737, _val738);",
                "                 }",
                "@@ -1372,6 +1372,6 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, struct.component_executors.size()));",
                "-            for (Map.Entry<String, Integer> _iter720 : struct.component_executors.entrySet())",
                "+            for (Map.Entry<String, Integer> _iter740 : struct.component_executors.entrySet())",
                "             {",
                "-              oprot.writeString(_iter720.getKey());",
                "-              oprot.writeI32(_iter720.getValue());",
                "+              oprot.writeString(_iter740.getKey());",
                "+              oprot.writeI32(_iter740.getValue());",
                "             }",
                "@@ -1413,6 +1413,6 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, struct.component_debug.size()));",
                "-            for (Map.Entry<String, DebugOptions> _iter721 : struct.component_debug.entrySet())",
                "+            for (Map.Entry<String, DebugOptions> _iter741 : struct.component_debug.entrySet())",
                "             {",
                "-              oprot.writeString(_iter721.getKey());",
                "-              _iter721.getValue().write(oprot);",
                "+              oprot.writeString(_iter741.getKey());",
                "+              _iter741.getValue().write(oprot);",
                "             }",
                "@@ -1486,6 +1486,6 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "           oprot.writeI32(struct.component_executors.size());",
                "-          for (Map.Entry<String, Integer> _iter722 : struct.component_executors.entrySet())",
                "+          for (Map.Entry<String, Integer> _iter742 : struct.component_executors.entrySet())",
                "           {",
                "-            oprot.writeString(_iter722.getKey());",
                "-            oprot.writeI32(_iter722.getValue());",
                "+            oprot.writeString(_iter742.getKey());",
                "+            oprot.writeI32(_iter742.getValue());",
                "           }",
                "@@ -1508,6 +1508,6 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "           oprot.writeI32(struct.component_debug.size());",
                "-          for (Map.Entry<String, DebugOptions> _iter723 : struct.component_debug.entrySet())",
                "+          for (Map.Entry<String, DebugOptions> _iter743 : struct.component_debug.entrySet())",
                "           {",
                "-            oprot.writeString(_iter723.getKey());",
                "-            _iter723.getValue().write(oprot);",
                "+            oprot.writeString(_iter743.getKey());",
                "+            _iter743.getValue().write(oprot);",
                "           }",
                "@@ -1535,11 +1535,11 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map724 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, iprot.readI32());",
                "-          struct.component_executors = new HashMap<String,Integer>(2*_map724.size);",
                "-          String _key725;",
                "-          int _val726;",
                "-          for (int _i727 = 0; _i727 < _map724.size; ++_i727)",
                "+          org.apache.thrift.protocol.TMap _map744 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, iprot.readI32());",
                "+          struct.component_executors = new HashMap<String,Integer>(2*_map744.size);",
                "+          String _key745;",
                "+          int _val746;",
                "+          for (int _i747 = 0; _i747 < _map744.size; ++_i747)",
                "           {",
                "-            _key725 = iprot.readString();",
                "-            _val726 = iprot.readI32();",
                "-            struct.component_executors.put(_key725, _val726);",
                "+            _key745 = iprot.readString();",
                "+            _val746 = iprot.readI32();",
                "+            struct.component_executors.put(_key745, _val746);",
                "           }",
                "@@ -1567,12 +1567,12 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map728 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-          struct.component_debug = new HashMap<String,DebugOptions>(2*_map728.size);",
                "-          String _key729;",
                "-          DebugOptions _val730;",
                "-          for (int _i731 = 0; _i731 < _map728.size; ++_i731)",
                "+          org.apache.thrift.protocol.TMap _map748 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+          struct.component_debug = new HashMap<String,DebugOptions>(2*_map748.size);",
                "+          String _key749;",
                "+          DebugOptions _val750;",
                "+          for (int _i751 = 0; _i751 < _map748.size; ++_i751)",
                "           {",
                "-            _key729 = iprot.readString();",
                "-            _val730 = new DebugOptions();",
                "-            _val730.read(iprot);",
                "-            struct.component_debug.put(_key729, _val730);",
                "+            _key749 = iprot.readString();",
                "+            _val750 = new DebugOptions();",
                "+            _val750.read(iprot);",
                "+            struct.component_debug.put(_key749, _val750);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/SupervisorInfo.java b/storm-client/src/jvm/org/apache/storm/generated/SupervisorInfo.java",
                "index d19bac447..45293ed59 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/SupervisorInfo.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/SupervisorInfo.java",
                "@@ -1087,9 +1087,9 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "               {",
                "-                org.apache.thrift.protocol.TList _list602 = iprot.readListBegin();",
                "-                struct.used_ports = new ArrayList<Long>(_list602.size);",
                "-                long _elem603;",
                "-                for (int _i604 = 0; _i604 < _list602.size; ++_i604)",
                "+                org.apache.thrift.protocol.TList _list622 = iprot.readListBegin();",
                "+                struct.used_ports = new ArrayList<Long>(_list622.size);",
                "+                long _elem623;",
                "+                for (int _i624 = 0; _i624 < _list622.size; ++_i624)",
                "                 {",
                "-                  _elem603 = iprot.readI64();",
                "-                  struct.used_ports.add(_elem603);",
                "+                  _elem623 = iprot.readI64();",
                "+                  struct.used_ports.add(_elem623);",
                "                 }",
                "@@ -1105,9 +1105,9 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "               {",
                "-                org.apache.thrift.protocol.TList _list605 = iprot.readListBegin();",
                "-                struct.meta = new ArrayList<Long>(_list605.size);",
                "-                long _elem606;",
                "-                for (int _i607 = 0; _i607 < _list605.size; ++_i607)",
                "+                org.apache.thrift.protocol.TList _list625 = iprot.readListBegin();",
                "+                struct.meta = new ArrayList<Long>(_list625.size);",
                "+                long _elem626;",
                "+                for (int _i627 = 0; _i627 < _list625.size; ++_i627)",
                "                 {",
                "-                  _elem606 = iprot.readI64();",
                "-                  struct.meta.add(_elem606);",
                "+                  _elem626 = iprot.readI64();",
                "+                  struct.meta.add(_elem626);",
                "                 }",
                "@@ -1123,11 +1123,11 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map608 = iprot.readMapBegin();",
                "-                struct.scheduler_meta = new HashMap<String,String>(2*_map608.size);",
                "-                String _key609;",
                "-                String _val610;",
                "-                for (int _i611 = 0; _i611 < _map608.size; ++_i611)",
                "+                org.apache.thrift.protocol.TMap _map628 = iprot.readMapBegin();",
                "+                struct.scheduler_meta = new HashMap<String,String>(2*_map628.size);",
                "+                String _key629;",
                "+                String _val630;",
                "+                for (int _i631 = 0; _i631 < _map628.size; ++_i631)",
                "                 {",
                "-                  _key609 = iprot.readString();",
                "-                  _val610 = iprot.readString();",
                "-                  struct.scheduler_meta.put(_key609, _val610);",
                "+                  _key629 = iprot.readString();",
                "+                  _val630 = iprot.readString();",
                "+                  struct.scheduler_meta.put(_key629, _val630);",
                "                 }",
                "@@ -1159,11 +1159,11 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map612 = iprot.readMapBegin();",
                "-                struct.resources_map = new HashMap<String,Double>(2*_map612.size);",
                "-                String _key613;",
                "-                double _val614;",
                "-                for (int _i615 = 0; _i615 < _map612.size; ++_i615)",
                "+                org.apache.thrift.protocol.TMap _map632 = iprot.readMapBegin();",
                "+                struct.resources_map = new HashMap<String,Double>(2*_map632.size);",
                "+                String _key633;",
                "+                double _val634;",
                "+                for (int _i635 = 0; _i635 < _map632.size; ++_i635)",
                "                 {",
                "-                  _key613 = iprot.readString();",
                "-                  _val614 = iprot.readDouble();",
                "-                  struct.resources_map.put(_key613, _val614);",
                "+                  _key633 = iprot.readString();",
                "+                  _val634 = iprot.readDouble();",
                "+                  struct.resources_map.put(_key633, _val634);",
                "                 }",
                "@@ -1209,5 +1209,5 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "             oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, struct.used_ports.size()));",
                "-            for (long _iter616 : struct.used_ports)",
                "+            for (long _iter636 : struct.used_ports)",
                "             {",
                "-              oprot.writeI64(_iter616);",
                "+              oprot.writeI64(_iter636);",
                "             }",
                "@@ -1223,5 +1223,5 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "             oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, struct.meta.size()));",
                "-            for (long _iter617 : struct.meta)",
                "+            for (long _iter637 : struct.meta)",
                "             {",
                "-              oprot.writeI64(_iter617);",
                "+              oprot.writeI64(_iter637);",
                "             }",
                "@@ -1237,6 +1237,6 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, struct.scheduler_meta.size()));",
                "-            for (Map.Entry<String, String> _iter618 : struct.scheduler_meta.entrySet())",
                "+            for (Map.Entry<String, String> _iter638 : struct.scheduler_meta.entrySet())",
                "             {",
                "-              oprot.writeString(_iter618.getKey());",
                "-              oprot.writeString(_iter618.getValue());",
                "+              oprot.writeString(_iter638.getKey());",
                "+              oprot.writeString(_iter638.getValue());",
                "             }",
                "@@ -1264,6 +1264,6 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, struct.resources_map.size()));",
                "-            for (Map.Entry<String, Double> _iter619 : struct.resources_map.entrySet())",
                "+            for (Map.Entry<String, Double> _iter639 : struct.resources_map.entrySet())",
                "             {",
                "-              oprot.writeString(_iter619.getKey());",
                "-              oprot.writeDouble(_iter619.getValue());",
                "+              oprot.writeString(_iter639.getKey());",
                "+              oprot.writeDouble(_iter639.getValue());",
                "             }",
                "@@ -1322,5 +1322,5 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "           oprot.writeI32(struct.used_ports.size());",
                "-          for (long _iter620 : struct.used_ports)",
                "+          for (long _iter640 : struct.used_ports)",
                "           {",
                "-            oprot.writeI64(_iter620);",
                "+            oprot.writeI64(_iter640);",
                "           }",
                "@@ -1331,5 +1331,5 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "           oprot.writeI32(struct.meta.size());",
                "-          for (long _iter621 : struct.meta)",
                "+          for (long _iter641 : struct.meta)",
                "           {",
                "-            oprot.writeI64(_iter621);",
                "+            oprot.writeI64(_iter641);",
                "           }",
                "@@ -1340,6 +1340,6 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "           oprot.writeI32(struct.scheduler_meta.size());",
                "-          for (Map.Entry<String, String> _iter622 : struct.scheduler_meta.entrySet())",
                "+          for (Map.Entry<String, String> _iter642 : struct.scheduler_meta.entrySet())",
                "           {",
                "-            oprot.writeString(_iter622.getKey());",
                "-            oprot.writeString(_iter622.getValue());",
                "+            oprot.writeString(_iter642.getKey());",
                "+            oprot.writeString(_iter642.getValue());",
                "           }",
                "@@ -1356,6 +1356,6 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "           oprot.writeI32(struct.resources_map.size());",
                "-          for (Map.Entry<String, Double> _iter623 : struct.resources_map.entrySet())",
                "+          for (Map.Entry<String, Double> _iter643 : struct.resources_map.entrySet())",
                "           {",
                "-            oprot.writeString(_iter623.getKey());",
                "-            oprot.writeDouble(_iter623.getValue());",
                "+            oprot.writeString(_iter643.getKey());",
                "+            oprot.writeDouble(_iter643.getValue());",
                "           }",
                "@@ -1379,9 +1379,9 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "         {",
                "-          org.apache.thrift.protocol.TList _list624 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "-          struct.used_ports = new ArrayList<Long>(_list624.size);",
                "-          long _elem625;",
                "-          for (int _i626 = 0; _i626 < _list624.size; ++_i626)",
                "+          org.apache.thrift.protocol.TList _list644 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "+          struct.used_ports = new ArrayList<Long>(_list644.size);",
                "+          long _elem645;",
                "+          for (int _i646 = 0; _i646 < _list644.size; ++_i646)",
                "           {",
                "-            _elem625 = iprot.readI64();",
                "-            struct.used_ports.add(_elem625);",
                "+            _elem645 = iprot.readI64();",
                "+            struct.used_ports.add(_elem645);",
                "           }",
                "@@ -1392,9 +1392,9 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "         {",
                "-          org.apache.thrift.protocol.TList _list627 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "-          struct.meta = new ArrayList<Long>(_list627.size);",
                "-          long _elem628;",
                "-          for (int _i629 = 0; _i629 < _list627.size; ++_i629)",
                "+          org.apache.thrift.protocol.TList _list647 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "+          struct.meta = new ArrayList<Long>(_list647.size);",
                "+          long _elem648;",
                "+          for (int _i649 = 0; _i649 < _list647.size; ++_i649)",
                "           {",
                "-            _elem628 = iprot.readI64();",
                "-            struct.meta.add(_elem628);",
                "+            _elem648 = iprot.readI64();",
                "+            struct.meta.add(_elem648);",
                "           }",
                "@@ -1405,11 +1405,11 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map630 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-          struct.scheduler_meta = new HashMap<String,String>(2*_map630.size);",
                "-          String _key631;",
                "-          String _val632;",
                "-          for (int _i633 = 0; _i633 < _map630.size; ++_i633)",
                "+          org.apache.thrift.protocol.TMap _map650 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+          struct.scheduler_meta = new HashMap<String,String>(2*_map650.size);",
                "+          String _key651;",
                "+          String _val652;",
                "+          for (int _i653 = 0; _i653 < _map650.size; ++_i653)",
                "           {",
                "-            _key631 = iprot.readString();",
                "-            _val632 = iprot.readString();",
                "-            struct.scheduler_meta.put(_key631, _val632);",
                "+            _key651 = iprot.readString();",
                "+            _val652 = iprot.readString();",
                "+            struct.scheduler_meta.put(_key651, _val652);",
                "           }",
                "@@ -1428,11 +1428,11 @@ public class SupervisorInfo implements org.apache.thrift.TBase<SupervisorInfo, S",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map634 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "-          struct.resources_map = new HashMap<String,Double>(2*_map634.size);",
                "-          String _key635;",
                "-          double _val636;",
                "-          for (int _i637 = 0; _i637 < _map634.size; ++_i637)",
                "+          org.apache.thrift.protocol.TMap _map654 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "+          struct.resources_map = new HashMap<String,Double>(2*_map654.size);",
                "+          String _key655;",
                "+          double _val656;",
                "+          for (int _i657 = 0; _i657 < _map654.size; ++_i657)",
                "           {",
                "-            _key635 = iprot.readString();",
                "-            _val636 = iprot.readDouble();",
                "-            struct.resources_map.put(_key635, _val636);",
                "+            _key655 = iprot.readString();",
                "+            _val656 = iprot.readDouble();",
                "+            struct.resources_map.put(_key655, _val656);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/TopologyHistoryInfo.java b/storm-client/src/jvm/org/apache/storm/generated/TopologyHistoryInfo.java",
                "index 58cf9bb4e..0fc7cab68 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/TopologyHistoryInfo.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/TopologyHistoryInfo.java",
                "@@ -366,9 +366,9 @@ public class TopologyHistoryInfo implements org.apache.thrift.TBase<TopologyHist",
                "               {",
                "-                org.apache.thrift.protocol.TList _list822 = iprot.readListBegin();",
                "-                struct.topo_ids = new ArrayList<String>(_list822.size);",
                "-                String _elem823;",
                "-                for (int _i824 = 0; _i824 < _list822.size; ++_i824)",
                "+                org.apache.thrift.protocol.TList _list842 = iprot.readListBegin();",
                "+                struct.topo_ids = new ArrayList<String>(_list842.size);",
                "+                String _elem843;",
                "+                for (int _i844 = 0; _i844 < _list842.size; ++_i844)",
                "                 {",
                "-                  _elem823 = iprot.readString();",
                "-                  struct.topo_ids.add(_elem823);",
                "+                  _elem843 = iprot.readString();",
                "+                  struct.topo_ids.add(_elem843);",
                "                 }",
                "@@ -398,5 +398,5 @@ public class TopologyHistoryInfo implements org.apache.thrift.TBase<TopologyHist",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, struct.topo_ids.size()));",
                "-          for (String _iter825 : struct.topo_ids)",
                "+          for (String _iter845 : struct.topo_ids)",
                "           {",
                "-            oprot.writeString(_iter825);",
                "+            oprot.writeString(_iter845);",
                "           }",
                "@@ -431,5 +431,5 @@ public class TopologyHistoryInfo implements org.apache.thrift.TBase<TopologyHist",
                "           oprot.writeI32(struct.topo_ids.size());",
                "-          for (String _iter826 : struct.topo_ids)",
                "+          for (String _iter846 : struct.topo_ids)",
                "           {",
                "-            oprot.writeString(_iter826);",
                "+            oprot.writeString(_iter846);",
                "           }",
                "@@ -445,9 +445,9 @@ public class TopologyHistoryInfo implements org.apache.thrift.TBase<TopologyHist",
                "         {",
                "-          org.apache.thrift.protocol.TList _list827 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-          struct.topo_ids = new ArrayList<String>(_list827.size);",
                "-          String _elem828;",
                "-          for (int _i829 = 0; _i829 < _list827.size; ++_i829)",
                "+          org.apache.thrift.protocol.TList _list847 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+          struct.topo_ids = new ArrayList<String>(_list847.size);",
                "+          String _elem848;",
                "+          for (int _i849 = 0; _i849 < _list847.size; ++_i849)",
                "           {",
                "-            _elem828 = iprot.readString();",
                "-            struct.topo_ids.add(_elem828);",
                "+            _elem848 = iprot.readString();",
                "+            struct.topo_ids.add(_elem848);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "index 99ef9cd9f..971e5f349 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "@@ -48,3 +48,2 @@ import org.apache.storm.windowing.TupleWindow;",
                " import org.json.simple.JSONValue;",
                "-import org.json.simple.parser.ParseException;",
                "@@ -579,3 +578,3 @@ public class TopologyBuilder {",
                "             String currConf = _commons.get(_id).get_json_conf();",
                "-            _commons.get(_id).set_json_conf(mergeIntoJson(parseJson(currConf), conf));",
                "+            _commons.get(_id).set_json_conf(mergeIntoJson(Utils.parseJson(currConf), conf));",
                "             return (T) this;",
                "@@ -697,15 +696,3 @@ public class TopologyBuilder {",
                "     }",
                "-    ",
                "-    private static Map parseJson(String json) {",
                "-        if (json==null) {",
                "-            return new HashMap();",
                "-        } else {",
                "-            try {",
                "-                return (Map) JSONValue.parseWithException(json);",
                "-            } catch (ParseException e) {",
                "-                throw new RuntimeException(e);",
                "-            }",
                "-        }",
                "-    }",
                "-    ",
                "+",
                "     private static String mergeIntoJson(Map into, Map newMap) {",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/Utils.java b/storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "index 0a38f6192..37c29630c 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "@@ -907,3 +907,3 @@ public class Utils {",
                "      * parses the arguments to extract jvm heap memory size in MB.",
                "-     * @param input",
                "+     * @param options",
                "      * @param defaultValue",
                "@@ -1290,2 +1290,14 @@ public class Utils {",
                "+    public static Map<String, Object> parseJson(String json) {",
                "+        if (json==null) {",
                "+            return new HashMap<>();",
                "+        } else {",
                "+            try {",
                "+                return (Map<String, Object>) JSONValue.parseWithException(json);",
                "+            } catch (ParseException e) {",
                "+                throw new RuntimeException(e);",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "     // Non-static impl methods exist for mocking purposes.",
                "diff --git a/storm-client/src/py/storm/Nimbus.py b/storm-client/src/py/storm/Nimbus.py",
                "index 545b05d76..522921b1c 100644",
                "--- a/storm-client/src/py/storm/Nimbus.py",
                "+++ b/storm-client/src/py/storm/Nimbus.py",
                "@@ -4979,7 +4979,7 @@ class getComponentPendingProfileActions_result:",
                "           self.success = []",
                "-          (_etype758, _size755) = iprot.readListBegin()",
                "-          for _i759 in xrange(_size755):",
                "-            _elem760 = ProfileRequest()",
                "-            _elem760.read(iprot)",
                "-            self.success.append(_elem760)",
                "+          (_etype776, _size773) = iprot.readListBegin()",
                "+          for _i777 in xrange(_size773):",
                "+            _elem778 = ProfileRequest()",
                "+            _elem778.read(iprot)",
                "+            self.success.append(_elem778)",
                "           iprot.readListEnd()",
                "@@ -5000,4 +5000,4 @@ class getComponentPendingProfileActions_result:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.success))",
                "-      for iter761 in self.success:",
                "-        iter761.write(oprot)",
                "+      for iter779 in self.success:",
                "+        iter779.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -10072,7 +10072,7 @@ class getOwnerResourceSummaries_result:",
                "           self.success = []",
                "-          (_etype765, _size762) = iprot.readListBegin()",
                "-          for _i766 in xrange(_size762):",
                "-            _elem767 = OwnerResourceSummary()",
                "-            _elem767.read(iprot)",
                "-            self.success.append(_elem767)",
                "+          (_etype783, _size780) = iprot.readListBegin()",
                "+          for _i784 in xrange(_size780):",
                "+            _elem785 = OwnerResourceSummary()",
                "+            _elem785.read(iprot)",
                "+            self.success.append(_elem785)",
                "           iprot.readListEnd()",
                "@@ -10099,4 +10099,4 @@ class getOwnerResourceSummaries_result:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.success))",
                "-      for iter768 in self.success:",
                "-        iter768.write(oprot)",
                "+      for iter786 in self.success:",
                "+        iter786.write(oprot)",
                "       oprot.writeListEnd()",
                "diff --git a/storm-client/src/py/storm/ttypes.py b/storm-client/src/py/storm/ttypes.py",
                "index 0daf3ffc5..6b744c6ed 100644",
                "--- a/storm-client/src/py/storm/ttypes.py",
                "+++ b/storm-client/src/py/storm/ttypes.py",
                "@@ -8357,2 +8357,5 @@ class RebalanceOptions:",
                "    - num_executors",
                "+   - topology_resources_overrides",
                "+   - topology_conf_overrides",
                "+   - principal",
                "   \"\"\"",
                "@@ -8364,5 +8367,8 @@ class RebalanceOptions:",
                "     (3, TType.MAP, 'num_executors', (TType.STRING,None,TType.I32,None), None, ), # 3",
                "+    (4, TType.MAP, 'topology_resources_overrides', (TType.STRING,None,TType.MAP,(TType.STRING,None,TType.DOUBLE,None)), None, ), # 4",
                "+    (5, TType.STRING, 'topology_conf_overrides', None, None, ), # 5",
                "+    (6, TType.STRING, 'principal', None, None, ), # 6",
                "   )",
                "-  def __init__(self, wait_secs=None, num_workers=None, num_executors=None,):",
                "+  def __init__(self, wait_secs=None, num_workers=None, num_executors=None, topology_resources_overrides=None, topology_conf_overrides=None, principal=None,):",
                "     self.wait_secs = wait_secs",
                "@@ -8370,2 +8376,5 @@ class RebalanceOptions:",
                "     self.num_executors = num_executors",
                "+    self.topology_resources_overrides = topology_resources_overrides",
                "+    self.topology_conf_overrides = topology_conf_overrides",
                "+    self.principal = principal",
                "@@ -8401,2 +8410,29 @@ class RebalanceOptions:",
                "           iprot.skip(ftype)",
                "+      elif fid == 4:",
                "+        if ftype == TType.MAP:",
                "+          self.topology_resources_overrides = {}",
                "+          (_ktype514, _vtype515, _size513 ) = iprot.readMapBegin()",
                "+          for _i517 in xrange(_size513):",
                "+            _key518 = iprot.readString().decode('utf-8')",
                "+            _val519 = {}",
                "+            (_ktype521, _vtype522, _size520 ) = iprot.readMapBegin()",
                "+            for _i524 in xrange(_size520):",
                "+              _key525 = iprot.readString().decode('utf-8')",
                "+              _val526 = iprot.readDouble()",
                "+              _val519[_key525] = _val526",
                "+            iprot.readMapEnd()",
                "+            self.topology_resources_overrides[_key518] = _val519",
                "+          iprot.readMapEnd()",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      elif fid == 5:",
                "+        if ftype == TType.STRING:",
                "+          self.topology_conf_overrides = iprot.readString().decode('utf-8')",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      elif fid == 6:",
                "+        if ftype == TType.STRING:",
                "+          self.principal = iprot.readString().decode('utf-8')",
                "+        else:",
                "+          iprot.skip(ftype)",
                "       else:",
                "@@ -8422,7 +8458,27 @@ class RebalanceOptions:",
                "       oprot.writeMapBegin(TType.STRING, TType.I32, len(self.num_executors))",
                "-      for kiter513,viter514 in self.num_executors.items():",
                "-        oprot.writeString(kiter513.encode('utf-8'))",
                "-        oprot.writeI32(viter514)",
                "+      for kiter527,viter528 in self.num_executors.items():",
                "+        oprot.writeString(kiter527.encode('utf-8'))",
                "+        oprot.writeI32(viter528)",
                "+      oprot.writeMapEnd()",
                "+      oprot.writeFieldEnd()",
                "+    if self.topology_resources_overrides is not None:",
                "+      oprot.writeFieldBegin('topology_resources_overrides', TType.MAP, 4)",
                "+      oprot.writeMapBegin(TType.STRING, TType.MAP, len(self.topology_resources_overrides))",
                "+      for kiter529,viter530 in self.topology_resources_overrides.items():",
                "+        oprot.writeString(kiter529.encode('utf-8'))",
                "+        oprot.writeMapBegin(TType.STRING, TType.DOUBLE, len(viter530))",
                "+        for kiter531,viter532 in viter530.items():",
                "+          oprot.writeString(kiter531.encode('utf-8'))",
                "+          oprot.writeDouble(viter532)",
                "+        oprot.writeMapEnd()",
                "       oprot.writeMapEnd()",
                "       oprot.writeFieldEnd()",
                "+    if self.topology_conf_overrides is not None:",
                "+      oprot.writeFieldBegin('topology_conf_overrides', TType.STRING, 5)",
                "+      oprot.writeString(self.topology_conf_overrides.encode('utf-8'))",
                "+      oprot.writeFieldEnd()",
                "+    if self.principal is not None:",
                "+      oprot.writeFieldBegin('principal', TType.STRING, 6)",
                "+      oprot.writeString(self.principal.encode('utf-8'))",
                "+      oprot.writeFieldEnd()",
                "     oprot.writeFieldStop()",
                "@@ -8439,2 +8495,5 @@ class RebalanceOptions:",
                "     value = (value * 31) ^ hash(self.num_executors)",
                "+    value = (value * 31) ^ hash(self.topology_resources_overrides)",
                "+    value = (value * 31) ^ hash(self.topology_conf_overrides)",
                "+    value = (value * 31) ^ hash(self.principal)",
                "     return value",
                "@@ -8478,7 +8537,7 @@ class Credentials:",
                "           self.creds = {}",
                "-          (_ktype516, _vtype517, _size515 ) = iprot.readMapBegin()",
                "-          for _i519 in xrange(_size515):",
                "-            _key520 = iprot.readString().decode('utf-8')",
                "-            _val521 = iprot.readString().decode('utf-8')",
                "-            self.creds[_key520] = _val521",
                "+          (_ktype534, _vtype535, _size533 ) = iprot.readMapBegin()",
                "+          for _i537 in xrange(_size533):",
                "+            _key538 = iprot.readString().decode('utf-8')",
                "+            _val539 = iprot.readString().decode('utf-8')",
                "+            self.creds[_key538] = _val539",
                "           iprot.readMapEnd()",
                "@@ -8499,5 +8558,5 @@ class Credentials:",
                "       oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.creds))",
                "-      for kiter522,viter523 in self.creds.items():",
                "-        oprot.writeString(kiter522.encode('utf-8'))",
                "-        oprot.writeString(viter523.encode('utf-8'))",
                "+      for kiter540,viter541 in self.creds.items():",
                "+        oprot.writeString(kiter540.encode('utf-8'))",
                "+        oprot.writeString(viter541.encode('utf-8'))",
                "       oprot.writeMapEnd()",
                "@@ -8734,7 +8793,7 @@ class SettableBlobMeta:",
                "           self.acl = []",
                "-          (_etype527, _size524) = iprot.readListBegin()",
                "-          for _i528 in xrange(_size524):",
                "-            _elem529 = AccessControl()",
                "-            _elem529.read(iprot)",
                "-            self.acl.append(_elem529)",
                "+          (_etype545, _size542) = iprot.readListBegin()",
                "+          for _i546 in xrange(_size542):",
                "+            _elem547 = AccessControl()",
                "+            _elem547.read(iprot)",
                "+            self.acl.append(_elem547)",
                "           iprot.readListEnd()",
                "@@ -8760,4 +8819,4 @@ class SettableBlobMeta:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.acl))",
                "-      for iter530 in self.acl:",
                "-        iter530.write(oprot)",
                "+      for iter548 in self.acl:",
                "+        iter548.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -8906,6 +8965,6 @@ class ListBlobsResult:",
                "           self.keys = []",
                "-          (_etype534, _size531) = iprot.readListBegin()",
                "-          for _i535 in xrange(_size531):",
                "-            _elem536 = iprot.readString().decode('utf-8')",
                "-            self.keys.append(_elem536)",
                "+          (_etype552, _size549) = iprot.readListBegin()",
                "+          for _i553 in xrange(_size549):",
                "+            _elem554 = iprot.readString().decode('utf-8')",
                "+            self.keys.append(_elem554)",
                "           iprot.readListEnd()",
                "@@ -8931,4 +8990,4 @@ class ListBlobsResult:",
                "       oprot.writeListBegin(TType.STRING, len(self.keys))",
                "-      for iter537 in self.keys:",
                "-        oprot.writeString(iter537.encode('utf-8'))",
                "+      for iter555 in self.keys:",
                "+        oprot.writeString(iter555.encode('utf-8'))",
                "       oprot.writeListEnd()",
                "@@ -9127,6 +9186,6 @@ class SupervisorInfo:",
                "           self.used_ports = []",
                "-          (_etype541, _size538) = iprot.readListBegin()",
                "-          for _i542 in xrange(_size538):",
                "-            _elem543 = iprot.readI64()",
                "-            self.used_ports.append(_elem543)",
                "+          (_etype559, _size556) = iprot.readListBegin()",
                "+          for _i560 in xrange(_size556):",
                "+            _elem561 = iprot.readI64()",
                "+            self.used_ports.append(_elem561)",
                "           iprot.readListEnd()",
                "@@ -9137,6 +9196,6 @@ class SupervisorInfo:",
                "           self.meta = []",
                "-          (_etype547, _size544) = iprot.readListBegin()",
                "-          for _i548 in xrange(_size544):",
                "-            _elem549 = iprot.readI64()",
                "-            self.meta.append(_elem549)",
                "+          (_etype565, _size562) = iprot.readListBegin()",
                "+          for _i566 in xrange(_size562):",
                "+            _elem567 = iprot.readI64()",
                "+            self.meta.append(_elem567)",
                "           iprot.readListEnd()",
                "@@ -9147,7 +9206,7 @@ class SupervisorInfo:",
                "           self.scheduler_meta = {}",
                "-          (_ktype551, _vtype552, _size550 ) = iprot.readMapBegin()",
                "-          for _i554 in xrange(_size550):",
                "-            _key555 = iprot.readString().decode('utf-8')",
                "-            _val556 = iprot.readString().decode('utf-8')",
                "-            self.scheduler_meta[_key555] = _val556",
                "+          (_ktype569, _vtype570, _size568 ) = iprot.readMapBegin()",
                "+          for _i572 in xrange(_size568):",
                "+            _key573 = iprot.readString().decode('utf-8')",
                "+            _val574 = iprot.readString().decode('utf-8')",
                "+            self.scheduler_meta[_key573] = _val574",
                "           iprot.readMapEnd()",
                "@@ -9168,7 +9227,7 @@ class SupervisorInfo:",
                "           self.resources_map = {}",
                "-          (_ktype558, _vtype559, _size557 ) = iprot.readMapBegin()",
                "-          for _i561 in xrange(_size557):",
                "-            _key562 = iprot.readString().decode('utf-8')",
                "-            _val563 = iprot.readDouble()",
                "-            self.resources_map[_key562] = _val563",
                "+          (_ktype576, _vtype577, _size575 ) = iprot.readMapBegin()",
                "+          for _i579 in xrange(_size575):",
                "+            _key580 = iprot.readString().decode('utf-8')",
                "+            _val581 = iprot.readDouble()",
                "+            self.resources_map[_key580] = _val581",
                "           iprot.readMapEnd()",
                "@@ -9201,4 +9260,4 @@ class SupervisorInfo:",
                "       oprot.writeListBegin(TType.I64, len(self.used_ports))",
                "-      for iter564 in self.used_ports:",
                "-        oprot.writeI64(iter564)",
                "+      for iter582 in self.used_ports:",
                "+        oprot.writeI64(iter582)",
                "       oprot.writeListEnd()",
                "@@ -9208,4 +9267,4 @@ class SupervisorInfo:",
                "       oprot.writeListBegin(TType.I64, len(self.meta))",
                "-      for iter565 in self.meta:",
                "-        oprot.writeI64(iter565)",
                "+      for iter583 in self.meta:",
                "+        oprot.writeI64(iter583)",
                "       oprot.writeListEnd()",
                "@@ -9215,5 +9274,5 @@ class SupervisorInfo:",
                "       oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.scheduler_meta))",
                "-      for kiter566,viter567 in self.scheduler_meta.items():",
                "-        oprot.writeString(kiter566.encode('utf-8'))",
                "-        oprot.writeString(viter567.encode('utf-8'))",
                "+      for kiter584,viter585 in self.scheduler_meta.items():",
                "+        oprot.writeString(kiter584.encode('utf-8'))",
                "+        oprot.writeString(viter585.encode('utf-8'))",
                "       oprot.writeMapEnd()",
                "@@ -9231,5 +9290,5 @@ class SupervisorInfo:",
                "       oprot.writeMapBegin(TType.STRING, TType.DOUBLE, len(self.resources_map))",
                "-      for kiter568,viter569 in self.resources_map.items():",
                "-        oprot.writeString(kiter568.encode('utf-8'))",
                "-        oprot.writeDouble(viter569)",
                "+      for kiter586,viter587 in self.resources_map.items():",
                "+        oprot.writeString(kiter586.encode('utf-8'))",
                "+        oprot.writeDouble(viter587)",
                "       oprot.writeMapEnd()",
                "@@ -9305,6 +9364,6 @@ class NodeInfo:",
                "           self.port = set()",
                "-          (_etype573, _size570) = iprot.readSetBegin()",
                "-          for _i574 in xrange(_size570):",
                "-            _elem575 = iprot.readI64()",
                "-            self.port.add(_elem575)",
                "+          (_etype591, _size588) = iprot.readSetBegin()",
                "+          for _i592 in xrange(_size588):",
                "+            _elem593 = iprot.readI64()",
                "+            self.port.add(_elem593)",
                "           iprot.readSetEnd()",
                "@@ -9329,4 +9388,4 @@ class NodeInfo:",
                "       oprot.writeSetBegin(TType.I64, len(self.port))",
                "-      for iter576 in self.port:",
                "-        oprot.writeI64(iter576)",
                "+      for iter594 in self.port:",
                "+        oprot.writeI64(iter594)",
                "       oprot.writeSetEnd()",
                "@@ -9547,7 +9606,7 @@ class Assignment:",
                "           self.node_host = {}",
                "-          (_ktype578, _vtype579, _size577 ) = iprot.readMapBegin()",
                "-          for _i581 in xrange(_size577):",
                "-            _key582 = iprot.readString().decode('utf-8')",
                "-            _val583 = iprot.readString().decode('utf-8')",
                "-            self.node_host[_key582] = _val583",
                "+          (_ktype596, _vtype597, _size595 ) = iprot.readMapBegin()",
                "+          for _i599 in xrange(_size595):",
                "+            _key600 = iprot.readString().decode('utf-8')",
                "+            _val601 = iprot.readString().decode('utf-8')",
                "+            self.node_host[_key600] = _val601",
                "           iprot.readMapEnd()",
                "@@ -9558,13 +9617,13 @@ class Assignment:",
                "           self.executor_node_port = {}",
                "-          (_ktype585, _vtype586, _size584 ) = iprot.readMapBegin()",
                "-          for _i588 in xrange(_size584):",
                "-            _key589 = []",
                "-            (_etype594, _size591) = iprot.readListBegin()",
                "-            for _i595 in xrange(_size591):",
                "-              _elem596 = iprot.readI64()",
                "-              _key589.append(_elem596)",
                "+          (_ktype603, _vtype604, _size602 ) = iprot.readMapBegin()",
                "+          for _i606 in xrange(_size602):",
                "+            _key607 = []",
                "+            (_etype612, _size609) = iprot.readListBegin()",
                "+            for _i613 in xrange(_size609):",
                "+              _elem614 = iprot.readI64()",
                "+              _key607.append(_elem614)",
                "             iprot.readListEnd()",
                "-            _val590 = NodeInfo()",
                "-            _val590.read(iprot)",
                "-            self.executor_node_port[_key589] = _val590",
                "+            _val608 = NodeInfo()",
                "+            _val608.read(iprot)",
                "+            self.executor_node_port[_key607] = _val608",
                "           iprot.readMapEnd()",
                "@@ -9575,12 +9634,12 @@ class Assignment:",
                "           self.executor_start_time_secs = {}",
                "-          (_ktype598, _vtype599, _size597 ) = iprot.readMapBegin()",
                "-          for _i601 in xrange(_size597):",
                "-            _key602 = []",
                "-            (_etype607, _size604) = iprot.readListBegin()",
                "-            for _i608 in xrange(_size604):",
                "-              _elem609 = iprot.readI64()",
                "-              _key602.append(_elem609)",
                "+          (_ktype616, _vtype617, _size615 ) = iprot.readMapBegin()",
                "+          for _i619 in xrange(_size615):",
                "+            _key620 = []",
                "+            (_etype625, _size622) = iprot.readListBegin()",
                "+            for _i626 in xrange(_size622):",
                "+              _elem627 = iprot.readI64()",
                "+              _key620.append(_elem627)",
                "             iprot.readListEnd()",
                "-            _val603 = iprot.readI64()",
                "-            self.executor_start_time_secs[_key602] = _val603",
                "+            _val621 = iprot.readI64()",
                "+            self.executor_start_time_secs[_key620] = _val621",
                "           iprot.readMapEnd()",
                "@@ -9591,9 +9650,9 @@ class Assignment:",
                "           self.worker_resources = {}",
                "-          (_ktype611, _vtype612, _size610 ) = iprot.readMapBegin()",
                "-          for _i614 in xrange(_size610):",
                "-            _key615 = NodeInfo()",
                "-            _key615.read(iprot)",
                "-            _val616 = WorkerResources()",
                "-            _val616.read(iprot)",
                "-            self.worker_resources[_key615] = _val616",
                "+          (_ktype629, _vtype630, _size628 ) = iprot.readMapBegin()",
                "+          for _i632 in xrange(_size628):",
                "+            _key633 = NodeInfo()",
                "+            _key633.read(iprot)",
                "+            _val634 = WorkerResources()",
                "+            _val634.read(iprot)",
                "+            self.worker_resources[_key633] = _val634",
                "           iprot.readMapEnd()",
                "@@ -9604,7 +9663,7 @@ class Assignment:",
                "           self.total_shared_off_heap = {}",
                "-          (_ktype618, _vtype619, _size617 ) = iprot.readMapBegin()",
                "-          for _i621 in xrange(_size617):",
                "-            _key622 = iprot.readString().decode('utf-8')",
                "-            _val623 = iprot.readDouble()",
                "-            self.total_shared_off_heap[_key622] = _val623",
                "+          (_ktype636, _vtype637, _size635 ) = iprot.readMapBegin()",
                "+          for _i639 in xrange(_size635):",
                "+            _key640 = iprot.readString().decode('utf-8')",
                "+            _val641 = iprot.readDouble()",
                "+            self.total_shared_off_heap[_key640] = _val641",
                "           iprot.readMapEnd()",
                "@@ -9634,5 +9693,5 @@ class Assignment:",
                "       oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.node_host))",
                "-      for kiter624,viter625 in self.node_host.items():",
                "-        oprot.writeString(kiter624.encode('utf-8'))",
                "-        oprot.writeString(viter625.encode('utf-8'))",
                "+      for kiter642,viter643 in self.node_host.items():",
                "+        oprot.writeString(kiter642.encode('utf-8'))",
                "+        oprot.writeString(viter643.encode('utf-8'))",
                "       oprot.writeMapEnd()",
                "@@ -9642,8 +9701,8 @@ class Assignment:",
                "       oprot.writeMapBegin(TType.LIST, TType.STRUCT, len(self.executor_node_port))",
                "-      for kiter626,viter627 in self.executor_node_port.items():",
                "-        oprot.writeListBegin(TType.I64, len(kiter626))",
                "-        for iter628 in kiter626:",
                "-          oprot.writeI64(iter628)",
                "+      for kiter644,viter645 in self.executor_node_port.items():",
                "+        oprot.writeListBegin(TType.I64, len(kiter644))",
                "+        for iter646 in kiter644:",
                "+          oprot.writeI64(iter646)",
                "         oprot.writeListEnd()",
                "-        viter627.write(oprot)",
                "+        viter645.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -9653,8 +9712,8 @@ class Assignment:",
                "       oprot.writeMapBegin(TType.LIST, TType.I64, len(self.executor_start_time_secs))",
                "-      for kiter629,viter630 in self.executor_start_time_secs.items():",
                "-        oprot.writeListBegin(TType.I64, len(kiter629))",
                "-        for iter631 in kiter629:",
                "-          oprot.writeI64(iter631)",
                "+      for kiter647,viter648 in self.executor_start_time_secs.items():",
                "+        oprot.writeListBegin(TType.I64, len(kiter647))",
                "+        for iter649 in kiter647:",
                "+          oprot.writeI64(iter649)",
                "         oprot.writeListEnd()",
                "-        oprot.writeI64(viter630)",
                "+        oprot.writeI64(viter648)",
                "       oprot.writeMapEnd()",
                "@@ -9664,5 +9723,5 @@ class Assignment:",
                "       oprot.writeMapBegin(TType.STRUCT, TType.STRUCT, len(self.worker_resources))",
                "-      for kiter632,viter633 in self.worker_resources.items():",
                "-        kiter632.write(oprot)",
                "-        viter633.write(oprot)",
                "+      for kiter650,viter651 in self.worker_resources.items():",
                "+        kiter650.write(oprot)",
                "+        viter651.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -9672,5 +9731,5 @@ class Assignment:",
                "       oprot.writeMapBegin(TType.STRING, TType.DOUBLE, len(self.total_shared_off_heap))",
                "-      for kiter634,viter635 in self.total_shared_off_heap.items():",
                "-        oprot.writeString(kiter634.encode('utf-8'))",
                "-        oprot.writeDouble(viter635)",
                "+      for kiter652,viter653 in self.total_shared_off_heap.items():",
                "+        oprot.writeString(kiter652.encode('utf-8'))",
                "+        oprot.writeDouble(viter653)",
                "       oprot.writeMapEnd()",
                "@@ -9863,7 +9922,7 @@ class StormBase:",
                "           self.component_executors = {}",
                "-          (_ktype637, _vtype638, _size636 ) = iprot.readMapBegin()",
                "-          for _i640 in xrange(_size636):",
                "-            _key641 = iprot.readString().decode('utf-8')",
                "-            _val642 = iprot.readI32()",
                "-            self.component_executors[_key641] = _val642",
                "+          (_ktype655, _vtype656, _size654 ) = iprot.readMapBegin()",
                "+          for _i658 in xrange(_size654):",
                "+            _key659 = iprot.readString().decode('utf-8')",
                "+            _val660 = iprot.readI32()",
                "+            self.component_executors[_key659] = _val660",
                "           iprot.readMapEnd()",
                "@@ -9895,8 +9954,8 @@ class StormBase:",
                "           self.component_debug = {}",
                "-          (_ktype644, _vtype645, _size643 ) = iprot.readMapBegin()",
                "-          for _i647 in xrange(_size643):",
                "-            _key648 = iprot.readString().decode('utf-8')",
                "-            _val649 = DebugOptions()",
                "-            _val649.read(iprot)",
                "-            self.component_debug[_key648] = _val649",
                "+          (_ktype662, _vtype663, _size661 ) = iprot.readMapBegin()",
                "+          for _i665 in xrange(_size661):",
                "+            _key666 = iprot.readString().decode('utf-8')",
                "+            _val667 = DebugOptions()",
                "+            _val667.read(iprot)",
                "+            self.component_debug[_key666] = _val667",
                "           iprot.readMapEnd()",
                "@@ -9939,5 +9998,5 @@ class StormBase:",
                "       oprot.writeMapBegin(TType.STRING, TType.I32, len(self.component_executors))",
                "-      for kiter650,viter651 in self.component_executors.items():",
                "-        oprot.writeString(kiter650.encode('utf-8'))",
                "-        oprot.writeI32(viter651)",
                "+      for kiter668,viter669 in self.component_executors.items():",
                "+        oprot.writeString(kiter668.encode('utf-8'))",
                "+        oprot.writeI32(viter669)",
                "       oprot.writeMapEnd()",
                "@@ -9963,5 +10022,5 @@ class StormBase:",
                "       oprot.writeMapBegin(TType.STRING, TType.STRUCT, len(self.component_debug))",
                "-      for kiter652,viter653 in self.component_debug.items():",
                "-        oprot.writeString(kiter652.encode('utf-8'))",
                "-        viter653.write(oprot)",
                "+      for kiter670,viter671 in self.component_debug.items():",
                "+        oprot.writeString(kiter670.encode('utf-8'))",
                "+        viter671.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -10055,9 +10114,9 @@ class ClusterWorkerHeartbeat:",
                "           self.executor_stats = {}",
                "-          (_ktype655, _vtype656, _size654 ) = iprot.readMapBegin()",
                "-          for _i658 in xrange(_size654):",
                "-            _key659 = ExecutorInfo()",
                "-            _key659.read(iprot)",
                "-            _val660 = ExecutorStats()",
                "-            _val660.read(iprot)",
                "-            self.executor_stats[_key659] = _val660",
                "+          (_ktype673, _vtype674, _size672 ) = iprot.readMapBegin()",
                "+          for _i676 in xrange(_size672):",
                "+            _key677 = ExecutorInfo()",
                "+            _key677.read(iprot)",
                "+            _val678 = ExecutorStats()",
                "+            _val678.read(iprot)",
                "+            self.executor_stats[_key677] = _val678",
                "           iprot.readMapEnd()",
                "@@ -10092,5 +10151,5 @@ class ClusterWorkerHeartbeat:",
                "       oprot.writeMapBegin(TType.STRUCT, TType.STRUCT, len(self.executor_stats))",
                "-      for kiter661,viter662 in self.executor_stats.items():",
                "-        kiter661.write(oprot)",
                "-        viter662.write(oprot)",
                "+      for kiter679,viter680 in self.executor_stats.items():",
                "+        kiter679.write(oprot)",
                "+        viter680.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -10247,8 +10306,8 @@ class LocalStateData:",
                "           self.serialized_parts = {}",
                "-          (_ktype664, _vtype665, _size663 ) = iprot.readMapBegin()",
                "-          for _i667 in xrange(_size663):",
                "-            _key668 = iprot.readString().decode('utf-8')",
                "-            _val669 = ThriftSerializedObject()",
                "-            _val669.read(iprot)",
                "-            self.serialized_parts[_key668] = _val669",
                "+          (_ktype682, _vtype683, _size681 ) = iprot.readMapBegin()",
                "+          for _i685 in xrange(_size681):",
                "+            _key686 = iprot.readString().decode('utf-8')",
                "+            _val687 = ThriftSerializedObject()",
                "+            _val687.read(iprot)",
                "+            self.serialized_parts[_key686] = _val687",
                "           iprot.readMapEnd()",
                "@@ -10269,5 +10328,5 @@ class LocalStateData:",
                "       oprot.writeMapBegin(TType.STRING, TType.STRUCT, len(self.serialized_parts))",
                "-      for kiter670,viter671 in self.serialized_parts.items():",
                "-        oprot.writeString(kiter670.encode('utf-8'))",
                "-        viter671.write(oprot)",
                "+      for kiter688,viter689 in self.serialized_parts.items():",
                "+        oprot.writeString(kiter688.encode('utf-8'))",
                "+        viter689.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -10342,7 +10401,7 @@ class LocalAssignment:",
                "           self.executors = []",
                "-          (_etype675, _size672) = iprot.readListBegin()",
                "-          for _i676 in xrange(_size672):",
                "-            _elem677 = ExecutorInfo()",
                "-            _elem677.read(iprot)",
                "-            self.executors.append(_elem677)",
                "+          (_etype693, _size690) = iprot.readListBegin()",
                "+          for _i694 in xrange(_size690):",
                "+            _elem695 = ExecutorInfo()",
                "+            _elem695.read(iprot)",
                "+            self.executors.append(_elem695)",
                "           iprot.readListEnd()",
                "@@ -10383,4 +10442,4 @@ class LocalAssignment:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.executors))",
                "-      for iter678 in self.executors:",
                "-        iter678.write(oprot)",
                "+      for iter696 in self.executors:",
                "+        iter696.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -10523,7 +10582,7 @@ class LSApprovedWorkers:",
                "           self.approved_workers = {}",
                "-          (_ktype680, _vtype681, _size679 ) = iprot.readMapBegin()",
                "-          for _i683 in xrange(_size679):",
                "-            _key684 = iprot.readString().decode('utf-8')",
                "-            _val685 = iprot.readI32()",
                "-            self.approved_workers[_key684] = _val685",
                "+          (_ktype698, _vtype699, _size697 ) = iprot.readMapBegin()",
                "+          for _i701 in xrange(_size697):",
                "+            _key702 = iprot.readString().decode('utf-8')",
                "+            _val703 = iprot.readI32()",
                "+            self.approved_workers[_key702] = _val703",
                "           iprot.readMapEnd()",
                "@@ -10544,5 +10603,5 @@ class LSApprovedWorkers:",
                "       oprot.writeMapBegin(TType.STRING, TType.I32, len(self.approved_workers))",
                "-      for kiter686,viter687 in self.approved_workers.items():",
                "-        oprot.writeString(kiter686.encode('utf-8'))",
                "-        oprot.writeI32(viter687)",
                "+      for kiter704,viter705 in self.approved_workers.items():",
                "+        oprot.writeString(kiter704.encode('utf-8'))",
                "+        oprot.writeI32(viter705)",
                "       oprot.writeMapEnd()",
                "@@ -10600,8 +10659,8 @@ class LSSupervisorAssignments:",
                "           self.assignments = {}",
                "-          (_ktype689, _vtype690, _size688 ) = iprot.readMapBegin()",
                "-          for _i692 in xrange(_size688):",
                "-            _key693 = iprot.readI32()",
                "-            _val694 = LocalAssignment()",
                "-            _val694.read(iprot)",
                "-            self.assignments[_key693] = _val694",
                "+          (_ktype707, _vtype708, _size706 ) = iprot.readMapBegin()",
                "+          for _i710 in xrange(_size706):",
                "+            _key711 = iprot.readI32()",
                "+            _val712 = LocalAssignment()",
                "+            _val712.read(iprot)",
                "+            self.assignments[_key711] = _val712",
                "           iprot.readMapEnd()",
                "@@ -10622,5 +10681,5 @@ class LSSupervisorAssignments:",
                "       oprot.writeMapBegin(TType.I32, TType.STRUCT, len(self.assignments))",
                "-      for kiter695,viter696 in self.assignments.items():",
                "-        oprot.writeI32(kiter695)",
                "-        viter696.write(oprot)",
                "+      for kiter713,viter714 in self.assignments.items():",
                "+        oprot.writeI32(kiter713)",
                "+        viter714.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -10697,7 +10756,7 @@ class LSWorkerHeartbeat:",
                "           self.executors = []",
                "-          (_etype700, _size697) = iprot.readListBegin()",
                "-          for _i701 in xrange(_size697):",
                "-            _elem702 = ExecutorInfo()",
                "-            _elem702.read(iprot)",
                "-            self.executors.append(_elem702)",
                "+          (_etype718, _size715) = iprot.readListBegin()",
                "+          for _i719 in xrange(_size715):",
                "+            _elem720 = ExecutorInfo()",
                "+            _elem720.read(iprot)",
                "+            self.executors.append(_elem720)",
                "           iprot.readListEnd()",
                "@@ -10731,4 +10790,4 @@ class LSWorkerHeartbeat:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.executors))",
                "-      for iter703 in self.executors:",
                "-        iter703.write(oprot)",
                "+      for iter721 in self.executors:",
                "+        iter721.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -10818,6 +10877,6 @@ class LSTopoHistory:",
                "           self.users = []",
                "-          (_etype707, _size704) = iprot.readListBegin()",
                "-          for _i708 in xrange(_size704):",
                "-            _elem709 = iprot.readString().decode('utf-8')",
                "-            self.users.append(_elem709)",
                "+          (_etype725, _size722) = iprot.readListBegin()",
                "+          for _i726 in xrange(_size722):",
                "+            _elem727 = iprot.readString().decode('utf-8')",
                "+            self.users.append(_elem727)",
                "           iprot.readListEnd()",
                "@@ -10828,6 +10887,6 @@ class LSTopoHistory:",
                "           self.groups = []",
                "-          (_etype713, _size710) = iprot.readListBegin()",
                "-          for _i714 in xrange(_size710):",
                "-            _elem715 = iprot.readString().decode('utf-8')",
                "-            self.groups.append(_elem715)",
                "+          (_etype731, _size728) = iprot.readListBegin()",
                "+          for _i732 in xrange(_size728):",
                "+            _elem733 = iprot.readString().decode('utf-8')",
                "+            self.groups.append(_elem733)",
                "           iprot.readListEnd()",
                "@@ -10856,4 +10915,4 @@ class LSTopoHistory:",
                "       oprot.writeListBegin(TType.STRING, len(self.users))",
                "-      for iter716 in self.users:",
                "-        oprot.writeString(iter716.encode('utf-8'))",
                "+      for iter734 in self.users:",
                "+        oprot.writeString(iter734.encode('utf-8'))",
                "       oprot.writeListEnd()",
                "@@ -10863,4 +10922,4 @@ class LSTopoHistory:",
                "       oprot.writeListBegin(TType.STRING, len(self.groups))",
                "-      for iter717 in self.groups:",
                "-        oprot.writeString(iter717.encode('utf-8'))",
                "+      for iter735 in self.groups:",
                "+        oprot.writeString(iter735.encode('utf-8'))",
                "       oprot.writeListEnd()",
                "@@ -10927,7 +10986,7 @@ class LSTopoHistoryList:",
                "           self.topo_history = []",
                "-          (_etype721, _size718) = iprot.readListBegin()",
                "-          for _i722 in xrange(_size718):",
                "-            _elem723 = LSTopoHistory()",
                "-            _elem723.read(iprot)",
                "-            self.topo_history.append(_elem723)",
                "+          (_etype739, _size736) = iprot.readListBegin()",
                "+          for _i740 in xrange(_size736):",
                "+            _elem741 = LSTopoHistory()",
                "+            _elem741.read(iprot)",
                "+            self.topo_history.append(_elem741)",
                "           iprot.readListEnd()",
                "@@ -10948,4 +11007,4 @@ class LSTopoHistoryList:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.topo_history))",
                "-      for iter724 in self.topo_history:",
                "-        iter724.write(oprot)",
                "+      for iter742 in self.topo_history:",
                "+        iter742.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -11284,8 +11343,8 @@ class LogConfig:",
                "           self.named_logger_level = {}",
                "-          (_ktype726, _vtype727, _size725 ) = iprot.readMapBegin()",
                "-          for _i729 in xrange(_size725):",
                "-            _key730 = iprot.readString().decode('utf-8')",
                "-            _val731 = LogLevel()",
                "-            _val731.read(iprot)",
                "-            self.named_logger_level[_key730] = _val731",
                "+          (_ktype744, _vtype745, _size743 ) = iprot.readMapBegin()",
                "+          for _i747 in xrange(_size743):",
                "+            _key748 = iprot.readString().decode('utf-8')",
                "+            _val749 = LogLevel()",
                "+            _val749.read(iprot)",
                "+            self.named_logger_level[_key748] = _val749",
                "           iprot.readMapEnd()",
                "@@ -11306,5 +11365,5 @@ class LogConfig:",
                "       oprot.writeMapBegin(TType.STRING, TType.STRUCT, len(self.named_logger_level))",
                "-      for kiter732,viter733 in self.named_logger_level.items():",
                "-        oprot.writeString(kiter732.encode('utf-8'))",
                "-        viter733.write(oprot)",
                "+      for kiter750,viter751 in self.named_logger_level.items():",
                "+        oprot.writeString(kiter750.encode('utf-8'))",
                "+        viter751.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -11360,6 +11419,6 @@ class TopologyHistoryInfo:",
                "           self.topo_ids = []",
                "-          (_etype737, _size734) = iprot.readListBegin()",
                "-          for _i738 in xrange(_size734):",
                "-            _elem739 = iprot.readString().decode('utf-8')",
                "-            self.topo_ids.append(_elem739)",
                "+          (_etype755, _size752) = iprot.readListBegin()",
                "+          for _i756 in xrange(_size752):",
                "+            _elem757 = iprot.readString().decode('utf-8')",
                "+            self.topo_ids.append(_elem757)",
                "           iprot.readListEnd()",
                "@@ -11380,4 +11439,4 @@ class TopologyHistoryInfo:",
                "       oprot.writeListBegin(TType.STRING, len(self.topo_ids))",
                "-      for iter740 in self.topo_ids:",
                "-        oprot.writeString(iter740.encode('utf-8'))",
                "+      for iter758 in self.topo_ids:",
                "+        oprot.writeString(iter758.encode('utf-8'))",
                "       oprot.writeListEnd()",
                "@@ -11966,7 +12025,7 @@ class HBRecords:",
                "           self.pulses = []",
                "-          (_etype744, _size741) = iprot.readListBegin()",
                "-          for _i745 in xrange(_size741):",
                "-            _elem746 = HBPulse()",
                "-            _elem746.read(iprot)",
                "-            self.pulses.append(_elem746)",
                "+          (_etype762, _size759) = iprot.readListBegin()",
                "+          for _i763 in xrange(_size759):",
                "+            _elem764 = HBPulse()",
                "+            _elem764.read(iprot)",
                "+            self.pulses.append(_elem764)",
                "           iprot.readListEnd()",
                "@@ -11987,4 +12046,4 @@ class HBRecords:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.pulses))",
                "-      for iter747 in self.pulses:",
                "-        iter747.write(oprot)",
                "+      for iter765 in self.pulses:",
                "+        iter765.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -12040,6 +12099,6 @@ class HBNodes:",
                "           self.pulseIds = []",
                "-          (_etype751, _size748) = iprot.readListBegin()",
                "-          for _i752 in xrange(_size748):",
                "-            _elem753 = iprot.readString().decode('utf-8')",
                "-            self.pulseIds.append(_elem753)",
                "+          (_etype769, _size766) = iprot.readListBegin()",
                "+          for _i770 in xrange(_size766):",
                "+            _elem771 = iprot.readString().decode('utf-8')",
                "+            self.pulseIds.append(_elem771)",
                "           iprot.readListEnd()",
                "@@ -12060,4 +12119,4 @@ class HBNodes:",
                "       oprot.writeListBegin(TType.STRING, len(self.pulseIds))",
                "-      for iter754 in self.pulseIds:",
                "-        oprot.writeString(iter754.encode('utf-8'))",
                "+      for iter772 in self.pulseIds:",
                "+        oprot.writeString(iter772.encode('utf-8'))",
                "       oprot.writeListEnd()",
                "diff --git a/storm-client/src/storm.thrift b/storm-client/src/storm.thrift",
                "index 11879d126..c21bcaaa2 100644",
                "--- a/storm-client/src/storm.thrift",
                "+++ b/storm-client/src/storm.thrift",
                "@@ -418,2 +418,6 @@ struct RebalanceOptions {",
                "   3: optional map<string, i32> num_executors;",
                "+  4: optional map<string, map<string, double>> topology_resources_overrides;",
                "+  5: optional string topology_conf_overrides;",
                "+  //This value is not intended to be explicitly set by end users and will be ignored if they do",
                "+  6: optional string principal",
                " }",
                "diff --git a/storm-core/src/jvm/org/apache/storm/command/Rebalance.java b/storm-core/src/jvm/org/apache/storm/command/Rebalance.java",
                "index 343658c10..ec34798f0 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/command/Rebalance.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/command/Rebalance.java",
                "@@ -23,2 +23,4 @@ import org.apache.storm.generated.RebalanceOptions;",
                " import org.apache.storm.utils.NimbusClient;",
                "+import org.apache.storm.utils.Utils;",
                "+import org.json.simple.JSONValue;",
                " import org.slf4j.Logger;",
                "@@ -39,2 +41,4 @@ public class Rebalance {",
                "             .opt(\"e\", \"executor\", null, new ExecutorParser(), CLI.INTO_MAP)",
                "+            .opt(\"r\", \"resources\", null, new ResourcesParser(), CLI.INTO_MAP)",
                "+            .opt(\"t\", \"topology-conf\", null, new ConfParser(), CLI.INTO_MAP)",
                "             .arg(\"topologyName\", CLI.FIRST_WINS)",
                "@@ -46,2 +50,4 @@ public class Rebalance {",
                "         Map<String, Integer> numExecutors = (Map<String, Integer>) cl.get(\"e\");",
                "+        Map<String, Map<String, Double>> resourceOverrides = (Map<String, Map<String, Double>>) cl.get(\"r\");",
                "+        Map<String, Object> confOverrides = (Map<String, Object>) cl.get(\"t\");",
                "@@ -57,2 +63,10 @@ public class Rebalance {",
                "+        if (null != resourceOverrides) {",
                "+            rebalanceOptions.set_topology_resources_overrides(resourceOverrides);",
                "+        }",
                "+",
                "+        if (null != confOverrides) {",
                "+            rebalanceOptions.set_topology_conf_overrides(JSONValue.toJSONString(confOverrides));",
                "+        }",
                "+",
                "         NimbusClient.withConfiguredClient(new NimbusClient.WithNimbus() {",
                "@@ -66,5 +80,45 @@ public class Rebalance {",
                "+    static final class ConfParser implements CLI.Parse {",
                "+        @Override",
                "+        public Object parse(String value) {",
                "+            if (value == null) {",
                "+                throw new RuntimeException(\"No arguments found for topology config override!\");",
                "+            }",
                "+            try {",
                "+                return Utils.parseJson(value);",
                "+            } catch (Exception e) {",
                "+                throw new RuntimeException(\"Error trying to parse topology config override\", e);",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    static final class ResourcesParser implements CLI.Parse {",
                "+        @Override",
                "+        public Object parse(String value) {",
                "+            if (value == null) {",
                "+                throw new RuntimeException(\"No arguments found for topology resources override!\");",
                "+            }",
                "+            try {",
                "+                //This is a bit ugly The JSON we are expecting should be in the form",
                "+                // {\"component\": {\"resource\": value, ...}, ...}",
                "+                // But because value is coming from JSON it is going to be a Number, and we want it to be a Double.",
                "+                // So the goal is to go through each entry and update it accordingly",
                "+                Map<String, Map<String, Double>> ret = new HashMap<>();",
                "+                for (Map.Entry<String, Object> compEntry: Utils.parseJson(value).entrySet()) {",
                "+                    String comp = compEntry.getKey();",
                "+                    Map<String, Number> numResources = (Map<String, Number>) compEntry.getValue();",
                "+                    Map<String, Double> doubleResource = new HashMap<>();",
                "+                    for (Map.Entry<String, Number> entry: numResources.entrySet()) {",
                "+                        doubleResource.put(entry.getKey(), entry.getValue().doubleValue());",
                "+                    }",
                "+                    ret.put(comp, doubleResource);",
                "+                }",
                "+                return ret;",
                "+            } catch (Exception e) {",
                "+                throw new RuntimeException(\"Error trying to parse resource override\", e);",
                "+            }",
                "+        }",
                "+    }",
                "     static final class ExecutorParser implements CLI.Parse {",
                "-",
                "         @Override",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index e54aaeb2f..d050f94d5 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -1201,2 +1201,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "         stormClusterState.updateStorm(topoId, updated);",
                "+        updateBlobStore(topoId, rbo, ServerUtils.principalNameToSubject(rbo.get_principal()));",
                "         mkAssignments(topoId);",
                "@@ -1277,2 +1278,29 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "     }",
                "+",
                "+    private void updateTopologyResources(String topoId, Map<String, Map<String, Double>> resourceOverrides, Subject subject)",
                "+        throws AuthorizationException, IOException, KeyNotFoundException {",
                "+        StormTopology topo = topoCache.readTopology(topoId, subject);",
                "+        topo = topo.deepCopy();",
                "+        ResourceUtils.updateStormTopologyResources(topo, resourceOverrides);",
                "+        topoCache.updateTopology(topoId, subject, topo);",
                "+    }",
                "+",
                "+    private void updateTopologyConf(String topoId, Map<String, Object> configOverride, Subject subject)",
                "+        throws AuthorizationException, IOException, KeyNotFoundException {",
                "+        Map<String, Object> topoConf = new HashMap<>(topoCache.readTopoConf(topoId, subject)); //Copy the data",
                "+        topoConf.putAll(configOverride);",
                "+        topoCache.updateTopoConf(topoId, subject,  topoConf);",
                "+    }",
                "+",
                "+    private void updateBlobStore(String topoId, RebalanceOptions rbo, Subject subject)",
                "+        throws AuthorizationException, IOException, KeyNotFoundException {",
                "+        Map<String, Map<String, Double>> resourceOverrides = rbo.get_topology_resources_overrides();",
                "+        if (resourceOverrides != null && !resourceOverrides.isEmpty()) {",
                "+            updateTopologyResources(topoId, resourceOverrides, subject);",
                "+        }",
                "+        String confOverride = rbo.get_topology_conf_overrides();",
                "+        if (confOverride != null && !confOverride.isEmpty()) {",
                "+            updateTopologyConf(topoId, Utils.parseJson(confOverride), subject);",
                "+        }",
                "+    }",
                "@@ -2765,2 +2793,4 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "             checkAuthorization(topoName, topoConf, operation);",
                "+            // Set principal in RebalanceOptions to nil because users are not suppose to set this",
                "+            options.set_principal(null);",
                "             Map<String, Integer> execOverrides = options.is_set_num_executors() ? options.get_num_executors() : Collections.emptyMap();",
                "@@ -2771,2 +2801,20 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "             }",
                "+            if (options.is_set_topology_conf_overrides()) {",
                "+                Map<String, Object> topoConfigOverrides = Utils.parseJson(options.get_topology_conf_overrides());",
                "+                //Clean up some things the user should not set.  (Not a security issue, just might confuse the topology)",
                "+                topoConfigOverrides.remove(Config.TOPOLOGY_SUBMITTER_PRINCIPAL);",
                "+                topoConfigOverrides.remove(Config.TOPOLOGY_SUBMITTER_USER);",
                "+                topoConfigOverrides.remove(Config.STORM_ZOOKEEPER_SUPERACL);",
                "+                topoConfigOverrides.remove(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME);",
                "+                topoConfigOverrides.remove(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);",
                "+                if ((boolean) conf.getOrDefault(DaemonConfig.STORM_TOPOLOGY_CLASSPATH_BEGINNING_ENABLED, false)) {",
                "+                    topoConfigOverrides.remove(Config.TOPOLOGY_CLASSPATH_BEGINNING);",
                "+                }",
                "+                options.set_topology_conf_overrides(JSONValue.toJSONString(topoConfigOverrides));",
                "+            }",
                "+            Subject subject = getSubject();",
                "+            if (subject != null) {",
                "+                options.set_principal(subject.getPrincipals().iterator().next().getName());",
                "+            }",
                "+",
                "             transitionName(topoName, TopologyActions.REBALANCE, options, true);",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "index d68e51227..6e92a9cb9 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "@@ -102,9 +102,2 @@ public class ReadClusterState implements Runnable, AutoCloseable {",
                "         }",
                "-",
                "-        //All the slots/assignments should be recovered now, so we can clean up anything that we don't expect to be here",
                "-        try {",
                "-            localizer.cleanupUnusedTopologies();",
                "-        } catch (Exception e) {",
                "-            LOG.warn(\"Error trying to clean up old topologies\", e);",
                "-        }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "index 6533d15fb..da6292534 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "@@ -21,3 +21,3 @@ package org.apache.storm.daemon.supervisor;",
                " import com.codahale.metrics.Meter;",
                "-",
                "+import com.google.common.annotations.VisibleForTesting;",
                " import java.io.IOException;",
                "@@ -29,4 +29,6 @@ import java.util.Map;",
                " import java.util.Set;",
                "+import java.util.concurrent.BlockingQueue;",
                " import java.util.concurrent.ExecutionException;",
                " import java.util.concurrent.Future;",
                "+import java.util.concurrent.LinkedBlockingQueue;",
                " import java.util.concurrent.TimeUnit;",
                "@@ -34,3 +36,2 @@ import java.util.concurrent.TimeoutException;",
                " import java.util.concurrent.atomic.AtomicReference;",
                "-",
                " import org.apache.storm.Config;",
                "@@ -46,2 +47,5 @@ import org.apache.storm.generated.ProfileRequest;",
                " import org.apache.storm.localizer.AsyncLocalizer;",
                "+import org.apache.storm.localizer.BlobChangingCallback;",
                "+import org.apache.storm.localizer.GoodToGo;",
                "+import org.apache.storm.localizer.LocallyCachedBlob;",
                " import org.apache.storm.metric.StormMetricsRegistry;",
                "@@ -55,3 +59,3 @@ import org.slf4j.LoggerFactory;",
                "-public class Slot extends Thread implements AutoCloseable {",
                "+public class Slot extends Thread implements AutoCloseable, BlobChangingCallback {",
                "     private static final Logger LOG = LoggerFactory.getLogger(Slot.class);",
                "@@ -76,4 +80,5 @@ public class Slot extends Thread implements AutoCloseable {",
                "         KILL,",
                "-        WAITING_FOR_BASIC_LOCALIZATION,",
                "-        WAITING_FOR_BLOB_LOCALIZATION;",
                "+        KILL_BLOB_UPDATE,",
                "+        WAITING_FOR_BLOB_LOCALIZATION,",
                "+        WAITING_FOR_BLOB_UPDATE;",
                "     }",
                "@@ -91,7 +96,9 @@ public class Slot extends Thread implements AutoCloseable {",
                "         public final LocalState localState;",
                "+        public final BlobChangingCallback changingCallback;",
                "         StaticState(AsyncLocalizer localizer, long hbTimeoutMs, long firstHbTimeoutMs,",
                "-                    long killSleepMs, long monitorFreqMs,",
                "-                    ContainerLauncher containerLauncher, String host, int port,",
                "-                    ISupervisor iSupervisor, LocalState localState) {",
                "+                long killSleepMs, long monitorFreqMs,",
                "+                ContainerLauncher containerLauncher, String host, int port,",
                "+                ISupervisor iSupervisor, LocalState localState,",
                "+                BlobChangingCallback changingCallback) {",
                "             this.localizer = localizer;",
                "@@ -106,5 +113,8 @@ public class Slot extends Thread implements AutoCloseable {",
                "             this.localState = localState;",
                "+            this.changingCallback = changingCallback;",
                "         }",
                "     }",
                "-    ",
                "+",
                "+    //TODO go through all of the state transitions and make sure we handle changingBlobs",
                "+    //TODO make sure to add in transition helpers that clean changingBlobs && pendingChangeingBlobs for not the current topology",
                "     static class DynamicState {",
                "@@ -118,2 +128,5 @@ public class Slot extends Thread implements AutoCloseable {",
                "         public final Set<TopoProfileAction> pendingStopProfileActions;",
                "+        public final Set<BlobChangeing> changingBlobs;",
                "+        public final LocalAssignment pendingChangingBlobsAssignment;",
                "+        public final Set<Future<Void>> pendingChangeingBlobs;",
                "@@ -141,11 +154,18 @@ public class Slot extends Thread implements AutoCloseable {",
                "             this.pendingDownload = null;",
                "-            this.profileActions = new HashSet<>();",
                "-            this.pendingStopProfileActions = new HashSet<>();",
                "+            this.profileActions = Collections.emptySet();",
                "+            this.pendingStopProfileActions = Collections.emptySet();",
                "+            this.changingBlobs = Collections.emptySet();",
                "+            this.pendingChangingBlobsAssignment = null;",
                "+            this.pendingChangeingBlobs = Collections.emptySet();",
                "         }",
                "-        ",
                "+",
                "         public DynamicState(final MachineState state, final LocalAssignment newAssignment,",
                "-                final Container container, final LocalAssignment currentAssignment,",
                "-                final LocalAssignment pendingLocalization, final long startTime,",
                "-                final Future<Void> pendingDownload, final Set<TopoProfileAction> profileActions, ",
                "-                final Set<TopoProfileAction> pendingStopProfileActions) {",
                "+                            final Container container, final LocalAssignment currentAssignment,",
                "+                            final LocalAssignment pendingLocalization, final long startTime,",
                "+                            final Future<Void> pendingDownload, final Set<TopoProfileAction> profileActions,",
                "+                            final Set<TopoProfileAction> pendingStopProfileActions,",
                "+                            final Set<BlobChangeing> changingBlobs,",
                "+                            final Set<Future<Void>> pendingChangingBlobs, final LocalAssignment pendingChaningBlobsAssignment) {",
                "+            assert pendingChangingBlobs != null;",
                "+            assert !(pendingChangingBlobs.isEmpty() ^ (pendingChaningBlobsAssignment == null));",
                "             this.state = state;",
                "@@ -159,2 +179,5 @@ public class Slot extends Thread implements AutoCloseable {",
                "             this.pendingStopProfileActions = pendingStopProfileActions;",
                "+            this.changingBlobs = changingBlobs;",
                "+            this.pendingChangeingBlobs = pendingChangingBlobs;",
                "+            this.pendingChangingBlobsAssignment = pendingChaningBlobsAssignment;",
                "         }",
                "@@ -181,14 +204,16 @@ public class Slot extends Thread implements AutoCloseable {",
                "             return new DynamicState(this.state, newAssignment,",
                "-                    this.container, this.currentAssignment,",
                "-                    this.pendingLocalization, this.startTime,",
                "-                    this.pendingDownload, this.profileActions,",
                "-                    this.pendingStopProfileActions);",
                "+                this.container, this.currentAssignment,",
                "+                this.pendingLocalization, this.startTime,",
                "+                this.pendingDownload, this.profileActions,",
                "+                this.pendingStopProfileActions, this.changingBlobs,",
                "+                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "         }",
                "-        ",
                "+",
                "         public DynamicState withPendingLocalization(LocalAssignment pendingLocalization, Future<Void> pendingDownload) {",
                "             return new DynamicState(this.state, this.newAssignment,",
                "-                    this.container, this.currentAssignment,",
                "-                    pendingLocalization, this.startTime,",
                "-                    pendingDownload, this.profileActions,",
                "-                    this.pendingStopProfileActions);",
                "+                this.container, this.currentAssignment,",
                "+                pendingLocalization, this.startTime,",
                "+                pendingDownload, this.profileActions,",
                "+                this.pendingStopProfileActions, this.changingBlobs,",
                "+                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "         }",
                "@@ -198,3 +223,3 @@ public class Slot extends Thread implements AutoCloseable {",
                "         }",
                "-        ",
                "+",
                "         public DynamicState withState(final MachineState state) {",
                "@@ -202,6 +227,7 @@ public class Slot extends Thread implements AutoCloseable {",
                "             return new DynamicState(state, this.newAssignment,",
                "-                    this.container, this.currentAssignment,",
                "-                    this.pendingLocalization, newStartTime,",
                "-                    this.pendingDownload, this.profileActions,",
                "-                    this.pendingStopProfileActions);",
                "+                this.container, this.currentAssignment,",
                "+                this.pendingLocalization, newStartTime,",
                "+                this.pendingDownload, this.profileActions,",
                "+                this.pendingStopProfileActions, this.changingBlobs,",
                "+                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "         }",
                "@@ -210,6 +236,7 @@ public class Slot extends Thread implements AutoCloseable {",
                "             return new DynamicState(this.state, this.newAssignment,",
                "-                    container, currentAssignment,",
                "-                    this.pendingLocalization, this.startTime,",
                "-                    this.pendingDownload, this.profileActions,",
                "-                    this.pendingStopProfileActions);",
                "+                container, currentAssignment,",
                "+                this.pendingLocalization, this.startTime,",
                "+                this.pendingDownload, this.profileActions,",
                "+                this.pendingStopProfileActions, this.changingBlobs,",
                "+                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "         }",
                "@@ -218,6 +245,30 @@ public class Slot extends Thread implements AutoCloseable {",
                "             return new DynamicState(this.state, this.newAssignment,",
                "-                    this.container, this.currentAssignment,",
                "-                    this.pendingLocalization, this.startTime,",
                "-                    this.pendingDownload, profileActions,",
                "-                    pendingStopProfileActions);",
                "+                this.container, this.currentAssignment,",
                "+                this.pendingLocalization, this.startTime,",
                "+                this.pendingDownload, profileActions,",
                "+                pendingStopProfileActions, this.changingBlobs,",
                "+                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "+        }",
                "+",
                "+        public DynamicState withChangingBlobs(Set<BlobChangeing> changingBlobs) {",
                "+            if (changingBlobs == this.changingBlobs) {",
                "+                return this;",
                "+            }",
                "+            return new DynamicState(this.state, this.newAssignment,",
                "+                this.container, this.currentAssignment,",
                "+                this.pendingLocalization, this.startTime,",
                "+                this.pendingDownload, profileActions,",
                "+                this.pendingStopProfileActions, changingBlobs,",
                "+                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "+        }",
                "+",
                "+        public DynamicState withPendingChangeingBlobs(Set<Future<Void>> pendingChangeingBlobs,",
                "+                                                      LocalAssignment pendingChangeingBlobsAssignment) {",
                "+            return new DynamicState(this.state, this.newAssignment,",
                "+                this.container, this.currentAssignment,",
                "+                this.pendingLocalization, this.startTime,",
                "+                this.pendingDownload, profileActions,",
                "+                this.pendingStopProfileActions, this.changingBlobs,",
                "+                pendingChangeingBlobs,",
                "+                pendingChangeingBlobsAssignment);",
                "         }",
                "@@ -265,3 +316,37 @@ public class Slot extends Thread implements AutoCloseable {",
                "     }",
                "-    ",
                "+",
                "+    /**",
                "+     * Holds the information about a blob that is changing.",
                "+     */",
                "+    static class BlobChangeing {",
                "+        private final LocalAssignment assignment;",
                "+        private final LocallyCachedBlob blob;",
                "+        private final GoodToGo.GoodToGoLatch latch;",
                "+",
                "+        public BlobChangeing(LocalAssignment assignment, LocallyCachedBlob blob, GoodToGo.GoodToGoLatch latch) {",
                "+            this.assignment = assignment;",
                "+            this.blob = blob;",
                "+            this.latch = latch;",
                "+        }",
                "+",
                "+        @Override",
                "+        public String toString() {",
                "+            return \"BLOB CHANGING \" + blob + \" \" + assignment;",
                "+        }",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    static boolean forSameTopology(LocalAssignment a, LocalAssignment b) {",
                "+        if (a == null && b == null) {",
                "+            return true;",
                "+        }",
                "+        if (a != null && b != null) {",
                "+            if (a.get_topology_id().equals(b.get_topology_id())) {",
                "+                return true;",
                "+            }",
                "+        }",
                "+        return false;",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "     static boolean equivalent(LocalAssignment a, LocalAssignment b) {",
                "@@ -300,2 +385,4 @@ public class Slot extends Thread implements AutoCloseable {",
                "                 return handleWaitingForWorkerStart(dynamicState, staticState);",
                "+            case KILL_BLOB_UPDATE:",
                "+                return handleKillBlobUpdate(dynamicState, staticState);",
                "             case KILL_AND_RELAUNCH:",
                "@@ -304,6 +391,6 @@ public class Slot extends Thread implements AutoCloseable {",
                "                 return handleKill(dynamicState, staticState);",
                "-            case WAITING_FOR_BASIC_LOCALIZATION:",
                "-                return handleWaitingForBasicLocalization(dynamicState, staticState);",
                "             case WAITING_FOR_BLOB_LOCALIZATION:",
                "                 return handleWaitingForBlobLocalization(dynamicState, staticState);",
                "+            case WAITING_FOR_BLOB_UPDATE:",
                "+                return handleWaitingForBlobUpdate(dynamicState, staticState);",
                "             default:",
                "@@ -327,4 +414,6 @@ public class Slot extends Thread implements AutoCloseable {",
                "         }",
                "-        Future<Void> pendingDownload = staticState.localizer.requestDownloadBaseTopologyBlobs(dynamicState.newAssignment, staticState.port);",
                "-        return dynamicState.withPendingLocalization(dynamicState.newAssignment, pendingDownload).withState(MachineState.WAITING_FOR_BASIC_LOCALIZATION);",
                "+        Future<Void> pendingDownload = staticState.localizer.requestDownloadTopologyBlobs(dynamicState.newAssignment,",
                "+            staticState.port, staticState.changingCallback);",
                "+        return dynamicState.withPendingLocalization(dynamicState.newAssignment, pendingDownload)",
                "+            .withState(MachineState.WAITING_FOR_BLOB_LOCALIZATION);",
                "     }",
                "@@ -332,3 +421,3 @@ public class Slot extends Thread implements AutoCloseable {",
                "     /**",
                "-     * Kill the current container and start downloading what the new assignment needs, if there is a new assignment",
                "+     * Kill the current container and start downloading what the new assignment needs, if there is a new assignment.",
                "      * PRECONDITION: container != null",
                "@@ -341,3 +430,3 @@ public class Slot extends Thread implements AutoCloseable {",
                "         assert(dynamicState.container != null);",
                "-        ",
                "+",
                "         staticState.iSupervisor.killedWorker(staticState.port);",
                "@@ -346,4 +435,6 @@ public class Slot extends Thread implements AutoCloseable {",
                "         if (dynamicState.newAssignment != null) {",
                "-            pendingDownload = staticState.localizer.requestDownloadBaseTopologyBlobs(dynamicState.newAssignment, staticState.port);",
                "+            pendingDownload = staticState.localizer.requestDownloadTopologyBlobs(dynamicState.newAssignment, staticState.port,",
                "+                staticState.changingCallback);",
                "         }",
                "+        dynamicState = drainAllChangingBlobs(dynamicState);",
                "         Time.sleep(staticState.killSleepMs);",
                "@@ -351,3 +442,20 @@ public class Slot extends Thread implements AutoCloseable {",
                "     }",
                "-    ",
                "+",
                "+    /**",
                "+     * Kill the current container, and wait go to the state to inform the localizer that we are ready to go.",
                "+     * PRECONDITION: container != null",
                "+     * @param dynamicState current state",
                "+     * @param staticState static data",
                "+     * @return the next state",
                "+     */",
                "+    private static DynamicState killContainerForChangedBlobs(DynamicState dynamicState, StaticState staticState) throws Exception {",
                "+        assert(dynamicState.container != null);",
                "+",
                "+        staticState.iSupervisor.killedWorker(staticState.port);",
                "+        dynamicState.container.kill();",
                "+",
                "+        Time.sleep(staticState.killSleepMs);",
                "+        return dynamicState.withState(MachineState.KILL_BLOB_UPDATE);",
                "+    }",
                "+",
                "     /**",
                "@@ -369,3 +477,3 @@ public class Slot extends Thread implements AutoCloseable {",
                "         mod.addAll(dynamicState.pendingStopProfileActions);",
                "-        return dynamicState.withState(MachineState.KILL_AND_RELAUNCH).withProfileActions(mod, Collections.<TopoProfileAction> emptySet());",
                "+        return dynamicState.withState(MachineState.KILL_AND_RELAUNCH).withProfileActions(mod, Collections.emptySet());",
                "     }",
                "@@ -373,3 +481,3 @@ public class Slot extends Thread implements AutoCloseable {",
                "     /**",
                "-     * Clean up a container",
                "+     * Clean up a container.",
                "      * PRECONDITION: All of the processes have died.",
                "@@ -393,3 +501,75 @@ public class Slot extends Thread implements AutoCloseable {",
                "     }",
                "-    ",
                "+",
                "+    /**",
                "+     * Drop all of the changingBlobs and pendingChangeingBlobs.",
                "+     * @param dynamicState current state.",
                "+     * @return the next state.",
                "+     */",
                "+    private static DynamicState drainAllChangingBlobs(DynamicState dynamicState) {",
                "+        if (!dynamicState.changingBlobs.isEmpty()) {",
                "+            for (BlobChangeing rc : dynamicState.changingBlobs) {",
                "+                rc.latch.countDown();",
                "+            }",
                "+            dynamicState = dynamicState.withChangingBlobs(Collections.emptySet());",
                "+        }",
                "+",
                "+        if (!dynamicState.pendingChangeingBlobs.isEmpty()) {",
                "+            dynamicState = dynamicState.withPendingChangeingBlobs(Collections.emptySet(), null);",
                "+        }",
                "+",
                "+        return dynamicState;",
                "+    }",
                "+",
                "+    /**",
                "+     * Informs the async localizer for all of blobs that the worker is dead.",
                "+     *",
                "+     * PRECONDITION: container is null",
                "+     * PRECONDITION: changingBlobs should only be for the given assignment.",
                "+     * @param dynamicState the current state",
                "+     * @return the futures for the current assignment.",
                "+     */",
                "+    private static DynamicState informChangedBlobs(DynamicState dynamicState, LocalAssignment assignment) {",
                "+        assert dynamicState.container == null;",
                "+        assert dynamicState.changingBlobs.stream().allMatch((cr) -> forSameTopology(cr.assignment, assignment));",
                "+",
                "+        Set<Future<Void>> futures = new HashSet<>(dynamicState.changingBlobs.size());",
                "+        if (forSameTopology(dynamicState.pendingChangingBlobsAssignment, assignment)) {",
                "+            //We need to add the new futures to the existing ones",
                "+            futures.addAll(dynamicState.pendingChangeingBlobs);",
                "+        }",
                "+        //Otherwise they will just be replaced",
                "+",
                "+        for (BlobChangeing rc: dynamicState.changingBlobs) {",
                "+            futures.add(rc.latch.countDown());",
                "+        }",
                "+",
                "+        LOG.debug(\"found changeing blobs {} moving them to pending...\", dynamicState.changingBlobs);",
                "+        return dynamicState.withChangingBlobs(Collections.emptySet())",
                "+            .withPendingChangeingBlobs(futures, assignment);",
                "+    }",
                "+",
                "+    /**",
                "+     * Filter all of the changing blobs to just be for those compatible with the given assignment.",
                "+     * All others will be released appropriately.",
                "+     *",
                "+     * @param dynamicState the current state",
                "+     * @param assignment the assignment to look for",
                "+     * @return the updated dynamicState",
                "+     */",
                "+    private static DynamicState filterChangingBlobsFor(DynamicState dynamicState, final LocalAssignment assignment) {",
                "+        if (!dynamicState.changingBlobs.isEmpty()) {",
                "+            return dynamicState;",
                "+        }",
                "+",
                "+        HashSet<BlobChangeing> savedBlobs = new HashSet<>(dynamicState.changingBlobs.size());",
                "+        for (BlobChangeing rc: dynamicState.changingBlobs) {",
                "+            if (forSameTopology(assignment, rc.assignment)) {",
                "+                savedBlobs.add(rc);",
                "+            } else {",
                "+                rc.latch.countDown();",
                "+            }",
                "+        }",
                "+        return dynamicState.withChangingBlobs(savedBlobs);",
                "+    }",
                "+",
                "     /**",
                "@@ -412,2 +592,9 @@ public class Slot extends Thread implements AutoCloseable {",
                "         try {",
                "+            //Release things that don't need to wait for us to finish downloading.",
                "+            dynamicState = filterChangingBlobsFor(dynamicState, dynamicState.pendingLocalization);",
                "+            if (!dynamicState.changingBlobs.isEmpty()) {",
                "+                //Unblock downloading by accepting the futures.",
                "+                dynamicState = informChangedBlobs(dynamicState, dynamicState.pendingLocalization);",
                "+            }",
                "+",
                "             dynamicState.pendingDownload.get(1000, TimeUnit.MILLISECONDS);",
                "@@ -417,4 +604,13 @@ public class Slot extends Thread implements AutoCloseable {",
                "                 staticState.localizer.releaseSlotFor(dynamicState.pendingLocalization, staticState.port);",
                "-                return prepareForNewAssignmentNoWorkersRunning(dynamicState, staticState);",
                "+                return prepareForNewAssignmentNoWorkersRunning(dynamicState.withPendingChangeingBlobs(Collections.emptySet(), null),",
                "+                    staticState);",
                "+            }",
                "+",
                "+            if (!dynamicState.pendingChangeingBlobs.isEmpty()) {",
                "+                LOG.info(\"There are pending changes, waiting for them to finish before launching container...\");",
                "+                //We cannot launch the container yet the resources may still be updating",
                "+                return dynamicState.withState(MachineState.WAITING_FOR_BLOB_UPDATE)",
                "+                    .withPendingLocalization(null, null);",
                "             }",
                "+",
                "             dynamicState = updateAssignmentIfNeeded(dynamicState);",
                "@@ -422,3 +618,5 @@ public class Slot extends Thread implements AutoCloseable {",
                "             Container c = staticState.containerLauncher.launchContainer(staticState.port, dynamicState.pendingLocalization, staticState.localState);",
                "-            return dynamicState.withCurrentAssignment(c, dynamicState.pendingLocalization).withState(MachineState.WAITING_FOR_WORKER_START).withPendingLocalization(null, null);",
                "+            return dynamicState",
                "+                .withCurrentAssignment(c, dynamicState.pendingLocalization).withState(MachineState.WAITING_FOR_WORKER_START)",
                "+                .withPendingLocalization(null, null);",
                "         } catch (TimeoutException e) {",
                "@@ -441,7 +639,12 @@ public class Slot extends Thread implements AutoCloseable {",
                "     }",
                "-    ",
                "+",
                "+    private static final long ONE_SEC_IN_NANO = TimeUnit.NANOSECONDS.convert(1, TimeUnit.SECONDS);",
                "+",
                "     /**",
                "-     * State Transitions for WAITING_FOR_BASIC_LOCALIZATION state.",
                "-     * PRECONDITION: neither pendingLocalization nor pendingDownload is null.",
                "-     * PRECONDITION: The slot should be empty",
                "+     * State Transitions for WAITING_FOR_BLOB_UPDATE state.",
                "+     *",
                "+     * PRECONDITION: container is null",
                "+     * PRECONDITION: pendingChangeingBlobs is not empty (otherwise why did we go to this state)",
                "+     * PRECONDITION: pendingChangingBlobsAssignment is not null.",
                "+     *",
                "      * @param dynamicState current state",
                "@@ -451,15 +654,45 @@ public class Slot extends Thread implements AutoCloseable {",
                "      */",
                "-    static DynamicState handleWaitingForBasicLocalization(DynamicState dynamicState, StaticState staticState) throws Exception {",
                "-        assert(dynamicState.pendingLocalization != null);",
                "-        assert(dynamicState.pendingDownload != null);",
                "-        assert(dynamicState.container == null);",
                "-        ",
                "-        //Ignore changes to scheduling while downloading the topology code",
                "-        // We don't support canceling the download through the future yet,",
                "-        // so to keep everything in sync, just wait",
                "+    private static DynamicState handleWaitingForBlobUpdate(DynamicState dynamicState, StaticState staticState)",
                "+        throws Exception {",
                "+        assert dynamicState.container == null;",
                "+        assert dynamicState.pendingChangingBlobsAssignment != null;",
                "+        assert !dynamicState.pendingChangeingBlobs.isEmpty();",
                "+",
                "+        if (!equivalent(dynamicState.newAssignment, dynamicState.currentAssignment)) {",
                "+            //We were rescheduled while waiting for the resources to be updated,",
                "+            // but the container is already not running.",
                "+            LOG.info(\"SLOT {}: Assignment Changed from {} to {}\", staticState.port,",
                "+                dynamicState.currentAssignment, dynamicState.newAssignment);",
                "+            if (dynamicState.currentAssignment != null) {",
                "+                staticState.localizer.releaseSlotFor(dynamicState.currentAssignment, staticState.port);",
                "+            }",
                "+            staticState.localizer.releaseSlotFor(dynamicState.pendingChangingBlobsAssignment, staticState.port);",
                "+            return prepareForNewAssignmentNoWorkersRunning(dynamicState.withCurrentAssignment(null, null)",
                "+                    .withPendingChangeingBlobs(Collections.emptySet(), null),",
                "+                staticState);",
                "+        }",
                "+",
                "+        dynamicState = filterChangingBlobsFor(dynamicState, dynamicState.pendingChangingBlobsAssignment);",
                "+        if (!dynamicState.changingBlobs.isEmpty()) {",
                "+            dynamicState = informChangedBlobs(dynamicState, dynamicState.pendingChangingBlobsAssignment);",
                "+        }",
                "+",
                "+        //We only have a set amount of time we can wait for before looping around again",
                "+        long start = Time.nanoTime();",
                "         try {",
                "-            dynamicState.pendingDownload.get(1000, TimeUnit.MILLISECONDS);",
                "-            Future<Void> pendingDownload = staticState.localizer.requestDownloadTopologyBlobs(dynamicState.pendingLocalization, staticState.port);",
                "-            return dynamicState.withPendingLocalization(pendingDownload).withState(MachineState.WAITING_FOR_BLOB_LOCALIZATION);",
                "-        } catch (TimeoutException e) {",
                "+            for (Future<Void> pending: dynamicState.pendingChangeingBlobs) {",
                "+                long now = Time.nanoTime();",
                "+                long timeLeft = ONE_SEC_IN_NANO - (now - start);",
                "+                if (timeLeft <= 0) {",
                "+                    throw new TimeoutException();",
                "+                }",
                "+                pending.get(timeLeft, TimeUnit.NANOSECONDS);",
                "+            }",
                "+            //All done we can launch the worker now",
                "+            Container c = staticState.containerLauncher.launchContainer(staticState.port, dynamicState.pendingChangingBlobsAssignment,",
                "+                staticState.localState);",
                "+            return dynamicState",
                "+                .withCurrentAssignment(c, dynamicState.pendingChangingBlobsAssignment).withState(MachineState.WAITING_FOR_WORKER_START)",
                "+                .withPendingChangeingBlobs(Collections.emptySet(), null);",
                "+        } catch (TimeoutException ex) {",
                "             return dynamicState;",
                "@@ -480,7 +713,7 @@ public class Slot extends Thread implements AutoCloseable {",
                "         assert(dynamicState.currentAssignment != null);",
                "-        ",
                "+",
                "         if (dynamicState.container.areAllProcessesDead()) {",
                "-            LOG.warn(\"SLOT {} all processes are dead...\", staticState.port);",
                "-            return cleanupCurrentContainer(dynamicState, staticState, ",
                "-                    dynamicState.pendingLocalization == null ? MachineState.EMPTY : MachineState.WAITING_FOR_BASIC_LOCALIZATION);",
                "+            LOG.info(\"SLOT {} all processes are dead...\", staticState.port);",
                "+            return cleanupCurrentContainer(dynamicState, staticState,",
                "+                dynamicState.pendingLocalization == null ? MachineState.EMPTY : MachineState.WAITING_FOR_BLOB_LOCALIZATION);",
                "         }",
                "@@ -526,2 +759,38 @@ public class Slot extends Thread implements AutoCloseable {",
                "+    /**",
                "+     * State Transitions for KILL_BLOB_UPDATE state.",
                "+     * PRECONDITION: container.kill() was called",
                "+     * PRECONDITION: container != null && currentAssignment != null",
                "+     *",
                "+     * @param dynamicState current state",
                "+     * @param staticState static data",
                "+     * @return the next state",
                "+     * @throws Exception on any error",
                "+     */",
                "+    private static DynamicState handleKillBlobUpdate(DynamicState dynamicState, StaticState staticState) throws Exception {",
                "+        assert(dynamicState.container != null);",
                "+        assert(dynamicState.currentAssignment != null);",
                "+",
                "+        //Release things that don't need to wait for us",
                "+        dynamicState = filterChangingBlobsFor(dynamicState, dynamicState.currentAssignment);",
                "+",
                "+        if (dynamicState.container.areAllProcessesDead()) {",
                "+            if (equivalent(dynamicState.newAssignment, dynamicState.currentAssignment)) {",
                "+                dynamicState.container.cleanUp();",
                "+                dynamicState = dynamicState.withCurrentAssignment(null, dynamicState.currentAssignment);",
                "+                return informChangedBlobs(dynamicState, dynamicState.currentAssignment)",
                "+                    .withState(MachineState.WAITING_FOR_BLOB_UPDATE);",
                "+            }",
                "+            //Scheduling changed after we killed all of the processes",
                "+            return prepareForNewAssignmentNoWorkersRunning(cleanupCurrentContainer(dynamicState, staticState, null), staticState);",
                "+        }",
                "+        //The child processes typically exit in < 1 sec.  If 2 mins later they are still around something is wrong",
                "+        if ((Time.currentTimeMillis() - dynamicState.startTime) > 120_000) {",
                "+            throw new RuntimeException(\"Not all processes in \" + dynamicState.container + \" exited after 120 seconds\");",
                "+        }",
                "+        dynamicState.container.forceKill();",
                "+        Time.sleep(staticState.killSleepMs);",
                "+        return dynamicState;",
                "+    }",
                "+",
                "     /**",
                "@@ -548,3 +817,4 @@ public class Slot extends Thread implements AutoCloseable {",
                "             //We were rescheduled while waiting for the worker to come up",
                "-            LOG.warn(\"SLOT {}: Assignment Changed from {} to {}\", staticState.port, dynamicState.currentAssignment, dynamicState.newAssignment);",
                "+            LOG.info(\"SLOT {}: Assignment Changed from {} to {}\", staticState.port, dynamicState.currentAssignment,",
                "+                dynamicState.newAssignment);",
                "             return killContainerForChangedAssignment(dynamicState, staticState);",
                "@@ -558,2 +828,8 @@ public class Slot extends Thread implements AutoCloseable {",
                "         }",
                "+",
                "+        dynamicState = filterChangingBlobsFor(dynamicState, dynamicState.currentAssignment);",
                "+        if (!dynamicState.changingBlobs.isEmpty()) {",
                "+            //Kill the container and restart it",
                "+            return killContainerForChangedBlobs(dynamicState, staticState);",
                "+        }",
                "         Time.sleep(1000);",
                "@@ -575,3 +851,4 @@ public class Slot extends Thread implements AutoCloseable {",
                "         if (!equivalent(dynamicState.newAssignment, dynamicState.currentAssignment)) {",
                "-            LOG.warn(\"SLOT {}: Assignment Changed from {} to {}\", staticState.port, dynamicState.currentAssignment, dynamicState.newAssignment);",
                "+            LOG.info(\"SLOT {}: Assignment Changed from {} to {}\", staticState.port, dynamicState.currentAssignment,",
                "+                dynamicState.newAssignment);",
                "             //Scheduling changed while running...",
                "@@ -581,2 +858,8 @@ public class Slot extends Thread implements AutoCloseable {",
                "+        dynamicState = filterChangingBlobsFor(dynamicState, dynamicState.currentAssignment);",
                "+        if (!dynamicState.changingBlobs.isEmpty()) {",
                "+            //Kill the container and restart it",
                "+            return killContainerForChangedBlobs(dynamicState, staticState);",
                "+        }",
                "+",
                "         if (dynamicState.container.didMainProcessExit()) {",
                "@@ -662,2 +945,4 @@ public class Slot extends Thread implements AutoCloseable {",
                "     static DynamicState handleEmpty(DynamicState dynamicState, StaticState staticState) throws InterruptedException, IOException {",
                "+        assert dynamicState.changingBlobs.isEmpty();",
                "+        assert dynamicState.pendingChangingBlobsAssignment == null;",
                "         if (!equivalent(dynamicState.newAssignment, dynamicState.currentAssignment)) {",
                "@@ -671,4 +956,6 @@ public class Slot extends Thread implements AutoCloseable {",
                "             LOG.warn(\"Dropping {} no topology is running\", dynamicState.profileActions);",
                "-            dynamicState = dynamicState.withProfileActions(Collections.<TopoProfileAction> emptySet(), Collections.<TopoProfileAction> emptySet());",
                "+            dynamicState = dynamicState.withProfileActions(Collections.emptySet(), Collections.emptySet());",
                "         }",
                "+        //Drop the change notifications we are not running anything right now",
                "+        dynamicState = drainAllChangingBlobs(dynamicState);",
                "         Time.sleep(1000);",
                "@@ -678,4 +965,4 @@ public class Slot extends Thread implements AutoCloseable {",
                "     private final AtomicReference<LocalAssignment> newAssignment = new AtomicReference<>();",
                "-    private final AtomicReference<Set<TopoProfileAction>> profiling =",
                "-            new AtomicReference<Set<TopoProfileAction>>(new HashSet<TopoProfileAction>());",
                "+    private final AtomicReference<Set<TopoProfileAction>> profiling = new AtomicReference<>(new HashSet<>());",
                "+    private final BlockingQueue<BlobChangeing> changingBlobs = new LinkedBlockingQueue<>();",
                "     private final StaticState staticState;",
                "@@ -687,7 +974,7 @@ public class Slot extends Thread implements AutoCloseable {",
                "     public Slot(AsyncLocalizer localizer, Map<String, Object> conf,",
                "-                ContainerLauncher containerLauncher, String host,",
                "-                int port, LocalState localState,",
                "-                IStormClusterState clusterState,",
                "-                ISupervisor iSupervisor,",
                "-                AtomicReference<Map<Long, LocalAssignment>> cachedCurrentAssignments) throws Exception {",
                "+            ContainerLauncher containerLauncher, String host,",
                "+            int port, LocalState localState,",
                "+            IStormClusterState clusterState,",
                "+            ISupervisor iSupervisor,",
                "+            AtomicReference<Map<Long, LocalAssignment>> cachedCurrentAssignments) throws Exception {",
                "         super(\"SLOT_\"+port);",
                "@@ -717,12 +1004,13 @@ public class Slot extends Thread implements AutoCloseable {",
                "         dynamicState = new DynamicState(currentAssignment, container, newAssignment);",
                "-        staticState = new StaticState(localizer, ",
                "-                ObjectReader.getInt(conf.get(Config.SUPERVISOR_WORKER_TIMEOUT_SECS)) * 1000,",
                "-                ObjectReader.getInt(conf.get(DaemonConfig.SUPERVISOR_WORKER_START_TIMEOUT_SECS)) * 1000,",
                "-                ObjectReader.getInt(conf.get(DaemonConfig.SUPERVISOR_WORKER_SHUTDOWN_SLEEP_SECS)) * 1000,",
                "-                ObjectReader.getInt(conf.get(DaemonConfig.SUPERVISOR_MONITOR_FREQUENCY_SECS)) * 1000,",
                "-                containerLauncher,",
                "-                host,",
                "-                port,",
                "-                iSupervisor,",
                "-                localState);",
                "+        staticState = new StaticState(localizer,",
                "+            ObjectReader.getInt(conf.get(Config.SUPERVISOR_WORKER_TIMEOUT_SECS)) * 1000,",
                "+            ObjectReader.getInt(conf.get(DaemonConfig.SUPERVISOR_WORKER_START_TIMEOUT_SECS)) * 1000,",
                "+            ObjectReader.getInt(conf.get(DaemonConfig.SUPERVISOR_WORKER_SHUTDOWN_SLEEP_SECS)) * 1000,",
                "+            ObjectReader.getInt(conf.get(DaemonConfig.SUPERVISOR_MONITOR_FREQUENCY_SECS)) * 1000,",
                "+            containerLauncher,",
                "+            host,",
                "+            port,",
                "+            iSupervisor,",
                "+            localState,",
                "+            this);",
                "         this.newAssignment.set(dynamicState.newAssignment);",
                "@@ -730,6 +1018,7 @@ public class Slot extends Thread implements AutoCloseable {",
                "             //We are running so we should recover the blobs.",
                "-            staticState.localizer.recoverRunningTopology(currentAssignment, port);",
                "+            staticState.localizer.recoverRunningTopology(currentAssignment, port, this);",
                "             saveNewAssignment(currentAssignment);",
                "         }",
                "-        LOG.warn(\"SLOT {}:{} Starting in state {} - assignment {}\", staticState.host, staticState.port, dynamicState.state, dynamicState.currentAssignment);",
                "+        LOG.info(\"SLOT {}:{} Starting in state {} - assignment {}\", staticState.host, staticState.port, dynamicState.state,",
                "+            dynamicState.currentAssignment);",
                "     }",
                "@@ -741,3 +1030,3 @@ public class Slot extends Thread implements AutoCloseable {",
                "     /**",
                "-     * Set a new assignment asynchronously",
                "+     * Set a new assignment asynchronously.",
                "      * @param newAssignment the new assignment for this slot to run, null to run nothing",
                "@@ -747,3 +1036,14 @@ public class Slot extends Thread implements AutoCloseable {",
                "     }",
                "-    ",
                "+",
                "+    @Override",
                "+    public void blobChanging(LocalAssignment assignment, int port, LocallyCachedBlob blob, GoodToGo go) {",
                "+        assert port == staticState.port : \"got a callaback that is not for us \" + port + \" != \" + staticState.port;",
                "+        //This is called async so lets assume that it is something we care about",
                "+        try {",
                "+            changingBlobs.put(new BlobChangeing(assignment, blob, go.getLatch()));",
                "+        } catch (InterruptedException e) {",
                "+            throw new RuntimeException(\"This should not have happend, but it did (the queue is unbounded)\", e);",
                "+        }",
                "+    }",
                "+",
                "     public void addProfilerActions(Set<TopoProfileAction> actions) {",
                "@@ -802,6 +1102,24 @@ public class Slot extends Thread implements AutoCloseable {",
                "                 Set<TopoProfileAction> removed = new HashSet<>(origProfileActions);",
                "-                ",
                "-                DynamicState nextState = ",
                "-                        stateMachineStep(dynamicState.withNewAssignment(newAssignment.get())",
                "-                                .withProfileActions(origProfileActions, dynamicState.pendingStopProfileActions), staticState);",
                "+",
                "+                Set<BlobChangeing> changingResourcesToHandle = dynamicState.changingBlobs;",
                "+                if (!changingBlobs.isEmpty()) {",
                "+                    changingResourcesToHandle = new HashSet<>(changingResourcesToHandle);",
                "+                    changingBlobs.drainTo(changingResourcesToHandle);",
                "+                    Iterator<BlobChangeing> it = changingResourcesToHandle.iterator();",
                "+",
                "+                    //Remove/Clean up changed requests that are not for us",
                "+                    while(it.hasNext()) {",
                "+                        BlobChangeing rc = it.next();",
                "+                        if (!forSameTopology(rc.assignment, dynamicState.currentAssignment) &&",
                "+                            !forSameTopology(rc.assignment, dynamicState.newAssignment)) {",
                "+                            rc.latch.countDown(); //Ignore the future",
                "+                            it.remove();",
                "+                        }",
                "+                    }",
                "+                }",
                "+",
                "+                DynamicState nextState =",
                "+                    stateMachineStep(dynamicState.withNewAssignment(newAssignment.get())",
                "+                        .withProfileActions(origProfileActions, dynamicState.pendingStopProfileActions)",
                "+                        .withChangingBlobs(changingResourcesToHandle), staticState);",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java b/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java",
                "index 043de8a18..4b4750246 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java",
                "@@ -26,9 +26,6 @@ import java.io.FileOutputStream;",
                " import java.io.FileWriter;",
                "-import java.io.FilenameFilter;",
                " import java.io.IOException;",
                "-import java.io.OutputStream;",
                " import java.io.PrintWriter;",
                "-import java.net.JarURLConnection;",
                "-import java.net.URL;",
                " import java.net.URLDecoder;",
                "+import java.nio.file.DirectoryStream;",
                " import java.nio.file.Files;",
                "@@ -60,3 +57,2 @@ import org.apache.storm.Config;",
                " import org.apache.storm.DaemonConfig;",
                "-import org.apache.storm.blobstore.BlobStore;",
                " import org.apache.storm.blobstore.ClientBlobStore;",
                "@@ -73,3 +69,2 @@ import org.apache.storm.utils.NimbusLeaderNotFoundException;",
                " import org.apache.storm.utils.ObjectReader;",
                "-import org.apache.storm.utils.ServerConfigUtils;",
                " import org.apache.storm.utils.ServerUtils;",
                "@@ -112,3 +107,2 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     private final boolean isLocalMode;",
                "-    private final Map<String, LocalDownloadedResource> basicPending;",
                "     private final Map<String, LocalDownloadedResource> blobPending;",
                "@@ -117,2 +111,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     private final boolean symlinksDisabled;",
                "+    private final ConcurrentHashMap<String, LocallyCachedBlob> topologyBlobs = new ConcurrentHashMap<>();",
                "+    private final ConcurrentHashMap<String, CompletableFuture<Void>> topologyBasicDownloaded = new ConcurrentHashMap<>();",
                "@@ -128,12 +124,6 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     private long cacheTargetSize;",
                "-    private long cacheCleanupPeriod;",
                "-",
                "-",
                "-    public AsyncLocalizer(Map<String, Object> conf, AtomicReference<Map<Long, LocalAssignment>> currAssignment,",
                "-                          Map<Integer, LocalAssignment> portToAssignments) throws IOException {",
                "-        this(conf, ConfigUtils.supervisorLocalDir(conf), AdvancedFSOps.make(conf), currAssignment, portToAssignments);",
                "-    }",
                "+    private final long cacheCleanupPeriod;",
                "     @VisibleForTesting",
                "-    AsyncLocalizer(Map<String, Object> conf, String baseDir, AdvancedFSOps ops,",
                "+    AsyncLocalizer(Map<String, Object> conf, AdvancedFSOps ops, String baseDir,",
                "                    AtomicReference<Map<Long, LocalAssignment>> currAssignment,",
                "@@ -148,3 +138,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "             10 * 1024).longValue() << 20;",
                "-        // default 30 seconds.",
                "+        // default 30 seconds. (we cache the size so it is cheap to do)",
                "         cacheCleanupPeriod = ObjectReader.getInt(conf.get(",
                "@@ -158,3 +148,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         execService = Executors.newScheduledThreadPool(threadPoolSize,",
                "-            new ThreadFactoryBuilder().setNameFormat(\"AsyncLocalizer Executor\").build());",
                "+            new ThreadFactoryBuilder().setNameFormat(\"AsyncLocalizer Executor - %d\").build());",
                "         reconstructLocalizedResources();",
                "@@ -162,3 +152,2 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         symlinksDisabled = (boolean)conf.getOrDefault(Config.DISABLE_SYMLINKS, false);",
                "-        basicPending = new HashMap<>();",
                "         blobPending = new HashMap<>();",
                "@@ -169,2 +158,127 @@ public class AsyncLocalizer implements AutoCloseable {",
                "+    public AsyncLocalizer(Map<String, Object> conf, AtomicReference<Map<Long, LocalAssignment>> currAssignment,",
                "+                          Map<Integer, LocalAssignment> portToAssignments) throws IOException {",
                "+        this(conf, AdvancedFSOps.make(conf), ConfigUtils.supervisorLocalDir(conf), currAssignment, portToAssignments);",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    LocallyCachedBlob getTopoJar(final String topologyId) throws IOException {",
                "+        String topoJarKey = ConfigUtils.masterStormJarKey(topologyId);",
                "+        LocallyCachedBlob topoJar = topologyBlobs.get(topoJarKey);",
                "+        if (topoJar == null) {",
                "+            topoJar = new LocallyCachedTopologyBlob(topologyId, isLocalMode, conf, fsOps,",
                "+                LocallyCachedTopologyBlob.TopologyBlobType.TOPO_JAR);",
                "+            topologyBlobs.put(topoJarKey, topoJar);",
                "+        }",
                "+        return topoJar;",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    LocallyCachedBlob getTopoCode(final String topologyId) throws IOException {",
                "+        String topoCodeKey = ConfigUtils.masterStormCodeKey(topologyId);",
                "+        LocallyCachedBlob topoCode = topologyBlobs.get(topoCodeKey);",
                "+        if (topoCode == null) {",
                "+            topoCode = new LocallyCachedTopologyBlob(topologyId, isLocalMode, conf, fsOps,",
                "+                LocallyCachedTopologyBlob.TopologyBlobType.TOPO_CODE);",
                "+            topologyBlobs.put(topoCodeKey, topoCode);",
                "+        }",
                "+        return topoCode;",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    LocallyCachedBlob getTopoConf(final String topologyId) throws IOException {",
                "+        String topoConfKey = ConfigUtils.masterStormConfKey(topologyId);",
                "+        LocallyCachedBlob topoConf = topologyBlobs.get(topoConfKey);",
                "+        if (topoConf == null) {",
                "+            topoConf = new LocallyCachedTopologyBlob(topologyId, isLocalMode, conf, fsOps,",
                "+                LocallyCachedTopologyBlob.TopologyBlobType.TOPO_CONF);",
                "+            topologyBlobs.put(topoConfKey, topoConf);",
                "+        }",
                "+        return topoConf;",
                "+    }",
                "+",
                "+    public synchronized CompletableFuture<Void> requestDownloadTopologyBlobs(final LocalAssignment assignment, final int port,",
                "+                                                                             final BlobChangingCallback cb) throws IOException {",
                "+        final String topologyId = assignment.get_topology_id();",
                "+",
                "+        CompletableFuture<Void> baseBlobs = requestDownloadBaseTopologyBlobs(assignment, port, cb);",
                "+        return baseBlobs.thenComposeAsync((v) -> {",
                "+            LocalDownloadedResource localResource = blobPending.get(topologyId);",
                "+            if (localResource == null) {",
                "+                Supplier<Void> supplier = new DownloadBlobs(topologyId, assignment.get_owner());",
                "+                localResource = new LocalDownloadedResource(CompletableFuture.supplyAsync(supplier, execService));",
                "+                blobPending.put(topologyId, localResource);",
                "+            }",
                "+            CompletableFuture<Void> r = localResource.reserve(port, assignment);",
                "+            LOG.debug(\"Reserved blobs {} {}\", topologyId, localResource);",
                "+            return r;",
                "+        });",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    synchronized CompletableFuture<Void> requestDownloadBaseTopologyBlobs(final LocalAssignment assignment, final int port,",
                "+                                                                          BlobChangingCallback cb) throws IOException {",
                "+        PortAndAssignment pna = new PortAndAssignment(port, assignment);",
                "+        final String topologyId = assignment.get_topology_id();",
                "+",
                "+        LocallyCachedBlob topoJar = getTopoJar(topologyId);",
                "+        topoJar.addReference(pna, cb);",
                "+",
                "+        LocallyCachedBlob topoCode = getTopoCode(topologyId);",
                "+        topoCode.addReference(pna, cb);",
                "+",
                "+        LocallyCachedBlob topoConf = getTopoConf(topologyId);",
                "+        topoConf.addReference(pna, cb);",
                "+",
                "+        CompletableFuture<Void> ret = topologyBasicDownloaded.get(topologyId);",
                "+        if (ret == null) {",
                "+            ret = downloadOrUpdate(topoJar, topoCode, topoConf);",
                "+        }",
                "+        return ret;",
                "+    }",
                "+",
                "+    private static final int ATTEMPTS_INTERVAL_TIME = 100;",
                "+",
                "+    private CompletableFuture<Void> downloadOrUpdate(LocallyCachedBlob ... blobs) {",
                "+        CompletableFuture<Void> [] all = new CompletableFuture[blobs.length];",
                "+        for (int i = 0; i < blobs.length; i++) {",
                "+            final LocallyCachedBlob blob = blobs[i];",
                "+            all[i] = CompletableFuture.runAsync(() -> {",
                "+                LOG.debug(\"STARTING download of {}\", blob);",
                "+                try (ClientBlobStore blobStore = ServerUtils.getClientBlobStoreForSupervisor(conf)) {",
                "+                    boolean done = false;",
                "+                    long failures = 0;",
                "+                    while (!done) {",
                "+                        try {",
                "+                            synchronized (blob) {",
                "+                                long localVersion = blob.getLocalVersion();",
                "+                                long remoteVersion = blob.getRemoteVersion(blobStore);",
                "+                                if (localVersion != remoteVersion) {",
                "+                                    try {",
                "+                                        long newVersion = blob.downloadToTempLocation(blobStore);",
                "+                                        blob.informAllOfChangeAndWaitForConsensus();",
                "+                                        blob.commitNewVersion(newVersion);",
                "+                                        blob.informAllChangeComplete();",
                "+                                    } finally {",
                "+                                        blob.cleanupOrphanedData();",
                "+                                    }",
                "+                                }",
                "+                            }",
                "+                            done = true;",
                "+                        } catch (Exception e) {",
                "+                            failures++;",
                "+                            if (failures > blobDownloadRetries) {",
                "+                                throw new RuntimeException(\"Could not download...\", e);",
                "+                            }",
                "+                            LOG.warn(\"Failed to download blob {} will try again in {} ms\", blob, ATTEMPTS_INTERVAL_TIME, e);",
                "+                            Utils.sleep(ATTEMPTS_INTERVAL_TIME);",
                "+                        }",
                "+                    }",
                "+                }",
                "+                LOG.debug(\"FINISHED download of {}\", blob);",
                "+            }, execService);",
                "+        }",
                "+        return CompletableFuture.allOf(all);",
                "+    }",
                "+",
                "     /**",
                "@@ -213,2 +327,11 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     private void updateBlobs() {",
                "+        for (LocallyCachedBlob blob : topologyBlobs.values()) {",
                "+            if (blob.isUsed()) {",
                "+                try {",
                "+                    downloadOrUpdate(blob);",
                "+                } catch (Exception e) {",
                "+                    LOG.error(\"Could not update {}\", blob, e);",
                "+                }",
                "+            }",
                "+        }",
                "         try {",
                "@@ -217,3 +340,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "                 .distinct()",
                "-                .collect(Collectors.toMap((p) -> p.getFirst(), (p) -> p.getSecond()));",
                "+                .collect(Collectors.toMap(Pair::getFirst, Pair::getSecond));",
                "             for (String topoId : readDownloadedTopologyIds(conf)) {",
                "@@ -274,125 +397,2 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     //ILocalizer",
                "-    private class DownloadBaseBlobsDistributed implements Supplier<Void> {",
                "-        protected final String topologyId;",
                "-        protected final File stormRoot;",
                "-        protected final LocalAssignment assignment;",
                "-        protected final String owner;",
                "-",
                "-        public DownloadBaseBlobsDistributed(String topologyId, LocalAssignment assignment) throws IOException {",
                "-            this.topologyId = topologyId;",
                "-            stormRoot = new File(ConfigUtils.supervisorStormDistRoot(conf, this.topologyId));",
                "-            this.assignment = assignment;",
                "-            owner = assignment.get_owner();",
                "-        }",
                "-",
                "-        protected void downloadBaseBlobs(File tmproot) throws Exception {",
                "-            String stormJarKey = ConfigUtils.masterStormJarKey(topologyId);",
                "-            String stormCodeKey = ConfigUtils.masterStormCodeKey(topologyId);",
                "-            String topoConfKey = ConfigUtils.masterStormConfKey(topologyId);",
                "-            String jarPath = ConfigUtils.supervisorStormJarPath(tmproot.getAbsolutePath());",
                "-            String codePath = ConfigUtils.supervisorStormCodePath(tmproot.getAbsolutePath());",
                "-            String confPath = ConfigUtils.supervisorStormConfPath(tmproot.getAbsolutePath());",
                "-            fsOps.forceMkdir(tmproot);",
                "-            fsOps.restrictDirectoryPermissions(tmproot);",
                "-            ClientBlobStore blobStore = ServerUtils.getClientBlobStoreForSupervisor(conf);",
                "-            try {",
                "-                ServerUtils.downloadResourcesAsSupervisor(stormJarKey, jarPath, blobStore);",
                "-                ServerUtils.downloadResourcesAsSupervisor(stormCodeKey, codePath, blobStore);",
                "-                ServerUtils.downloadResourcesAsSupervisor(topoConfKey, confPath, blobStore);",
                "-            } finally {",
                "-                blobStore.shutdown();",
                "-            }",
                "-            ServerUtils.extractDirFromJar(jarPath, ServerConfigUtils.RESOURCES_SUBDIR, tmproot);",
                "-        }",
                "-",
                "-        @Override",
                "-        public Void get() {",
                "-            try {",
                "-                if (fsOps.fileExists(stormRoot)) {",
                "-                    if (!fsOps.supportsAtomicDirectoryMove()) {",
                "-                        LOG.warn(\"{} may have partially downloaded blobs, recovering\", topologyId);",
                "-                        fsOps.deleteIfExists(stormRoot);",
                "-                    } else {",
                "-                        LOG.warn(\"{} already downloaded blobs, skipping\", topologyId);",
                "-                        return null;",
                "-                    }",
                "-                }",
                "-                boolean deleteAll = true;",
                "-                String tmproot = ServerConfigUtils.supervisorTmpDir(conf) + File.separator + Utils.uuid();",
                "-                File tr = new File(tmproot);",
                "-                try {",
                "-                    downloadBaseBlobs(tr);",
                "-                    if (assignment.is_set_total_node_shared()) {",
                "-                        File sharedMemoryDirTmpLocation = new File(tr, \"shared_by_topology\");",
                "-                        //We need to create a directory for shared memory to write to (we should not encourage this though)",
                "-                        Path path = sharedMemoryDirTmpLocation.toPath();",
                "-                        Files.createDirectories(path);",
                "-                    }",
                "-                    fsOps.moveDirectoryPreferAtomic(tr, stormRoot);",
                "-                    fsOps.setupStormCodeDir(owner, stormRoot);",
                "-                    if (assignment.is_set_total_node_shared()) {",
                "-                        File sharedMemoryDir = new File(stormRoot, \"shared_by_topology\");",
                "-                        fsOps.setupWorkerArtifactsDir(owner, sharedMemoryDir);",
                "-                    }",
                "-                    deleteAll = false;",
                "-                } finally {",
                "-                    if (deleteAll) {",
                "-                        LOG.warn(\"Failed to download basic resources for topology-id {}\", topologyId);",
                "-                        fsOps.deleteIfExists(tr);",
                "-                        fsOps.deleteIfExists(stormRoot);",
                "-                    }",
                "-                }",
                "-                return null;",
                "-            } catch (Exception e) {",
                "-                LOG.warn(\"Caught Exception While Downloading (rethrowing)... \", e);",
                "-                throw new RuntimeException(e);",
                "-            }",
                "-        }",
                "-    }",
                "-",
                "-    private class DownloadBaseBlobsLocal extends DownloadBaseBlobsDistributed {",
                "-",
                "-        public DownloadBaseBlobsLocal(String topologyId, LocalAssignment assignment) throws IOException {",
                "-            super(topologyId, assignment);",
                "-        }",
                "-",
                "-        @Override",
                "-        protected void downloadBaseBlobs(File tmproot) throws Exception {",
                "-            fsOps.forceMkdir(tmproot);",
                "-            String stormCodeKey = ConfigUtils.masterStormCodeKey(topologyId);",
                "-            String topoConfKey = ConfigUtils.masterStormConfKey(topologyId);",
                "-            File codePath = new File(ConfigUtils.supervisorStormCodePath(tmproot.getAbsolutePath()));",
                "-            File confPath = new File(ConfigUtils.supervisorStormConfPath(tmproot.getAbsolutePath()));",
                "-            BlobStore blobStore = ServerUtils.getNimbusBlobStore(conf, null);",
                "-            try {",
                "-                try (OutputStream codeOutStream = fsOps.getOutputStream(codePath)){",
                "-                    blobStore.readBlobTo(stormCodeKey, codeOutStream, null);",
                "-                }",
                "-                try (OutputStream confOutStream = fsOps.getOutputStream(confPath)) {",
                "-                    blobStore.readBlobTo(topoConfKey, confOutStream, null);",
                "-                }",
                "-            } finally {",
                "-                blobStore.shutdown();",
                "-            }",
                "-",
                "-            ClassLoader classloader = Thread.currentThread().getContextClassLoader();",
                "-            String resourcesJar = resourcesJar();",
                "-            URL url = classloader.getResource(ServerConfigUtils.RESOURCES_SUBDIR);",
                "-",
                "-            String targetDir = tmproot + File.separator;",
                "-            if (resourcesJar != null) {",
                "-                LOG.info(\"Extracting resources from jar at {} to {}\", resourcesJar, targetDir);",
                "-                ServerUtils.extractDirFromJar(resourcesJar, ServerConfigUtils.RESOURCES_SUBDIR, new File(targetDir));",
                "-            } else if (url != null) {",
                "-                LOG.info(\"Copying resources at {} to {}\", url, targetDir);",
                "-                if (\"jar\".equals(url.getProtocol())) {",
                "-                    JarURLConnection urlConnection = (JarURLConnection) url.openConnection();",
                "-                    ServerUtils.extractDirFromJar(urlConnection.getJarFileURL().getFile(), ServerConfigUtils.RESOURCES_SUBDIR, new File(targetDir));",
                "-                } else {",
                "-                    fsOps.copyDirectory(new File(url.getFile()), new File(targetDir, ConfigUtils.RESOURCES_SUBDIR));",
                "-                }",
                "-            }",
                "-        }",
                "-    }",
                "-",
                "     private class DownloadBlobs implements Supplier<Void> {",
                "@@ -474,47 +474,17 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    public synchronized CompletableFuture<Void> requestDownloadBaseTopologyBlobs(final LocalAssignment assignment, final int port) throws IOException {",
                "+    public synchronized void recoverRunningTopology(final LocalAssignment assignment, final int port,",
                "+                                                    final BlobChangingCallback cb) throws IOException {",
                "+        PortAndAssignment pna = new PortAndAssignment(port, assignment);",
                "         final String topologyId = assignment.get_topology_id();",
                "-        LocalDownloadedResource localResource = basicPending.get(topologyId);",
                "-        if (localResource == null) {",
                "-            Supplier<Void> supplier;",
                "-            if (isLocalMode) {",
                "-                supplier = new DownloadBaseBlobsLocal(topologyId, assignment);",
                "-            } else {",
                "-                supplier = new DownloadBaseBlobsDistributed(topologyId, assignment);",
                "-            }",
                "-            localResource = new LocalDownloadedResource(CompletableFuture.supplyAsync(supplier, execService));",
                "-            basicPending.put(topologyId, localResource);",
                "-        }",
                "-        CompletableFuture<Void> ret = localResource.reserve(port, assignment);",
                "-        LOG.debug(\"Reserved basic {} {}\", topologyId, localResource);",
                "-        return ret;",
                "-    }",
                "-    private static String resourcesJar() throws IOException {",
                "-        String path = ServerUtils.currentClasspath();",
                "-        if (path == null) {",
                "-            return null;",
                "-        }",
                "+        LocallyCachedBlob topoJar = getTopoJar(topologyId);",
                "+        topoJar.addReference(pna, cb);",
                "-        for (String jpath : path.split(File.pathSeparator)) {",
                "-            if (jpath.endsWith(\".jar\")) {",
                "-                if (ServerUtils.zipDoesContainDir(jpath, ServerConfigUtils.RESOURCES_SUBDIR)) {",
                "-                    return jpath;",
                "-                }",
                "-            }",
                "-        }",
                "-        return null;",
                "-    }",
                "+        LocallyCachedBlob topoCode = getTopoCode(topologyId);",
                "+        topoCode.addReference(pna, cb);",
                "-    public synchronized void recoverRunningTopology(LocalAssignment assignment, int port) {",
                "-        final String topologyId = assignment.get_topology_id();",
                "-        LocalDownloadedResource localResource = basicPending.get(topologyId);",
                "-        if (localResource == null) {",
                "-            localResource = new LocalDownloadedResource(ALL_DONE_FUTURE);",
                "-            basicPending.put(topologyId, localResource);",
                "-        }",
                "-        localResource.reserve(port, assignment);",
                "-        LOG.debug(\"Recovered basic {} {}\", topologyId, localResource);",
                "+        LocallyCachedBlob topoConf = getTopoConf(topologyId);",
                "+        topoConf.addReference(pna, cb);",
                "-        localResource = blobPending.get(topologyId);",
                "+        LocalDownloadedResource localResource = blobPending.get(topologyId);",
                "         if (localResource == null) {",
                "@@ -527,15 +497,2 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    public synchronized CompletableFuture<Void> requestDownloadTopologyBlobs(LocalAssignment assignment, int port) {",
                "-        final String topologyId = assignment.get_topology_id();",
                "-        LocalDownloadedResource localResource = blobPending.get(topologyId);",
                "-        if (localResource == null) {",
                "-            Supplier<Void> supplier = new DownloadBlobs(topologyId, assignment.get_owner());",
                "-            localResource = new LocalDownloadedResource(CompletableFuture.supplyAsync(supplier, execService));",
                "-            blobPending.put(topologyId, localResource);",
                "-        }",
                "-        CompletableFuture<Void> ret = localResource.reserve(port, assignment);",
                "-        LOG.debug(\"Reserved blobs {} {}\", topologyId, localResource);",
                "-        return ret;",
                "-    }",
                "-",
                "     /**",
                "@@ -548,4 +505,25 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     public synchronized void releaseSlotFor(LocalAssignment assignment, int port) throws IOException {",
                "+        PortAndAssignment pna = new PortAndAssignment(port, assignment);",
                "         final String topologyId = assignment.get_topology_id();",
                "         LOG.debug(\"Releasing slot for {} {}\", topologyId, port);",
                "+",
                "+        String topoJarKey = ConfigUtils.masterStormJarKey(topologyId);",
                "+        String topoCodeKey = ConfigUtils.masterStormCodeKey(topologyId);",
                "+        String topoConfKey = ConfigUtils.masterStormConfKey(topologyId);",
                "+",
                "+        LocallyCachedBlob topoJar = topologyBlobs.get(topoJarKey);",
                "+        if (topoJar != null) {",
                "+            topoJar.removeReference(pna);",
                "+        }",
                "+",
                "+        LocallyCachedBlob topoCode = topologyBlobs.get(topoCodeKey);",
                "+        if (topoCode != null) {",
                "+            topoCode.removeReference(pna);",
                "+        }",
                "+",
                "+        LocallyCachedBlob topoConfBlob = topologyBlobs.get(topoConfKey);",
                "+        if (topoConfBlob != null) {",
                "+            topoConfBlob.removeReference(pna);",
                "+        }",
                "+",
                "         LocalDownloadedResource localResource = blobPending.get(topologyId);",
                "@@ -576,28 +554,2 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         }",
                "-",
                "-        localResource = basicPending.get(topologyId);",
                "-        if (localResource == null || !localResource.release(port, assignment)) {",
                "-            LOG.warn(\"Released basic reference {} {} for something that we didn't have {}\", topologyId, port, localResource);",
                "-        } else if (localResource.isDone()){",
                "-            LOG.info(\"Released blob reference {} {} Cleaning up basic files...\", topologyId, port);",
                "-            basicPending.remove(topologyId);",
                "-            String path = ConfigUtils.supervisorStormDistRoot(conf, topologyId);",
                "-            fsOps.deleteIfExists(new File(path), null, \"rmr \"+topologyId);",
                "-        } else {",
                "-            LOG.debug(\"Released basic reference {} {} still waiting on {}\", topologyId, port, localResource);",
                "-        }",
                "-    }",
                "-",
                "-    public synchronized void cleanupUnusedTopologies() throws IOException {",
                "-        File distRoot = new File(ConfigUtils.supervisorStormDistRoot(conf));",
                "-        LOG.info(\"Cleaning up unused topologies in {}\", distRoot);",
                "-        File[] children = distRoot.listFiles();",
                "-        if (children != null) {",
                "-            for (File topoDir : children) {",
                "-                String topoId = URLDecoder.decode(topoDir.getName(), \"UTF-8\");",
                "-                if (basicPending.get(topoId) == null && blobPending.get(topoId) == null) {",
                "-                    fsOps.deleteIfExists(topoDir, null, \"rmr \" + topoId);",
                "-                }",
                "-            }",
                "-        }",
                "     }",
                "@@ -606,4 +558,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    // For testing, it allows setting size in bytes",
                "-    protected void setTargetCacheSize(long size) {",
                "+    @VisibleForTesting",
                "+    void setTargetCacheSize(long size) {",
                "         cacheTargetSize = size;",
                "@@ -617,3 +569,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     // baseDir/supervisor/usercache/",
                "-    protected File getUserCacheDir() {",
                "+    private File getUserCacheDir() {",
                "         return new File(localBaseDir, USERCACHE);",
                "@@ -622,3 +574,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     // baseDir/supervisor/usercache/user1/",
                "-    protected File getLocalUserDir(String userName) {",
                "+    File getLocalUserDir(String userName) {",
                "         return new File(getUserCacheDir(), userName);",
                "@@ -627,3 +579,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     // baseDir/supervisor/usercache/user1/filecache",
                "-    public File getLocalUserFileCacheDir(String userName) {",
                "+    File getLocalUserFileCacheDir(String userName) {",
                "         return new File(getLocalUserDir(userName), FILECACHE);",
                "@@ -632,3 +584,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     // baseDir/supervisor/usercache/user1/filecache/files",
                "-    protected File getCacheDirForFiles(File dir) {",
                "+    private File getCacheDirForFiles(File dir) {",
                "         return new File(dir, FILESDIR);",
                "@@ -638,3 +590,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     // baseDir/supervisor/usercache/user1/filecache/archives",
                "-    protected File getCacheDirForArchives(File dir) {",
                "+    private File getCacheDirForArchives(File dir) {",
                "         return new File(dir, ARCHIVESDIR);",
                "@@ -642,4 +594,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    protected void addLocalizedResourceInDir(String dir, LocalizedResourceSet lrsrcSet,",
                "-                                             boolean uncompress) {",
                "+    private void addLocalizedResourceInDir(String dir, LocalizedResourceSet lrsrcSet,",
                "+                                           boolean uncompress) {",
                "         File[] lrsrcs = readCurrentBlobs(dir);",
                "@@ -664,3 +616,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     // Looks for files in the directory with .current suffix",
                "-    protected File[] readCurrentBlobs(String location) {",
                "+    private File[] readCurrentBlobs(String location) {",
                "         File dir = new File(location);",
                "@@ -668,8 +620,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         if (dir.exists()) {",
                "-            files = dir.listFiles(new FilenameFilter() {",
                "-                @Override",
                "-                public boolean accept(File dir, String name) {",
                "-                    return name.toLowerCase().endsWith(ServerUtils.DEFAULT_CURRENT_BLOB_SUFFIX);",
                "-                }",
                "-            });",
                "+            files = dir.listFiles((d, name) -> name.toLowerCase().endsWith(ServerUtils.DEFAULT_CURRENT_BLOB_SUFFIX));",
                "         }",
                "@@ -679,3 +626,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     // Check to see if there are any existing files already localized.",
                "-    protected void reconstructLocalizedResources() {",
                "+    private void reconstructLocalizedResources() {",
                "         try {",
                "@@ -707,4 +654,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     // ignores invalid user/topo/key",
                "-    public synchronized void removeBlobReference(String key, String user, String topo,",
                "-                                                 boolean uncompress) throws AuthorizationException, KeyNotFoundException {",
                "+    synchronized void removeBlobReference(String key, String user, String topo,",
                "+                                          boolean uncompress) throws AuthorizationException, KeyNotFoundException {",
                "         LocalizedResourceSet lrsrcSet = userRsrc.get(user);",
                "@@ -725,4 +672,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    public synchronized void addReferences(List<LocalResource> localresource, String user,",
                "-                                           String topo) {",
                "+    synchronized void addReferences(List<LocalResource> localresource, String user,",
                "+                                    String topo) {",
                "         LocalizedResourceSet lrsrcSet = userRsrc.get(user);",
                "@@ -748,5 +695,5 @@ public class AsyncLocalizer implements AutoCloseable {",
                "      */",
                "-    public LocalizedResource getBlob(LocalResource localResource, String user, String topo,",
                "+    LocalizedResource getBlob(LocalResource localResource, String user, String topo,",
                "                                      File userFileDir) throws AuthorizationException, KeyNotFoundException, IOException {",
                "-        ArrayList<LocalResource> arr = new ArrayList<LocalResource>();",
                "+        ArrayList<LocalResource> arr = new ArrayList<>();",
                "         arr.add(localResource);",
                "@@ -760,3 +707,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    protected boolean isLocalizedResourceDownloaded(LocalizedResource lrsrc) {",
                "+    private boolean isLocalizedResourceDownloaded(LocalizedResource lrsrc) {",
                "         File rsrcFileCurrent = new File(lrsrc.getCurrentSymlinkPath());",
                "@@ -767,4 +714,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    protected boolean isLocalizedResourceUpToDate(LocalizedResource lrsrc,",
                "-                                                  ClientBlobStore blobstore) throws AuthorizationException, KeyNotFoundException {",
                "+    private boolean isLocalizedResourceUpToDate(LocalizedResource lrsrc,",
                "+                                                ClientBlobStore blobstore) throws AuthorizationException, KeyNotFoundException {",
                "         String localFile = lrsrc.getFilePath();",
                "@@ -783,4 +730,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "      */",
                "-    public List<LocalizedResource> updateBlobs(List<LocalResource> localResources,",
                "-                                               String user) throws AuthorizationException, KeyNotFoundException, IOException {",
                "+    List<LocalizedResource> updateBlobs(List<LocalResource> localResources,",
                "+                                        String user) throws AuthorizationException, KeyNotFoundException, IOException {",
                "         LocalizedResourceSet lrsrcSet = userRsrc.get(user);",
                "@@ -801,6 +748,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "                     LOG.warn(\"blob requested for update doesn't exist: {}\", key);",
                "-                    continue;",
                "                 } else if ((boolean) conf.getOrDefault(Config.DISABLE_SYMLINKS, false)) {",
                "                     LOG.warn(\"symlinks are disabled so blobs cannot be downloaded.\");",
                "-                    continue;",
                "                 } else {",
                "@@ -857,4 +802,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "      */",
                "-    public synchronized List<LocalizedResource> getBlobs(List<LocalResource> localResources,",
                "-                                                         String user, String topo, File userFileDir)",
                "+    synchronized List<LocalizedResource> getBlobs(List<LocalResource> localResources,",
                "+                                                  String user, String topo, File userFileDir)",
                "         throws AuthorizationException, KeyNotFoundException, IOException {",
                "@@ -944,4 +889,4 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-        public DownloadBlob(AsyncLocalizer localizer, Map<String, Object> conf, String key, File localFile,",
                "-                            String user, boolean uncompress, boolean update) {",
                "+        DownloadBlob(AsyncLocalizer localizer, Map<String, Object> conf, String key, File localFile,",
                "+                     String user, boolean uncompress, boolean update) {",
                "             this.localizer = localizer;",
                "@@ -1100,3 +1045,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "-    public void setBlobPermissions(Map<String, Object> conf, String user, String path)",
                "+    private void setBlobPermissions(Map<String, Object> conf, String user, String path)",
                "         throws IOException {",
                "@@ -1111,3 +1056,3 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         }",
                "-        List<String> command = new ArrayList<String>(Arrays.asList(wlCommand, user, \"blob\", path));",
                "+        List<String> command = new ArrayList<>(Arrays.asList(wlCommand, user, \"blob\", path));",
                "@@ -1129,4 +1074,22 @@ public class AsyncLocalizer implements AutoCloseable {",
                "+    private interface ConsumePathAndId {",
                "+        void accept(Path path, String topologyId) throws IOException;",
                "+    }",
                "-    public synchronized void cleanup() {",
                "+    private void forEachTopologyDistDir(ConsumePathAndId consumer) throws IOException {",
                "+        Path stormCodeRoot = Paths.get(ConfigUtils.supervisorStormDistRoot(conf));",
                "+        if (Files.exists(stormCodeRoot) && Files.isDirectory(stormCodeRoot)) {",
                "+            try (DirectoryStream<Path> children = Files.newDirectoryStream(stormCodeRoot)) {",
                "+                for (Path child : children) {",
                "+                    if (Files.isDirectory(child)) {",
                "+                        String topologyId = child.getFileName().toString();",
                "+                        consumer.accept(child, topologyId);",
                "+                    }",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    synchronized void cleanup() {",
                "         LocalizedResourceRetentionSet toClean = new LocalizedResourceRetentionSet(cacheTargetSize);",
                "@@ -1137,3 +1100,24 @@ public class AsyncLocalizer implements AutoCloseable {",
                "         }",
                "+",
                "+        toClean.addResources(topologyBlobs);",
                "         toClean.cleanup();",
                "+",
                "+        HashSet<String> safeTopologyIds = new HashSet<>();",
                "+        for (String blobKey : topologyBlobs.keySet()) {",
                "+            safeTopologyIds.add(ConfigUtils.getIdFromBlobKey(blobKey));",
                "+        }",
                "+",
                "+        //Deleting this early does not hurt anything",
                "+        topologyBasicDownloaded.keySet().removeIf(topoId -> !safeTopologyIds.contains(topoId));",
                "+",
                "+        try {",
                "+            forEachTopologyDistDir((p, topologyId) -> {",
                "+                if (!safeTopologyIds.contains(topologyId)) {",
                "+                    fsOps.deleteIfExists(p.toFile());",
                "+                }",
                "+            });",
                "+        } catch (Exception e) {",
                "+            LOG.error(\"Could not read topology directories for cleanup\", e);",
                "+        }",
                "+",
                "         LOG.debug(\"Resource cleanup: {}\", toClean);",
                "@@ -1156,3 +1140,2 @@ public class AsyncLocalizer implements AutoCloseable {",
                "     }",
                "-",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/BlobChangingCallback.java b/storm-server/src/main/java/org/apache/storm/localizer/BlobChangingCallback.java",
                "new file mode 100644",
                "index 000000000..e99f95b28",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/BlobChangingCallback.java",
                "@@ -0,0 +1,40 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.localizer;",
                "+",
                "+import org.apache.storm.generated.LocalAssignment;",
                "+",
                "+/**",
                "+ * Callback for when a localized blob is going to change.",
                "+ */",
                "+public interface BlobChangingCallback {",
                "+",
                "+    /**",
                "+     * Informs the listener that a blob has changed and is ready to update and replace a localized blob that has been marked as",
                "+     * tied to the life cycle of the worker process.",
                "+     *",
                "+     * If `go.getLatch()` is never called before the method completes it is assumed that",
                "+     * the listener is good with the blob changing.",
                "+     * @param assignment the assignment this resource and callback are registered with.",
                "+     * @param port the port that this resource and callback are registered with.",
                "+     * @param blob the blob that is going to change.",
                "+     * @param go a way to indicate if the listener is ready for the resource to change.",
                "+     */",
                "+    void blobChanging(LocalAssignment assignment, int port, LocallyCachedBlob blob, GoodToGo go);",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/GoodToGo.java b/storm-server/src/main/java/org/apache/storm/localizer/GoodToGo.java",
                "new file mode 100644",
                "index 000000000..04c7a0656",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/GoodToGo.java",
                "@@ -0,0 +1,70 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.localizer;",
                "+",
                "+import java.util.concurrent.CountDownLatch;",
                "+import java.util.concurrent.Future;",
                "+",
                "+/**",
                "+ * Used as a way to give feedback that the listener is ready for the caller to change the blob.",
                "+ * By calling @{link GoodToGo#getLatch()} the listener indicates that it wants to block",
                "+ * changing the blob until the CountDownLatch is triggered with a call to @{link CountDownLatch#countDown()}.",
                "+ */",
                "+public class GoodToGo {",
                "+    public static class GoodToGoLatch {",
                "+        private final CountDownLatch latch;",
                "+        private final Future<Void> doneChangeing;",
                "+        private boolean wasCounted = false;",
                "+",
                "+        public GoodToGoLatch(CountDownLatch latch, Future<Void> doneChangeing) {",
                "+            this.latch = latch;",
                "+            this.doneChangeing = doneChangeing;",
                "+        }",
                "+",
                "+        public synchronized Future<Void> countDown() {",
                "+            if (!wasCounted) {",
                "+                latch.countDown();",
                "+                wasCounted = true;",
                "+            }",
                "+            return doneChangeing;",
                "+        }",
                "+    }",
                "+",
                "+    private final GoodToGoLatch latch;",
                "+    private boolean gotLatch = false;",
                "+",
                "+    public GoodToGo(CountDownLatch latch, Future<Void> doneChangeing) {",
                "+        this.latch = new GoodToGoLatch(latch, doneChangeing);",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the latch and indicate that you want to block the blob being changed.",
                "+     * @return the latch to use when you are ready.",
                "+     */",
                "+    public synchronized GoodToGoLatch getLatch() {",
                "+        gotLatch = true;",
                "+        return latch;",
                "+    }",
                "+",
                "+    synchronized void countDownIfLatchWasNotGotten() {",
                "+        if (!gotLatch) {",
                "+            latch.countDown();",
                "+        }",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocalDownloadedResource.java b/storm-server/src/main/java/org/apache/storm/localizer/LocalDownloadedResource.java",
                "index f019374d9..2d2e87983 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocalDownloadedResource.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocalDownloadedResource.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.localizer;",
                "@@ -22,6 +23,2 @@ import java.util.Set;",
                " import java.util.concurrent.CompletableFuture;",
                "-import java.util.concurrent.ExecutionException;",
                "-import java.util.concurrent.Future;",
                "-import java.util.concurrent.TimeUnit;",
                "-import java.util.concurrent.TimeoutException;",
                "@@ -31,35 +28,10 @@ import org.slf4j.LoggerFactory;",
                "+/**",
                "+ * Used for accounting to keep track of who is waiting for specific resources to be downloaded.",
                "+ */",
                " public class LocalDownloadedResource {",
                "     private static final Logger LOG = LoggerFactory.getLogger(LocalDownloadedResource.class);",
                "-    private static class PortNAssignment {",
                "-        private final int _port;",
                "-        private final LocalAssignment _assignment;",
                "-        ",
                "-        public PortNAssignment(int port, LocalAssignment assignment) {",
                "-            _port = port;",
                "-            _assignment = assignment;",
                "-        }",
                "-        ",
                "-        @Override",
                "-        public boolean equals(Object other) {",
                "-            if (!(other instanceof PortNAssignment)) {",
                "-                return false;",
                "-            }",
                "-            PortNAssignment pna = (PortNAssignment) other;",
                "-            return pna._port == _port && _assignment.equals(pna._assignment); ",
                "-        }",
                "-        ",
                "-        @Override",
                "-        public int hashCode() {",
                "-            return (17 * _port) + _assignment.hashCode();",
                "-        }",
                "-        ",
                "-        @Override",
                "-        public String toString() {",
                "-            return \"{\"+ _port + \" \" + _assignment +\"}\";",
                "-        }",
                "-    }",
                "-    private final CompletableFuture<Void> _pending;",
                "-    private final Set<PortNAssignment> _references;",
                "-    private boolean _isDone;",
                "+    private final CompletableFuture<Void> pending;",
                "+    private final Set<PortAndAssignment> references;",
                "+    private boolean isDone;",
                "@@ -67,5 +39,5 @@ public class LocalDownloadedResource {",
                "     public LocalDownloadedResource(CompletableFuture<Void> pending) {",
                "-        _pending = pending;",
                "-        _references = new HashSet<>();",
                "-        _isDone = false;",
                "+        this.pending = pending;",
                "+        references = new HashSet<>();",
                "+        isDone = false;",
                "     }",
                "@@ -79,7 +51,7 @@ public class LocalDownloadedResource {",
                "     public synchronized CompletableFuture<Void> reserve(int port, LocalAssignment la) {",
                "-        PortNAssignment pna = new PortNAssignment(port, la);",
                "-        if (!_references.add(pna)) {",
                "-            LOG.warn(\"Resources {} already reserved {} for this topology\", pna, _references);",
                "+        PortAndAssignment pna = new PortAndAssignment(port, la);",
                "+        if (!references.add(pna)) {",
                "+            LOG.warn(\"Resources {} already reserved {} for this topology\", pna, references);",
                "         }",
                "-        return _pending;",
                "+        return pending;",
                "     }",
                "@@ -93,6 +65,6 @@ public class LocalDownloadedResource {",
                "     public synchronized boolean release(int port, LocalAssignment la) {",
                "-        PortNAssignment pna = new PortNAssignment(port, la);",
                "-        boolean ret = _references.remove(pna);",
                "-        if (ret && _references.isEmpty()) {",
                "-            _isDone = true;",
                "+        PortAndAssignment pna = new PortAndAssignment(port, la);",
                "+        boolean ret = references.remove(pna);",
                "+        if (ret && references.isEmpty()) {",
                "+            isDone = true;",
                "         }",
                "@@ -106,3 +78,3 @@ public class LocalDownloadedResource {",
                "     public synchronized boolean isDone() {",
                "-        return _isDone;",
                "+        return isDone;",
                "     }",
                "@@ -111,3 +83,3 @@ public class LocalDownloadedResource {",
                "     public String toString() {",
                "-        return _references.toString();",
                "+        return references.toString();",
                "     }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java b/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java",
                "index b6b5cd18e..7241976b2 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java",
                "@@ -129,2 +129,15 @@ public class LocalizedResource {",
                "+  @Override",
                "+  public boolean equals(Object other) {",
                "+    if (other instanceof LocalizedResource) {",
                "+        LocalizedResource l = (LocalizedResource)other;",
                "+        return _key.equals(l._key) && _uncompressed == l._uncompressed && _localPath.equals(l._localPath);",
                "+    }",
                "+    return false;",
                "+  }",
                "+",
                "+  @Override",
                "+  public int hashCode() {",
                "+     return _key.hashCode() + Boolean.hashCode(_uncompressed) + _localPath.hashCode();",
                "+  }",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java b/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java",
                "index 9f42b47b4..826bf9869 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java",
                "@@ -19,2 +19,5 @@ package org.apache.storm.localizer;",
                "+import com.google.common.annotations.VisibleForTesting;",
                "+import java.nio.file.Path;",
                "+import java.util.concurrent.ConcurrentHashMap;",
                " import org.apache.commons.io.FileUtils;",
                "@@ -31,2 +34,3 @@ import java.util.SortedMap;",
                " import java.util.TreeMap;",
                "+",
                " /**",
                "@@ -36,105 +40,317 @@ import java.util.TreeMap;",
                " public class LocalizedResourceRetentionSet {",
                "-  public static final Logger LOG = LoggerFactory.getLogger(LocalizedResourceRetentionSet.class);",
                "-  private long _delSize;",
                "-  private long _currentSize;",
                "-  // targetSize in Bytes",
                "-  private long _targetSize;",
                "-  private final SortedMap<LocalizedResource, LocalizedResourceSet> _noReferences;",
                "-",
                "-  LocalizedResourceRetentionSet(long targetSize) {",
                "-    this(targetSize, new LRUComparator());",
                "-  }",
                "-",
                "-  LocalizedResourceRetentionSet(long targetSize, Comparator<? super LocalizedResource> cmp) {",
                "-    this(targetSize, new TreeMap<LocalizedResource, LocalizedResourceSet>(cmp));",
                "-  }",
                "-",
                "-  LocalizedResourceRetentionSet(long targetSize,",
                "-                                SortedMap<LocalizedResource, LocalizedResourceSet> retain) {",
                "-    this._noReferences = retain;",
                "-    this._targetSize = targetSize;",
                "-  }",
                "-",
                "-  // for testing",
                "-  protected int getSizeWithNoReferences() {",
                "-    return _noReferences.size();",
                "-  }",
                "-",
                "-  protected void addResourcesForSet(Iterator<LocalizedResource> setIter, LocalizedResourceSet set) {",
                "-    for (Iterator<LocalizedResource> iter = setIter; setIter.hasNext(); ) {",
                "-      LocalizedResource lrsrc = iter.next();",
                "-      _currentSize += lrsrc.getSize();",
                "-      if (lrsrc.getRefCount() > 0) {",
                "-        // always retain resources in use",
                "-        continue;",
                "-      }",
                "-      LOG.debug(\"adding {} to be checked for cleaning\", lrsrc.getKey());",
                "-      _noReferences.put(lrsrc, set);",
                "-    }",
                "-  }",
                "-",
                "-  public void addResources(LocalizedResourceSet set) {",
                "-    addResourcesForSet(set.getLocalFilesIterator(), set);",
                "-    addResourcesForSet(set.getLocalArchivesIterator(), set);",
                "-  }",
                "-",
                "-  public void cleanup() {",
                "-    LOG.debug(\"cleanup target size: {} current size is: {}\", _targetSize, _currentSize);",
                "-    for (Iterator<Map.Entry<LocalizedResource, LocalizedResourceSet>> i =",
                "-             _noReferences.entrySet().iterator();",
                "-         _currentSize - _delSize > _targetSize && i.hasNext();) {",
                "-      Map.Entry<LocalizedResource, LocalizedResourceSet> rsrc = i.next();",
                "-      LocalizedResource resource = rsrc.getKey();",
                "-      LocalizedResourceSet set = rsrc.getValue();",
                "-      if (resource != null && set.remove(resource)) {",
                "-        if (deleteResource(resource)) {",
                "-          _delSize += resource.getSize();",
                "-          LOG.info(\"deleting: \" + resource.getFilePath() + \" size of: \" + resource.getSize());",
                "-          i.remove();",
                "-        } else {",
                "-          // since it failed to delete add it back so it gets retried",
                "-          set.add(resource.getKey(), resource, resource.isUncompressed());",
                "-        }",
                "-      }",
                "-    }",
                "-  }",
                "-",
                "-  protected boolean deleteResource(LocalizedResource resource){",
                "-    try {",
                "-      String fileWithVersion = resource.getFilePathWithVersion();",
                "-      String currentSymlinkName = resource.getCurrentSymlinkPath();",
                "-      String versionFile = resource.getVersionFilePath();",
                "-      File deletePath = new File(fileWithVersion);",
                "-      if (resource.isUncompressed()) {",
                "-        // this doesn't follow symlinks, which is what we want",
                "-        FileUtils.deleteDirectory(deletePath);",
                "-      } else {",
                "-        Files.delete(deletePath.toPath());",
                "-      }",
                "-      Files.delete(new File(currentSymlinkName).toPath());",
                "-      Files.delete(new File(versionFile).toPath());",
                "-      return true;",
                "-    } catch (IOException e) {",
                "-      LOG.warn(\"Could not delete: {}\", resource.getFilePath());",
                "-    }",
                "-    return false;",
                "-  }",
                "-",
                "-  @Override",
                "-  public String toString() {",
                "-    StringBuilder sb = new StringBuilder();",
                "-    sb.append(\"Cache: \").append(_currentSize).append(\", \");",
                "-    sb.append(\"Deleted: \").append(_delSize);",
                "-    return sb.toString();",
                "-  }",
                "-",
                "-  static class LRUComparator implements Comparator<LocalizedResource> {",
                "-    public int compare(LocalizedResource r1, LocalizedResource r2) {",
                "-      long ret = r1.getLastAccessTime() - r2.getLastAccessTime();",
                "-      if (0 == ret) {",
                "-        return System.identityHashCode(r1) - System.identityHashCode(r2);",
                "-      }",
                "-      return ret > 0 ? 1 : -1;",
                "-    }",
                "-  }",
                "+    public static final Logger LOG = LoggerFactory.getLogger(LocalizedResourceRetentionSet.class);",
                "+    private long delSize;",
                "+    private long currentSize;",
                "+    // targetSize in Bytes",
                "+    private long targetSize;",
                "+    @VisibleForTesting",
                "+    final SortedMap<ComparableResource, CleanableResourceSet> noReferences;",
                "+    private int resourceCount = 0;",
                "+",
                "+    LocalizedResourceRetentionSet(long targetSize) {",
                "+        this(targetSize, new LRUComparator());",
                "+    }",
                "+",
                "+    LocalizedResourceRetentionSet(long targetSize, Comparator<? super ComparableResource> cmp) {",
                "+        this(targetSize, new TreeMap<>(cmp));",
                "+    }",
                "+",
                "+    LocalizedResourceRetentionSet(long targetSize,",
                "+                                  SortedMap<ComparableResource, CleanableResourceSet> retain) {",
                "+        this.noReferences = retain;",
                "+        this.targetSize = targetSize;",
                "+    }",
                "+",
                "+    // for testing",
                "+    protected int getSizeWithNoReferences() {",
                "+        return noReferences.size();",
                "+    }",
                "+",
                "+    protected void addResourcesForSet(Iterator<LocalizedResource> setIter, LocalizedResourceSet set) {",
                "+        CleanableLocalizedResourceSet cleanset = new CleanableLocalizedResourceSet(set);",
                "+        for (Iterator<LocalizedResource> iter = setIter; setIter.hasNext(); ) {",
                "+            LocalizedResource lrsrc = iter.next();",
                "+            currentSize += lrsrc.getSize();",
                "+            resourceCount ++;",
                "+            if (lrsrc.getRefCount() > 0) {",
                "+                // always retain resources in use",
                "+                continue;",
                "+            }",
                "+            noReferences.put(new LocalizedBlobComparableResource(lrsrc), cleanset);",
                "+        }",
                "+    }",
                "+",
                "+    public void addResources(LocalizedResourceSet set) {",
                "+        addResourcesForSet(set.getLocalFilesIterator(), set);",
                "+        addResourcesForSet(set.getLocalArchivesIterator(), set);",
                "+    }",
                "+",
                "+    public void addResources(ConcurrentHashMap<String, LocallyCachedBlob> blobs) {",
                "+        CleanableLocalizedLocallyCachedBlob set = new CleanableLocalizedLocallyCachedBlob(blobs);",
                "+        for (LocallyCachedBlob b: blobs.values()) {",
                "+            currentSize += b.getSizeOnDisk();",
                "+            resourceCount ++;",
                "+            if (b.isUsed()) {",
                "+                // always retain resources in use",
                "+                continue;",
                "+            }",
                "+            LocallyCachedBlobComparableResource cb = new LocallyCachedBlobComparableResource(b);",
                "+            noReferences.put(cb, set);",
                "+        }",
                "+    }",
                "+",
                "+    public void cleanup() {",
                "+        LOG.debug(\"cleanup target size: {} current size is: {}\", targetSize, currentSize);",
                "+        for (Iterator<Map.Entry<ComparableResource, CleanableResourceSet>> i =",
                "+             noReferences.entrySet().iterator();",
                "+             currentSize - delSize > targetSize && i.hasNext();) {",
                "+            Map.Entry<ComparableResource, CleanableResourceSet> rsrc = i.next();",
                "+            ComparableResource resource = rsrc.getKey();",
                "+            CleanableResourceSet set = rsrc.getValue();",
                "+            if (resource != null && set.remove(resource)) {",
                "+                if (set.deleteUnderlyingResource(resource)) {",
                "+                    delSize += resource.getSize();",
                "+                    LOG.info(\"deleting: {} with size of: {}\", resource.getNameForDebug(), resource.getSize());",
                "+                    i.remove();",
                "+                } else {",
                "+                    // since it failed to delete add it back so it gets retried",
                "+                    set.add(resource.getKey(), resource);",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    public boolean deleteResource(CleanableResourceSet set, ComparableResource resource) {",
                "+        return set.deleteUnderlyingResource(resource);",
                "+    }",
                "+",
                "+    public long getCurrentSize() {",
                "+        return currentSize;",
                "+    }",
                "+",
                "+    public int getResourceCount() {",
                "+        return resourceCount;",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        StringBuilder sb = new StringBuilder();",
                "+        sb.append(\"Cache: \").append(currentSize).append(\", \");",
                "+        sb.append(\"Deleted: \").append(delSize);",
                "+        return sb.toString();",
                "+    }",
                "+",
                "+    interface ComparableResource {",
                "+        long getLastAccessTime();",
                "+",
                "+        long getSize();",
                "+",
                "+        String getNameForDebug();",
                "+",
                "+        String getKey();",
                "+    }",
                "+",
                "+    interface CleanableResourceSet {",
                "+        boolean remove(ComparableResource resource);",
                "+",
                "+        void add(String key, ComparableResource resource);",
                "+",
                "+        boolean deleteUnderlyingResource(ComparableResource resource);",
                "+    }",
                "+",
                "+    public static class LocallyCachedBlobComparableResource implements ComparableResource {",
                "+        private final LocallyCachedBlob blob;",
                "+",
                "+        public LocallyCachedBlobComparableResource(LocallyCachedBlob blob) {",
                "+            this.blob = blob;",
                "+        }",
                "+",
                "+        @Override",
                "+        public long getLastAccessTime() {",
                "+            return blob.getLastUsed();",
                "+        }",
                "+",
                "+        @Override",
                "+        public long getSize() {",
                "+            return blob.getSizeOnDisk();",
                "+        }",
                "+",
                "+        @Override",
                "+        public String getNameForDebug() {",
                "+            return blob.getKey();",
                "+        }",
                "+",
                "+        @Override",
                "+        public String getKey() {",
                "+            return blob.getKey();",
                "+        }",
                "+",
                "+        @Override",
                "+        public String toString() {",
                "+            return blob.toString();",
                "+        }",
                "+",
                "+        @Override",
                "+        public boolean equals(Object other) {",
                "+            if (other instanceof LocallyCachedBlobComparableResource) {",
                "+                return blob.equals(((LocallyCachedBlobComparableResource) other).blob);",
                "+            }",
                "+            return false;",
                "+        }",
                "+",
                "+        @Override",
                "+        public int hashCode() {",
                "+            return blob.hashCode();",
                "+        }",
                "+    }",
                "+",
                "+    private static class CleanableLocalizedLocallyCachedBlob implements CleanableResourceSet {",
                "+        private final ConcurrentHashMap<String, LocallyCachedBlob> blobs;",
                "+",
                "+        public CleanableLocalizedLocallyCachedBlob(ConcurrentHashMap<String, LocallyCachedBlob> blobs) {",
                "+            this.blobs = blobs;",
                "+        }",
                "+",
                "+        @Override",
                "+        public boolean remove(ComparableResource resource) {",
                "+            if (!(resource instanceof LocallyCachedBlobComparableResource)) {",
                "+                throw new IllegalStateException(resource + \" must be a LocallyCachedBlobComparableResource\");",
                "+            }",
                "+            LocallyCachedBlob blob = ((LocallyCachedBlobComparableResource)resource).blob;",
                "+            synchronized (blob) {",
                "+                if (!blob.isUsed()) {",
                "+                    try {",
                "+                        blob.completelyRemove();",
                "+                    } catch (Exception e) {",
                "+                        LOG.warn(\"Tried to remove {} but failed with\", blob, e);",
                "+                    }",
                "+                    blobs.remove(blob.getKey());",
                "+                    return true;",
                "+                }",
                "+                return false;",
                "+            }",
                "+        }",
                "+",
                "+        @Override",
                "+        public void add(String key, ComparableResource resource) {",
                "+            ///NOOP not used",
                "+        }",
                "+",
                "+        @Override",
                "+        public boolean deleteUnderlyingResource(ComparableResource resource) {",
                "+            //NOOP not used",
                "+            return true;",
                "+        }",
                "+    }",
                "+",
                "+    private static class LocalizedBlobComparableResource implements ComparableResource {",
                "+        private final LocalizedResource resource;",
                "+",
                "+        private LocalizedBlobComparableResource(LocalizedResource resource) {",
                "+            this.resource = resource;",
                "+        }",
                "+",
                "+        @Override",
                "+        public long getLastAccessTime() {",
                "+            return resource.getLastAccessTime();",
                "+        }",
                "+",
                "+        @Override",
                "+        public long getSize() {",
                "+            return resource.getSize();",
                "+        }",
                "+",
                "+        @Override",
                "+        public String getNameForDebug() {",
                "+            return resource.getFilePath();",
                "+        }",
                "+",
                "+        @Override",
                "+        public String getKey() {",
                "+            return resource.getKey();",
                "+        }",
                "+",
                "+        @Override",
                "+        public String toString() {",
                "+            return resource.getKey() + \" at \" + resource.getFilePathWithVersion();",
                "+        }",
                "+",
                "+        @Override",
                "+        public boolean equals(Object other) {",
                "+            if (other instanceof LocalizedBlobComparableResource) {",
                "+                return resource.equals(((LocalizedBlobComparableResource) other).resource);",
                "+            }",
                "+            return false;",
                "+        }",
                "+",
                "+        @Override",
                "+        public int hashCode() {",
                "+            return resource.hashCode();",
                "+        }",
                "+    }",
                "+",
                "+    private static class CleanableLocalizedResourceSet implements CleanableResourceSet {",
                "+        private final LocalizedResourceSet set;",
                "+",
                "+        public CleanableLocalizedResourceSet(LocalizedResourceSet set) {",
                "+            this.set = set;",
                "+        }",
                "+",
                "+        @Override",
                "+        public boolean remove(ComparableResource resource) {",
                "+            if (!(resource instanceof LocalizedBlobComparableResource)) {",
                "+                throw new IllegalStateException(resource + \" must be a LocalizedBlobComparableResource\");",
                "+            }",
                "+            return set.remove(((LocalizedBlobComparableResource)resource).resource);",
                "+        }",
                "+",
                "+        @Override",
                "+        public void add(String key, ComparableResource resource) {",
                "+            if (!(resource instanceof LocalizedBlobComparableResource)) {",
                "+                throw new IllegalStateException(resource + \" must be a LocalizedBlobComparableResource\");",
                "+            }",
                "+            LocalizedResource r = ((LocalizedBlobComparableResource)resource).resource;",
                "+            set.add(key, r, r.isUncompressed());",
                "+        }",
                "+",
                "+        @Override",
                "+        public boolean deleteUnderlyingResource(ComparableResource resource) {",
                "+            if (resource instanceof LocalizedBlobComparableResource) {",
                "+                LocalizedResource lr = ((LocalizedBlobComparableResource) resource).resource;",
                "+                try {",
                "+                    Path fileWithVersion = new File(lr.getFilePathWithVersion()).toPath();",
                "+                    Path currentSymLink = new File(lr.getCurrentSymlinkPath()).toPath();",
                "+                    Path versionFile = new File(lr.getVersionFilePath()).toPath();",
                "+",
                "+                    if (lr.isUncompressed()) {",
                "+                        if (Files.exists(fileWithVersion)) {",
                "+                            // this doesn't follow symlinks, which is what we want",
                "+                            FileUtils.deleteDirectory(fileWithVersion.toFile());",
                "+                        }",
                "+                    } else {",
                "+                        Files.deleteIfExists(fileWithVersion);",
                "+                    }",
                "+                    Files.deleteIfExists(currentSymLink);",
                "+                    Files.deleteIfExists(versionFile);",
                "+                    return true;",
                "+                } catch (IOException e) {",
                "+                    LOG.warn(\"Could not delete: {}\", resource.getNameForDebug(), e);",
                "+                }",
                "+                return false;",
                "+            }  else {",
                "+                throw new IllegalArgumentException(\"Don't know how to handle a \" + resource.getClass());",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    static class LRUComparator implements Comparator<ComparableResource> {",
                "+        public int compare(ComparableResource r1, ComparableResource r2) {",
                "+            long ret = r1.getLastAccessTime() - r2.getLastAccessTime();",
                "+            if (0 == ret) {",
                "+                return System.identityHashCode(r1) - System.identityHashCode(r2);",
                "+            }",
                "+            return ret > 0 ? 1 : -1;",
                "+        }",
                "+    }",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java",
                "new file mode 100644",
                "index 000000000..c09108da1",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java",
                "@@ -0,0 +1,220 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * <p>",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ * <p>",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.localizer;",
                "+",
                "+import java.io.IOException;",
                "+import java.nio.file.Files;",
                "+import java.nio.file.LinkOption;",
                "+import java.nio.file.Path;",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+import java.util.concurrent.CompletableFuture;",
                "+import java.util.concurrent.CountDownLatch;",
                "+import java.util.concurrent.TimeUnit;",
                "+import org.apache.storm.blobstore.BlobStore;",
                "+import org.apache.storm.blobstore.ClientBlobStore;",
                "+import org.apache.storm.generated.AuthorizationException;",
                "+import org.apache.storm.generated.KeyNotFoundException;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * Represents a blob that is cached locally on disk by the supervisor.",
                "+ */",
                "+public abstract class LocallyCachedBlob {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(LocallyCachedBlob.class);",
                "+    public static final long NOT_DOWNLOADED_VERSION = -1;",
                "+    // A callback that does nothing.",
                "+    private static final BlobChangingCallback NOOP_CB = (assignment, port, resource, go) -> {};",
                "+",
                "+    private long lastUsed = System.currentTimeMillis();",
                "+    private final Map<PortAndAssignment, BlobChangingCallback> references = new HashMap<>();",
                "+    private final String blobDescription;",
                "+    private final String blobKey;",
                "+    private CompletableFuture<Void> doneUpdating = null;",
                "+",
                "+    /**",
                "+     * Create a new LocallyCachedBlob.",
                "+     * @param blobDescription a description of the blob this represents.  Typically it should at least be the blob key, but ideally also",
                "+     * include if it is an archive or not, what user or topology it is for, or if it is a storm.jar etc.",
                "+     */",
                "+    protected LocallyCachedBlob(String blobDescription, String blobKey) {",
                "+        this.blobDescription = blobDescription;",
                "+        this.blobKey = blobKey;",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the version of the blob cached locally.  If the version is unknown or it has not been downloaded NOT_DOWNLOADED_VERSION",
                "+     * should be returned.",
                "+     * PRECONDITION: this can only be called with a lock on this instance held.",
                "+     */",
                "+    public abstract long getLocalVersion();",
                "+",
                "+    /**",
                "+     * Get the version of the blob in the blob store.",
                "+     * PRECONDITION: this can only be called with a lock on this instance held.",
                "+     */",
                "+    public abstract long getRemoteVersion(ClientBlobStore store) throws KeyNotFoundException, AuthorizationException;",
                "+",
                "+    /**",
                "+     * Download the latest version to a temp location. This may also include unzipping some or all of the data to a temp location.",
                "+     * PRECONDITION: this can only be called with a lock on this instance held.",
                "+     * @param store the store to us to download the data.",
                "+     * @return the version that was downloaded.",
                "+     */",
                "+    public abstract long downloadToTempLocation(ClientBlobStore store) throws IOException, KeyNotFoundException, AuthorizationException;",
                "+",
                "+    /**",
                "+     * Commit the new version and make it available for the end user.",
                "+     * PRECONDITION: uncompressToTempLocationIfNeeded will have been called.",
                "+     * PRECONDITION: this can only be called with a lock on this instance held.",
                "+     * @param version the version of the blob to commit.",
                "+     */",
                "+    public abstract void commitNewVersion(long version) throws IOException;",
                "+",
                "+    /**",
                "+     * Clean up any temporary files.  This will be called after updating a blob, either successfully or if an error has occured.",
                "+     * The goal is to find any files that may be left over and remove them so space is not leaked.",
                "+     * PRECONDITION: this can only be called with a lock on this instance held.",
                "+     */",
                "+    public abstract void cleanupOrphanedData() throws IOException;",
                "+",
                "+    /**",
                "+     * Completely remove anything that is cached locally for this blob and all tracking files also stored for it.",
                "+     * This will be called after the blob was determined to no longer be needed in the cache.",
                "+     * PRECONDITION: this can only be called with a lock on this instance held.",
                "+     */",
                "+    public abstract void completelyRemove() throws IOException;",
                "+",
                "+    /**",
                "+     * Get the amount of disk space that is used by this blob.  If the blob is uncompressed it should be the sum of the space used by all",
                "+     * of the uncompressed files.  In general this will not be called with any locks held so it is a good idea to cache it and updated it",
                "+     * when committing a new version.",
                "+     */",
                "+    public abstract long getSizeOnDisk();",
                "+",
                "+    /**",
                "+     * Updates the last updated time.  This should be called when references are added or removed.",
                "+     */",
                "+    private synchronized void touch() {",
                "+        lastUsed = System.currentTimeMillis();",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the last time that this used for LRU calculations.",
                "+     */",
                "+    public synchronized long getLastUsed() {",
                "+        return lastUsed;",
                "+    }",
                "+",
                "+    /**",
                "+     * Return true if this blob is actively being used, else false (meaning it can be deleted, but might not be).",
                "+     */",
                "+    public synchronized boolean isUsed() {",
                "+        return !references.isEmpty();",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the size of p in bytes.",
                "+     * @param p the path to read.",
                "+     * @return the size of p in bytes.",
                "+     */",
                "+    protected long getSizeOnDisk(Path p) throws IOException {",
                "+        if (!Files.exists(p)) {",
                "+            return 0;",
                "+        } else if (Files.isRegularFile(p)) {",
                "+            return Files.size(p);",
                "+        } else {",
                "+            //We will not follow sym links",
                "+            return Files.walk(p)",
                "+                .filter((subp) -> Files.isRegularFile(subp, LinkOption.NOFOLLOW_LINKS))",
                "+                .mapToLong((subp) -> {",
                "+                    try {",
                "+                        return Files.size(subp);",
                "+                    } catch (IOException e) {",
                "+                        LOG.warn(\"Could not get the size of \");",
                "+                    }",
                "+                    return 0;",
                "+                }).sum();",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Mark that a given port and assignemnt are using this.",
                "+     * @param pna the slot and assignment that are using this blob.",
                "+     * @param cb an optional callback indicating that they want to know/synchronize when a blob is updated.",
                "+     */",
                "+    public void addReference(final PortAndAssignment pna, BlobChangingCallback cb) {",
                "+        if (cb == null) {",
                "+            cb = NOOP_CB;",
                "+        }",
                "+        if (references.put(pna, cb) != null) {",
                "+            LOG.warn(\"{} already has a reservation for {}\", pna, blobDescription);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Removes a reservation for this blob from a given slot and assignemnt.",
                "+     * @param pna the slot + assignment that no longer needs this blob.",
                "+     */",
                "+    public void removeReference(final PortAndAssignment pna) {",
                "+        if (references.remove(pna) == null) {",
                "+            LOG.warn(\"{} had no reservation for {}\", pna, blobDescription);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Inform all of the callbacks that a change is going to happen and then wait for",
                "+     * them to all get back that it is OK to make that change.",
                "+     */",
                "+    public synchronized void informAllOfChangeAndWaitForConsensus() {",
                "+        CountDownLatch cdl = new CountDownLatch(references.size());",
                "+        doneUpdating = new CompletableFuture<>();",
                "+        for (Map.Entry<PortAndAssignment, BlobChangingCallback> entry : references.entrySet()) {",
                "+            GoodToGo gtg = new GoodToGo(cdl, doneUpdating);",
                "+            try {",
                "+                PortAndAssignment pna = entry.getKey();",
                "+                BlobChangingCallback cb = entry.getValue();",
                "+                //TODO we probably want to not use this, or make it just return something that has less power to modify things",
                "+                cb.blobChanging(pna.getAssignment(), pna.getPort(), this, gtg);",
                "+            } finally {",
                "+                gtg.countDownIfLatchWasNotGotten();",
                "+            }",
                "+        }",
                "+        try {",
                "+            cdl.await(3, TimeUnit.MINUTES);",
                "+        } catch (InterruptedException e) {",
                "+            //TODO need to think about error handling here in general.",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Inform all of the callbacks that the change to the blob is complete.",
                "+     */",
                "+    public synchronized void informAllChangeComplete() {",
                "+        doneUpdating.complete(null);",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the key for this blob.",
                "+     */",
                "+    public String getKey() {",
                "+        return blobKey;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java",
                "new file mode 100644",
                "index 000000000..35371b537",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java",
                "@@ -0,0 +1,364 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * <p>",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ * <p>",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.localizer;",
                "+",
                "+import java.io.File;",
                "+import java.io.FileOutputStream;",
                "+import java.io.IOException;",
                "+import java.io.InputStream;",
                "+import java.io.OutputStream;",
                "+import java.net.JarURLConnection;",
                "+import java.net.URL;",
                "+import java.nio.file.DirectoryStream;",
                "+import java.nio.file.Files;",
                "+import java.nio.file.Path;",
                "+import java.nio.file.Paths;",
                "+import java.util.Enumeration;",
                "+import java.util.Map;",
                "+import java.util.jar.JarEntry;",
                "+import java.util.jar.JarFile;",
                "+import java.util.regex.Matcher;",
                "+import java.util.regex.Pattern;",
                "+",
                "+import org.apache.commons.io.FileUtils;",
                "+import org.apache.commons.io.IOUtils;",
                "+import org.apache.storm.blobstore.ClientBlobStore;",
                "+import org.apache.storm.blobstore.InputStreamWithMeta;",
                "+import org.apache.storm.daemon.supervisor.AdvancedFSOps;",
                "+import org.apache.storm.generated.AuthorizationException;",
                "+import org.apache.storm.generated.KeyNotFoundException;",
                "+import org.apache.storm.utils.ConfigUtils;",
                "+import org.apache.storm.utils.ServerConfigUtils;",
                "+import org.apache.storm.utils.ServerUtils;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * A locally cached blob for the topology.  storm.jar, stormcode.ser, or stormconf.ser.",
                "+ * The version number of the blob's file will be stored in `${basename}.version`",
                "+ */",
                "+public class LocallyCachedTopologyBlob extends LocallyCachedBlob {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(LocallyCachedTopologyBlob.class);",
                "+    public static final long LOCAL_MODE_JAR_VERSION = 1;",
                "+",
                "+    private static String resourcesJar() throws IOException {",
                "+        String path = ServerUtils.currentClasspath();",
                "+        if (path == null) {",
                "+            return null;",
                "+        }",
                "+",
                "+        for (String jpath : path.split(File.pathSeparator)) {",
                "+            if (jpath.endsWith(\".jar\")) {",
                "+                if (ServerUtils.zipDoesContainDir(jpath, ServerConfigUtils.RESOURCES_SUBDIR)) {",
                "+                    return jpath;",
                "+                }",
                "+            }",
                "+        }",
                "+        return null;",
                "+    }",
                "+",
                "+    public enum TopologyBlobType {",
                "+        TOPO_JAR(\"stormjar.jar\", \"-stormjar.jar\", \"resources\"),",
                "+        TOPO_CODE(\"stormcode.ser\", \"-stormcode.ser\", null),",
                "+        TOPO_CONF(\"stormconf.ser\", \"-stormconf.ser\", null);",
                "+",
                "+        private final String fileName;",
                "+        private final String keySuffix;",
                "+        private final String extractionDir;",
                "+",
                "+        TopologyBlobType(String fileName, String keySuffix, String extractionDir) {",
                "+            this.fileName = fileName;",
                "+            this.keySuffix = keySuffix;",
                "+            this.extractionDir = extractionDir;",
                "+        }",
                "+",
                "+        public String getFileName() {",
                "+            return fileName;",
                "+        }",
                "+",
                "+        public String getTempFileName(long version) {",
                "+            return fileName + \".\" + version;",
                "+        }",
                "+",
                "+        public String getVersionFileName() {",
                "+            return fileName + \".version\";",
                "+        }",
                "+",
                "+        public String getKey(String topologyId) {",
                "+            return topologyId + keySuffix;",
                "+        }",
                "+",
                "+        public boolean needsExtraction() {",
                "+            return extractionDir != null;",
                "+        }",
                "+",
                "+        public String getExtractionDir() {",
                "+            return extractionDir;",
                "+        }",
                "+",
                "+        public String getTempExtractionDir(long version) {",
                "+            return extractionDir + \".\" + version;",
                "+        }",
                "+    };",
                "+",
                "+    private final TopologyBlobType type;",
                "+    private final String topologyId;",
                "+    private final boolean isLocalMode;",
                "+    private final Path topologyBasicBlobsRootDir;",
                "+    private final AdvancedFSOps fsOps;",
                "+    private final Map<String, Object> conf;",
                "+    private volatile long version = NOT_DOWNLOADED_VERSION;",
                "+    private volatile long size = 0;",
                "+",
                "+    /**",
                "+     * Create a new LocallyCachedBlob.",
                "+     *",
                "+     * @param topologyId the ID of the topology.",
                "+     * @param type the type of the blob.",
                "+     */",
                "+    protected LocallyCachedTopologyBlob(final String topologyId, final boolean isLocalMode, final Map<String, Object> conf,",
                "+                                        final AdvancedFSOps fsOps, final TopologyBlobType type) throws IOException {",
                "+        super(topologyId + \" \" + type.getFileName(), type.getKey(topologyId));",
                "+        this.topologyId = topologyId;",
                "+        this.type = type;",
                "+        this.isLocalMode = isLocalMode;",
                "+        this.fsOps = fsOps;",
                "+        this.conf = conf;",
                "+        topologyBasicBlobsRootDir = Paths.get(ConfigUtils.supervisorStormDistRoot(conf, topologyId));",
                "+        readVersion();",
                "+        updateSizeOnDisk();",
                "+    }",
                "+",
                "+    private void updateSizeOnDisk() throws IOException {",
                "+        long total = getSizeOnDisk(topologyBasicBlobsRootDir.resolve(type.getFileName()));",
                "+        if (type.needsExtraction()) {",
                "+            total += getSizeOnDisk(topologyBasicBlobsRootDir.resolve(type.getExtractionDir()));",
                "+        }",
                "+        size = total;",
                "+    }",
                "+",
                "+    private void readVersion() throws IOException {",
                "+        Path versionFile = topologyBasicBlobsRootDir.resolve(type.getVersionFileName());",
                "+        if (!fsOps.fileExists(versionFile)) {",
                "+            version = NOT_DOWNLOADED_VERSION;",
                "+        } else {",
                "+            String ver = FileUtils.readFileToString(versionFile.toFile(), \"UTF8\").trim();",
                "+            version = Long.parseLong(ver);",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public long getLocalVersion() {",
                "+        LOG.debug(\"LOCAL VERSION {}/{} is {}\", type, topologyId, version);",
                "+        return version;",
                "+    }",
                "+",
                "+    @Override",
                "+    public long getRemoteVersion(ClientBlobStore store) throws KeyNotFoundException, AuthorizationException {",
                "+        if (isLocalMode && type == TopologyBlobType.TOPO_JAR) {",
                "+            LOG.debug(\"REMOTE VERSION LOCAL JAR {}\", LOCAL_MODE_JAR_VERSION);",
                "+            return LOCAL_MODE_JAR_VERSION;",
                "+        }",
                "+        return store.getBlobMeta(type.getKey(topologyId)).get_version();",
                "+    }",
                "+",
                "+    @Override",
                "+    public long downloadToTempLocation(ClientBlobStore store)",
                "+        throws IOException, KeyNotFoundException, AuthorizationException {",
                "+        if (isLocalMode && type == TopologyBlobType.TOPO_JAR) {",
                "+            LOG.debug(\"DOWNLOADING LOCAL JAR to TEMP LOCATION... {}\", topologyId);",
                "+            //This is a special case where the jar was not uploaded so we will not download it (it is already on the classpath)",
                "+            ClassLoader classloader = Thread.currentThread().getContextClassLoader();",
                "+            String resourcesJar = resourcesJar();",
                "+            URL url = classloader.getResource(ServerConfigUtils.RESOURCES_SUBDIR);",
                "+            Path extractionDest = topologyBasicBlobsRootDir.resolve(type.getTempExtractionDir(LOCAL_MODE_JAR_VERSION));",
                "+            if (resourcesJar != null) {",
                "+                LOG.info(\"Extracting resources from jar at {} to {}\", resourcesJar, extractionDest);",
                "+                extractDirFromJar(resourcesJar, ServerConfigUtils.RESOURCES_SUBDIR, extractionDest);",
                "+            } else if (url != null) {",
                "+                LOG.info(\"Copying resources at {} to {}\", url, extractionDest);",
                "+                if (\"jar\".equals(url.getProtocol())) {",
                "+                    JarURLConnection urlConnection = (JarURLConnection) url.openConnection();",
                "+                    extractDirFromJar(urlConnection.getJarFileURL().getFile(), ServerConfigUtils.RESOURCES_SUBDIR, extractionDest);",
                "+                } else {",
                "+                    fsOps.copyDirectory(new File(url.getFile()), extractionDest.toFile());",
                "+                }",
                "+            }",
                "+            return LOCAL_MODE_JAR_VERSION;",
                "+        }",
                "+",
                "+        long newVersion;",
                "+        Path tmpLocation;",
                "+        String key = type.getKey(topologyId);",
                "+        try (InputStreamWithMeta in = store.getBlob(key)) {",
                "+            newVersion = in.getVersion();",
                "+            long expectedSize = in.getFileLength();",
                "+            if (newVersion == version) {",
                "+                throw new RuntimeException(\"The version did not change, but we tried to download it. \" + version + \" \" + key);",
                "+            }",
                "+            tmpLocation = topologyBasicBlobsRootDir.resolve(type.getTempFileName(newVersion));",
                "+            long totalRead = 0;",
                "+            //Make sure the parent directory is there and ready to go",
                "+            fsOps.forceMkdir(tmpLocation.getParent());",
                "+            try (OutputStream outStream = fsOps.getOutputStream(tmpLocation.toFile())) {",
                "+                byte [] buffer = new byte[4096];",
                "+                int read = 0;",
                "+                while ((read = in.read(buffer)) > 0) {",
                "+                    outStream.write(buffer, 0, read);",
                "+                    totalRead += read;",
                "+                }",
                "+            }",
                "+            if (totalRead != expectedSize) {",
                "+                throw new IOException(\"We expected to download \" + expectedSize + \" bytes but found we got \" + totalRead);",
                "+            }",
                "+        }",
                "+",
                "+        if (type.needsExtraction()) {",
                "+            Path extractionDest = topologyBasicBlobsRootDir.resolve(type.getTempExtractionDir(newVersion));",
                "+            extractDirFromJar(tmpLocation.toAbsolutePath().toString(), ServerConfigUtils.RESOURCES_SUBDIR,",
                "+                extractionDest);",
                "+        }",
                "+        return newVersion;",
                "+    }",
                "+",
                "+    protected void extractDirFromJar(String jarpath, String dir, Path dest) throws IOException {",
                "+        LOG.debug(\"EXTRACTING {} from {} and placing it at {}\", dir, jarpath, dest);",
                "+        try (JarFile jarFile = new JarFile(jarpath)) {",
                "+            String toRemove = dir + '/';",
                "+            Enumeration<JarEntry> jarEnums = jarFile.entries();",
                "+            while (jarEnums.hasMoreElements()) {",
                "+                JarEntry entry = jarEnums.nextElement();",
                "+                String name = entry.getName();",
                "+                if (!entry.isDirectory() && name.startsWith(toRemove)) {",
                "+                    String shortenedName = name.replace(toRemove, \"\");",
                "+                    Path aFile = dest.resolve(shortenedName);",
                "+                    LOG.debug(\"EXTRACTING {} SHORTENED to {} into {}\", name, shortenedName, aFile);",
                "+                    fsOps.forceMkdir(aFile.getParent());",
                "+                    try (FileOutputStream out = new FileOutputStream(aFile.toFile());",
                "+                         InputStream in = jarFile.getInputStream(entry)) {",
                "+                        IOUtils.copy(in, out);",
                "+                    }",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public void commitNewVersion(long newVersion) throws IOException {",
                "+        //This is not atomic (so if something bad happens in the middle we need to be able to recover",
                "+        Path tempLoc = topologyBasicBlobsRootDir.resolve(type.getTempFileName(newVersion));",
                "+        Path dest = topologyBasicBlobsRootDir.resolve(type.getFileName());",
                "+        Path versionFile = topologyBasicBlobsRootDir.resolve(type.getVersionFileName());",
                "+",
                "+        LOG.debug(\"Removing version file {} to force download on failure\", versionFile);",
                "+        fsOps.deleteIfExists(versionFile.toFile()); //So if we fail we are forced to try again",
                "+        LOG.debug(\"Removing destination file {} in preparation for move\", dest);",
                "+        fsOps.deleteIfExists(dest.toFile());",
                "+        if (type.needsExtraction()) {",
                "+            Path extractionTemp = topologyBasicBlobsRootDir.resolve(type.getTempExtractionDir(newVersion));",
                "+            Path extractionDest = topologyBasicBlobsRootDir.resolve(type.getExtractionDir());",
                "+            LOG.debug(\"Removing extraction dest {} in preparation for extraction\", extractionDest);",
                "+            fsOps.deleteIfExists(extractionDest.toFile());",
                "+            if (fsOps.fileExists(extractionTemp)) {",
                "+                fsOps.moveDirectoryPreferAtomic(extractionTemp.toFile(), extractionDest.toFile());",
                "+            }",
                "+        }",
                "+        if (!(isLocalMode && type == TopologyBlobType.TOPO_JAR)) {",
                "+            //Don't try to move the JAR file in local mode, it does not exist because it was not uploaded",
                "+            Files.move(tempLoc, dest);",
                "+        }",
                "+        LOG.debug(\"Writing out version file {} with version {}\", versionFile, newVersion);",
                "+        FileUtils.write(versionFile.toFile(), Long.toString(newVersion), \"UTF8\");",
                "+        this.version = newVersion;",
                "+        updateSizeOnDisk();",
                "+        LOG.debug(\"New version of {} - {} committed {}\", topologyId, type, newVersion);",
                "+    }",
                "+",
                "+    @Override",
                "+    public void cleanupOrphanedData() throws IOException {",
                "+        cleanUpTemp(type.getFileName());",
                "+        if (type.needsExtraction()) {",
                "+            cleanUpTemp(type.getExtractionDir());",
                "+        }",
                "+    }",
                "+",
                "+    private static final Pattern EXTRACT_BASE_NAME_AND_VERSION = Pattern.compile(\"^(.*)\\\\.([0-9]+)$\");",
                "+",
                "+    private void cleanUpTemp(String baseName) throws IOException {",
                "+        LOG.debug(\"Cleaning up temporary data in {}\", topologyBasicBlobsRootDir);",
                "+        try (DirectoryStream<Path> children = fsOps.newDirectoryStream(topologyBasicBlobsRootDir,",
                "+            (p) -> {",
                "+                String fileName = p.getFileName().toString();",
                "+                Matcher m = EXTRACT_BASE_NAME_AND_VERSION.matcher(fileName);",
                "+                return m.matches() && baseName.equals(m.group(1));",
                "+            })) {",
                "+            //children is only ever null if topologyBasicBlobsRootDir does not exist.  This happens during unit tests",
                "+            // And because a non-existant directory is by definition clean we are ignoring it.",
                "+            if (children != null) {",
                "+                for (Path p : children) {",
                "+                    LOG.debug(\"Cleaning up {}\", p);",
                "+                    fsOps.deleteIfExists(p.toFile());",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public void completelyRemove() throws IOException {",
                "+        removeAll(type.getFileName());",
                "+        if (type.needsExtraction()) {",
                "+            removeAll(type.getExtractionDir());",
                "+        }",
                "+    }",
                "+",
                "+    private void removeAll(String baseName) throws IOException {",
                "+        try (DirectoryStream<Path> children = fsOps.newDirectoryStream(topologyBasicBlobsRootDir)) {",
                "+            for (Path p : children) {",
                "+                String fileName = p.getFileName().toString();",
                "+                if (fileName.startsWith(baseName)) {",
                "+                    fsOps.deleteIfExists(p.toFile());",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public long getSizeOnDisk() {",
                "+        return size;",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean equals(Object other) {",
                "+        if (other instanceof LocallyCachedTopologyBlob) {",
                "+            LocallyCachedTopologyBlob o = (LocallyCachedTopologyBlob)other;",
                "+            return topologyId.equals(o.topologyId) && type == o.type && topologyBasicBlobsRootDir.equals(o.topologyBasicBlobsRootDir);",
                "+        }",
                "+        return false;",
                "+    }",
                "+",
                "+    @Override",
                "+    public int hashCode() {",
                "+        return topologyId.hashCode() + type.hashCode() + topologyBasicBlobsRootDir.hashCode();",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        return \"LOCAL TOPO BLOB \" + type + \" \" + topologyId;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/PortAndAssignment.java b/storm-server/src/main/java/org/apache/storm/localizer/PortAndAssignment.java",
                "new file mode 100644",
                "index 000000000..081c81174",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/PortAndAssignment.java",
                "@@ -0,0 +1,71 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.localizer;",
                "+",
                "+import org.apache.storm.generated.LocalAssignment;",
                "+",
                "+/**",
                "+ * A Port and a LocalAssignment used to reference count Resources",
                "+ */",
                "+class PortAndAssignment {",
                "+    private final int port;",
                "+    private final LocalAssignment assignment;",
                "+",
                "+    public PortAndAssignment(int port, LocalAssignment assignment) {",
                "+        this.port = port;",
                "+        this.assignment = assignment;",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean equals(Object other) {",
                "+        if (!(other instanceof PortAndAssignment)) {",
                "+            return false;",
                "+        }",
                "+        PortAndAssignment pna = (PortAndAssignment) other;",
                "+        return pna.port == port && assignment.equals(pna.assignment);",
                "+    }",
                "+",
                "+    public String getToplogyId() {",
                "+        return assignment.get_topology_id();",
                "+    }",
                "+",
                "+    @Override",
                "+    public int hashCode() {",
                "+        return (17 * port) + assignment.hashCode();",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        return \"{\" + port + \" \" + assignment + \"}\";",
                "+    }",
                "+",
                "+    /**",
                "+     * Return the port associated with this.",
                "+     */",
                "+    public int getPort() {",
                "+        return port;",
                "+    }",
                "+",
                "+    /**",
                "+     * return the assigment for this.",
                "+     */",
                "+    public LocalAssignment getAssignment() {",
                "+        return assignment;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "index a50dcedb0..ab8eb14de 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "@@ -26,2 +26,3 @@ import org.apache.storm.Config;",
                " import org.apache.storm.generated.Bolt;",
                "+import org.apache.storm.generated.ComponentCommon;",
                " import org.apache.storm.generated.SpoutSpec;",
                "@@ -65,2 +66,67 @@ public class ResourceUtils {",
                "+    public static void updateStormTopologyResources(StormTopology topology, Map<String, Map<String, Double>> resourceUpdatesMap) {",
                "+        Map<String, Map<String, Double>> componentsUpdated = new HashMap<>();",
                "+        if (topology.get_spouts() != null) {",
                "+            for (Map.Entry<String, SpoutSpec> spout : topology.get_spouts().entrySet()) {",
                "+                SpoutSpec spoutSpec = spout.getValue();",
                "+                String spoutName = spout.getKey();",
                "+",
                "+                if (resourceUpdatesMap.containsKey(spoutName)) {",
                "+                    ComponentCommon spoutCommon = spoutSpec.get_common();",
                "+                    Map<String, Double> resourcesUpdate = resourceUpdatesMap.get(spoutName);",
                "+                    String newJsonConf = getJsonWithUpdatedResources(spoutCommon.get_json_conf(), resourcesUpdate);",
                "+                    spoutCommon.set_json_conf(newJsonConf);",
                "+                    componentsUpdated.put(spoutName, resourcesUpdate);",
                "+                }",
                "+            }",
                "+        }",
                "+",
                "+        if (topology.get_bolts() != null) {",
                "+            for (Map.Entry<String, Bolt> bolt : topology.get_bolts().entrySet()) {",
                "+                Bolt boltObj = bolt.getValue();",
                "+                String boltName = bolt.getKey();",
                "+",
                "+                if(resourceUpdatesMap.containsKey(boltName)) {",
                "+                    ComponentCommon boltCommon = boltObj.get_common();",
                "+                    Map<String, Double> resourcesUpdate = resourceUpdatesMap.get(boltName);",
                "+                    String newJsonConf = getJsonWithUpdatedResources(boltCommon.get_json_conf(), resourceUpdatesMap.get(boltName));",
                "+                    boltCommon.set_json_conf(newJsonConf);",
                "+                    componentsUpdated.put(boltName, resourcesUpdate);",
                "+                }",
                "+            }",
                "+        }",
                "+        LOG.info(\"Component resources updated: {}\", componentsUpdated);",
                "+        Map<String, Map<String, Double>> notUpdated = new HashMap<String, Map<String, Double>>();",
                "+        for (String component : resourceUpdatesMap.keySet()) {",
                "+            if (!componentsUpdated.containsKey(component)) {",
                "+                notUpdated.put(component, resourceUpdatesMap.get(component));",
                "+            }",
                "+        }",
                "+        LOG.info(\"Component resource updates ignored: {}\", notUpdated);",
                "+    }",
                "+",
                "+    public static String getJsonWithUpdatedResources(String jsonConf, Map<String, Double> resourceUpdates) {",
                "+        try {",
                "+            JSONParser parser = new JSONParser();",
                "+            Object obj = parser.parse(jsonConf);",
                "+            JSONObject jsonObject = (JSONObject) obj;",
                "+",
                "+            if (resourceUpdates.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB)) {",
                "+                Double topoMemOnHeap = resourceUpdates.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB);",
                "+                jsonObject.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, topoMemOnHeap);",
                "+            }",
                "+            if (resourceUpdates.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB)) {",
                "+                Double topoMemOffHeap = resourceUpdates.get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB);",
                "+                jsonObject.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, topoMemOffHeap);",
                "+            }",
                "+            if (resourceUpdates.containsKey(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT)) {",
                "+                Double topoCPU = resourceUpdates.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT);",
                "+                jsonObject.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, topoCPU);",
                "+            }",
                "+            return jsonObject.toJSONString();",
                "+        } catch (ParseException ex) {",
                "+            throw new RuntimeException(\"Failed to parse component resources with json: \" +  jsonConf);",
                "+        }",
                "+    }",
                "+",
                "     public static void checkIntialization(Map<String, Double> topologyResources, String com,",
                "diff --git a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "index 9f2b95b5a..6a4454a0a 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "@@ -20,2 +20,3 @@ package org.apache.storm.utils;",
                "+import javax.security.auth.Subject;",
                " import org.apache.commons.compress.archivers.tar.TarArchiveEntry;",
                "@@ -34,2 +35,3 @@ import org.apache.storm.blobstore.InputStreamWithMeta;",
                " import org.apache.storm.blobstore.LocalFsBlobStore;",
                "+import org.apache.storm.blobstore.LocalModeClientBlobStore;",
                " import org.apache.storm.daemon.StormCommon;",
                "@@ -45,2 +47,3 @@ import org.apache.storm.nimbus.NimbusInfo;",
                " import org.apache.storm.scheduler.resource.ResourceUtils;",
                "+import org.apache.storm.security.auth.SingleUserPrincipal;",
                " import org.apache.thrift.TException;",
                "@@ -214,4 +217,9 @@ public class ServerUtils {",
                "     public static ClientBlobStore getClientBlobStoreForSupervisor(Map<String, Object> conf) {",
                "-        ClientBlobStore store = (ClientBlobStore) ReflectionUtils.newInstance(",
                "+        ClientBlobStore store;",
                "+        if (ConfigUtils.isLocalMode(conf)) {",
                "+            store = new LocalModeClientBlobStore(getNimbusBlobStore(conf, null));",
                "+        } else {",
                "+            store = (ClientBlobStore) ReflectionUtils.newInstance(",
                "                 (String) conf.get(DaemonConfig.SUPERVISOR_BLOBSTORE));",
                "+        }",
                "         store.prepare(conf);",
                "@@ -769,2 +777,9 @@ public class ServerUtils {",
                "     }",
                "+",
                "+    public static Subject principalNameToSubject(String name) {",
                "+        SingleUserPrincipal principal = new SingleUserPrincipal(name);",
                "+        Subject sub = new Subject();",
                "+        sub.getPrincipals().add(principal);",
                "+        return sub;",
                "+    }",
                " }"
            ],
            "changed_files": [
                "bin/storm.py",
                "external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsClientBlobStore.java",
                "storm-client/src/jvm/org/apache/storm/blobstore/ClientBlobStore.java",
                "storm-client/src/jvm/org/apache/storm/blobstore/LocalModeClientBlobStore.java",
                "storm-client/src/jvm/org/apache/storm/daemon/supervisor/AdvancedFSOps.java",
                "storm-client/src/jvm/org/apache/storm/daemon/supervisor/IAdvancedFSOps.java",
                "storm-client/src/jvm/org/apache/storm/generated/Assignment.java",
                "storm-client/src/jvm/org/apache/storm/generated/ClusterWorkerHeartbeat.java",
                "storm-client/src/jvm/org/apache/storm/generated/Credentials.java",
                "storm-client/src/jvm/org/apache/storm/generated/HBNodes.java",
                "storm-client/src/jvm/org/apache/storm/generated/HBRecords.java",
                "storm-client/src/jvm/org/apache/storm/generated/LSApprovedWorkers.java",
                "storm-client/src/jvm/org/apache/storm/generated/LSSupervisorAssignments.java",
                "storm-client/src/jvm/org/apache/storm/generated/LSTopoHistory.java",
                "storm-client/src/jvm/org/apache/storm/generated/LSTopoHistoryList.java",
                "storm-client/src/jvm/org/apache/storm/generated/LSWorkerHeartbeat.java",
                "storm-client/src/jvm/org/apache/storm/generated/ListBlobsResult.java",
                "storm-client/src/jvm/org/apache/storm/generated/LocalAssignment.java",
                "storm-client/src/jvm/org/apache/storm/generated/LocalStateData.java",
                "storm-client/src/jvm/org/apache/storm/generated/LogConfig.java",
                "storm-client/src/jvm/org/apache/storm/generated/Nimbus.java",
                "storm-client/src/jvm/org/apache/storm/generated/NodeInfo.java",
                "storm-client/src/jvm/org/apache/storm/generated/RebalanceOptions.java",
                "storm-client/src/jvm/org/apache/storm/generated/SettableBlobMeta.java",
                "storm-client/src/jvm/org/apache/storm/generated/StormBase.java",
                "storm-client/src/jvm/org/apache/storm/generated/SupervisorInfo.java",
                "storm-client/src/jvm/org/apache/storm/generated/TopologyHistoryInfo.java",
                "storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "storm-client/src/py/storm/Nimbus.py",
                "storm-client/src/py/storm/ttypes.py",
                "storm-client/src/storm.thrift",
                "storm-core/src/jvm/org/apache/storm/command/Rebalance.java",
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java",
                "storm-server/src/main/java/org/apache/storm/localizer/BlobChangingCallback.java",
                "storm-server/src/main/java/org/apache/storm/localizer/GoodToGo.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocalDownloadedResource.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocalizedResourceRetentionSet.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java",
                "storm-server/src/main/java/org/apache/storm/localizer/PortAndAssignment.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2438": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2438",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d568408e4d284199cf53baed7dd1471ab9628ca4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510477909,
            "hunks": 2,
            "message": "STORM-2811: Fix NPE in Nimbus when killing the same topology multiple times, fix integration test killing the same topology multiple times",
            "diff": [
                "diff --git a/integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java b/integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java",
                "index 690121244..34c2b6559 100644",
                "--- a/integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java",
                "+++ b/integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java",
                "@@ -19,3 +19,2 @@ package org.apache.storm.st.utils;",
                "-import java.nio.charset.StandardCharsets;",
                " import org.apache.commons.lang.StringUtils;",
                "diff --git a/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java b/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java",
                "index 674865bee..051ccd537 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java",
                "@@ -170,4 +170,4 @@ public interface IStormClusterState {",
                "         for (String topoId: activeStorms()) {",
                "-            String name = stormBase(topoId, null).get_name();",
                "-            if (topologyName.equals(name)) {",
                "+            StormBase base = stormBase(topoId, null);",
                "+            if(base != null && topologyName.equals(base.get_name())) {",
                "                 ret = topoId;"
            ],
            "changed_files": [
                "integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java",
                "storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2811": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2811",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8e2f7e7ef621ff20d485bc0af9e6d18803777564",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515705304,
            "hunks": 4,
            "message": "STORM-2153: Deprecate old user defined metrics in favor of metrics v2",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/task/TopologyContext.java b/storm-core/src/jvm/org/apache/storm/task/TopologyContext.java",
                "index 330fee123..20684392e 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/task/TopologyContext.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/task/TopologyContext.java",
                "@@ -315,2 +315,3 @@ public class TopologyContext extends WorkerTopologyContext implements IMetricsCo",
                "      */",
                "+    @Deprecated",
                "     public <T extends IMetric> T registerMetric(String name, T metric, int timeBucketSizeInSecs) {",
                "@@ -361,2 +362,3 @@ public class TopologyContext extends WorkerTopologyContext implements IMetricsCo",
                "      */",
                "+    @Deprecated",
                "     public IMetric getRegisteredMetricByName(String name) {",
                "@@ -381,2 +383,3 @@ public class TopologyContext extends WorkerTopologyContext implements IMetricsCo",
                "      */",
                "+    @Deprecated",
                "     public ReducedMetric registerMetric(String name, IReducer reducer, int timeBucketSizeInSecs) {",
                "@@ -387,2 +390,3 @@ public class TopologyContext extends WorkerTopologyContext implements IMetricsCo",
                "      */",
                "+    @Deprecated",
                "     public CombinedMetric registerMetric(String name, ICombiner combiner, int timeBucketSizeInSecs) {"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/task/TopologyContext.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: user",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "08d7d8db51df3b2620461f7167cae787c9cf96dc",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515568728,
            "hunks": 1,
            "message": "[STORM-2893] fix storm distribution build by using POSIX tar There is a known issue with the maven-assembly-plugin version 2.5+ where it fails with errors like the following: ``` [ERROR] Failed to execute goal org.apache.maven.plugins:maven-assembly-plugin:2.6:single (default) on project final-package: Execution default of goal org.apache.maven.plugins:maven-assembly-plugin:2.6:single failed: group id '906175167' is too big ( > 2097151 ). Use STAR or POSIX extensions to overcome this limit -> [Help 1] ``` This became an issue with change 84a4314d96b9e4e377a3d5d81d0a042d96a0625e when the maven-assembly-plugin was upgraded from version 2.2.2 to 2.6, as inherited from the apache pom version 18. To fix this we set the assembly plugin's `tarLongFileMode` configuration setting to `posix`.",
            "diff": [
                "diff --git a/storm-dist/binary/final-package/pom.xml b/storm-dist/binary/final-package/pom.xml",
                "index ef3235cec..71a464784 100644",
                "--- a/storm-dist/binary/final-package/pom.xml",
                "+++ b/storm-dist/binary/final-package/pom.xml",
                "@@ -56,2 +56,3 @@",
                "                     <attach>true</attach>",
                "+                    <tarLongFileMode>posix</tarLongFileMode>",
                "                     <runOnlyAtExecutionRoot>false</runOnlyAtExecutionRoot>"
            ],
            "changed_files": [
                "storm-dist/binary/final-package/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2893": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: execute",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2893",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6a4aeb9101bb1e6b196fa348ece03e55807391d1",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510683360,
            "hunks": 129,
            "message": "STORM-2813: Use a class for normalized resources not a map.",
            "diff": [
                "diff --git a/storm-client/pom.xml b/storm-client/pom.xml",
                "index 6097b359e..22c8bcc04 100644",
                "--- a/storm-client/pom.xml",
                "+++ b/storm-client/pom.xml",
                "@@ -263,3 +263,3 @@",
                "                     <excludes>**/generated/**</excludes>",
                "-                    <maxAllowedViolations>10298</maxAllowedViolations>",
                "+                    <maxAllowedViolations>10263</maxAllowedViolations>",
                "                 </configuration>",
                "diff --git a/storm-client/src/jvm/org/apache/storm/Constants.java b/storm-client/src/jvm/org/apache/storm/Constants.java",
                "index c8d3b0966..a8e2c0127 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/Constants.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/Constants.java",
                "@@ -66,14 +66,2 @@ public class Constants {",
                "     public static final String COMMON_TOTAL_MEMORY_RESOURCE_NAME = \"memory.mb\";",
                "-",
                "-    public static final Map<String, String> resourceNameMapping;",
                "-",
                "-    static {",
                "-        Map<String, String> tmp = new HashMap<>();",
                "-        tmp.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, COMMON_CPU_RESOURCE_NAME);",
                "-        tmp.put(Config.SUPERVISOR_CPU_CAPACITY, COMMON_CPU_RESOURCE_NAME);",
                "-        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "-        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "-        tmp.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, COMMON_TOTAL_MEMORY_RESOURCE_NAME);",
                "-        resourceNameMapping = Collections.unmodifiableMap(tmp);",
                "-    }",
                " }",
                "diff --git a/storm-server/pom.xml b/storm-server/pom.xml",
                "index 18fec7f6b..d6554de45 100644",
                "--- a/storm-server/pom.xml",
                "+++ b/storm-server/pom.xml",
                "@@ -132,3 +132,3 @@",
                "                 <configuration>",
                "-                    <maxAllowedViolations>2655</maxAllowedViolations>",
                "+                    <maxAllowedViolations>2630</maxAllowedViolations>",
                "                 </configuration>",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index fc427a8e8..ea3c9c4a3 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -54,2 +54,3 @@ import java.util.regex.Matcher;",
                " import java.util.regex.Pattern;",
                "+import java.util.stream.Collectors;",
                " import javax.security.auth.Subject;",
                "@@ -150,2 +151,3 @@ import org.apache.storm.scheduler.blacklist.BlacklistScheduler;",
                " import org.apache.storm.scheduler.multitenant.MultitenantScheduler;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                " import org.apache.storm.scheduler.resource.ResourceAwareScheduler;",
                "@@ -969,9 +971,8 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "-    private static Map<String, Double> setResourcesDefaultIfNotSet(Map<String, Map<String, Double>> compResourcesMap, String compId, Map<String, Object> topoConf) {",
                "-        Map<String, Double> resourcesMap = compResourcesMap.get(compId);",
                "-        if (resourcesMap == null) {",
                "-            resourcesMap = new HashMap<>();",
                "+    private static void setResourcesDefaultIfNotSet(Map<String, NormalizedResourceRequest> compResourcesMap, String compId,",
                "+                                                    Map<String, Object> topoConf) {",
                "+        NormalizedResourceRequest resources = compResourcesMap.get(compId);",
                "+        if (resources == null) {",
                "+            compResourcesMap.put(compId, new NormalizedResourceRequest(topoConf));",
                "         }",
                "-        ResourceUtils.checkInitialization(resourcesMap, compId, topoConf);",
                "-        return resourcesMap;",
                "     }",
                "@@ -2565,7 +2566,5 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "         double largestMemoryOperator = 0.0;",
                "-        for (Map<String, Double> entry :",
                "+        for (NormalizedResourceRequest entry :",
                "             ResourceUtils.getBoltsResources(topology, topologyConf).values()) {",
                "-            double memoryRequirement =",
                "-                entry.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "-                    + entry.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+            double memoryRequirement = entry.getTotalMemoryMb();",
                "             if (memoryRequirement > largestMemoryOperator) {",
                "@@ -2574,7 +2573,5 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "         }",
                "-        for (Map<String, Double> entry :",
                "+        for (NormalizedResourceRequest entry :",
                "             ResourceUtils.getSpoutsResources(topology, topologyConf).values()) {",
                "-            double memoryRequirement =",
                "-                entry.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "-                    + entry.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+            double memoryRequirement = entry.getTotalMemoryMb();",
                "             if (memoryRequirement > largestMemoryOperator) {",
                "@@ -3651,12 +3648,14 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "-            Map<String, Map<String, Double>> spoutResources = ResourceUtils.getSpoutsResources(topology, topoConf);",
                "+            Map<String, NormalizedResourceRequest> spoutResources = ResourceUtils.getSpoutsResources(topology, topoConf);",
                "             for (Entry<String, ComponentAggregateStats> entry: topoPageInfo.get_id_to_spout_agg_stats().entrySet()) {",
                "                 CommonAggregateStats commonStats = entry.getValue().get_common_stats();",
                "-                commonStats.set_resources_map(setResourcesDefaultIfNotSet(spoutResources, entry.getKey(), topoConf));",
                "+                setResourcesDefaultIfNotSet(spoutResources, entry.getKey(), topoConf);",
                "+                commonStats.set_resources_map(spoutResources.get(entry.getKey()).toNormalizedMap());",
                "             }",
                "-            Map<String, Map<String, Double>> boltResources = ResourceUtils.getBoltsResources(topology, topoConf);",
                "+            Map<String, NormalizedResourceRequest> boltResources = ResourceUtils.getBoltsResources(topology, topoConf);",
                "             for (Entry<String, ComponentAggregateStats> entry: topoPageInfo.get_id_to_bolt_agg_stats().entrySet()) {",
                "                 CommonAggregateStats commonStats = entry.getValue().get_common_stats();",
                "-                commonStats.set_resources_map(setResourcesDefaultIfNotSet(boltResources, entry.getKey(), topoConf));",
                "+                setResourcesDefaultIfNotSet(boltResources, entry.getKey(), topoConf);",
                "+                commonStats.set_resources_map(boltResources.get(entry.getKey()).toNormalizedMap());",
                "             }",
                "@@ -3818,7 +3817,13 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "             if (compPageInfo.get_component_type() == ComponentType.SPOUT) {",
                "-                compPageInfo.set_resources_map(setResourcesDefaultIfNotSet(",
                "-                        ResourceUtils.getSpoutsResources(topology, topoConf), componentId, topoConf));",
                "+                NormalizedResourceRequest spoutResources = ResourceUtils.getSpoutResources(topology, topoConf, componentId);",
                "+                if (spoutResources == null) {",
                "+                    spoutResources = new NormalizedResourceRequest(topoConf);",
                "+                }",
                "+                compPageInfo.set_resources_map(spoutResources.toNormalizedMap());",
                "             } else { //bolt",
                "-                compPageInfo.set_resources_map(setResourcesDefaultIfNotSet(",
                "-                        ResourceUtils.getBoltsResources(topology, topoConf), componentId, topoConf));",
                "+                NormalizedResourceRequest boltResources = ResourceUtils.getBoltResources(topology, topoConf, componentId);",
                "+                if (boltResources == null) {",
                "+                    boltResources = new NormalizedResourceRequest(topoConf);",
                "+                }",
                "+                compPageInfo.set_resources_map(boltResources.toNormalizedMap());",
                "             }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "index 26fb72923..87e310ea4 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "@@ -28,3 +28,2 @@ import org.apache.storm.utils.Time;",
                " import java.util.ArrayList;",
                "-import java.util.Collection;",
                " import java.util.Collections;",
                "@@ -34,3 +33,3 @@ import java.util.Map;",
                "-import static org.apache.storm.scheduler.resource.ResourceUtils.normalizedResourceMap;",
                "+import static org.apache.storm.scheduler.resource.NormalizedResources.normalizedResourceMap;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "index 93e4dfaba..c311897b5 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "@@ -21,3 +21,2 @@ package org.apache.storm.scheduler;",
                " import com.google.common.annotations.VisibleForTesting;",
                "-",
                " import java.util.ArrayList;",
                "@@ -31,3 +30,2 @@ import java.util.Map.Entry;",
                " import java.util.Set;",
                "-",
                " import org.apache.storm.Config;",
                "@@ -38,2 +36,4 @@ import org.apache.storm.generated.WorkerResources;",
                " import org.apache.storm.networktopography.DNSToSwitchMapping;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                " import org.apache.storm.utils.ConfigUtils;",
                "@@ -45,3 +45,3 @@ import org.slf4j.LoggerFactory;",
                "-import static org.apache.storm.scheduler.resource.ResourceUtils.normalizedResourceMap;",
                "+import static org.apache.storm.scheduler.resource.NormalizedResources.normalizedResourceMap;",
                "@@ -81,3 +81,3 @@ public class Cluster implements ISchedulingState {",
                "     private final Topologies topologies;",
                "-    private final Map<String, Double> scheduledCPUCache = new HashMap<>();",
                "+    private final Map<String, Double> scheduledCpuCache = new HashMap<>();",
                "     private final Map<String, Double> scheduledMemoryCache = new HashMap<>();",
                "@@ -329,6 +329,6 @@ public class Cluster implements ISchedulingState {",
                "     public Set<Integer> getAssignablePorts(SupervisorDetails supervisor) {",
                "-        if (isBlackListed(supervisor.id)) {",
                "+        if (isBlackListed(supervisor.getId())) {",
                "             return Collections.emptySet();",
                "         }",
                "-        return supervisor.allPorts;",
                "+        return supervisor.getAllPorts();",
                "     }",
                "@@ -419,6 +419,6 @@ public class Cluster implements ISchedulingState {",
                "         TopologyDetails td, Collection<ExecutorDetails> executors) {",
                "-        Map<String, Double> totalResources = new HashMap<>();",
                "+        NormalizedResourceRequest totalResources = new NormalizedResourceRequest();",
                "         Map<String, Double> sharedTotalResources = new HashMap<>();",
                "         for (ExecutorDetails exec : executors) {",
                "-            Map<String, Double> allResources = td.getTotalResources(exec);",
                "+            NormalizedResourceRequest allResources = td.getTotalResources(exec);",
                "             if (allResources == null) {",
                "@@ -426,22 +426,7 @@ public class Cluster implements ISchedulingState {",
                "             }",
                "-            for (Entry<String, Double> resource : allResources.entrySet()) {",
                "-",
                "-                if (!totalResources.containsKey(resource.getKey())) {",
                "-                    totalResources.put(resource.getKey(), 0.0);",
                "-                }",
                "-                totalResources.put(",
                "-                        resource.getKey(),",
                "-                        totalResources.get(resource.getKey()) + resource.getValue());",
                "-            }",
                "+            totalResources.add(allResources);",
                "         }",
                "-        totalResources = normalizedResourceMap(totalResources);",
                "         for (SharedMemory shared : td.getSharedMemoryRequests(executors)) {",
                "-            addResource(",
                "-                    totalResources,",
                "-                    Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, shared.get_off_heap_worker()",
                "-            );",
                "-            addResource(",
                "-                    totalResources,",
                "-                    Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, shared.get_on_heap()",
                "-            );",
                "+            totalResources.addOffHeap(shared.get_off_heap_worker());",
                "+            totalResources.addOnHeap(shared.get_off_heap_worker());",
                "@@ -458,17 +443,8 @@ public class Cluster implements ISchedulingState {",
                "         WorkerResources ret = new WorkerResources();",
                "-        ret.set_resources(totalResources);",
                "+        ret.set_resources(totalResources.toNormalizedMap());",
                "         ret.set_shared_resources(sharedTotalResources);",
                "-        ret.set_cpu(",
                "-                totalResources.getOrDefault(",
                "-                        Constants.COMMON_CPU_RESOURCE_NAME, 0.0)",
                "-        );",
                "-        ret.set_mem_off_heap(",
                "-                totalResources.getOrDefault(",
                "-                        Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "-        );",
                "-        ret.set_mem_on_heap(",
                "-                totalResources.getOrDefault(",
                "-                        Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "-        );",
                "+        ret.set_cpu(totalResources.getTotalCpu());",
                "+        ret.set_mem_off_heap(totalResources.getOffHeapMemoryMb());",
                "+        ret.set_mem_on_heap(totalResources.getOnHeapMemoryMb());",
                "         ret.set_shared_mem_off_heap(",
                "@@ -489,28 +465,8 @@ public class Cluster implements ISchedulingState {",
                "         TopologyDetails td,",
                "-        Map<String, Double> resourcesAvailable,",
                "+        NormalizedResourceOffer resourcesAvailable,",
                "         double maxHeap) {",
                "-        Map<String, Double> requestedResources = td.getTotalResources(exec);",
                "-",
                "-        for (Entry resourceNeededEntry : requestedResources.entrySet()) {",
                "-            String resourceName = resourceNeededEntry.getKey().toString();",
                "-            if (resourceName.equals(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME) ||",
                "-                    resourceName.equals(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME)) {",
                "-                continue;",
                "-            }",
                "-            Double resourceNeeded = ObjectReader.getDouble(resourceNeededEntry.getValue());",
                "-            Double resourceAvailable = ObjectReader.getDouble(resourcesAvailable.get(resourceName), 0.0);",
                "-            if (resourceNeeded > resourceAvailable) {",
                "-                if (LOG.isTraceEnabled()) {",
                "-                    LOG.trace(\"Could not schedule {}:{} on {} not enough {} {} > {}\",",
                "-                            td.getName(),",
                "-                            exec,",
                "-                            ws,",
                "-                            resourceName,",
                "-                            resourceNeeded,",
                "-                            resourceAvailable);",
                "-                }",
                "-                //Not enough resources - stop trying",
                "-                return false;",
                "-            }",
                "+        NormalizedResourceRequest requestedResources = td.getTotalResources(exec);",
                "+        if (!resourcesAvailable.couldHoldIgnoringMemory(requestedResources)) {",
                "+            return false;",
                "         }",
                "@@ -541,4 +497,3 @@ public class Cluster implements ISchedulingState {",
                "         double memoryAdded = afterTotal - currentTotal;",
                "-        double memoryAvailable = ObjectReader.getDouble(resourcesAvailable.get(",
                "-                Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME), 0.0);",
                "+        double memoryAvailable = resourcesAvailable.getTotalMemoryMb();",
                "@@ -615,3 +570,3 @@ public class Cluster implements ISchedulingState {",
                "             nodeId, calculateSharedOffHeapMemory(nodeId, assignment));",
                "-        scheduledCPUCache.remove(nodeId);",
                "+        scheduledCpuCache.remove(nodeId);",
                "         scheduledMemoryCache.remove(nodeId);",
                "@@ -686,3 +641,3 @@ public class Cluster implements ISchedulingState {",
                "                     nodeId, calculateSharedOffHeapMemory(nodeId, assignment));",
                "-                scheduledCPUCache.remove(nodeId);",
                "+                scheduledCpuCache.remove(nodeId);",
                "                 scheduledMemoryCache.remove(nodeId);",
                "@@ -797,3 +752,3 @@ public class Cluster implements ISchedulingState {",
                "         for (SupervisorDetails sup : supervisors.values()) {",
                "-            sum += sup.getTotalCPU();",
                "+            sum += sup.getTotalCpu();",
                "         }",
                "@@ -937,3 +892,3 @@ public class Cluster implements ISchedulingState {",
                "         for (SupervisorDetails sd : supervisors.values()) {",
                "-            ret.put(sd.getId(), new SupervisorResources(sd.getTotalMemory(), sd.getTotalCPU(), 0, 0));",
                "+            ret.put(sd.getId(), new SupervisorResources(sd.getTotalMemory(), sd.getTotalCpu(), 0, 0));",
                "         }",
                "@@ -988,4 +943,4 @@ public class Cluster implements ISchedulingState {",
                "     @Override",
                "-    public Map<String, Double> getAllScheduledResourcesForNode(String nodeId) {",
                "-        Map<String, Double> totalScheduledResources = new HashMap<>();",
                "+    public NormalizedResourceRequest getAllScheduledResourcesForNode(String nodeId) {",
                "+        NormalizedResourceRequest totalScheduledResources = new NormalizedResourceRequest();",
                "         for (SchedulerAssignmentImpl assignment : assignments.values()) {",
                "@@ -994,10 +949,3 @@ public class Cluster implements ISchedulingState {",
                "                 if (nodeId.equals(entry.getKey().getNodeId())) {",
                "-                    WorkerResources resources = entry.getValue();",
                "-                    for (Map.Entry<String, Double> resourceEntry : resources.get_resources().entrySet()) {",
                "-                        Double currentResourceValue = totalScheduledResources.getOrDefault(resourceEntry.getKey(), 0.0);",
                "-                        totalScheduledResources.put(",
                "-                                resourceEntry.getKey(),",
                "-                                currentResourceValue + ObjectReader.getDouble(resourceEntry.getValue()));",
                "-                    }",
                "-",
                "+                    totalScheduledResources.add(entry.getValue());",
                "                 }",
                "@@ -1006,5 +954,3 @@ public class Cluster implements ISchedulingState {",
                "             if (sharedOffHeap != null) {",
                "-                String resourceName = Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME;",
                "-                Double currentResourceValue = totalScheduledResources.getOrDefault(resourceName, 0.0);",
                "-                totalScheduledResources.put(resourceName, currentResourceValue + sharedOffHeap);",
                "+                totalScheduledResources.addOffHeap(sharedOffHeap);",
                "             }",
                "@@ -1040,3 +986,3 @@ public class Cluster implements ISchedulingState {",
                "     public double getScheduledCpuForNode(String nodeId) {",
                "-        Double ret = scheduledCPUCache.get(nodeId);",
                "+        Double ret = scheduledCpuCache.get(nodeId);",
                "         if (ret != null) {",
                "@@ -1054,3 +1000,3 @@ public class Cluster implements ISchedulingState {",
                "         }",
                "-        scheduledCPUCache.put(nodeId, totalCpu);",
                "+        scheduledCpuCache.put(nodeId, totalCpu);",
                "         return totalCpu;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/ExecutorDetails.java b/storm-server/src/main/java/org/apache/storm/scheduler/ExecutorDetails.java",
                "new file mode 100644",
                "index 000000000..bbbbf3f2e",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/ExecutorDetails.java",
                "@@ -0,0 +1,57 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler;",
                "+",
                "+public class ExecutorDetails {",
                "+    public final int startTask;",
                "+    public final int endTask;",
                "+",
                "+    public ExecutorDetails(int startTask, int endTask) {",
                "+        this.startTask = startTask;",
                "+        this.endTask = endTask;",
                "+    }",
                "+",
                "+    public int getStartTask() {",
                "+        return startTask;",
                "+    }",
                "+",
                "+    public int getEndTask() {",
                "+        return endTask;",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean equals(Object other) {",
                "+        if (other == null || !(other instanceof ExecutorDetails)) {",
                "+            return false;",
                "+        }",
                "+        ",
                "+        ExecutorDetails executor = (ExecutorDetails)other;",
                "+        return (this.startTask == executor.startTask) && (this.endTask == executor.endTask);",
                "+    }",
                "+",
                "+    @Override",
                "+    public int hashCode() {",
                "+        return this.startTask + 13 * this.endTask;",
                "+    }",
                "+    ",
                "+    @Override",
                "+    public String toString() {",
                "+        return \"[\" + this.startTask + \", \" + this.endTask + \"]\";",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java b/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "index b539b1f93..7175b7e72 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "@@ -27,2 +27,4 @@ import org.apache.storm.daemon.nimbus.TopologyResources;",
                " import org.apache.storm.generated.WorkerResources;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                "@@ -181,3 +183,3 @@ public interface ISchedulingState {",
                "         TopologyDetails td,",
                "-        Map<String, Double> resourcesAvailable,",
                "+        NormalizedResourceOffer resourcesAvailable,",
                "         double maxHeap);",
                "@@ -207,4 +209,4 @@ public interface ISchedulingState {",
                "-    /** Get all scheduled resources for node **/",
                "-    Map<String, Double> getAllScheduledResourcesForNode(String nodeId);",
                "+    /** Get all scheduled resources for node. **/",
                "+    NormalizedResourceRequest getAllScheduledResourcesForNode(String nodeId);",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/SchedulerAssignment.java b/storm-server/src/main/java/org/apache/storm/scheduler/SchedulerAssignment.java",
                "new file mode 100644",
                "index 000000000..c16a67f42",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/SchedulerAssignment.java",
                "@@ -0,0 +1,85 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler;",
                "+",
                "+import java.util.Collection;",
                "+import java.util.Map;",
                "+import java.util.Set;",
                "+",
                "+import org.apache.storm.generated.Assignment;",
                "+import org.apache.storm.generated.WorkerResources;",
                "+",
                "+public interface SchedulerAssignment {",
                "+    /**",
                "+     * Is this slot part of this assignment or not.",
                "+     * @param slot the slot to check.",
                "+     * @return true if the slot is occupied by this assignment else false.",
                "+     */",
                "+    public boolean isSlotOccupied(WorkerSlot slot);",
                "+",
                "+    /**",
                "+     * Is the executor assigned or not.",
                "+     * ",
                "+     * @param executor the executor to check it if is assigned.",
                "+     * @return true if it is assigned else false",
                "+     */",
                "+    public boolean isExecutorAssigned(ExecutorDetails executor);",
                "+    ",
                "+    /**",
                "+     * Return the ID of the topology.",
                "+     * @return the topology-id this assignment is for.",
                "+     */",
                "+    public String getTopologyId();",
                "+",
                "+    /**",
                "+     * Get the map of executor to WorkerSlot.",
                "+     * @return the executor -> slot map.",
                "+     */",
                "+    public Map<ExecutorDetails, WorkerSlot> getExecutorToSlot();",
                "+",
                "+    /**",
                "+     * Get the set of all executors.",
                "+     * @return  the executors covered by this assignments",
                "+     */",
                "+    public Set<ExecutorDetails> getExecutors();",
                "+",
                "+    /**",
                "+     * Get the set of all slots that are a part of this.",
                "+     * @return the set of all slots.",
                "+     */",
                "+    public Set<WorkerSlot> getSlots();",
                "+",
                "+    /**",
                "+     * Get the mapping of slot to executors on that slot.",
                "+     * @return the slot to the executors assigned to that slot.",
                "+     */",
                "+    public Map<WorkerSlot, Collection<ExecutorDetails>> getSlotToExecutors();",
                "+    ",
                "+    /**",
                "+     * Get the slot to resource mapping.",
                "+     * @return The slot to resource mapping",
                "+     */",
                "+    public Map<WorkerSlot, WorkerResources> getScheduledResources();",
                "+    ",
                "+    /**",
                "+     * Get the total shared off heap memory mapping.",
                "+     * @return host to total shared off heap memory mapping.",
                "+     */",
                "+    public Map<String, Double> getNodeIdToTotalSharedOffHeapMemory();",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/SchedulerAssignmentImpl.java b/storm-server/src/main/java/org/apache/storm/scheduler/SchedulerAssignmentImpl.java",
                "new file mode 100644",
                "index 000000000..f92c3d173",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/SchedulerAssignmentImpl.java",
                "@@ -0,0 +1,251 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler;",
                "+",
                "+import java.util.ArrayList;",
                "+import java.util.Collection;",
                "+import java.util.HashMap;",
                "+import java.util.HashSet;",
                "+import java.util.LinkedList;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import java.util.Set;",
                "+",
                "+import org.apache.storm.generated.WorkerResources;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+public class SchedulerAssignmentImpl implements SchedulerAssignment {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(SchedulerAssignmentImpl.class);",
                "+",
                "+    /**",
                "+     * topology-id this assignment is for.",
                "+     */",
                "+    private final String topologyId;",
                "+",
                "+    /**",
                "+     * assignment detail, a mapping from executor to <code>WorkerSlot</code>.",
                "+     */",
                "+    private final Map<ExecutorDetails, WorkerSlot> executorToSlot = new HashMap<>();",
                "+    private final Map<WorkerSlot, WorkerResources> resources = new HashMap<>();",
                "+    private final Map<String, Double> nodeIdToTotalSharedOffHeap = new HashMap<>();",
                "+    //Used to cache the slotToExecutors mapping.",
                "+    private Map<WorkerSlot, Collection<ExecutorDetails>> slotToExecutorsCache = null;",
                "+",
                "+    /**",
                "+     * Create a new assignment.",
                "+     * @param topologyId the id of the topology the assignment is for.",
                "+     * @param executorToSlot the executor to slot mapping for the assignment.  Can be null and set through other methods later.",
                "+     * @param resources the resources for the current assignments.  Can be null and set through other methods later.",
                "+     * @param nodeIdToTotalSharedOffHeap the shared memory for this assignment can be null and set through other methods later.",
                "+     */",
                "+    public SchedulerAssignmentImpl(String topologyId, Map<ExecutorDetails, WorkerSlot> executorToSlot,",
                "+            Map<WorkerSlot, WorkerResources> resources, Map<String, Double> nodeIdToTotalSharedOffHeap) {",
                "+        this.topologyId = topologyId;       ",
                "+        if (executorToSlot != null) {",
                "+            if (executorToSlot.entrySet().stream().anyMatch((entry) -> entry.getKey() == null || entry.getValue() == null)) {",
                "+                throw new RuntimeException(\"Cannot create a scheduling with a null in it \" + executorToSlot);",
                "+            }",
                "+            this.executorToSlot.putAll(executorToSlot);",
                "+        }",
                "+        if (resources != null) {",
                "+            if (resources.entrySet().stream().anyMatch((entry) -> entry.getKey() == null || entry.getValue() == null)) {",
                "+                throw new RuntimeException(\"Cannot create resources with a null in it \" + resources);",
                "+            }",
                "+            this.resources.putAll(resources);",
                "+        }",
                "+        if (nodeIdToTotalSharedOffHeap != null) {",
                "+            if (nodeIdToTotalSharedOffHeap.entrySet().stream().anyMatch((entry) -> entry.getKey() == null || entry.getValue() == null)) {",
                "+                throw new RuntimeException(\"Cannot create off heap with a null in it \" + nodeIdToTotalSharedOffHeap);",
                "+            }",
                "+            this.nodeIdToTotalSharedOffHeap.putAll(nodeIdToTotalSharedOffHeap);",
                "+        }",
                "+    }",
                "+",
                "+    public SchedulerAssignmentImpl(String topologyId) {",
                "+        this(topologyId, null, null, null);",
                "+    }",
                "+",
                "+    public SchedulerAssignmentImpl(SchedulerAssignment assignment) {",
                "+        this(assignment.getTopologyId(), assignment.getExecutorToSlot(), ",
                "+                assignment.getScheduledResources(), assignment.getNodeIdToTotalSharedOffHeapMemory());",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        return this.getClass().getSimpleName() + \" topo: \" + topologyId + \" execToSlots: \" + executorToSlot;",
                "+    }",
                "+",
                "+    /**",
                "+     * Like the equals command, but ignores the resources.",
                "+     * @param other the object to check for equality against.",
                "+     * @return true if they are equal, ignoring resources, else false.",
                "+     */",
                "+    public boolean equalsIgnoreResources(Object other) {",
                "+        if (other == this) {",
                "+            return true;",
                "+        }",
                "+        if (!(other instanceof SchedulerAssignmentImpl)) {",
                "+            return false;",
                "+        }",
                "+        SchedulerAssignmentImpl o = (SchedulerAssignmentImpl) other;",
                "+        ",
                "+        return topologyId.equals(o.topologyId)",
                "+            && executorToSlot.equals(o.executorToSlot);",
                "+    }",
                "+    ",
                "+    @Override",
                "+    public int hashCode() {",
                "+        final int prime = 31;",
                "+        int result = 1;",
                "+        result = prime * result + ((topologyId == null) ? 0 : topologyId.hashCode());",
                "+        result = prime * result + ((executorToSlot == null) ? 0 : executorToSlot.hashCode());",
                "+        return result;",
                "+    }",
                "+    ",
                "+    @Override",
                "+    public boolean equals(Object other) {",
                "+        if (!equalsIgnoreResources(other)) {",
                "+            return false;",
                "+        }",
                "+        SchedulerAssignmentImpl o = (SchedulerAssignmentImpl) other;",
                "+",
                "+        return resources.equals(o.resources)",
                "+            && nodeIdToTotalSharedOffHeap.equals(o.nodeIdToTotalSharedOffHeap);",
                "+    }",
                "+    ",
                "+    @Override",
                "+    public Set<WorkerSlot> getSlots() {",
                "+        return new HashSet<>(executorToSlot.values());",
                "+    }    ",
                "+",
                "+    @Deprecated",
                "+    public void assign(WorkerSlot slot, Collection<ExecutorDetails> executors) {",
                "+        assign(slot, executors, null);",
                "+    }",
                "+",
                "+    /**",
                "+     * Assign the slot to executors.",
                "+     */",
                "+    public void assign(WorkerSlot slot, Collection<ExecutorDetails> executors, WorkerResources slotResources) {",
                "+        assert slot != null;",
                "+        for (ExecutorDetails executor : executors) {",
                "+            this.executorToSlot.put(executor, slot);",
                "+        }",
                "+        if (slotResources != null) {",
                "+            resources.put(slot, slotResources);",
                "+        } else {",
                "+            resources.remove(slot);",
                "+        }",
                "+        //Clear the cache scheduling changed",
                "+        slotToExecutorsCache = null;",
                "+    }",
                "+",
                "+    /**",
                "+     * Release the slot occupied by this assignment.",
                "+     */",
                "+    public void unassignBySlot(WorkerSlot slot) {",
                "+        //Clear the cache scheduling is going to change",
                "+        slotToExecutorsCache = null;",
                "+        List<ExecutorDetails> executors = new ArrayList<>();",
                "+        for (ExecutorDetails executor : executorToSlot.keySet()) {",
                "+            WorkerSlot ws = executorToSlot.get(executor);",
                "+            if (ws.equals(slot)) {",
                "+                executors.add(executor);",
                "+            }",
                "+        }",
                "+",
                "+        // remove",
                "+        for (ExecutorDetails executor : executors) {",
                "+            executorToSlot.remove(executor);",
                "+        }",
                "+",
                "+        resources.remove(slot);",
                "+",
                "+        String node = slot.getNodeId();",
                "+        boolean isFound = false;",
                "+        for (WorkerSlot ws: executorToSlot.values()) {",
                "+            if (node.equals(ws.getNodeId())) {",
                "+                isFound = true;",
                "+                break;",
                "+            }",
                "+        }",
                "+        if (!isFound) {",
                "+            nodeIdToTotalSharedOffHeap.remove(node);",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean isSlotOccupied(WorkerSlot slot) {",
                "+        return this.executorToSlot.containsValue(slot);",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean isExecutorAssigned(ExecutorDetails executor) {",
                "+        return this.executorToSlot.containsKey(executor);",
                "+    }",
                "+",
                "+    @Override",
                "+    public String getTopologyId() {",
                "+        return this.topologyId;",
                "+    }",
                "+",
                "+    @Override",
                "+    public Map<ExecutorDetails, WorkerSlot> getExecutorToSlot() {",
                "+        return this.executorToSlot;",
                "+    }",
                "+",
                "+    @Override",
                "+    public Set<ExecutorDetails> getExecutors() {",
                "+        return this.executorToSlot.keySet();",
                "+    }",
                "+",
                "+    @Override",
                "+    public Map<WorkerSlot, Collection<ExecutorDetails>> getSlotToExecutors() {",
                "+        Map<WorkerSlot, Collection<ExecutorDetails>> ret = slotToExecutorsCache;",
                "+        if (ret != null) {",
                "+            return ret;",
                "+        }",
                "+        ret = new HashMap<>();",
                "+        for (Map.Entry<ExecutorDetails, WorkerSlot> entry : executorToSlot.entrySet()) {",
                "+            ExecutorDetails exec = entry.getKey();",
                "+            WorkerSlot ws = entry.getValue();",
                "+            if (!ret.containsKey(ws)) {",
                "+                ret.put(ws, new LinkedList<ExecutorDetails>());",
                "+            }",
                "+            ret.get(ws).add(exec);",
                "+        }",
                "+        slotToExecutorsCache = ret;",
                "+        return ret;",
                "+    }",
                "+",
                "+    @Override",
                "+    public Map<WorkerSlot, WorkerResources> getScheduledResources() {",
                "+        return resources;",
                "+    }",
                "+",
                "+    public void setTotalSharedOffHeapMemory(String node, double value) {",
                "+        nodeIdToTotalSharedOffHeap.put(node, value);",
                "+    }",
                "+    ",
                "+    @Override",
                "+    public Map<String, Double> getNodeIdToTotalSharedOffHeapMemory() {",
                "+        return nodeIdToTotalSharedOffHeap;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/SupervisorDetails.java b/storm-server/src/main/java/org/apache/storm/scheduler/SupervisorDetails.java",
                "new file mode 100644",
                "index 000000000..3d087155a",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/SupervisorDetails.java",
                "@@ -0,0 +1,153 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler;",
                "+",
                "+import java.util.Collection;",
                "+import java.util.HashSet;",
                "+import java.util.Map;",
                "+import java.util.Set;",
                "+import org.apache.storm.Constants;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+public class SupervisorDetails {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(SupervisorDetails.class);",
                "+",
                "+    private final String id;",
                "+    /**",
                "+     * hostname of this supervisor.",
                "+     */",
                "+    private final String host;",
                "+    private final Object meta;",
                "+    /**",
                "+     * meta data configured for this supervisor.",
                "+     */",
                "+    private final Object schedulerMeta;",
                "+    /**",
                "+     * all the ports of the supervisor.",
                "+     */",
                "+    private Set<Integer> allPorts;",
                "+    /**",
                "+     * Map containing a manifest of resources for the node the supervisor resides.",
                "+     */",
                "+    private final NormalizedResourceOffer totalResources;",
                "+",
                "+    /**",
                "+     * Create the details of a new supervisor.",
                "+     * @param id the ID as reported by the supervisor.",
                "+     * @param host the host the supervisor is on.",
                "+     * @param meta meta data reported by the supervisor (should be a collection of the ports on the supervisor).",
                "+     * @param schedulerMeta Not used and can probably be removed.",
                "+     * @param allPorts all of the ports for the supervisor (a better version of meta)",
                "+     * @param totalResources all of the resources for this supervisor.",
                "+     */",
                "+    public SupervisorDetails(String id, String host, Object meta, Object schedulerMeta,",
                "+                             Collection<? extends Number> allPorts, Map<String, Double> totalResources) {",
                "+",
                "+        this.id = id;",
                "+        this.host = host;",
                "+        this.meta = meta;",
                "+        this.schedulerMeta = schedulerMeta;",
                "+        if (allPorts != null) {",
                "+            setAllPorts(allPorts);",
                "+        } else {",
                "+            this.allPorts = new HashSet<>();",
                "+        }",
                "+        this.totalResources = new NormalizedResourceOffer(totalResources);",
                "+        LOG.debug(\"Creating a new supervisor ({}-{}) with resources: {}\", this.host, this.id, totalResources);",
                "+    }",
                "+",
                "+    public SupervisorDetails(String id, Object meta) {",
                "+        this(id, null, meta, null, null, null);",
                "+    }",
                "+",
                "+    public SupervisorDetails(String id, Object meta, Map<String, Double> totalResources) {",
                "+        this(id, null, meta, null, null, totalResources);",
                "+    }",
                "+",
                "+    public SupervisorDetails(String id, Object meta, Collection<? extends Number> allPorts) {",
                "+        this(id, null, meta, null, allPorts, null);",
                "+    }",
                "+",
                "+    public SupervisorDetails(String id, String host, Object schedulerMeta, Collection<? extends Number> allPorts) {",
                "+        this(id, host, null, schedulerMeta, allPorts, null);",
                "+    }",
                "+",
                "+    public SupervisorDetails(String id, String host, Object schedulerMeta,",
                "+                             Collection<? extends Number> allPorts, Map<String, Double> totalResources) {",
                "+        this(id, host, null, schedulerMeta, allPorts, totalResources);",
                "+    }",
                "+    ",
                "+    @Override",
                "+    public String toString() {",
                "+        return getClass().getSimpleName() + \" ID: \" + id + \" HOST: \" + host + \" META: \" + meta",
                "+                + \" SCHED_META: \" + schedulerMeta + \" PORTS: \" + allPorts;",
                "+    }",
                "+",
                "+    private void setAllPorts(Collection<? extends Number> allPorts) {",
                "+        this.allPorts = new HashSet<>();",
                "+        if (allPorts != null) {",
                "+            for (Number n: allPorts) {",
                "+                this.allPorts.add(n.intValue());",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    public String getId() {",
                "+        return id;",
                "+    }",
                "+",
                "+    public String getHost() {",
                "+        return host;",
                "+    }",
                "+",
                "+    public Object getMeta() {",
                "+        return meta;",
                "+    }",
                "+    ",
                "+    public Set<Integer> getAllPorts() {",
                "+        return allPorts;",
                "+    }",
                "+",
                "+    public Object getSchedulerMeta() {",
                "+        return this.schedulerMeta;",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the total Memory on this supervisor in MB.",
                "+     */",
                "+    public double getTotalMemory() {",
                "+        return totalResources.getTotalMemoryMb();",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the total CPU on this supervisor in % CPU.",
                "+     */",
                "+    public double getTotalCpu() {",
                "+        return totalResources.getTotalCpu();",
                "+    }",
                "+",
                "+    /**",
                "+     * Get all resources for this Supervisor.",
                "+     */",
                "+    public NormalizedResourceOffer getTotalResources() {",
                "+        return totalResources;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java b/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "index d92c80af5..8e4c1d532 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "@@ -37,3 +37,3 @@ import org.apache.storm.generated.SpoutSpec;",
                " import org.apache.storm.generated.StormTopology;",
                "-import org.apache.storm.scheduler.resource.ResourceUtils;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                " import org.apache.storm.utils.ObjectReader;",
                "@@ -44,4 +44,2 @@ import org.slf4j.LoggerFactory;",
                "-import static org.apache.storm.scheduler.resource.ResourceUtils.normalizedResourceMap;",
                "-",
                " public class TopologyDetails {",
                "@@ -53,3 +51,3 @@ public class TopologyDetails {",
                "     //<ExecutorDetails - Task, Map<String - Type of resource, Map<String - type of that resource, Double - amount>>>",
                "-    private Map<ExecutorDetails, Map<String, Double>> resourceList;",
                "+    private Map<ExecutorDetails, NormalizedResourceRequest> resourceList;",
                "     //Max heap size for a worker used by topology",
                "@@ -140,8 +138,6 @@ public class TopologyDetails {",
                "                 //the json_conf is populated by TopologyBuilder (e.g. boltDeclarer.setMemoryLoad)",
                "-                Map<String, Double> topologyResources =",
                "-                    ResourceUtils.parseResources(bolt.getValue().get_common().get_json_conf());",
                "-                ResourceUtils.checkInitialization(topologyResources, bolt.getKey(), this.topologyConf);",
                "+                NormalizedResourceRequest topologyResources = new NormalizedResourceRequest(bolt.getValue().get_common(), topologyConf);",
                "                 for (Map.Entry<ExecutorDetails, String> anExecutorToComponent :",
                "                     executorToComponent.entrySet()) {",
                "-                    if (bolt.getKey().equals(anExecutorToComponent.getValue()) && !topologyResources.isEmpty()) {",
                "+                    if (bolt.getKey().equals(anExecutorToComponent.getValue())) {",
                "                         resourceList.put(anExecutorToComponent.getKey(), topologyResources);",
                "@@ -154,8 +150,6 @@ public class TopologyDetails {",
                "             for (Map.Entry<String, SpoutSpec> spout : topology.get_spouts().entrySet()) {",
                "-                Map<String, Double> topologyResources =",
                "-                    ResourceUtils.parseResources(spout.getValue().get_common().get_json_conf());",
                "-                ResourceUtils.checkInitialization(topologyResources, spout.getKey(), this.topologyConf);",
                "+                NormalizedResourceRequest topologyResources = new NormalizedResourceRequest(spout.getValue().get_common(), topologyConf);",
                "                 for (Map.Entry<ExecutorDetails, String> anExecutorToComponent :",
                "                     executorToComponent.entrySet()) {",
                "-                    if (spout.getKey().equals(anExecutorToComponent.getValue()) && !topologyResources.isEmpty()) {",
                "+                    if (spout.getKey().equals(anExecutorToComponent.getValue())) {",
                "                         resourceList.put(anExecutorToComponent.getKey(), topologyResources);",
                "@@ -263,4 +257,3 @@ public class TopologyDetails {",
                "             ret = resourceList",
                "-                    .get(exec)",
                "-                    .get(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "+                    .get(exec).getOnHeapMemoryMb();;",
                "         }",
                "@@ -278,4 +271,3 @@ public class TopologyDetails {",
                "             ret = resourceList",
                "-                    .get(exec)",
                "-                    .get(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "+                    .get(exec).getOffHeapMemoryMb();",
                "         }",
                "@@ -337,3 +329,3 @@ public class TopologyDetails {",
                "      */",
                "-    public Map<String, Double> getTotalResources(ExecutorDetails exec) {",
                "+    public NormalizedResourceRequest getTotalResources(ExecutorDetails exec) {",
                "         if (hasExecInTopo(exec)) {",
                "@@ -352,4 +344,3 @@ public class TopologyDetails {",
                "             return resourceList",
                "-                    .get(exec)",
                "-                    .get(Constants.COMMON_CPU_RESOURCE_NAME);",
                "+                    .get(exec).getTotalCpu();",
                "         }",
                "@@ -442,3 +433,3 @@ public class TopologyDetails {",
                "      */",
                "-    public Map<String, Double> getTaskResourceReqList(ExecutorDetails exec) {",
                "+    public NormalizedResourceRequest getTaskResourceReqList(ExecutorDetails exec) {",
                "         if (hasExecInTopo(exec)) {",
                "@@ -460,3 +451,3 @@ public class TopologyDetails {",
                "      */",
                "-    public void addResourcesForExec(ExecutorDetails exec, Map<String, Double> resourceList) {",
                "+    public void addResourcesForExec(ExecutorDetails exec, NormalizedResourceRequest resourceList) {",
                "         if (hasExecInTopo(exec)) {",
                "@@ -472,45 +463,3 @@ public class TopologyDetails {",
                "     private void addDefaultResforExec(ExecutorDetails exec) {",
                "-        Double topologyComponentCpuPcorePercent =",
                "-            ObjectReader.getDouble(",
                "-                topologyConf.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT), null);",
                "-        Double topologyComponentResourcesOffheapMemoryMb =",
                "-            ObjectReader.getDouble(",
                "-                topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB), null);",
                "-        Double topologyComponentResourcesOnheapMemoryMb =",
                "-            ObjectReader.getDouble(",
                "-                topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB), null);",
                "-",
                "-        assert topologyComponentCpuPcorePercent != null;",
                "-        assert topologyComponentResourcesOffheapMemoryMb != null;",
                "-        assert topologyComponentResourcesOnheapMemoryMb != null;",
                "-",
                "-        Map<String, Double> defaultResourceList = new HashMap<>();",
                "-        defaultResourceList.put(",
                "-            Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, topologyComponentCpuPcorePercent);",
                "-        defaultResourceList.put(",
                "-            Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB,",
                "-            topologyComponentResourcesOffheapMemoryMb);",
                "-        defaultResourceList.put(",
                "-            Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB,",
                "-            topologyComponentResourcesOnheapMemoryMb);",
                "-",
                "-        adjustResourcesForExec(exec, defaultResourceList);",
                "-",
                "-        Map<String,Double> topologyComponentResourcesMap = (",
                "-                Map<String, Double>) this.topologyConf.getOrDefault(",
                "-                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<>());",
                "-",
                "-        topologyComponentResourcesMap = normalizedResourceMap(topologyComponentResourcesMap);",
                "-",
                "-        LOG.info(\"Scheduling Executor: {} with resource requirement as {}\",",
                "-                exec, topologyComponentResourcesMap);",
                "-        LOG.debug(",
                "-            \"Scheduling Executor: {} {} with memory requirement as onHeap: {} - offHeap: {} \"",
                "-                + \"and CPU requirement: {}\",",
                "-            getExecutorToComponent().get(exec),",
                "-            exec,",
                "-            topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB),",
                "-            topologyConf.get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB),",
                "-            topologyConf.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT));",
                "-        addResourcesForExec(exec, normalizedResourceMap(defaultResourceList));",
                "+        addResourcesForExec(exec, new NormalizedResourceRequest(topologyConf));",
                "     }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java",
                "new file mode 100644",
                "index 000000000..262365552",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java",
                "@@ -0,0 +1,89 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource;",
                "+",
                "+import java.util.HashMap;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import org.apache.storm.Constants;",
                "+import org.apache.storm.generated.WorkerResources;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * An offer of resources that has been normalized.",
                "+ */",
                "+public class NormalizedResourceOffer extends NormalizedResources {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(NormalizedResourceOffer.class);",
                "+    private double totalMemory;",
                "+",
                "+    /**",
                "+     * Create a new normalized set of resources.  Note that memory is not covered here becasue it is not consistent in requests vs offers",
                "+     * because of how on heap vs off heap is used.",
                "+     *",
                "+     * @param resources the resources to be normalized.",
                "+     */",
                "+    public NormalizedResourceOffer(Map<String, ? extends Number> resources) {",
                "+        super(resources, null);",
                "+    }",
                "+",
                "+    public NormalizedResourceOffer() {",
                "+        super(null, null);",
                "+    }",
                "+",
                "+    public NormalizedResourceOffer(NormalizedResourceOffer other) {",
                "+        super(other);",
                "+        this.totalMemory = other.totalMemory;",
                "+    }",
                "+",
                "+    @Override",
                "+    protected void initializeMemory(Map<String, Double> normalizedResources) {",
                "+        totalMemory = normalizedResources.getOrDefault(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, 0.0);",
                "+    }",
                "+",
                "+    @Override",
                "+    public double getTotalMemoryMb() {",
                "+        return totalMemory;",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        return super.toString() + \" MEM: \" + totalMemory;",
                "+    }",
                "+",
                "+    @Override",
                "+    public Map<String,Double> toNormalizedMap() {",
                "+        Map<String, Double> ret = super.toNormalizedMap();",
                "+        ret.put(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, totalMemory);",
                "+        return ret;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void add(NormalizedResources other) {",
                "+        super.add(other);",
                "+        totalMemory += other.getTotalMemoryMb();",
                "+    }",
                "+",
                "+    @Override",
                "+    public void remove(NormalizedResources other) {",
                "+        super.remove(other);",
                "+        totalMemory -= other.getTotalMemoryMb();",
                "+        assert totalMemory >= 0.0;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java",
                "new file mode 100644",
                "index 000000000..926184c35",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java",
                "@@ -0,0 +1,194 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource;",
                "+",
                "+import java.util.Collections;",
                "+import java.util.HashMap;",
                "+import java.util.HashSet;",
                "+import java.util.Map;",
                "+import java.util.Set;",
                "+import java.util.stream.Collectors;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.Constants;",
                "+import org.apache.storm.generated.ComponentCommon;",
                "+import org.apache.storm.generated.WorkerResources;",
                "+import org.apache.storm.utils.ObjectReader;",
                "+import org.json.simple.JSONObject;",
                "+import org.json.simple.parser.JSONParser;",
                "+import org.json.simple.parser.ParseException;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * A request that has been normalized.",
                "+ */",
                "+public class NormalizedResourceRequest extends NormalizedResources {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(NormalizedResourceRequest.class);",
                "+",
                "+    private static void putIfMissing(Map<String, Double> dest, String destKey, Map<String, Object> src, String srcKey) {",
                "+        if (!dest.containsKey(destKey)) {",
                "+            Number value = (Number)src.get(srcKey);",
                "+            if (value != null) {",
                "+                dest.put(destKey, value.doubleValue());",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    private static Map<String, Double> getDefaultResources(Map<String, Object> topoConf) {",
                "+        Map<String, Double> ret = normalizedResourceMap((Map<String, Number>) topoConf.getOrDefault(",
                "+            Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<>()));",
                "+        putIfMissing(ret, Constants.COMMON_CPU_RESOURCE_NAME, topoConf, Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT);",
                "+        putIfMissing(ret, Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, topoConf, Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB);",
                "+        putIfMissing(ret, Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, topoConf, Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB);",
                "+        return ret;",
                "+    }",
                "+",
                "+    private static Map<String, Double> parseResources(String input) {",
                "+        Map<String, Double> topologyResources = new HashMap<>();",
                "+        JSONParser parser = new JSONParser();",
                "+        LOG.debug(\"Input to parseResources {}\", input);",
                "+        try {",
                "+            if (input != null) {",
                "+                Object obj = parser.parse(input);",
                "+                JSONObject jsonObject = (JSONObject) obj;",
                "+",
                "+                // Legacy resource parsing",
                "+                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB)) {",
                "+                    Double topoMemOnHeap = ObjectReader",
                "+                        .getDouble(jsonObject.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB), null);",
                "+                    topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, topoMemOnHeap);",
                "+                }",
                "+                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB)) {",
                "+                    Double topoMemOffHeap = ObjectReader",
                "+                        .getDouble(jsonObject.get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB), null);",
                "+                    topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, topoMemOffHeap);",
                "+                }",
                "+                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT)) {",
                "+                    Double topoCpu = ObjectReader.getDouble(jsonObject.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT),",
                "+                        null);",
                "+                    topologyResources.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, topoCpu);",
                "+                }",
                "+",
                "+                // If resource is also present in resources map will overwrite the above",
                "+                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "+                    Map<String, Number> rawResourcesMap =",
                "+                        (Map<String, Number>) jsonObject.computeIfAbsent(",
                "+                            Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "+",
                "+                    for (Map.Entry<String, Number> stringNumberEntry : rawResourcesMap.entrySet()) {",
                "+                        topologyResources.put(",
                "+                            stringNumberEntry.getKey(), stringNumberEntry.getValue().doubleValue());",
                "+                    }",
                "+",
                "+",
                "+                }",
                "+            }",
                "+        } catch (ParseException e) {",
                "+            LOG.error(\"Failed to parse component resources is:\" + e.toString(), e);",
                "+            return null;",
                "+        }",
                "+        return topologyResources;",
                "+    }",
                "+",
                "+    private double onHeap;",
                "+    private double offHeap;",
                "+",
                "+    /**",
                "+     * Create a new normalized set of resources.  Note that memory is not covered here becasue it is not consistent in requests vs offers",
                "+     * because of how on heap vs off heap is used.",
                "+     *",
                "+     * @param resources the resources to be normalized.",
                "+     * @param topologyConf the config for the topology",
                "+     */",
                "+    private NormalizedResourceRequest(Map<String, ? extends Number> resources,",
                "+                                     Map<String, Object> topologyConf) {",
                "+        super(resources, getDefaultResources(topologyConf));",
                "+    }",
                "+",
                "+    public NormalizedResourceRequest(ComponentCommon component, Map<String, Object> topoConf) {",
                "+        this(parseResources(component.get_json_conf()), topoConf);",
                "+    }",
                "+",
                "+    public NormalizedResourceRequest(Map<String, Object> topoConf) {",
                "+        this((Map<String, ? extends Number>) null, topoConf);",
                "+    }",
                "+",
                "+    public NormalizedResourceRequest() {",
                "+        super(null, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public Map<String,Double> toNormalizedMap() {",
                "+        Map<String, Double> ret = super.toNormalizedMap();",
                "+        ret.put(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, offHeap);",
                "+        ret.put(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, onHeap);",
                "+        return ret;",
                "+    }",
                "+",
                "+    @Override",
                "+    protected void initializeMemory(Map<String, Double> normalizedResources) {",
                "+        onHeap = normalizedResources.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+        offHeap = normalizedResources.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+    }",
                "+",
                "+    public double getOnHeapMemoryMb() {",
                "+        return onHeap;",
                "+    }",
                "+",
                "+    public void addOnHeap(final double onHeap) {",
                "+        this.onHeap += onHeap;",
                "+    }",
                "+",
                "+    public double getOffHeapMemoryMb() {",
                "+        return offHeap;",
                "+    }",
                "+",
                "+    public void addOffHeap(final double offHeap) {",
                "+        this.offHeap += offHeap;",
                "+    }",
                "+",
                "+    /**",
                "+     * Add the resources in other to this.",
                "+     * @param other the other Request to add to this.",
                "+     */",
                "+    public void add(NormalizedResourceRequest other) {",
                "+        super.add(other);",
                "+        onHeap += other.onHeap;",
                "+        offHeap += other.offHeap;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void add(WorkerResources value) {",
                "+        super.add(value);",
                "+        //The resources are already normalized",
                "+        Map<String, Double> resources = value.get_resources();",
                "+        onHeap += resources.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+        offHeap += resources.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+    }",
                "+",
                "+    @Override",
                "+    public double getTotalMemoryMb() {",
                "+        return getOnHeapMemoryMb() + getOffHeapMemoryMb();",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        return super.toString() + \" onHeap: \" + onHeap + \" offHeap: \" + offHeap;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "new file mode 100644",
                "index 000000000..8ed1a5728",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "@@ -0,0 +1,290 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource;",
                "+",
                "+import static org.apache.storm.Constants.*;",
                "+",
                "+import java.util.Arrays;",
                "+import java.util.Collections;",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+import java.util.concurrent.ConcurrentHashMap;",
                "+import java.util.concurrent.ConcurrentMap;",
                "+import java.util.concurrent.atomic.AtomicInteger;",
                "+import java.util.stream.Collectors;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.Constants;",
                "+import org.apache.storm.generated.WorkerResources;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * Resources that have been normalized.",
                "+ */",
                "+public abstract class NormalizedResources {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(NormalizedResources.class);",
                "+    public static final Map<String, String> RESOURCE_NAME_MAPPING;",
                "+",
                "+    static {",
                "+        Map<String, String> tmp = new HashMap<>();",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, COMMON_CPU_RESOURCE_NAME);",
                "+        tmp.put(Config.SUPERVISOR_CPU_CAPACITY, COMMON_CPU_RESOURCE_NAME);",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "+        tmp.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, COMMON_TOTAL_MEMORY_RESOURCE_NAME);",
                "+        RESOURCE_NAME_MAPPING = Collections.unmodifiableMap(tmp);",
                "+    }",
                "+",
                "+    private static double[] makeArray(Map<String, Double> normalizedResources) {",
                "+        //To avoid locking we will go through the map twice.  It should be small so it is probably not a big deal",
                "+        for (String key : normalizedResources.keySet()) {",
                "+            //We are going to skip over CPU and Memory, because they are captured elsewhere",
                "+            if (!COMMON_CPU_RESOURCE_NAME.equals(key)",
                "+                && !COMMON_TOTAL_MEMORY_RESOURCE_NAME.equals(key)",
                "+                && !COMMON_OFFHEAP_MEMORY_RESOURCE_NAME.equals(key)",
                "+                && !COMMON_ONHEAP_MEMORY_RESOURCE_NAME.equals(key)) {",
                "+                resourceNames.computeIfAbsent(key, (k) -> counter.getAndIncrement());",
                "+            }",
                "+        }",
                "+        //By default all of the values are 0",
                "+        double [] ret = new double[counter.get()];",
                "+        for (Map.Entry<String, Double> entry : normalizedResources.entrySet()) {",
                "+            Integer index = resourceNames.get(entry.getKey());",
                "+            if (index != null) {",
                "+                //index == null if it is memory or CPU",
                "+                ret[index] = entry.getValue();",
                "+            }",
                "+        }",
                "+        return ret;",
                "+    }",
                "+",
                "+    private static final ConcurrentMap<String, Integer> resourceNames = new ConcurrentHashMap<>();",
                "+    private static final AtomicInteger counter = new AtomicInteger(0);",
                "+    private double cpu;",
                "+    private double[] otherResources;",
                "+",
                "+    public NormalizedResources(NormalizedResources other) {",
                "+        cpu = other.cpu;",
                "+        otherResources = Arrays.copyOf(other.otherResources, other.otherResources.length);",
                "+    }",
                "+",
                "+    /**",
                "+     * Create a new normalized set of resources.  Note that memory is not",
                "+     * covered here because it is not consistent in requests vs offers because",
                "+     * of how on heap vs off heap is used.",
                "+     * @param resources the resources to be normalized.",
                "+     * @param defaults the default resources that will also be normalized and combined with the real resources.",
                "+     */",
                "+    public NormalizedResources(Map<String, ? extends Number> resources, Map<String, ? extends Number> defaults) {",
                "+        Map<String, Double> normalizedResources = normalizedResourceMap(defaults);",
                "+        normalizedResources.putAll(normalizedResourceMap(resources));",
                "+        cpu = normalizedResources.getOrDefault(Constants.COMMON_CPU_RESOURCE_NAME, 0.0);",
                "+        otherResources = makeArray(normalizedResources);",
                "+        initializeMemory(normalizedResources);",
                "+    }",
                "+",
                "+    /**",
                "+     * Initialize any memory usage from the normalized map.",
                "+     * @param normalizedResources the normalized resource map.",
                "+     */",
                "+    protected abstract void initializeMemory(Map<String, Double> normalizedResources);",
                "+",
                "+    /**",
                "+     * Normalizes a supervisor resource map or topology details map's keys to universal resource names.",
                "+     * @param resourceMap resource map of either Supervisor or Topology",
                "+     * @return the resource map with common resource names",
                "+     */",
                "+    public static Map<String, Double> normalizedResourceMap(Map<String, ? extends Number> resourceMap) {",
                "+        if (resourceMap == null) {",
                "+            return new HashMap<>();",
                "+        }",
                "+        return new HashMap<>(resourceMap.entrySet().stream()",
                "+            .collect(Collectors.toMap(",
                "+                //Map the key if needed",
                "+                (e) -> RESOURCE_NAME_MAPPING.getOrDefault(e.getKey(), e.getKey()),",
                "+                //Map the value",
                "+                (e) -> e.getValue().doubleValue())));",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the total amount of memory.",
                "+     * @return the total amount of memory requested or provided.",
                "+     */",
                "+    public abstract double getTotalMemoryMb();",
                "+",
                "+    /**",
                "+     * Get the total amount of cpu.",
                "+     * @return the amount of cpu.",
                "+     */",
                "+    public double getTotalCpu() {",
                "+        return cpu;",
                "+    }",
                "+",
                "+    private void add(double[] resourceArray) {",
                "+        int otherLength = resourceArray.length;",
                "+        int length = otherResources.length;",
                "+        if (otherLength > length) {",
                "+            double [] newResources = new double[otherLength];",
                "+            System.arraycopy(newResources, 0, otherResources, 0, length);",
                "+            otherResources = newResources;",
                "+        }",
                "+        for (int i = 0; i < otherLength; i++) {",
                "+            otherResources[i] += resourceArray[i];",
                "+        }",
                "+    }",
                "+",
                "+    public void add(NormalizedResources other) {",
                "+        this.cpu += other.cpu;",
                "+        add(other.otherResources);",
                "+    }",
                "+",
                "+    /**",
                "+     * Add the resources from a worker to this.",
                "+     * @param value the worker resources that should be added to this.",
                "+     */",
                "+    public void add(WorkerResources value) {",
                "+        Map<String, Double> normalizedResources = value.get_resources();",
                "+        cpu += normalizedResources.getOrDefault(Constants.COMMON_CPU_RESOURCE_NAME, 0.0);",
                "+        add(makeArray(normalizedResources));",
                "+    }",
                "+",
                "+    /**",
                "+     * Remove the resources from other.  This is the same as subtracting the resources in other from this.",
                "+     * @param other the resources we want removed.",
                "+     */",
                "+    public void remove(NormalizedResources other) {",
                "+        this.cpu -= other.cpu;",
                "+        assert cpu >= 0.0;",
                "+        int otherLength = other.otherResources.length;",
                "+        int length = otherResources.length;",
                "+        if (otherLength > length) {",
                "+            double [] newResources = new double[otherLength];",
                "+            System.arraycopy(newResources, 0, otherResources, 0, length);",
                "+            otherResources = newResources;",
                "+        }",
                "+        for (int i = 0; i < Math.min(length, otherLength); i++) {",
                "+            otherResources[i] -= other.otherResources[i];",
                "+            assert otherResources[i] >= 0.0;",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        return \"CPU: \" + cpu;",
                "+    }",
                "+",
                "+    /**",
                "+     * Return a Map of the normalized resource name to a double.  This should only",
                "+     * be used when returning thrift resource requests to the end user.",
                "+     */",
                "+    public Map<String,Double> toNormalizedMap() {",
                "+        HashMap<String, Double> ret = new HashMap<>();",
                "+        ret.put(Constants.COMMON_CPU_RESOURCE_NAME, cpu);",
                "+        int length = otherResources.length;",
                "+        for (Map.Entry<String, Integer> entry: resourceNames.entrySet()) {",
                "+            int index = entry.getValue();",
                "+            if (index < length) {",
                "+                ret.put(entry.getKey(), otherResources[index]);",
                "+            }",
                "+        }",
                "+        return ret;",
                "+    }",
                "+",
                "+    private double getResourceAt(int index) {",
                "+        if (index >= otherResources.length) {",
                "+            return 0.0;",
                "+        }",
                "+        return otherResources[index];",
                "+    }",
                "+",
                "+    /**",
                "+     * A simple sanity check to see if all of the resources in this would be large enough to hold the resources in other ignoring memory.",
                "+     * It does not check memory because with shared memory it is beyond the scope of this.",
                "+     * @param other the resources that we want to check if they would fit in this.",
                "+     * @return true if it might fit, else false if it could not possibly fit.",
                "+     */",
                "+    public boolean couldHoldIgnoringMemory(NormalizedResources other) {",
                "+        if (this.cpu < other.getTotalCpu()) {",
                "+            return false;",
                "+        }",
                "+        int length = Math.max(this.otherResources.length, other.otherResources.length);",
                "+        for (int i = 0; i < length; i++) {",
                "+            if (getResourceAt(i) < other.getResourceAt(i)) {",
                "+                return false;",
                "+            }",
                "+        }",
                "+        return true;",
                "+    }",
                "+",
                "+    /**",
                "+     * Calculate the average resource usage percentage with this being the total resources and",
                "+     * used being the amounts used.",
                "+     * @param used the amount of resources used.",
                "+     * @return the average percentage used 0.0 to 100.0.",
                "+     */",
                "+    public double calculateAveragePercentageUsedBy(NormalizedResources used) {",
                "+        double total = 0.0;",
                "+        double totalMemory = getTotalMemoryMb();",
                "+        if (totalMemory != 0.0) {",
                "+            total += used.getTotalMemoryMb() / totalMemory;",
                "+        }",
                "+        double totalCpu = getTotalCpu();",
                "+        if (totalCpu != 0.0) {",
                "+            total += used.getTotalCpu() / getTotalCpu();",
                "+        }",
                "+        //If total is 0 we add in a 0% used, so we can just skip over anything that is not in both.",
                "+        int length = Math.min(used.otherResources.length, otherResources.length);",
                "+        for (int i = 0; i < length; i++) {",
                "+            if (otherResources[i] != 0.0) {",
                "+                total += used.otherResources[i] / otherResources[i];",
                "+            }",
                "+        }",
                "+        //To get the count we divide by we need to take the maximum length because we are doing an average.",
                "+        return (total * 100.0) / (2 + Math.max(otherResources.length, used.otherResources.length));",
                "+    }",
                "+",
                "+    /**",
                "+     * Calculate the minimum resource usage percentage with this being the total resources and",
                "+     * used being the amounts used.",
                "+     * @param used the amount of resources used.",
                "+     * @return the minimum percentage used 0.0 to 100.0.",
                "+     */",
                "+    public double calculateMinPercentageUsedBy(NormalizedResources used) {",
                "+        double totalMemory = getTotalMemoryMb();",
                "+        double totalCpu = getTotalCpu();",
                "+        if (used.otherResources.length != otherResources.length",
                "+            || totalMemory == 0.0",
                "+            || totalCpu == 0.0) {",
                "+            //If the lengths don't match one of the resources will be 0, which means we would calculate the percentage to be 0.0",
                "+            // and so the min would be 0.0 (assuming that we can never go negative on a resource being used.",
                "+            return 0.0;",
                "+        }",
                "+        double min = used.getTotalMemoryMb() / totalMemory;",
                "+        min = Math.min(min, used.getTotalCpu() / getTotalCpu());",
                "+",
                "+        for (int i = 0; i < otherResources.length; i++) {",
                "+            if (otherResources[i] != 0.0) {",
                "+                min = Math.min(min, used.otherResources[i] / otherResources[i]);",
                "+            } else {",
                "+                return 0.0; //0 will be the minimum, because we count values not in here as 0",
                "+            }",
                "+        }",
                "+        return min * 100.0;",
                "+    }",
                "+}",
                "\\ No newline at end of file",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "index 633fb5c16..535000576 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "@@ -28,3 +28,2 @@ import java.util.Set;",
                "-import org.apache.storm.Config;",
                " import org.apache.storm.Constants;",
                "@@ -57,3 +56,2 @@ public class RAS_Node {",
                "     private final Set<WorkerSlot> originallyFreeSlots;",
                "-    private final Map<String, Double> totalResources;",
                "@@ -91,8 +89,2 @@ public class RAS_Node {",
                "-        if (isAlive) {",
                "-            totalResources = getTotalResources();",
                "-        } else {",
                "-            totalResources = new HashMap<>();",
                "-        }",
                "-",
                "         HashSet<String> freeById = new HashSet<>(slots.keySet());",
                "@@ -369,3 +361,3 @@ public class RAS_Node {",
                "             td,",
                "-            this.getTotalAvailableResources(),",
                "+            getTotalAvailableResources(),",
                "             td.getTopologyWorkerMaxHeapSize()",
                "@@ -431,6 +423,4 @@ public class RAS_Node {",
                "      */",
                "-    public Double getAvailableMemoryResources() {",
                "-        Map<String, Double> allAvailableResources = getTotalAvailableResources();",
                "-        return allAvailableResources.getOrDefault(",
                "-                Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, 0.0);",
                "+    public double getAvailableMemoryResources() {",
                "+        return getTotalAvailableResources().getTotalMemoryMb();",
                "     }",
                "@@ -439,6 +429,4 @@ public class RAS_Node {",
                "      * Gets total resources for this node.",
                "-     *",
                "-     * @return Map<String, Double> of all resources",
                "      */",
                "-    public Map<String, Double> getTotalResources() {",
                "+    public NormalizedResourceOffer getTotalResources() {",
                "         if (sup != null) {",
                "@@ -446,3 +434,3 @@ public class RAS_Node {",
                "         } else {",
                "-            return new HashMap<>();",
                "+            return new NormalizedResourceOffer();",
                "         }",
                "@@ -453,26 +441,11 @@ public class RAS_Node {",
                "      *",
                "-     * @return Map<String, Double> of all resources",
                "+     * @return All of the available resources.",
                "      */",
                "-    public Map<String, Double> getTotalAvailableResources() {",
                "+    public NormalizedResourceOffer getTotalAvailableResources() {",
                "         if (sup != null) {",
                "-            Map<String, Double> totalResources = sup.getTotalResources();",
                "-            Map<String, Double> scheduledResources = cluster.getAllScheduledResourcesForNode(sup.getId());",
                "-            Map<String, Double> availableResources = new HashMap<>();",
                "-            for (Entry resource : totalResources.entrySet()) {",
                "-                if(resource.getKey() == Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME) {",
                "-                    availableResources.put(resource.getKey().toString(),",
                "-                            ObjectReader.getDouble(resource.getValue())",
                "-                                    - (scheduledResources.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0)",
                "-                                    + scheduledResources.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0))",
                "-                    );",
                "-                    continue;",
                "-                }",
                "-                availableResources.put(resource.getKey().toString(),",
                "-                        ObjectReader.getDouble(resource.getValue())",
                "-                                - scheduledResources.getOrDefault(resource.getKey(), 0.0));",
                "-",
                "-            }",
                "+            NormalizedResourceOffer availableResources = new NormalizedResourceOffer(sup.getTotalResources());",
                "+            availableResources.remove(cluster.getAllScheduledResourcesForNode(sup.getId()));",
                "             return availableResources;",
                "         } else {",
                "-            return new HashMap<>();",
                "+            return new NormalizedResourceOffer();",
                "         }",
                "@@ -485,3 +458,3 @@ public class RAS_Node {",
                "      */",
                "-    public Double getTotalMemoryResources() {",
                "+    public double getTotalMemoryResources() {",
                "         if (sup != null) {",
                "@@ -499,3 +472,3 @@ public class RAS_Node {",
                "     public double getAvailableCpuResources() {",
                "-        return getTotalAvailableResources().getOrDefault(Constants.COMMON_CPU_RESOURCE_NAME, 0.0);",
                "+        return getTotalAvailableResources().getTotalCpu();",
                "     }",
                "@@ -507,5 +480,5 @@ public class RAS_Node {",
                "      */",
                "-    public Double getTotalCpuResources() {",
                "+    public double getTotalCpuResources() {",
                "         if (sup != null) {",
                "-            return sup.getTotalCPU();",
                "+            return sup.getTotalCpu();",
                "         } else {",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "index 9b0ee1532..83ee4cb6d 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "@@ -20,8 +20,4 @@ package org.apache.storm.scheduler.resource;",
                "-import java.util.Collection;",
                "-import java.util.Collections;",
                " import java.util.HashMap;",
                "-import java.util.HashSet;",
                " import java.util.Map;",
                "-import java.util.Set;",
                "@@ -32,3 +28,2 @@ import org.apache.storm.generated.SpoutSpec;",
                " import org.apache.storm.generated.StormTopology;",
                "-import org.apache.storm.utils.ObjectReader;",
                " import org.json.simple.JSONObject;",
                "@@ -39,4 +34,2 @@ import org.slf4j.LoggerFactory;",
                "-import static org.apache.storm.Constants.resourceNameMapping;",
                "-",
                " public class ResourceUtils {",
                "@@ -44,9 +37,17 @@ public class ResourceUtils {",
                "-    public static Map<String, Map<String, Double>> getBoltsResources(StormTopology topology,",
                "+    public static NormalizedResourceRequest getBoltResources(StormTopology topology, Map<String, Object> topologyConf,",
                "+                                                              String componentId) {",
                "+        if (topology.get_bolts() != null) {",
                "+            Bolt bolt = topology.get_bolts().get(componentId);",
                "+            return new NormalizedResourceRequest(bolt.get_common(), topologyConf);",
                "+        }",
                "+        return null;",
                "+    }",
                "+",
                "+    public static Map<String, NormalizedResourceRequest> getBoltsResources(StormTopology topology,",
                "                                                                      Map<String, Object> topologyConf) {",
                "-        Map<String, Map<String, Double>> boltResources = new HashMap<>();",
                "+        Map<String, NormalizedResourceRequest> boltResources = new HashMap<>();",
                "         if (topology.get_bolts() != null) {",
                "             for (Map.Entry<String, Bolt> bolt : topology.get_bolts().entrySet()) {",
                "-                Map<String, Double> topologyResources = parseResources(bolt.getValue().get_common().get_json_conf());",
                "-                checkInitialization(topologyResources, bolt.getValue().toString(), topologyConf);",
                "+                NormalizedResourceRequest topologyResources = new NormalizedResourceRequest(bolt.getValue().get_common(), topologyConf);",
                "                 if (LOG.isTraceEnabled()) {",
                "@@ -60,9 +61,17 @@ public class ResourceUtils {",
                "-    public static Map<String, Map<String, Double>> getSpoutsResources(StormTopology topology,",
                "+    public static NormalizedResourceRequest getSpoutResources(StormTopology topology, Map<String, Object> topologyConf,",
                "+                                                              String componentId) {",
                "+        if (topology.get_spouts() != null) {",
                "+            SpoutSpec spout = topology.get_spouts().get(componentId);",
                "+            return new NormalizedResourceRequest(spout.get_common(), topologyConf);",
                "+        }",
                "+        return null;",
                "+    }",
                "+",
                "+    public static Map<String, NormalizedResourceRequest> getSpoutsResources(StormTopology topology,",
                "                                                                       Map<String, Object> topologyConf) {",
                "-        Map<String, Map<String, Double>> spoutResources = new HashMap<>();",
                "+        Map<String, NormalizedResourceRequest> spoutResources = new HashMap<>();",
                "         if (topology.get_spouts() != null) {",
                "             for (Map.Entry<String, SpoutSpec> spout : topology.get_spouts().entrySet()) {",
                "-                Map<String, Double> topologyResources = parseResources(spout.getValue().get_common().get_json_conf());",
                "-                checkInitialization(topologyResources, spout.getValue().toString(), topologyConf);",
                "+                NormalizedResourceRequest topologyResources = new NormalizedResourceRequest(spout.getValue().get_common(), topologyConf);",
                "                 if (LOG.isTraceEnabled()) {",
                "@@ -141,185 +150,2 @@ public class ResourceUtils {",
                "-    public static void checkInitialization(Map<String, Double> topologyResources,",
                "-                                           String componentId, Map<String, Object> topologyConf) {",
                "-        StringBuilder msgBuilder = new StringBuilder();",
                "-",
                "-        Set<String> resourceNameSet = new HashSet<>();",
                "-",
                "-        resourceNameSet.add(",
                "-                Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT",
                "-        );",
                "-        resourceNameSet.add(",
                "-                Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB",
                "-        );",
                "-        resourceNameSet.add(",
                "-                Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB",
                "-        );",
                "-",
                "-        Map<String, Double> topologyComponentResourcesMap =",
                "-                (Map<String, Double>) topologyConf.getOrDefault(",
                "-                        Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<>());",
                "-",
                "-        resourceNameSet.addAll(topologyResources.keySet());",
                "-        resourceNameSet.addAll(topologyComponentResourcesMap.keySet());",
                "-",
                "-        for (String resourceName : resourceNameSet) {",
                "-            msgBuilder.append(checkInitResource(topologyResources, topologyConf, topologyComponentResourcesMap, resourceName));",
                "-        }",
                "-",
                "-        Map<String, Double> normalizedTopologyResources = normalizedResourceMap(topologyResources);",
                "-        topologyResources.clear();",
                "-        topologyResources.putAll(normalizedTopologyResources);",
                "-",
                "-        if (msgBuilder.length() > 0) {",
                "-            String resourceDefaults = msgBuilder.toString();",
                "-            LOG.debug(",
                "-                    \"Unable to extract resource requirement for Component {} \\n Resources : {}\",",
                "-                    componentId, resourceDefaults);",
                "-        }",
                "-    }",
                "-",
                "-    private static String checkInitResource(Map<String, Double> topologyResources, Map topologyConf,",
                "-                                            Map<String, Double> topologyComponentResourcesMap, String resourceName) {",
                "-        StringBuilder msgBuilder = new StringBuilder();",
                "-        String normalizedResourceName = resourceNameMapping.getOrDefault(resourceName, resourceName);",
                "-        if (!topologyResources.containsKey(normalizedResourceName)) {",
                "-            if (topologyConf.containsKey(resourceName)) {",
                "-                Double resourceValue = ObjectReader.getDouble(topologyConf.get(resourceName));",
                "-                if (resourceValue != null) {",
                "-                    topologyResources.put(normalizedResourceName, resourceValue);",
                "-                }",
                "-            }",
                "-",
                "-            if (topologyComponentResourcesMap.containsKey(normalizedResourceName)) {",
                "-                Double resourceValue = ObjectReader.getDouble(topologyComponentResourcesMap.get(resourceName));",
                "-                if (resourceValue != null) {",
                "-                    topologyResources.put(normalizedResourceName, resourceValue);",
                "-                }",
                "-            }",
                "-        }",
                "-",
                "-        return msgBuilder.toString();",
                "-    }",
                "-",
                "-    public static Map<String, Double> parseResources(String input) {",
                "-        Map<String, Double> topologyResources = new HashMap<>();",
                "-        JSONParser parser = new JSONParser();",
                "-        LOG.debug(\"Input to parseResources {}\", input);",
                "-        try {",
                "-            if (input != null) {",
                "-                Object obj = parser.parse(input);",
                "-                JSONObject jsonObject = (JSONObject) obj;",
                "-",
                "-                // Legacy resource parsing",
                "-                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB)) {",
                "-                    Double topoMemOnHeap = ObjectReader",
                "-                            .getDouble(jsonObject.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB), null);",
                "-                    topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, topoMemOnHeap);",
                "-                }",
                "-                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB)) {",
                "-                    Double topoMemOffHeap = ObjectReader",
                "-                            .getDouble(jsonObject.get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB), null);",
                "-                    topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, topoMemOffHeap);",
                "-                }",
                "-                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT)) {",
                "-                    Double topoCpu = ObjectReader.getDouble(jsonObject.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT),",
                "-                        null);",
                "-                    topologyResources.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, topoCpu);",
                "-                }",
                "-",
                "-                // If resource is also present in resources map will overwrite the above",
                "-                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "-                    Map<String, Number> rawResourcesMap =",
                "-                            (Map<String, Number>) jsonObject.computeIfAbsent(",
                "-                                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "-",
                "-                    for (Map.Entry<String, Number> stringNumberEntry : rawResourcesMap.entrySet()) {",
                "-                        topologyResources.put(",
                "-                                stringNumberEntry.getKey(), stringNumberEntry.getValue().doubleValue());",
                "-                    }",
                "-",
                "-",
                "-                }",
                "-            }",
                "-        } catch (ParseException e) {",
                "-            LOG.error(\"Failed to parse component resources is:\" + e.toString(), e);",
                "-            return null;",
                "-        }",
                "-        return normalizedResourceMap(topologyResources);",
                "-    }",
                "-",
                "-    /**",
                "-     * Calculate the sum of a collection of doubles.",
                "-     * @param list collection of doubles",
                "-     * @return the sum of of collection of doubles",
                "-     */",
                "-    public static double sum(Collection<Double> list) {",
                "-        double sum = 0.0;",
                "-        for (Double elem : list) {",
                "-            sum += elem;",
                "-        }",
                "-        return sum;",
                "-    }",
                "-",
                "-    /**",
                "-     * Calculate the average of a collection of doubles.",
                "-     * @param list a collection of doubles",
                "-     * @return the average of collection of doubles",
                "-     */",
                "-    public static double avg(Collection<Double> list) {",
                "-        return sum(list) / list.size();",
                "-    }",
                "-",
                "-    /**",
                "-     * Normalizes a supervisor resource map or topology details map's keys to universal resource names.",
                "-     * @param resourceMap resource map of either Supervisor or Topology",
                "-     * @return the resource map with common resource names",
                "-     */",
                "-    public static Map<String, Double> normalizedResourceMap(Map<String, Double> resourceMap) {",
                "-        Map<String, Double> result = new HashMap();",
                "-",
                "-        result.putAll(resourceMap);",
                "-        for (Map.Entry entry: resourceMap.entrySet()) {",
                "-            if (resourceNameMapping.containsKey(entry.getKey())) {",
                "-                result.put(resourceNameMapping.get(entry.getKey()), ObjectReader.getDouble(entry.getValue(), 0.0));",
                "-                result.remove(entry.getKey());",
                "-            }",
                "-        }",
                "-        return result;",
                "-    }",
                "-",
                "-    public static Map<String, Double> addResources(Map<String, Double> resourceMap1, Map<String, Double> resourceMap2) {",
                "-        Map<String, Double> result = new HashMap();",
                "-",
                "-        result.putAll(resourceMap1);",
                "-",
                "-        for (Map.Entry<String, Double> entry: resourceMap2.entrySet()) {",
                "-            if (result.containsKey(entry.getKey())) {",
                "-                result.put(entry.getKey(), ObjectReader.getDouble(entry.getValue(),",
                "-                        0.0) + ObjectReader.getDouble(resourceMap1.get(entry.getKey()), 0.0));",
                "-            } else {",
                "-                result.put(entry.getKey(), entry.getValue());",
                "-            }",
                "-        }",
                "-        return result;",
                "-",
                "-    }",
                "-",
                "-    public static Double getMinValuePresentInResourceMap(Map<String, Double> resourceMap) {",
                "-        return Collections.min(resourceMap.values());",
                "-    }",
                "-",
                "-    public static Map<String, Double> getPercentageOfTotalResourceMap(Map<String, Double> resourceMap, Map<String, Double> totalResourceMap) {",
                "-        Map<String, Double> result = new HashMap();",
                "-",
                "-        for(Map.Entry<String, Double> entry: totalResourceMap.entrySet()) {",
                "-            if (resourceMap.containsKey(entry.getKey())) {",
                "-                result.put(entry.getKey(),  (ObjectReader.getDouble(resourceMap.get(entry.getKey()))/ entry.getValue()) * 100.0) ;",
                "-            } else {",
                "-                result.put(entry.getKey(), 0.0);",
                "-            }",
                "-        }",
                "-        return result;",
                "-",
                "-    }",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "index 07c908e95..67bc86743 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "@@ -41,2 +41,3 @@ import org.apache.storm.scheduler.TopologyDetails;",
                " import org.apache.storm.scheduler.WorkerSlot;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                " import org.apache.storm.scheduler.resource.RAS_Node;",
                "@@ -131,5 +132,5 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "     static class AllResources {",
                "-        List<ObjectResources> objectResources = new LinkedList<ObjectResources>();",
                "-        Map<String, Double> availableResourcesOverall = new HashMap<>();",
                "-        Map<String, Double> totalResourcesOverall = new HashMap<>();",
                "+        List<ObjectResources> objectResources = new LinkedList<>();",
                "+        NormalizedResourceOffer availableResourcesOverall = new NormalizedResourceOffer();",
                "+        NormalizedResourceOffer totalResourcesOverall = new NormalizedResourceOffer();",
                "         String identifier;",
                "@@ -141,6 +142,6 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "         public AllResources(AllResources other) {",
                "-            this(   null,",
                "-                    new HashMap<>(other.availableResourcesOverall),",
                "-                    new HashMap(other.totalResourcesOverall),",
                "-                    other.identifier);",
                "+            this (null,",
                "+                new NormalizedResourceOffer(other.availableResourcesOverall),",
                "+                new NormalizedResourceOffer(other.totalResourcesOverall),",
                "+                other.identifier);",
                "             List<ObjectResources> objectResourcesList = new ArrayList<>();",
                "@@ -150,6 +151,6 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "             this.objectResources = objectResourcesList;",
                "-",
                "         }",
                "-        public AllResources(List<ObjectResources> objectResources, Map<String, Double> availableResourcesOverall, Map<String, Double> totalResourcesOverall, String identifier) {",
                "+        public AllResources(List<ObjectResources> objectResources, NormalizedResourceOffer availableResourcesOverall,",
                "+                            NormalizedResourceOffer totalResourcesOverall, String identifier) {",
                "             this.objectResources = objectResources;",
                "@@ -165,6 +166,6 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "     static class ObjectResources {",
                "-        String id;",
                "-        Map<String, Double> availableResources = new HashMap<>();",
                "-        Map<String, Double> totalResources = new HashMap<>();",
                "-        double effectiveResources = 0.0;",
                "+        public final String id;",
                "+        public NormalizedResourceOffer availableResources = new NormalizedResourceOffer();",
                "+        public NormalizedResourceOffer totalResources = new NormalizedResourceOffer();",
                "+        public double effectiveResources = 0.0;",
                "@@ -178,3 +179,4 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "-        public ObjectResources(String id, Map<String, Double> availableResources, Map<String, Double> totalResources, double effectiveResources) {",
                "+        public ObjectResources(String id, NormalizedResourceOffer availableResources, NormalizedResourceOffer totalResources,",
                "+                               double effectiveResources) {",
                "             this.id = id;",
                "@@ -222,6 +224,4 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "             nodes.add(node);",
                "-            allResources.availableResourcesOverall = ResourceUtils.addResources(",
                "-                    allResources.availableResourcesOverall, node.availableResources);",
                "-            allResources.totalResourcesOverall = ResourceUtils.addResources(",
                "-                    allResources.totalResourcesOverall, node.totalResources);",
                "+            allResources.availableResourcesOverall.add(node.availableResources);",
                "+            allResources.totalResourcesOverall.add(node.totalResources);",
                "@@ -236,3 +236,3 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "         String topoId = topologyDetails.getId();",
                "-        return this.sortObjectResources(",
                "+        return sortObjectResources(",
                "             allResources,",
                "@@ -245,3 +245,3 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "                     //Get execs already assigned in rack",
                "-                    Collection<ExecutorDetails> execs = new LinkedList<ExecutorDetails>();",
                "+                    Collection<ExecutorDetails> execs = new LinkedList<>();",
                "                     if (cluster.getAssignmentById(topoId) != null) {",
                "@@ -333,4 +333,4 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "                 RAS_Node node = nodes.getNodeById(nodeHostnameToId(nodeId));",
                "-                rack.availableResources = ResourceUtils.addResources(rack.availableResources, node.getTotalAvailableResources());",
                "-                rack.totalResources = ResourceUtils.addResources(rack.totalResources, node.getTotalResources());",
                "+                rack.availableResources.add(node.getTotalAvailableResources());",
                "+                rack.totalResources.add(node.getTotalAvailableResources());",
                "@@ -338,4 +338,4 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "-                allResources.totalResourcesOverall = ResourceUtils.addResources(allResources.totalResourcesOverall, rack.totalResources);",
                "-                allResources.availableResourcesOverall = ResourceUtils.addResources(allResources.availableResourcesOverall, rack.availableResources);",
                "+                allResources.totalResourcesOverall.add(rack.totalResources);",
                "+                allResources.availableResourcesOverall.add(rack.availableResources);",
                "@@ -349,3 +349,3 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "         String topoId = topologyDetails.getId();",
                "-        return this.sortObjectResources(",
                "+        return sortObjectResources(",
                "             allResources,",
                "@@ -353,22 +353,19 @@ public abstract class BaseResourceAwareStrategy implements IStrategy {",
                "             topologyDetails,",
                "-            new ExistingScheduleFunc() {",
                "-                @Override",
                "-                public int getNumExistingSchedule(String objectId) {",
                "-                    String rackId = objectId;",
                "-                    //Get execs already assigned in rack",
                "-                    Collection<ExecutorDetails> execs = new LinkedList<ExecutorDetails>();",
                "-                    if (cluster.getAssignmentById(topoId) != null) {",
                "-                        for (Map.Entry<ExecutorDetails, WorkerSlot> entry :",
                "-                            cluster.getAssignmentById(topoId).getExecutorToSlot().entrySet()) {",
                "-                            String nodeId = entry.getValue().getNodeId();",
                "-                            String hostname = idToNode(nodeId).getHostname();",
                "-                            ExecutorDetails exec = entry.getKey();",
                "-                            if (nodeIdToRackId.get(hostname) != null",
                "-                                && nodeIdToRackId.get(hostname).equals(rackId)) {",
                "-                                execs.add(exec);",
                "-                            }",
                "+            (objectId) -> {",
                "+                String rackId = objectId;",
                "+                //Get execs already assigned in rack",
                "+                Collection<ExecutorDetails> execs = new LinkedList<>();",
                "+                if (cluster.getAssignmentById(topoId) != null) {",
                "+                    for (Map.Entry<ExecutorDetails, WorkerSlot> entry :",
                "+                        cluster.getAssignmentById(topoId).getExecutorToSlot().entrySet()) {",
                "+                        String nodeId = entry.getValue().getNodeId();",
                "+                        String hostname = idToNode(nodeId).getHostname();",
                "+                        ExecutorDetails exec1 = entry.getKey();",
                "+                        if (nodeIdToRackId.get(hostname) != null",
                "+                            && nodeIdToRackId.get(hostname).equals(rackId)) {",
                "+                            execs.add(exec1);",
                "                         }",
                "                     }",
                "-                    return execs.size();",
                "                 }",
                "+                return execs.size();",
                "             });",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "index adf05f542..fc746b084 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "@@ -70,3 +70,3 @@ public class DefaultResourceAwareStrategy extends BaseResourceAwareStrategy impl",
                "         List<String> unFavoredNodes = (List<String>) td.getConf().get(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES);",
                "-        final List<ObjectResources> sortedNodes = this.sortAllNodes(td, null, favoredNodes, unFavoredNodes);",
                "+        final List<ObjectResources> sortedNodes = sortAllNodes(td, null, favoredNodes, unFavoredNodes);",
                "@@ -129,28 +129,4 @@ public class DefaultResourceAwareStrategy extends BaseResourceAwareStrategy impl",
                "         for (ObjectResources objectResources : allResources.objectResources) {",
                "-            StringBuilder sb = new StringBuilder();",
                "-            if (ResourceUtils.getMinValuePresentInResourceMap(objectResources.availableResources) <= 0) {",
                "-                objectResources.effectiveResources = 0.0;",
                "-            } else {",
                "-                List<Double> values = new LinkedList<>();",
                "-",
                "-                Map<String, Double> percentageTotal = ResourceUtils.getPercentageOfTotalResourceMap(",
                "-                        objectResources.availableResources, allResources.availableResourcesOverall",
                "-                );",
                "-                for(Map.Entry<String, Double> percentageEntry : percentageTotal.entrySet()) {",
                "-                    values.add(percentageEntry.getValue());",
                "-                    sb.append(String.format(\"%s %f(%f%%) \", percentageEntry.getKey(),",
                "-                            objectResources.availableResources.get(percentageEntry.getKey()),",
                "-                            percentageEntry.getValue())",
                "-                    );",
                "-",
                "-                }",
                "-",
                "-                objectResources.effectiveResources = Collections.min(values);",
                "-            }",
                "-            LOG.debug(",
                "-                    \"{}: Avail [ {} ] Total [ {} ] effective resources: {}\",",
                "-                    objectResources.id,",
                "-                    sb.toString(),",
                "-                    objectResources.totalResources,",
                "-                    objectResources.effectiveResources);",
                "+            objectResources.effectiveResources =",
                "+                allResources.availableResourcesOverall.calculateMinPercentageUsedBy(objectResources.availableResources);",
                "         }",
                "@@ -171,9 +147,4 @@ public class DefaultResourceAwareStrategy extends BaseResourceAwareStrategy impl",
                "                         } else {",
                "-                            Collection<Double> o1Values = ResourceUtils.getPercentageOfTotalResourceMap(",
                "-                                    o1.availableResources, allResources.availableResourcesOverall).values();",
                "-                            Collection<Double> o2Values = ResourceUtils.getPercentageOfTotalResourceMap(",
                "-                                    o2.availableResources, allResources.availableResourcesOverall).values();",
                "-",
                "-                            double o1Avg = ResourceUtils.avg(o1Values);",
                "-                            double o2Avg = ResourceUtils.avg(o2Values);",
                "+                            double o1Avg = allResources.availableResourcesOverall.calculateAveragePercentageUsedBy(o1.availableResources);",
                "+                            double o2Avg = allResources.availableResourcesOverall.calculateAveragePercentageUsedBy(o2.availableResources);",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "index a4de7c29d..893e2898e 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "@@ -20,4 +20,2 @@ package org.apache.storm.scheduler.resource.strategies.scheduling;",
                "-import com.google.common.annotations.VisibleForTesting;",
                "-",
                " import java.util.ArrayList;",
                "@@ -29,3 +27,2 @@ import java.util.Map;",
                " import java.util.TreeSet;",
                "-",
                " import org.apache.storm.Config;",
                "@@ -135,27 +132,3 @@ public class GenericResourceAwareStrategy extends BaseResourceAwareStrategy impl",
                "-        Map<String, Double> requestedResources = topologyDetails.getTotalResources(exec);",
                "         AllResources affinityBasedAllResources = new AllResources(allResources);",
                "-        for (ObjectResources objectResources : affinityBasedAllResources.objectResources) {",
                "-            StringBuilder sb = new StringBuilder();",
                "-            List<Double> values = new LinkedList<>();",
                "-",
                "-            for (Map.Entry<String, Double> availableResourcesEntry : objectResources.availableResources.entrySet()) {",
                "-                if (!requestedResources.containsKey(availableResourcesEntry.getKey())) {",
                "-                    objectResources.availableResources.put(availableResourcesEntry.getKey(), -1.0 * availableResourcesEntry.getValue());",
                "-                }",
                "-            }",
                "-",
                "-            Map<String, Double> percentageTotal = ResourceUtils.getPercentageOfTotalResourceMap(",
                "-                    objectResources.availableResources, allResources.availableResourcesOverall",
                "-            );",
                "-            for(Map.Entry<String, Double> percentageEntry : percentageTotal.entrySet()) {",
                "-                values.add(percentageEntry.getValue());",
                "-                sb.append(String.format(\"%s %f(%f%%) \", percentageEntry.getKey(),",
                "-                        objectResources.availableResources.get(percentageEntry.getKey()),",
                "-                        percentageEntry.getValue())",
                "-                );",
                "-",
                "-            }",
                "-",
                "-        }",
                "@@ -170,9 +143,4 @@ public class GenericResourceAwareStrategy extends BaseResourceAwareStrategy impl",
                "                     } else {",
                "-                        Collection<Double> o1Values = ResourceUtils.getPercentageOfTotalResourceMap(",
                "-                                o1.availableResources, allResources.availableResourcesOverall).values();",
                "-                        Collection<Double> o2Values = ResourceUtils.getPercentageOfTotalResourceMap(",
                "-                                o2.availableResources, allResources.availableResourcesOverall).values();",
                "-",
                "-                        double o1Avg = ResourceUtils.avg(o1Values);",
                "-                        double o2Avg = ResourceUtils.avg(o2Values);",
                "+                        double o1Avg = allResources.availableResourcesOverall.calculateAveragePercentageUsedBy(o1.availableResources);",
                "+                        double o2Avg = allResources.availableResourcesOverall.calculateAveragePercentageUsedBy(o2.availableResources);",
                "diff --git a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "index bd4c4feba..a64c9b4ed 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "@@ -75,2 +75,3 @@ import org.apache.storm.generated.StormTopology;",
                " import org.apache.storm.nimbus.NimbusInfo;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                " import org.apache.storm.scheduler.resource.ResourceUtils;",
                "@@ -717,3 +718,4 @@ public class ServerUtils {",
                "-    public static int getEstimatedWorkerCountForRASTopo(Map<String, Object> topoConf, StormTopology topology) throws InvalidTopologyException {",
                "+    public static int getEstimatedWorkerCountForRASTopo(Map<String, Object> topoConf, StormTopology topology)",
                "+        throws InvalidTopologyException {",
                "         return (int) Math.ceil(getEstimatedTotalHeapMemoryRequiredByTopo(topoConf, topology) /",
                "@@ -722,3 +724,4 @@ public class ServerUtils {",
                "-    public static double getEstimatedTotalHeapMemoryRequiredByTopo(Map<String, Object> topoConf, StormTopology topology) throws InvalidTopologyException {",
                "+    public static double getEstimatedTotalHeapMemoryRequiredByTopo(Map<String, Object> topoConf, StormTopology topology)",
                "+        throws InvalidTopologyException {",
                "         Map<String, Integer> componentParallelism = getComponentParallelism(topoConf, topology);",
                "@@ -726,5 +729,5 @@ public class ServerUtils {",
                "-        for(Map.Entry<String, Map<String, Double>> entry: ResourceUtils.getBoltsResources(topology, topoConf).entrySet()) {",
                "+        for (Map.Entry<String, NormalizedResourceRequest> entry: ResourceUtils.getBoltsResources(topology, topoConf).entrySet()) {",
                "             int parallelism = componentParallelism.getOrDefault(entry.getKey(), 1);",
                "-            double memoryRequirement = entry.getValue().get(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "+            double memoryRequirement = entry.getValue().getOnHeapMemoryMb();",
                "             totalMemoryRequired += memoryRequirement * parallelism;",
                "@@ -732,5 +735,5 @@ public class ServerUtils {",
                "-        for(Map.Entry<String, Map<String, Double>> entry: ResourceUtils.getSpoutsResources(topology, topoConf).entrySet()) {",
                "+        for (Map.Entry<String, NormalizedResourceRequest> entry: ResourceUtils.getSpoutsResources(topology, topoConf).entrySet()) {",
                "             int parallelism = componentParallelism.getOrDefault(entry.getKey(), 1);",
                "-            double memoryRequirement = entry.getValue().get(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "+            double memoryRequirement = entry.getValue().getOnHeapMemoryMb();",
                "             totalMemoryRequired += memoryRequirement * parallelism;",
                "@@ -740,3 +743,4 @@ public class ServerUtils {",
                "-    public static Map<String, Integer> getComponentParallelism(Map<String, Object> topoConf, StormTopology topology) throws InvalidTopologyException {",
                "+    public static Map<String, Integer> getComponentParallelism(Map<String, Object> topoConf, StormTopology topology)",
                "+        throws InvalidTopologyException {",
                "         Map<String, Integer> ret = new HashMap<>();"
            ],
            "changed_files": [
                "storm-client/pom.xml",
                "storm-client/src/jvm/org/apache/storm/Constants.java",
                "storm-server/pom.xml",
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/ExecutorDetails.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/SchedulerAssignment.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/SchedulerAssignmentImpl.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/SupervisorDetails.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2813": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2813",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "48e23a99bf90c50f5d4864dbd449b0079ab2d5e2",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512668163,
            "hunks": 370,
            "message": "STORM-2887: store metrics into RocksDB",
            "diff": [
                "diff --git a/conf/defaults.yaml b/conf/defaults.yaml",
                "index 2260a0b70..e15a26545 100644",
                "--- a/conf/defaults.yaml",
                "+++ b/conf/defaults.yaml",
                "@@ -297,2 +297,8 @@ storm.daemon.metrics.reporter.plugins:",
                "+storm.metricstore.class: \"org.apache.storm.metricstore.rocksdb.RocksDbStore\"",
                "+storm.metricstore.rocksdb.location: \"storm_rocks\"",
                "+storm.metricstore.rocksdb.create_if_missing: true",
                "+storm.metricstore.rocksdb.metadata_string_cache_capacity: 4000",
                "+storm.metricstore.rocksdb.retention_hours: 240",
                "+",
                " # configuration of cluster metrics consumer",
                "diff --git a/pom.xml b/pom.xml",
                "index 3c924e3ac..ccace428a 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -326,2 +326,3 @@",
                "         <jaxb-version>2.3.0</jaxb-version>",
                "+        <rocksdb-version>5.8.6</rocksdb-version>",
                "@@ -1050,2 +1051,7 @@",
                "             </dependency>",
                "+            <dependency>",
                "+                <groupId>org.rocksdb</groupId>",
                "+                <artifactId>rocksdbjni</artifactId>",
                "+                <version>${rocksdb-version}</version>",
                "+            </dependency>",
                "         </dependencies>",
                "@@ -1093,2 +1099,5 @@",
                "                         <reuseForks>true</reuseForks>",
                "+                        <systemPropertyVariables>",
                "+                            <storm.home>${project.basedir}/target/testhome</storm.home>",
                "+                        </systemPropertyVariables>",
                "                     </configuration>",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/Assignment.java b/storm-client/src/jvm/org/apache/storm/generated/Assignment.java",
                "index c3436d52f..e8741dfd7 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/Assignment.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/Assignment.java",
                "@@ -968,11 +968,11 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map666 = iprot.readMapBegin();",
                "-                struct.node_host = new HashMap<String,String>(2*_map666.size);",
                "-                String _key667;",
                "-                String _val668;",
                "-                for (int _i669 = 0; _i669 < _map666.size; ++_i669)",
                "+                org.apache.thrift.protocol.TMap _map686 = iprot.readMapBegin();",
                "+                struct.node_host = new HashMap<String,String>(2*_map686.size);",
                "+                String _key687;",
                "+                String _val688;",
                "+                for (int _i689 = 0; _i689 < _map686.size; ++_i689)",
                "                 {",
                "-                  _key667 = iprot.readString();",
                "-                  _val668 = iprot.readString();",
                "-                  struct.node_host.put(_key667, _val668);",
                "+                  _key687 = iprot.readString();",
                "+                  _val688 = iprot.readString();",
                "+                  struct.node_host.put(_key687, _val688);",
                "                 }",
                "@@ -988,16 +988,16 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map670 = iprot.readMapBegin();",
                "-                struct.executor_node_port = new HashMap<List<Long>,NodeInfo>(2*_map670.size);",
                "-                List<Long> _key671;",
                "-                NodeInfo _val672;",
                "-                for (int _i673 = 0; _i673 < _map670.size; ++_i673)",
                "+                org.apache.thrift.protocol.TMap _map690 = iprot.readMapBegin();",
                "+                struct.executor_node_port = new HashMap<List<Long>,NodeInfo>(2*_map690.size);",
                "+                List<Long> _key691;",
                "+                NodeInfo _val692;",
                "+                for (int _i693 = 0; _i693 < _map690.size; ++_i693)",
                "                 {",
                "                   {",
                "-                    org.apache.thrift.protocol.TList _list674 = iprot.readListBegin();",
                "-                    _key671 = new ArrayList<Long>(_list674.size);",
                "-                    long _elem675;",
                "-                    for (int _i676 = 0; _i676 < _list674.size; ++_i676)",
                "+                    org.apache.thrift.protocol.TList _list694 = iprot.readListBegin();",
                "+                    _key691 = new ArrayList<Long>(_list694.size);",
                "+                    long _elem695;",
                "+                    for (int _i696 = 0; _i696 < _list694.size; ++_i696)",
                "                     {",
                "-                      _elem675 = iprot.readI64();",
                "-                      _key671.add(_elem675);",
                "+                      _elem695 = iprot.readI64();",
                "+                      _key691.add(_elem695);",
                "                     }",
                "@@ -1005,5 +1005,5 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "                   }",
                "-                  _val672 = new NodeInfo();",
                "-                  _val672.read(iprot);",
                "-                  struct.executor_node_port.put(_key671, _val672);",
                "+                  _val692 = new NodeInfo();",
                "+                  _val692.read(iprot);",
                "+                  struct.executor_node_port.put(_key691, _val692);",
                "                 }",
                "@@ -1019,16 +1019,16 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map677 = iprot.readMapBegin();",
                "-                struct.executor_start_time_secs = new HashMap<List<Long>,Long>(2*_map677.size);",
                "-                List<Long> _key678;",
                "-                long _val679;",
                "-                for (int _i680 = 0; _i680 < _map677.size; ++_i680)",
                "+                org.apache.thrift.protocol.TMap _map697 = iprot.readMapBegin();",
                "+                struct.executor_start_time_secs = new HashMap<List<Long>,Long>(2*_map697.size);",
                "+                List<Long> _key698;",
                "+                long _val699;",
                "+                for (int _i700 = 0; _i700 < _map697.size; ++_i700)",
                "                 {",
                "                   {",
                "-                    org.apache.thrift.protocol.TList _list681 = iprot.readListBegin();",
                "-                    _key678 = new ArrayList<Long>(_list681.size);",
                "-                    long _elem682;",
                "-                    for (int _i683 = 0; _i683 < _list681.size; ++_i683)",
                "+                    org.apache.thrift.protocol.TList _list701 = iprot.readListBegin();",
                "+                    _key698 = new ArrayList<Long>(_list701.size);",
                "+                    long _elem702;",
                "+                    for (int _i703 = 0; _i703 < _list701.size; ++_i703)",
                "                     {",
                "-                      _elem682 = iprot.readI64();",
                "-                      _key678.add(_elem682);",
                "+                      _elem702 = iprot.readI64();",
                "+                      _key698.add(_elem702);",
                "                     }",
                "@@ -1036,4 +1036,4 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "                   }",
                "-                  _val679 = iprot.readI64();",
                "-                  struct.executor_start_time_secs.put(_key678, _val679);",
                "+                  _val699 = iprot.readI64();",
                "+                  struct.executor_start_time_secs.put(_key698, _val699);",
                "                 }",
                "@@ -1049,13 +1049,13 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map684 = iprot.readMapBegin();",
                "-                struct.worker_resources = new HashMap<NodeInfo,WorkerResources>(2*_map684.size);",
                "-                NodeInfo _key685;",
                "-                WorkerResources _val686;",
                "-                for (int _i687 = 0; _i687 < _map684.size; ++_i687)",
                "+                org.apache.thrift.protocol.TMap _map704 = iprot.readMapBegin();",
                "+                struct.worker_resources = new HashMap<NodeInfo,WorkerResources>(2*_map704.size);",
                "+                NodeInfo _key705;",
                "+                WorkerResources _val706;",
                "+                for (int _i707 = 0; _i707 < _map704.size; ++_i707)",
                "                 {",
                "-                  _key685 = new NodeInfo();",
                "-                  _key685.read(iprot);",
                "-                  _val686 = new WorkerResources();",
                "-                  _val686.read(iprot);",
                "-                  struct.worker_resources.put(_key685, _val686);",
                "+                  _key705 = new NodeInfo();",
                "+                  _key705.read(iprot);",
                "+                  _val706 = new WorkerResources();",
                "+                  _val706.read(iprot);",
                "+                  struct.worker_resources.put(_key705, _val706);",
                "                 }",
                "@@ -1071,11 +1071,11 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map688 = iprot.readMapBegin();",
                "-                struct.total_shared_off_heap = new HashMap<String,Double>(2*_map688.size);",
                "-                String _key689;",
                "-                double _val690;",
                "-                for (int _i691 = 0; _i691 < _map688.size; ++_i691)",
                "+                org.apache.thrift.protocol.TMap _map708 = iprot.readMapBegin();",
                "+                struct.total_shared_off_heap = new HashMap<String,Double>(2*_map708.size);",
                "+                String _key709;",
                "+                double _val710;",
                "+                for (int _i711 = 0; _i711 < _map708.size; ++_i711)",
                "                 {",
                "-                  _key689 = iprot.readString();",
                "-                  _val690 = iprot.readDouble();",
                "-                  struct.total_shared_off_heap.put(_key689, _val690);",
                "+                  _key709 = iprot.readString();",
                "+                  _val710 = iprot.readDouble();",
                "+                  struct.total_shared_off_heap.put(_key709, _val710);",
                "                 }",
                "@@ -1119,6 +1119,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, struct.node_host.size()));",
                "-            for (Map.Entry<String, String> _iter692 : struct.node_host.entrySet())",
                "+            for (Map.Entry<String, String> _iter712 : struct.node_host.entrySet())",
                "             {",
                "-              oprot.writeString(_iter692.getKey());",
                "-              oprot.writeString(_iter692.getValue());",
                "+              oprot.writeString(_iter712.getKey());",
                "+              oprot.writeString(_iter712.getValue());",
                "             }",
                "@@ -1134,9 +1134,9 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.STRUCT, struct.executor_node_port.size()));",
                "-            for (Map.Entry<List<Long>, NodeInfo> _iter693 : struct.executor_node_port.entrySet())",
                "+            for (Map.Entry<List<Long>, NodeInfo> _iter713 : struct.executor_node_port.entrySet())",
                "             {",
                "               {",
                "-                oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, _iter693.getKey().size()));",
                "-                for (long _iter694 : _iter693.getKey())",
                "+                oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, _iter713.getKey().size()));",
                "+                for (long _iter714 : _iter713.getKey())",
                "                 {",
                "-                  oprot.writeI64(_iter694);",
                "+                  oprot.writeI64(_iter714);",
                "                 }",
                "@@ -1144,3 +1144,3 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               }",
                "-              _iter693.getValue().write(oprot);",
                "+              _iter713.getValue().write(oprot);",
                "             }",
                "@@ -1156,9 +1156,9 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.I64, struct.executor_start_time_secs.size()));",
                "-            for (Map.Entry<List<Long>, Long> _iter695 : struct.executor_start_time_secs.entrySet())",
                "+            for (Map.Entry<List<Long>, Long> _iter715 : struct.executor_start_time_secs.entrySet())",
                "             {",
                "               {",
                "-                oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, _iter695.getKey().size()));",
                "-                for (long _iter696 : _iter695.getKey())",
                "+                oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, _iter715.getKey().size()));",
                "+                for (long _iter716 : _iter715.getKey())",
                "                 {",
                "-                  oprot.writeI64(_iter696);",
                "+                  oprot.writeI64(_iter716);",
                "                 }",
                "@@ -1166,3 +1166,3 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "               }",
                "-              oprot.writeI64(_iter695.getValue());",
                "+              oprot.writeI64(_iter715.getValue());",
                "             }",
                "@@ -1178,6 +1178,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, struct.worker_resources.size()));",
                "-            for (Map.Entry<NodeInfo, WorkerResources> _iter697 : struct.worker_resources.entrySet())",
                "+            for (Map.Entry<NodeInfo, WorkerResources> _iter717 : struct.worker_resources.entrySet())",
                "             {",
                "-              _iter697.getKey().write(oprot);",
                "-              _iter697.getValue().write(oprot);",
                "+              _iter717.getKey().write(oprot);",
                "+              _iter717.getValue().write(oprot);",
                "             }",
                "@@ -1193,6 +1193,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, struct.total_shared_off_heap.size()));",
                "-            for (Map.Entry<String, Double> _iter698 : struct.total_shared_off_heap.entrySet())",
                "+            for (Map.Entry<String, Double> _iter718 : struct.total_shared_off_heap.entrySet())",
                "             {",
                "-              oprot.writeString(_iter698.getKey());",
                "-              oprot.writeDouble(_iter698.getValue());",
                "+              oprot.writeString(_iter718.getKey());",
                "+              oprot.writeDouble(_iter718.getValue());",
                "             }",
                "@@ -1251,6 +1251,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "           oprot.writeI32(struct.node_host.size());",
                "-          for (Map.Entry<String, String> _iter699 : struct.node_host.entrySet())",
                "+          for (Map.Entry<String, String> _iter719 : struct.node_host.entrySet())",
                "           {",
                "-            oprot.writeString(_iter699.getKey());",
                "-            oprot.writeString(_iter699.getValue());",
                "+            oprot.writeString(_iter719.getKey());",
                "+            oprot.writeString(_iter719.getValue());",
                "           }",
                "@@ -1261,12 +1261,12 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "           oprot.writeI32(struct.executor_node_port.size());",
                "-          for (Map.Entry<List<Long>, NodeInfo> _iter700 : struct.executor_node_port.entrySet())",
                "+          for (Map.Entry<List<Long>, NodeInfo> _iter720 : struct.executor_node_port.entrySet())",
                "           {",
                "             {",
                "-              oprot.writeI32(_iter700.getKey().size());",
                "-              for (long _iter701 : _iter700.getKey())",
                "+              oprot.writeI32(_iter720.getKey().size());",
                "+              for (long _iter721 : _iter720.getKey())",
                "               {",
                "-                oprot.writeI64(_iter701);",
                "+                oprot.writeI64(_iter721);",
                "               }",
                "             }",
                "-            _iter700.getValue().write(oprot);",
                "+            _iter720.getValue().write(oprot);",
                "           }",
                "@@ -1277,12 +1277,12 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "           oprot.writeI32(struct.executor_start_time_secs.size());",
                "-          for (Map.Entry<List<Long>, Long> _iter702 : struct.executor_start_time_secs.entrySet())",
                "+          for (Map.Entry<List<Long>, Long> _iter722 : struct.executor_start_time_secs.entrySet())",
                "           {",
                "             {",
                "-              oprot.writeI32(_iter702.getKey().size());",
                "-              for (long _iter703 : _iter702.getKey())",
                "+              oprot.writeI32(_iter722.getKey().size());",
                "+              for (long _iter723 : _iter722.getKey())",
                "               {",
                "-                oprot.writeI64(_iter703);",
                "+                oprot.writeI64(_iter723);",
                "               }",
                "             }",
                "-            oprot.writeI64(_iter702.getValue());",
                "+            oprot.writeI64(_iter722.getValue());",
                "           }",
                "@@ -1293,6 +1293,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "           oprot.writeI32(struct.worker_resources.size());",
                "-          for (Map.Entry<NodeInfo, WorkerResources> _iter704 : struct.worker_resources.entrySet())",
                "+          for (Map.Entry<NodeInfo, WorkerResources> _iter724 : struct.worker_resources.entrySet())",
                "           {",
                "-            _iter704.getKey().write(oprot);",
                "-            _iter704.getValue().write(oprot);",
                "+            _iter724.getKey().write(oprot);",
                "+            _iter724.getValue().write(oprot);",
                "           }",
                "@@ -1303,6 +1303,6 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "           oprot.writeI32(struct.total_shared_off_heap.size());",
                "-          for (Map.Entry<String, Double> _iter705 : struct.total_shared_off_heap.entrySet())",
                "+          for (Map.Entry<String, Double> _iter725 : struct.total_shared_off_heap.entrySet())",
                "           {",
                "-            oprot.writeString(_iter705.getKey());",
                "-            oprot.writeDouble(_iter705.getValue());",
                "+            oprot.writeString(_iter725.getKey());",
                "+            oprot.writeDouble(_iter725.getValue());",
                "           }",
                "@@ -1323,11 +1323,11 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map706 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-          struct.node_host = new HashMap<String,String>(2*_map706.size);",
                "-          String _key707;",
                "-          String _val708;",
                "-          for (int _i709 = 0; _i709 < _map706.size; ++_i709)",
                "+          org.apache.thrift.protocol.TMap _map726 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+          struct.node_host = new HashMap<String,String>(2*_map726.size);",
                "+          String _key727;",
                "+          String _val728;",
                "+          for (int _i729 = 0; _i729 < _map726.size; ++_i729)",
                "           {",
                "-            _key707 = iprot.readString();",
                "-            _val708 = iprot.readString();",
                "-            struct.node_host.put(_key707, _val708);",
                "+            _key727 = iprot.readString();",
                "+            _val728 = iprot.readString();",
                "+            struct.node_host.put(_key727, _val728);",
                "           }",
                "@@ -1338,21 +1338,21 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map710 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-          struct.executor_node_port = new HashMap<List<Long>,NodeInfo>(2*_map710.size);",
                "-          List<Long> _key711;",
                "-          NodeInfo _val712;",
                "-          for (int _i713 = 0; _i713 < _map710.size; ++_i713)",
                "+          org.apache.thrift.protocol.TMap _map730 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+          struct.executor_node_port = new HashMap<List<Long>,NodeInfo>(2*_map730.size);",
                "+          List<Long> _key731;",
                "+          NodeInfo _val732;",
                "+          for (int _i733 = 0; _i733 < _map730.size; ++_i733)",
                "           {",
                "             {",
                "-              org.apache.thrift.protocol.TList _list714 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "-              _key711 = new ArrayList<Long>(_list714.size);",
                "-              long _elem715;",
                "-              for (int _i716 = 0; _i716 < _list714.size; ++_i716)",
                "+              org.apache.thrift.protocol.TList _list734 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "+              _key731 = new ArrayList<Long>(_list734.size);",
                "+              long _elem735;",
                "+              for (int _i736 = 0; _i736 < _list734.size; ++_i736)",
                "               {",
                "-                _elem715 = iprot.readI64();",
                "-                _key711.add(_elem715);",
                "+                _elem735 = iprot.readI64();",
                "+                _key731.add(_elem735);",
                "               }",
                "             }",
                "-            _val712 = new NodeInfo();",
                "-            _val712.read(iprot);",
                "-            struct.executor_node_port.put(_key711, _val712);",
                "+            _val732 = new NodeInfo();",
                "+            _val732.read(iprot);",
                "+            struct.executor_node_port.put(_key731, _val732);",
                "           }",
                "@@ -1363,20 +1363,20 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map717 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "-          struct.executor_start_time_secs = new HashMap<List<Long>,Long>(2*_map717.size);",
                "-          List<Long> _key718;",
                "-          long _val719;",
                "-          for (int _i720 = 0; _i720 < _map717.size; ++_i720)",
                "+          org.apache.thrift.protocol.TMap _map737 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.LIST, org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "+          struct.executor_start_time_secs = new HashMap<List<Long>,Long>(2*_map737.size);",
                "+          List<Long> _key738;",
                "+          long _val739;",
                "+          for (int _i740 = 0; _i740 < _map737.size; ++_i740)",
                "           {",
                "             {",
                "-              org.apache.thrift.protocol.TList _list721 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "-              _key718 = new ArrayList<Long>(_list721.size);",
                "-              long _elem722;",
                "-              for (int _i723 = 0; _i723 < _list721.size; ++_i723)",
                "+              org.apache.thrift.protocol.TList _list741 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.I64, iprot.readI32());",
                "+              _key738 = new ArrayList<Long>(_list741.size);",
                "+              long _elem742;",
                "+              for (int _i743 = 0; _i743 < _list741.size; ++_i743)",
                "               {",
                "-                _elem722 = iprot.readI64();",
                "-                _key718.add(_elem722);",
                "+                _elem742 = iprot.readI64();",
                "+                _key738.add(_elem742);",
                "               }",
                "             }",
                "-            _val719 = iprot.readI64();",
                "-            struct.executor_start_time_secs.put(_key718, _val719);",
                "+            _val739 = iprot.readI64();",
                "+            struct.executor_start_time_secs.put(_key738, _val739);",
                "           }",
                "@@ -1387,13 +1387,13 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map724 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-          struct.worker_resources = new HashMap<NodeInfo,WorkerResources>(2*_map724.size);",
                "-          NodeInfo _key725;",
                "-          WorkerResources _val726;",
                "-          for (int _i727 = 0; _i727 < _map724.size; ++_i727)",
                "+          org.apache.thrift.protocol.TMap _map744 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+          struct.worker_resources = new HashMap<NodeInfo,WorkerResources>(2*_map744.size);",
                "+          NodeInfo _key745;",
                "+          WorkerResources _val746;",
                "+          for (int _i747 = 0; _i747 < _map744.size; ++_i747)",
                "           {",
                "-            _key725 = new NodeInfo();",
                "-            _key725.read(iprot);",
                "-            _val726 = new WorkerResources();",
                "-            _val726.read(iprot);",
                "-            struct.worker_resources.put(_key725, _val726);",
                "+            _key745 = new NodeInfo();",
                "+            _key745.read(iprot);",
                "+            _val746 = new WorkerResources();",
                "+            _val746.read(iprot);",
                "+            struct.worker_resources.put(_key745, _val746);",
                "           }",
                "@@ -1404,11 +1404,11 @@ public class Assignment implements org.apache.thrift.TBase<Assignment, Assignmen",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map728 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "-          struct.total_shared_off_heap = new HashMap<String,Double>(2*_map728.size);",
                "-          String _key729;",
                "-          double _val730;",
                "-          for (int _i731 = 0; _i731 < _map728.size; ++_i731)",
                "+          org.apache.thrift.protocol.TMap _map748 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "+          struct.total_shared_off_heap = new HashMap<String,Double>(2*_map748.size);",
                "+          String _key749;",
                "+          double _val750;",
                "+          for (int _i751 = 0; _i751 < _map748.size; ++_i751)",
                "           {",
                "-            _key729 = iprot.readString();",
                "-            _val730 = iprot.readDouble();",
                "-            struct.total_shared_off_heap.put(_key729, _val730);",
                "+            _key749 = iprot.readString();",
                "+            _val750 = iprot.readDouble();",
                "+            struct.total_shared_off_heap.put(_key749, _val750);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/ClusterWorkerHeartbeat.java b/storm-client/src/jvm/org/apache/storm/generated/ClusterWorkerHeartbeat.java",
                "index 1613778f7..a2651cd4d 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/ClusterWorkerHeartbeat.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/ClusterWorkerHeartbeat.java",
                "@@ -637,13 +637,13 @@ public class ClusterWorkerHeartbeat implements org.apache.thrift.TBase<ClusterWo",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map752 = iprot.readMapBegin();",
                "-                struct.executor_stats = new HashMap<ExecutorInfo,ExecutorStats>(2*_map752.size);",
                "-                ExecutorInfo _key753;",
                "-                ExecutorStats _val754;",
                "-                for (int _i755 = 0; _i755 < _map752.size; ++_i755)",
                "+                org.apache.thrift.protocol.TMap _map772 = iprot.readMapBegin();",
                "+                struct.executor_stats = new HashMap<ExecutorInfo,ExecutorStats>(2*_map772.size);",
                "+                ExecutorInfo _key773;",
                "+                ExecutorStats _val774;",
                "+                for (int _i775 = 0; _i775 < _map772.size; ++_i775)",
                "                 {",
                "-                  _key753 = new ExecutorInfo();",
                "-                  _key753.read(iprot);",
                "-                  _val754 = new ExecutorStats();",
                "-                  _val754.read(iprot);",
                "-                  struct.executor_stats.put(_key753, _val754);",
                "+                  _key773 = new ExecutorInfo();",
                "+                  _key773.read(iprot);",
                "+                  _val774 = new ExecutorStats();",
                "+                  _val774.read(iprot);",
                "+                  struct.executor_stats.put(_key773, _val774);",
                "                 }",
                "@@ -694,6 +694,6 @@ public class ClusterWorkerHeartbeat implements org.apache.thrift.TBase<ClusterWo",
                "           oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, struct.executor_stats.size()));",
                "-          for (Map.Entry<ExecutorInfo, ExecutorStats> _iter756 : struct.executor_stats.entrySet())",
                "+          for (Map.Entry<ExecutorInfo, ExecutorStats> _iter776 : struct.executor_stats.entrySet())",
                "           {",
                "-            _iter756.getKey().write(oprot);",
                "-            _iter756.getValue().write(oprot);",
                "+            _iter776.getKey().write(oprot);",
                "+            _iter776.getValue().write(oprot);",
                "           }",
                "@@ -729,6 +729,6 @@ public class ClusterWorkerHeartbeat implements org.apache.thrift.TBase<ClusterWo",
                "         oprot.writeI32(struct.executor_stats.size());",
                "-        for (Map.Entry<ExecutorInfo, ExecutorStats> _iter757 : struct.executor_stats.entrySet())",
                "+        for (Map.Entry<ExecutorInfo, ExecutorStats> _iter777 : struct.executor_stats.entrySet())",
                "         {",
                "-          _iter757.getKey().write(oprot);",
                "-          _iter757.getValue().write(oprot);",
                "+          _iter777.getKey().write(oprot);",
                "+          _iter777.getValue().write(oprot);",
                "         }",
                "@@ -745,13 +745,13 @@ public class ClusterWorkerHeartbeat implements org.apache.thrift.TBase<ClusterWo",
                "       {",
                "-        org.apache.thrift.protocol.TMap _map758 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.executor_stats = new HashMap<ExecutorInfo,ExecutorStats>(2*_map758.size);",
                "-        ExecutorInfo _key759;",
                "-        ExecutorStats _val760;",
                "-        for (int _i761 = 0; _i761 < _map758.size; ++_i761)",
                "+        org.apache.thrift.protocol.TMap _map778 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRUCT, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.executor_stats = new HashMap<ExecutorInfo,ExecutorStats>(2*_map778.size);",
                "+        ExecutorInfo _key779;",
                "+        ExecutorStats _val780;",
                "+        for (int _i781 = 0; _i781 < _map778.size; ++_i781)",
                "         {",
                "-          _key759 = new ExecutorInfo();",
                "-          _key759.read(iprot);",
                "-          _val760 = new ExecutorStats();",
                "-          _val760.read(iprot);",
                "-          struct.executor_stats.put(_key759, _val760);",
                "+          _key779 = new ExecutorInfo();",
                "+          _key779.read(iprot);",
                "+          _val780 = new ExecutorStats();",
                "+          _val780.read(iprot);",
                "+          struct.executor_stats.put(_key779, _val780);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/HBNodes.java b/storm-client/src/jvm/org/apache/storm/generated/HBNodes.java",
                "index 75c5c6d4a..887dbc0f9 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/HBNodes.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/HBNodes.java",
                "@@ -366,9 +366,9 @@ public class HBNodes implements org.apache.thrift.TBase<HBNodes, HBNodes._Fields",
                "               {",
                "-                org.apache.thrift.protocol.TList _list858 = iprot.readListBegin();",
                "-                struct.pulseIds = new ArrayList<String>(_list858.size);",
                "-                String _elem859;",
                "-                for (int _i860 = 0; _i860 < _list858.size; ++_i860)",
                "+                org.apache.thrift.protocol.TList _list886 = iprot.readListBegin();",
                "+                struct.pulseIds = new ArrayList<String>(_list886.size);",
                "+                String _elem887;",
                "+                for (int _i888 = 0; _i888 < _list886.size; ++_i888)",
                "                 {",
                "-                  _elem859 = iprot.readString();",
                "-                  struct.pulseIds.add(_elem859);",
                "+                  _elem887 = iprot.readString();",
                "+                  struct.pulseIds.add(_elem887);",
                "                 }",
                "@@ -398,5 +398,5 @@ public class HBNodes implements org.apache.thrift.TBase<HBNodes, HBNodes._Fields",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, struct.pulseIds.size()));",
                "-          for (String _iter861 : struct.pulseIds)",
                "+          for (String _iter889 : struct.pulseIds)",
                "           {",
                "-            oprot.writeString(_iter861);",
                "+            oprot.writeString(_iter889);",
                "           }",
                "@@ -431,5 +431,5 @@ public class HBNodes implements org.apache.thrift.TBase<HBNodes, HBNodes._Fields",
                "           oprot.writeI32(struct.pulseIds.size());",
                "-          for (String _iter862 : struct.pulseIds)",
                "+          for (String _iter890 : struct.pulseIds)",
                "           {",
                "-            oprot.writeString(_iter862);",
                "+            oprot.writeString(_iter890);",
                "           }",
                "@@ -445,9 +445,9 @@ public class HBNodes implements org.apache.thrift.TBase<HBNodes, HBNodes._Fields",
                "         {",
                "-          org.apache.thrift.protocol.TList _list863 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-          struct.pulseIds = new ArrayList<String>(_list863.size);",
                "-          String _elem864;",
                "-          for (int _i865 = 0; _i865 < _list863.size; ++_i865)",
                "+          org.apache.thrift.protocol.TList _list891 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+          struct.pulseIds = new ArrayList<String>(_list891.size);",
                "+          String _elem892;",
                "+          for (int _i893 = 0; _i893 < _list891.size; ++_i893)",
                "           {",
                "-            _elem864 = iprot.readString();",
                "-            struct.pulseIds.add(_elem864);",
                "+            _elem892 = iprot.readString();",
                "+            struct.pulseIds.add(_elem892);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/HBRecords.java b/storm-client/src/jvm/org/apache/storm/generated/HBRecords.java",
                "index f726e5cf3..cfed78518 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/HBRecords.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/HBRecords.java",
                "@@ -369,10 +369,10 @@ public class HBRecords implements org.apache.thrift.TBase<HBRecords, HBRecords._",
                "               {",
                "-                org.apache.thrift.protocol.TList _list850 = iprot.readListBegin();",
                "-                struct.pulses = new ArrayList<HBPulse>(_list850.size);",
                "-                HBPulse _elem851;",
                "-                for (int _i852 = 0; _i852 < _list850.size; ++_i852)",
                "+                org.apache.thrift.protocol.TList _list878 = iprot.readListBegin();",
                "+                struct.pulses = new ArrayList<HBPulse>(_list878.size);",
                "+                HBPulse _elem879;",
                "+                for (int _i880 = 0; _i880 < _list878.size; ++_i880)",
                "                 {",
                "-                  _elem851 = new HBPulse();",
                "-                  _elem851.read(iprot);",
                "-                  struct.pulses.add(_elem851);",
                "+                  _elem879 = new HBPulse();",
                "+                  _elem879.read(iprot);",
                "+                  struct.pulses.add(_elem879);",
                "                 }",
                "@@ -402,5 +402,5 @@ public class HBRecords implements org.apache.thrift.TBase<HBRecords, HBRecords._",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.pulses.size()));",
                "-          for (HBPulse _iter853 : struct.pulses)",
                "+          for (HBPulse _iter881 : struct.pulses)",
                "           {",
                "-            _iter853.write(oprot);",
                "+            _iter881.write(oprot);",
                "           }",
                "@@ -435,5 +435,5 @@ public class HBRecords implements org.apache.thrift.TBase<HBRecords, HBRecords._",
                "           oprot.writeI32(struct.pulses.size());",
                "-          for (HBPulse _iter854 : struct.pulses)",
                "+          for (HBPulse _iter882 : struct.pulses)",
                "           {",
                "-            _iter854.write(oprot);",
                "+            _iter882.write(oprot);",
                "           }",
                "@@ -449,10 +449,10 @@ public class HBRecords implements org.apache.thrift.TBase<HBRecords, HBRecords._",
                "         {",
                "-          org.apache.thrift.protocol.TList _list855 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-          struct.pulses = new ArrayList<HBPulse>(_list855.size);",
                "-          HBPulse _elem856;",
                "-          for (int _i857 = 0; _i857 < _list855.size; ++_i857)",
                "+          org.apache.thrift.protocol.TList _list883 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+          struct.pulses = new ArrayList<HBPulse>(_list883.size);",
                "+          HBPulse _elem884;",
                "+          for (int _i885 = 0; _i885 < _list883.size; ++_i885)",
                "           {",
                "-            _elem856 = new HBPulse();",
                "-            _elem856.read(iprot);",
                "-            struct.pulses.add(_elem856);",
                "+            _elem884 = new HBPulse();",
                "+            _elem884.read(iprot);",
                "+            struct.pulses.add(_elem884);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LSApprovedWorkers.java b/storm-client/src/jvm/org/apache/storm/generated/LSApprovedWorkers.java",
                "index 9e8e5cf7d..9b137248b 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LSApprovedWorkers.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LSApprovedWorkers.java",
                "@@ -367,11 +367,11 @@ public class LSApprovedWorkers implements org.apache.thrift.TBase<LSApprovedWork",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map780 = iprot.readMapBegin();",
                "-                struct.approved_workers = new HashMap<String,Integer>(2*_map780.size);",
                "-                String _key781;",
                "-                int _val782;",
                "-                for (int _i783 = 0; _i783 < _map780.size; ++_i783)",
                "+                org.apache.thrift.protocol.TMap _map800 = iprot.readMapBegin();",
                "+                struct.approved_workers = new HashMap<String,Integer>(2*_map800.size);",
                "+                String _key801;",
                "+                int _val802;",
                "+                for (int _i803 = 0; _i803 < _map800.size; ++_i803)",
                "                 {",
                "-                  _key781 = iprot.readString();",
                "-                  _val782 = iprot.readI32();",
                "-                  struct.approved_workers.put(_key781, _val782);",
                "+                  _key801 = iprot.readString();",
                "+                  _val802 = iprot.readI32();",
                "+                  struct.approved_workers.put(_key801, _val802);",
                "                 }",
                "@@ -401,6 +401,6 @@ public class LSApprovedWorkers implements org.apache.thrift.TBase<LSApprovedWork",
                "           oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, struct.approved_workers.size()));",
                "-          for (Map.Entry<String, Integer> _iter784 : struct.approved_workers.entrySet())",
                "+          for (Map.Entry<String, Integer> _iter804 : struct.approved_workers.entrySet())",
                "           {",
                "-            oprot.writeString(_iter784.getKey());",
                "-            oprot.writeI32(_iter784.getValue());",
                "+            oprot.writeString(_iter804.getKey());",
                "+            oprot.writeI32(_iter804.getValue());",
                "           }",
                "@@ -429,6 +429,6 @@ public class LSApprovedWorkers implements org.apache.thrift.TBase<LSApprovedWork",
                "         oprot.writeI32(struct.approved_workers.size());",
                "-        for (Map.Entry<String, Integer> _iter785 : struct.approved_workers.entrySet())",
                "+        for (Map.Entry<String, Integer> _iter805 : struct.approved_workers.entrySet())",
                "         {",
                "-          oprot.writeString(_iter785.getKey());",
                "-          oprot.writeI32(_iter785.getValue());",
                "+          oprot.writeString(_iter805.getKey());",
                "+          oprot.writeI32(_iter805.getValue());",
                "         }",
                "@@ -441,11 +441,11 @@ public class LSApprovedWorkers implements org.apache.thrift.TBase<LSApprovedWork",
                "       {",
                "-        org.apache.thrift.protocol.TMap _map786 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, iprot.readI32());",
                "-        struct.approved_workers = new HashMap<String,Integer>(2*_map786.size);",
                "-        String _key787;",
                "-        int _val788;",
                "-        for (int _i789 = 0; _i789 < _map786.size; ++_i789)",
                "+        org.apache.thrift.protocol.TMap _map806 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, iprot.readI32());",
                "+        struct.approved_workers = new HashMap<String,Integer>(2*_map806.size);",
                "+        String _key807;",
                "+        int _val808;",
                "+        for (int _i809 = 0; _i809 < _map806.size; ++_i809)",
                "         {",
                "-          _key787 = iprot.readString();",
                "-          _val788 = iprot.readI32();",
                "-          struct.approved_workers.put(_key787, _val788);",
                "+          _key807 = iprot.readString();",
                "+          _val808 = iprot.readI32();",
                "+          struct.approved_workers.put(_key807, _val808);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LSSupervisorAssignments.java b/storm-client/src/jvm/org/apache/storm/generated/LSSupervisorAssignments.java",
                "index 64c36e27e..95f78cab0 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LSSupervisorAssignments.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LSSupervisorAssignments.java",
                "@@ -378,12 +378,12 @@ public class LSSupervisorAssignments implements org.apache.thrift.TBase<LSSuperv",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map790 = iprot.readMapBegin();",
                "-                struct.assignments = new HashMap<Integer,LocalAssignment>(2*_map790.size);",
                "-                int _key791;",
                "-                LocalAssignment _val792;",
                "-                for (int _i793 = 0; _i793 < _map790.size; ++_i793)",
                "+                org.apache.thrift.protocol.TMap _map810 = iprot.readMapBegin();",
                "+                struct.assignments = new HashMap<Integer,LocalAssignment>(2*_map810.size);",
                "+                int _key811;",
                "+                LocalAssignment _val812;",
                "+                for (int _i813 = 0; _i813 < _map810.size; ++_i813)",
                "                 {",
                "-                  _key791 = iprot.readI32();",
                "-                  _val792 = new LocalAssignment();",
                "-                  _val792.read(iprot);",
                "-                  struct.assignments.put(_key791, _val792);",
                "+                  _key811 = iprot.readI32();",
                "+                  _val812 = new LocalAssignment();",
                "+                  _val812.read(iprot);",
                "+                  struct.assignments.put(_key811, _val812);",
                "                 }",
                "@@ -413,6 +413,6 @@ public class LSSupervisorAssignments implements org.apache.thrift.TBase<LSSuperv",
                "           oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.I32, org.apache.thrift.protocol.TType.STRUCT, struct.assignments.size()));",
                "-          for (Map.Entry<Integer, LocalAssignment> _iter794 : struct.assignments.entrySet())",
                "+          for (Map.Entry<Integer, LocalAssignment> _iter814 : struct.assignments.entrySet())",
                "           {",
                "-            oprot.writeI32(_iter794.getKey());",
                "-            _iter794.getValue().write(oprot);",
                "+            oprot.writeI32(_iter814.getKey());",
                "+            _iter814.getValue().write(oprot);",
                "           }",
                "@@ -441,6 +441,6 @@ public class LSSupervisorAssignments implements org.apache.thrift.TBase<LSSuperv",
                "         oprot.writeI32(struct.assignments.size());",
                "-        for (Map.Entry<Integer, LocalAssignment> _iter795 : struct.assignments.entrySet())",
                "+        for (Map.Entry<Integer, LocalAssignment> _iter815 : struct.assignments.entrySet())",
                "         {",
                "-          oprot.writeI32(_iter795.getKey());",
                "-          _iter795.getValue().write(oprot);",
                "+          oprot.writeI32(_iter815.getKey());",
                "+          _iter815.getValue().write(oprot);",
                "         }",
                "@@ -453,12 +453,12 @@ public class LSSupervisorAssignments implements org.apache.thrift.TBase<LSSuperv",
                "       {",
                "-        org.apache.thrift.protocol.TMap _map796 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.I32, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.assignments = new HashMap<Integer,LocalAssignment>(2*_map796.size);",
                "-        int _key797;",
                "-        LocalAssignment _val798;",
                "-        for (int _i799 = 0; _i799 < _map796.size; ++_i799)",
                "+        org.apache.thrift.protocol.TMap _map816 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.I32, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.assignments = new HashMap<Integer,LocalAssignment>(2*_map816.size);",
                "+        int _key817;",
                "+        LocalAssignment _val818;",
                "+        for (int _i819 = 0; _i819 < _map816.size; ++_i819)",
                "         {",
                "-          _key797 = iprot.readI32();",
                "-          _val798 = new LocalAssignment();",
                "-          _val798.read(iprot);",
                "-          struct.assignments.put(_key797, _val798);",
                "+          _key817 = iprot.readI32();",
                "+          _val818 = new LocalAssignment();",
                "+          _val818.read(iprot);",
                "+          struct.assignments.put(_key817, _val818);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistory.java b/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistory.java",
                "index 6de53df9b..432b9b600 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistory.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistory.java",
                "@@ -658,9 +658,9 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "               {",
                "-                org.apache.thrift.protocol.TList _list808 = iprot.readListBegin();",
                "-                struct.users = new ArrayList<String>(_list808.size);",
                "-                String _elem809;",
                "-                for (int _i810 = 0; _i810 < _list808.size; ++_i810)",
                "+                org.apache.thrift.protocol.TList _list828 = iprot.readListBegin();",
                "+                struct.users = new ArrayList<String>(_list828.size);",
                "+                String _elem829;",
                "+                for (int _i830 = 0; _i830 < _list828.size; ++_i830)",
                "                 {",
                "-                  _elem809 = iprot.readString();",
                "-                  struct.users.add(_elem809);",
                "+                  _elem829 = iprot.readString();",
                "+                  struct.users.add(_elem829);",
                "                 }",
                "@@ -676,9 +676,9 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "               {",
                "-                org.apache.thrift.protocol.TList _list811 = iprot.readListBegin();",
                "-                struct.groups = new ArrayList<String>(_list811.size);",
                "-                String _elem812;",
                "-                for (int _i813 = 0; _i813 < _list811.size; ++_i813)",
                "+                org.apache.thrift.protocol.TList _list831 = iprot.readListBegin();",
                "+                struct.groups = new ArrayList<String>(_list831.size);",
                "+                String _elem832;",
                "+                for (int _i833 = 0; _i833 < _list831.size; ++_i833)",
                "                 {",
                "-                  _elem812 = iprot.readString();",
                "-                  struct.groups.add(_elem812);",
                "+                  _elem832 = iprot.readString();",
                "+                  struct.groups.add(_elem832);",
                "                 }",
                "@@ -716,5 +716,5 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, struct.users.size()));",
                "-          for (String _iter814 : struct.users)",
                "+          for (String _iter834 : struct.users)",
                "           {",
                "-            oprot.writeString(_iter814);",
                "+            oprot.writeString(_iter834);",
                "           }",
                "@@ -728,5 +728,5 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, struct.groups.size()));",
                "-          for (String _iter815 : struct.groups)",
                "+          for (String _iter835 : struct.groups)",
                "           {",
                "-            oprot.writeString(_iter815);",
                "+            oprot.writeString(_iter835);",
                "           }",
                "@@ -757,5 +757,5 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "         oprot.writeI32(struct.users.size());",
                "-        for (String _iter816 : struct.users)",
                "+        for (String _iter836 : struct.users)",
                "         {",
                "-          oprot.writeString(_iter816);",
                "+          oprot.writeString(_iter836);",
                "         }",
                "@@ -764,5 +764,5 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "         oprot.writeI32(struct.groups.size());",
                "-        for (String _iter817 : struct.groups)",
                "+        for (String _iter837 : struct.groups)",
                "         {",
                "-          oprot.writeString(_iter817);",
                "+          oprot.writeString(_iter837);",
                "         }",
                "@@ -779,9 +779,9 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "       {",
                "-        org.apache.thrift.protocol.TList _list818 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-        struct.users = new ArrayList<String>(_list818.size);",
                "-        String _elem819;",
                "-        for (int _i820 = 0; _i820 < _list818.size; ++_i820)",
                "+        org.apache.thrift.protocol.TList _list838 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+        struct.users = new ArrayList<String>(_list838.size);",
                "+        String _elem839;",
                "+        for (int _i840 = 0; _i840 < _list838.size; ++_i840)",
                "         {",
                "-          _elem819 = iprot.readString();",
                "-          struct.users.add(_elem819);",
                "+          _elem839 = iprot.readString();",
                "+          struct.users.add(_elem839);",
                "         }",
                "@@ -790,9 +790,9 @@ public class LSTopoHistory implements org.apache.thrift.TBase<LSTopoHistory, LST",
                "       {",
                "-        org.apache.thrift.protocol.TList _list821 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-        struct.groups = new ArrayList<String>(_list821.size);",
                "-        String _elem822;",
                "-        for (int _i823 = 0; _i823 < _list821.size; ++_i823)",
                "+        org.apache.thrift.protocol.TList _list841 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+        struct.groups = new ArrayList<String>(_list841.size);",
                "+        String _elem842;",
                "+        for (int _i843 = 0; _i843 < _list841.size; ++_i843)",
                "         {",
                "-          _elem822 = iprot.readString();",
                "-          struct.groups.add(_elem822);",
                "+          _elem842 = iprot.readString();",
                "+          struct.groups.add(_elem842);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistoryList.java b/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistoryList.java",
                "index 790a6fba3..461bd088c 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistoryList.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LSTopoHistoryList.java",
                "@@ -373,10 +373,10 @@ public class LSTopoHistoryList implements org.apache.thrift.TBase<LSTopoHistoryL",
                "               {",
                "-                org.apache.thrift.protocol.TList _list824 = iprot.readListBegin();",
                "-                struct.topo_history = new ArrayList<LSTopoHistory>(_list824.size);",
                "-                LSTopoHistory _elem825;",
                "-                for (int _i826 = 0; _i826 < _list824.size; ++_i826)",
                "+                org.apache.thrift.protocol.TList _list844 = iprot.readListBegin();",
                "+                struct.topo_history = new ArrayList<LSTopoHistory>(_list844.size);",
                "+                LSTopoHistory _elem845;",
                "+                for (int _i846 = 0; _i846 < _list844.size; ++_i846)",
                "                 {",
                "-                  _elem825 = new LSTopoHistory();",
                "-                  _elem825.read(iprot);",
                "-                  struct.topo_history.add(_elem825);",
                "+                  _elem845 = new LSTopoHistory();",
                "+                  _elem845.read(iprot);",
                "+                  struct.topo_history.add(_elem845);",
                "                 }",
                "@@ -406,5 +406,5 @@ public class LSTopoHistoryList implements org.apache.thrift.TBase<LSTopoHistoryL",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.topo_history.size()));",
                "-          for (LSTopoHistory _iter827 : struct.topo_history)",
                "+          for (LSTopoHistory _iter847 : struct.topo_history)",
                "           {",
                "-            _iter827.write(oprot);",
                "+            _iter847.write(oprot);",
                "           }",
                "@@ -433,5 +433,5 @@ public class LSTopoHistoryList implements org.apache.thrift.TBase<LSTopoHistoryL",
                "         oprot.writeI32(struct.topo_history.size());",
                "-        for (LSTopoHistory _iter828 : struct.topo_history)",
                "+        for (LSTopoHistory _iter848 : struct.topo_history)",
                "         {",
                "-          _iter828.write(oprot);",
                "+          _iter848.write(oprot);",
                "         }",
                "@@ -444,10 +444,10 @@ public class LSTopoHistoryList implements org.apache.thrift.TBase<LSTopoHistoryL",
                "       {",
                "-        org.apache.thrift.protocol.TList _list829 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.topo_history = new ArrayList<LSTopoHistory>(_list829.size);",
                "-        LSTopoHistory _elem830;",
                "-        for (int _i831 = 0; _i831 < _list829.size; ++_i831)",
                "+        org.apache.thrift.protocol.TList _list849 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.topo_history = new ArrayList<LSTopoHistory>(_list849.size);",
                "+        LSTopoHistory _elem850;",
                "+        for (int _i851 = 0; _i851 < _list849.size; ++_i851)",
                "         {",
                "-          _elem830 = new LSTopoHistory();",
                "-          _elem830.read(iprot);",
                "-          struct.topo_history.add(_elem830);",
                "+          _elem850 = new LSTopoHistory();",
                "+          _elem850.read(iprot);",
                "+          struct.topo_history.add(_elem850);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LSWorkerHeartbeat.java b/storm-client/src/jvm/org/apache/storm/generated/LSWorkerHeartbeat.java",
                "index 6cf386f10..73b9c122b 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LSWorkerHeartbeat.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LSWorkerHeartbeat.java",
                "@@ -640,10 +640,10 @@ public class LSWorkerHeartbeat implements org.apache.thrift.TBase<LSWorkerHeartb",
                "               {",
                "-                org.apache.thrift.protocol.TList _list800 = iprot.readListBegin();",
                "-                struct.executors = new ArrayList<ExecutorInfo>(_list800.size);",
                "-                ExecutorInfo _elem801;",
                "-                for (int _i802 = 0; _i802 < _list800.size; ++_i802)",
                "+                org.apache.thrift.protocol.TList _list820 = iprot.readListBegin();",
                "+                struct.executors = new ArrayList<ExecutorInfo>(_list820.size);",
                "+                ExecutorInfo _elem821;",
                "+                for (int _i822 = 0; _i822 < _list820.size; ++_i822)",
                "                 {",
                "-                  _elem801 = new ExecutorInfo();",
                "-                  _elem801.read(iprot);",
                "-                  struct.executors.add(_elem801);",
                "+                  _elem821 = new ExecutorInfo();",
                "+                  _elem821.read(iprot);",
                "+                  struct.executors.add(_elem821);",
                "                 }",
                "@@ -689,5 +689,5 @@ public class LSWorkerHeartbeat implements org.apache.thrift.TBase<LSWorkerHeartb",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.executors.size()));",
                "-          for (ExecutorInfo _iter803 : struct.executors)",
                "+          for (ExecutorInfo _iter823 : struct.executors)",
                "           {",
                "-            _iter803.write(oprot);",
                "+            _iter823.write(oprot);",
                "           }",
                "@@ -721,5 +721,5 @@ public class LSWorkerHeartbeat implements org.apache.thrift.TBase<LSWorkerHeartb",
                "         oprot.writeI32(struct.executors.size());",
                "-        for (ExecutorInfo _iter804 : struct.executors)",
                "+        for (ExecutorInfo _iter824 : struct.executors)",
                "         {",
                "-          _iter804.write(oprot);",
                "+          _iter824.write(oprot);",
                "         }",
                "@@ -737,10 +737,10 @@ public class LSWorkerHeartbeat implements org.apache.thrift.TBase<LSWorkerHeartb",
                "       {",
                "-        org.apache.thrift.protocol.TList _list805 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.executors = new ArrayList<ExecutorInfo>(_list805.size);",
                "-        ExecutorInfo _elem806;",
                "-        for (int _i807 = 0; _i807 < _list805.size; ++_i807)",
                "+        org.apache.thrift.protocol.TList _list825 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.executors = new ArrayList<ExecutorInfo>(_list825.size);",
                "+        ExecutorInfo _elem826;",
                "+        for (int _i827 = 0; _i827 < _list825.size; ++_i827)",
                "         {",
                "-          _elem806 = new ExecutorInfo();",
                "-          _elem806.read(iprot);",
                "-          struct.executors.add(_elem806);",
                "+          _elem826 = new ExecutorInfo();",
                "+          _elem826.read(iprot);",
                "+          struct.executors.add(_elem826);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LocalAssignment.java b/storm-client/src/jvm/org/apache/storm/generated/LocalAssignment.java",
                "index e4d83aafe..38141248a 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LocalAssignment.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LocalAssignment.java",
                "@@ -712,10 +712,10 @@ public class LocalAssignment implements org.apache.thrift.TBase<LocalAssignment,",
                "               {",
                "-                org.apache.thrift.protocol.TList _list772 = iprot.readListBegin();",
                "-                struct.executors = new ArrayList<ExecutorInfo>(_list772.size);",
                "-                ExecutorInfo _elem773;",
                "-                for (int _i774 = 0; _i774 < _list772.size; ++_i774)",
                "+                org.apache.thrift.protocol.TList _list792 = iprot.readListBegin();",
                "+                struct.executors = new ArrayList<ExecutorInfo>(_list792.size);",
                "+                ExecutorInfo _elem793;",
                "+                for (int _i794 = 0; _i794 < _list792.size; ++_i794)",
                "                 {",
                "-                  _elem773 = new ExecutorInfo();",
                "-                  _elem773.read(iprot);",
                "-                  struct.executors.add(_elem773);",
                "+                  _elem793 = new ExecutorInfo();",
                "+                  _elem793.read(iprot);",
                "+                  struct.executors.add(_elem793);",
                "                 }",
                "@@ -775,5 +775,5 @@ public class LocalAssignment implements org.apache.thrift.TBase<LocalAssignment,",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.executors.size()));",
                "-          for (ExecutorInfo _iter775 : struct.executors)",
                "+          for (ExecutorInfo _iter795 : struct.executors)",
                "           {",
                "-            _iter775.write(oprot);",
                "+            _iter795.write(oprot);",
                "           }",
                "@@ -822,5 +822,5 @@ public class LocalAssignment implements org.apache.thrift.TBase<LocalAssignment,",
                "         oprot.writeI32(struct.executors.size());",
                "-        for (ExecutorInfo _iter776 : struct.executors)",
                "+        for (ExecutorInfo _iter796 : struct.executors)",
                "         {",
                "-          _iter776.write(oprot);",
                "+          _iter796.write(oprot);",
                "         }",
                "@@ -855,10 +855,10 @@ public class LocalAssignment implements org.apache.thrift.TBase<LocalAssignment,",
                "       {",
                "-        org.apache.thrift.protocol.TList _list777 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.executors = new ArrayList<ExecutorInfo>(_list777.size);",
                "-        ExecutorInfo _elem778;",
                "-        for (int _i779 = 0; _i779 < _list777.size; ++_i779)",
                "+        org.apache.thrift.protocol.TList _list797 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.executors = new ArrayList<ExecutorInfo>(_list797.size);",
                "+        ExecutorInfo _elem798;",
                "+        for (int _i799 = 0; _i799 < _list797.size; ++_i799)",
                "         {",
                "-          _elem778 = new ExecutorInfo();",
                "-          _elem778.read(iprot);",
                "-          struct.executors.add(_elem778);",
                "+          _elem798 = new ExecutorInfo();",
                "+          _elem798.read(iprot);",
                "+          struct.executors.add(_elem798);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LocalStateData.java b/storm-client/src/jvm/org/apache/storm/generated/LocalStateData.java",
                "index 3536c0b17..a99475fcf 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LocalStateData.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LocalStateData.java",
                "@@ -378,12 +378,12 @@ public class LocalStateData implements org.apache.thrift.TBase<LocalStateData, L",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map762 = iprot.readMapBegin();",
                "-                struct.serialized_parts = new HashMap<String,ThriftSerializedObject>(2*_map762.size);",
                "-                String _key763;",
                "-                ThriftSerializedObject _val764;",
                "-                for (int _i765 = 0; _i765 < _map762.size; ++_i765)",
                "+                org.apache.thrift.protocol.TMap _map782 = iprot.readMapBegin();",
                "+                struct.serialized_parts = new HashMap<String,ThriftSerializedObject>(2*_map782.size);",
                "+                String _key783;",
                "+                ThriftSerializedObject _val784;",
                "+                for (int _i785 = 0; _i785 < _map782.size; ++_i785)",
                "                 {",
                "-                  _key763 = iprot.readString();",
                "-                  _val764 = new ThriftSerializedObject();",
                "-                  _val764.read(iprot);",
                "-                  struct.serialized_parts.put(_key763, _val764);",
                "+                  _key783 = iprot.readString();",
                "+                  _val784 = new ThriftSerializedObject();",
                "+                  _val784.read(iprot);",
                "+                  struct.serialized_parts.put(_key783, _val784);",
                "                 }",
                "@@ -413,6 +413,6 @@ public class LocalStateData implements org.apache.thrift.TBase<LocalStateData, L",
                "           oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, struct.serialized_parts.size()));",
                "-          for (Map.Entry<String, ThriftSerializedObject> _iter766 : struct.serialized_parts.entrySet())",
                "+          for (Map.Entry<String, ThriftSerializedObject> _iter786 : struct.serialized_parts.entrySet())",
                "           {",
                "-            oprot.writeString(_iter766.getKey());",
                "-            _iter766.getValue().write(oprot);",
                "+            oprot.writeString(_iter786.getKey());",
                "+            _iter786.getValue().write(oprot);",
                "           }",
                "@@ -441,6 +441,6 @@ public class LocalStateData implements org.apache.thrift.TBase<LocalStateData, L",
                "         oprot.writeI32(struct.serialized_parts.size());",
                "-        for (Map.Entry<String, ThriftSerializedObject> _iter767 : struct.serialized_parts.entrySet())",
                "+        for (Map.Entry<String, ThriftSerializedObject> _iter787 : struct.serialized_parts.entrySet())",
                "         {",
                "-          oprot.writeString(_iter767.getKey());",
                "-          _iter767.getValue().write(oprot);",
                "+          oprot.writeString(_iter787.getKey());",
                "+          _iter787.getValue().write(oprot);",
                "         }",
                "@@ -453,12 +453,12 @@ public class LocalStateData implements org.apache.thrift.TBase<LocalStateData, L",
                "       {",
                "-        org.apache.thrift.protocol.TMap _map768 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-        struct.serialized_parts = new HashMap<String,ThriftSerializedObject>(2*_map768.size);",
                "-        String _key769;",
                "-        ThriftSerializedObject _val770;",
                "-        for (int _i771 = 0; _i771 < _map768.size; ++_i771)",
                "+        org.apache.thrift.protocol.TMap _map788 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+        struct.serialized_parts = new HashMap<String,ThriftSerializedObject>(2*_map788.size);",
                "+        String _key789;",
                "+        ThriftSerializedObject _val790;",
                "+        for (int _i791 = 0; _i791 < _map788.size; ++_i791)",
                "         {",
                "-          _key769 = iprot.readString();",
                "-          _val770 = new ThriftSerializedObject();",
                "-          _val770.read(iprot);",
                "-          struct.serialized_parts.put(_key769, _val770);",
                "+          _key789 = iprot.readString();",
                "+          _val790 = new ThriftSerializedObject();",
                "+          _val790.read(iprot);",
                "+          struct.serialized_parts.put(_key789, _val790);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/LogConfig.java b/storm-client/src/jvm/org/apache/storm/generated/LogConfig.java",
                "index e783e6c45..f44203ffe 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/LogConfig.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/LogConfig.java",
                "@@ -370,12 +370,12 @@ public class LogConfig implements org.apache.thrift.TBase<LogConfig, LogConfig._",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map832 = iprot.readMapBegin();",
                "-                struct.named_logger_level = new HashMap<String,LogLevel>(2*_map832.size);",
                "-                String _key833;",
                "-                LogLevel _val834;",
                "-                for (int _i835 = 0; _i835 < _map832.size; ++_i835)",
                "+                org.apache.thrift.protocol.TMap _map852 = iprot.readMapBegin();",
                "+                struct.named_logger_level = new HashMap<String,LogLevel>(2*_map852.size);",
                "+                String _key853;",
                "+                LogLevel _val854;",
                "+                for (int _i855 = 0; _i855 < _map852.size; ++_i855)",
                "                 {",
                "-                  _key833 = iprot.readString();",
                "-                  _val834 = new LogLevel();",
                "-                  _val834.read(iprot);",
                "-                  struct.named_logger_level.put(_key833, _val834);",
                "+                  _key853 = iprot.readString();",
                "+                  _val854 = new LogLevel();",
                "+                  _val854.read(iprot);",
                "+                  struct.named_logger_level.put(_key853, _val854);",
                "                 }",
                "@@ -406,6 +406,6 @@ public class LogConfig implements org.apache.thrift.TBase<LogConfig, LogConfig._",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, struct.named_logger_level.size()));",
                "-            for (Map.Entry<String, LogLevel> _iter836 : struct.named_logger_level.entrySet())",
                "+            for (Map.Entry<String, LogLevel> _iter856 : struct.named_logger_level.entrySet())",
                "             {",
                "-              oprot.writeString(_iter836.getKey());",
                "-              _iter836.getValue().write(oprot);",
                "+              oprot.writeString(_iter856.getKey());",
                "+              _iter856.getValue().write(oprot);",
                "             }",
                "@@ -441,6 +441,6 @@ public class LogConfig implements org.apache.thrift.TBase<LogConfig, LogConfig._",
                "           oprot.writeI32(struct.named_logger_level.size());",
                "-          for (Map.Entry<String, LogLevel> _iter837 : struct.named_logger_level.entrySet())",
                "+          for (Map.Entry<String, LogLevel> _iter857 : struct.named_logger_level.entrySet())",
                "           {",
                "-            oprot.writeString(_iter837.getKey());",
                "-            _iter837.getValue().write(oprot);",
                "+            oprot.writeString(_iter857.getKey());",
                "+            _iter857.getValue().write(oprot);",
                "           }",
                "@@ -456,12 +456,12 @@ public class LogConfig implements org.apache.thrift.TBase<LogConfig, LogConfig._",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map838 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-          struct.named_logger_level = new HashMap<String,LogLevel>(2*_map838.size);",
                "-          String _key839;",
                "-          LogLevel _val840;",
                "-          for (int _i841 = 0; _i841 < _map838.size; ++_i841)",
                "+          org.apache.thrift.protocol.TMap _map858 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+          struct.named_logger_level = new HashMap<String,LogLevel>(2*_map858.size);",
                "+          String _key859;",
                "+          LogLevel _val860;",
                "+          for (int _i861 = 0; _i861 < _map858.size; ++_i861)",
                "           {",
                "-            _key839 = iprot.readString();",
                "-            _val840 = new LogLevel();",
                "-            _val840.read(iprot);",
                "-            struct.named_logger_level.put(_key839, _val840);",
                "+            _key859 = iprot.readString();",
                "+            _val860 = new LogLevel();",
                "+            _val860.read(iprot);",
                "+            struct.named_logger_level.put(_key859, _val860);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/Nimbus.java b/storm-client/src/jvm/org/apache/storm/generated/Nimbus.java",
                "index ea4dcc90b..8b0254859 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/Nimbus.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/Nimbus.java",
                "@@ -172,2 +172,4 @@ public class Nimbus {",
                "+    public void processWorkerMetrics(WorkerMetrics metrics) throws org.apache.thrift.TException;",
                "+",
                "   }",
                "@@ -268,2 +270,4 @@ public class Nimbus {",
                "+    public void processWorkerMetrics(WorkerMetrics metrics, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException;",
                "+",
                "   }",
                "@@ -1524,2 +1528,22 @@ public class Nimbus {",
                "+    public void processWorkerMetrics(WorkerMetrics metrics) throws org.apache.thrift.TException",
                "+    {",
                "+      send_processWorkerMetrics(metrics);",
                "+      recv_processWorkerMetrics();",
                "+    }",
                "+",
                "+    public void send_processWorkerMetrics(WorkerMetrics metrics) throws org.apache.thrift.TException",
                "+    {",
                "+      processWorkerMetrics_args args = new processWorkerMetrics_args();",
                "+      args.set_metrics(metrics);",
                "+      sendBase(\"processWorkerMetrics\", args);",
                "+    }",
                "+",
                "+    public void recv_processWorkerMetrics() throws org.apache.thrift.TException",
                "+    {",
                "+      processWorkerMetrics_result result = new processWorkerMetrics_result();",
                "+      receiveBase(result, \"processWorkerMetrics\");",
                "+      return;",
                "+    }",
                "+",
                "   }",
                "@@ -3092,2 +3116,34 @@ public class Nimbus {",
                "+    public void processWorkerMetrics(WorkerMetrics metrics, org.apache.thrift.async.AsyncMethodCallback resultHandler) throws org.apache.thrift.TException {",
                "+      checkReady();",
                "+      processWorkerMetrics_call method_call = new processWorkerMetrics_call(metrics, resultHandler, this, ___protocolFactory, ___transport);",
                "+      this.___currentMethod = method_call;",
                "+      ___manager.call(method_call);",
                "+    }",
                "+",
                "+    public static class processWorkerMetrics_call extends org.apache.thrift.async.TAsyncMethodCall {",
                "+      private WorkerMetrics metrics;",
                "+      public processWorkerMetrics_call(WorkerMetrics metrics, org.apache.thrift.async.AsyncMethodCallback resultHandler, org.apache.thrift.async.TAsyncClient client, org.apache.thrift.protocol.TProtocolFactory protocolFactory, org.apache.thrift.transport.TNonblockingTransport transport) throws org.apache.thrift.TException {",
                "+        super(client, protocolFactory, transport, resultHandler, false);",
                "+        this.metrics = metrics;",
                "+      }",
                "+",
                "+      public void write_args(org.apache.thrift.protocol.TProtocol prot) throws org.apache.thrift.TException {",
                "+        prot.writeMessageBegin(new org.apache.thrift.protocol.TMessage(\"processWorkerMetrics\", org.apache.thrift.protocol.TMessageType.CALL, 0));",
                "+        processWorkerMetrics_args args = new processWorkerMetrics_args();",
                "+        args.set_metrics(metrics);",
                "+        args.write(prot);",
                "+        prot.writeMessageEnd();",
                "+      }",
                "+",
                "+      public void getResult() throws org.apache.thrift.TException {",
                "+        if (getState() != org.apache.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {",
                "+          throw new IllegalStateException(\"Method call not finished!\");",
                "+        }",
                "+        org.apache.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());",
                "+        org.apache.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);",
                "+        (new Client(prot)).recv_processWorkerMetrics();",
                "+      }",
                "+    }",
                "+",
                "   }",
                "@@ -3151,2 +3207,3 @@ public class Nimbus {",
                "       processMap.put(\"getOwnerResourceSummaries\", new getOwnerResourceSummaries());",
                "+      processMap.put(\"processWorkerMetrics\", new processWorkerMetrics());",
                "       return processMap;",
                "@@ -4295,2 +4352,22 @@ public class Nimbus {",
                "+    public static class processWorkerMetrics<I extends Iface> extends org.apache.thrift.ProcessFunction<I, processWorkerMetrics_args> {",
                "+      public processWorkerMetrics() {",
                "+        super(\"processWorkerMetrics\");",
                "+      }",
                "+",
                "+      public processWorkerMetrics_args getEmptyArgsInstance() {",
                "+        return new processWorkerMetrics_args();",
                "+      }",
                "+",
                "+      protected boolean isOneway() {",
                "+        return false;",
                "+      }",
                "+",
                "+      public processWorkerMetrics_result getResult(I iface, processWorkerMetrics_args args) throws org.apache.thrift.TException {",
                "+        processWorkerMetrics_result result = new processWorkerMetrics_result();",
                "+        iface.processWorkerMetrics(args.metrics);",
                "+        return result;",
                "+      }",
                "+    }",
                "+",
                "   }",
                "@@ -4354,2 +4431,3 @@ public class Nimbus {",
                "       processMap.put(\"getOwnerResourceSummaries\", new getOwnerResourceSummaries());",
                "+      processMap.put(\"processWorkerMetrics\", new processWorkerMetrics());",
                "       return processMap;",
                "@@ -7072,2 +7150,52 @@ public class Nimbus {",
                "+    public static class processWorkerMetrics<I extends AsyncIface> extends org.apache.thrift.AsyncProcessFunction<I, processWorkerMetrics_args, Void> {",
                "+      public processWorkerMetrics() {",
                "+        super(\"processWorkerMetrics\");",
                "+      }",
                "+",
                "+      public processWorkerMetrics_args getEmptyArgsInstance() {",
                "+        return new processWorkerMetrics_args();",
                "+      }",
                "+",
                "+      public AsyncMethodCallback<Void> getResultHandler(final AsyncFrameBuffer fb, final int seqid) {",
                "+        final org.apache.thrift.AsyncProcessFunction fcall = this;",
                "+        return new AsyncMethodCallback<Void>() { ",
                "+          public void onComplete(Void o) {",
                "+            processWorkerMetrics_result result = new processWorkerMetrics_result();",
                "+            try {",
                "+              fcall.sendResponse(fb,result, org.apache.thrift.protocol.TMessageType.REPLY,seqid);",
                "+              return;",
                "+            } catch (Exception e) {",
                "+              LOGGER.error(\"Exception writing to internal frame buffer\", e);",
                "+            }",
                "+            fb.close();",
                "+          }",
                "+          public void onError(Exception e) {",
                "+            byte msgType = org.apache.thrift.protocol.TMessageType.REPLY;",
                "+            org.apache.thrift.TBase msg;",
                "+            processWorkerMetrics_result result = new processWorkerMetrics_result();",
                "+            {",
                "+              msgType = org.apache.thrift.protocol.TMessageType.EXCEPTION;",
                "+              msg = (org.apache.thrift.TBase)new org.apache.thrift.TApplicationException(org.apache.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());",
                "+            }",
                "+            try {",
                "+              fcall.sendResponse(fb,msg,msgType,seqid);",
                "+              return;",
                "+            } catch (Exception ex) {",
                "+              LOGGER.error(\"Exception writing to internal frame buffer\", ex);",
                "+            }",
                "+            fb.close();",
                "+          }",
                "+        };",
                "+      }",
                "+",
                "+      protected boolean isOneway() {",
                "+        return false;",
                "+      }",
                "+",
                "+      public void start(I iface, processWorkerMetrics_args args, org.apache.thrift.async.AsyncMethodCallback<Void> resultHandler) throws TException {",
                "+        iface.processWorkerMetrics(args.metrics,resultHandler);",
                "+      }",
                "+    }",
                "+",
                "   }",
                "@@ -18289,10 +18417,10 @@ public class Nimbus {",
                "                 {",
                "-                  org.apache.thrift.protocol.TList _list866 = iprot.readListBegin();",
                "-                  struct.success = new ArrayList<ProfileRequest>(_list866.size);",
                "-                  ProfileRequest _elem867;",
                "-                  for (int _i868 = 0; _i868 < _list866.size; ++_i868)",
                "+                  org.apache.thrift.protocol.TList _list894 = iprot.readListBegin();",
                "+                  struct.success = new ArrayList<ProfileRequest>(_list894.size);",
                "+                  ProfileRequest _elem895;",
                "+                  for (int _i896 = 0; _i896 < _list894.size; ++_i896)",
                "                   {",
                "-                    _elem867 = new ProfileRequest();",
                "-                    _elem867.read(iprot);",
                "-                    struct.success.add(_elem867);",
                "+                    _elem895 = new ProfileRequest();",
                "+                    _elem895.read(iprot);",
                "+                    struct.success.add(_elem895);",
                "                   }",
                "@@ -18322,5 +18450,5 @@ public class Nimbus {",
                "             oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.success.size()));",
                "-            for (ProfileRequest _iter869 : struct.success)",
                "+            for (ProfileRequest _iter897 : struct.success)",
                "             {",
                "-              _iter869.write(oprot);",
                "+              _iter897.write(oprot);",
                "             }",
                "@@ -18355,5 +18483,5 @@ public class Nimbus {",
                "             oprot.writeI32(struct.success.size());",
                "-            for (ProfileRequest _iter870 : struct.success)",
                "+            for (ProfileRequest _iter898 : struct.success)",
                "             {",
                "-              _iter870.write(oprot);",
                "+              _iter898.write(oprot);",
                "             }",
                "@@ -18369,10 +18497,10 @@ public class Nimbus {",
                "           {",
                "-            org.apache.thrift.protocol.TList _list871 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-            struct.success = new ArrayList<ProfileRequest>(_list871.size);",
                "-            ProfileRequest _elem872;",
                "-            for (int _i873 = 0; _i873 < _list871.size; ++_i873)",
                "+            org.apache.thrift.protocol.TList _list899 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+            struct.success = new ArrayList<ProfileRequest>(_list899.size);",
                "+            ProfileRequest _elem900;",
                "+            for (int _i901 = 0; _i901 < _list899.size; ++_i901)",
                "             {",
                "-              _elem872 = new ProfileRequest();",
                "-              _elem872.read(iprot);",
                "-              struct.success.add(_elem872);",
                "+              _elem900 = new ProfileRequest();",
                "+              _elem900.read(iprot);",
                "+              struct.success.add(_elem900);",
                "             }",
                "@@ -48139,10 +48267,10 @@ public class Nimbus {",
                "                 {",
                "-                  org.apache.thrift.protocol.TList _list874 = iprot.readListBegin();",
                "-                  struct.success = new ArrayList<OwnerResourceSummary>(_list874.size);",
                "-                  OwnerResourceSummary _elem875;",
                "-                  for (int _i876 = 0; _i876 < _list874.size; ++_i876)",
                "+                  org.apache.thrift.protocol.TList _list902 = iprot.readListBegin();",
                "+                  struct.success = new ArrayList<OwnerResourceSummary>(_list902.size);",
                "+                  OwnerResourceSummary _elem903;",
                "+                  for (int _i904 = 0; _i904 < _list902.size; ++_i904)",
                "                   {",
                "-                    _elem875 = new OwnerResourceSummary();",
                "-                    _elem875.read(iprot);",
                "-                    struct.success.add(_elem875);",
                "+                    _elem903 = new OwnerResourceSummary();",
                "+                    _elem903.read(iprot);",
                "+                    struct.success.add(_elem903);",
                "                   }",
                "@@ -48181,5 +48309,5 @@ public class Nimbus {",
                "             oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.success.size()));",
                "-            for (OwnerResourceSummary _iter877 : struct.success)",
                "+            for (OwnerResourceSummary _iter905 : struct.success)",
                "             {",
                "-              _iter877.write(oprot);",
                "+              _iter905.write(oprot);",
                "             }",
                "@@ -48222,5 +48350,5 @@ public class Nimbus {",
                "             oprot.writeI32(struct.success.size());",
                "-            for (OwnerResourceSummary _iter878 : struct.success)",
                "+            for (OwnerResourceSummary _iter906 : struct.success)",
                "             {",
                "-              _iter878.write(oprot);",
                "+              _iter906.write(oprot);",
                "             }",
                "@@ -48239,10 +48367,10 @@ public class Nimbus {",
                "           {",
                "-            org.apache.thrift.protocol.TList _list879 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-            struct.success = new ArrayList<OwnerResourceSummary>(_list879.size);",
                "-            OwnerResourceSummary _elem880;",
                "-            for (int _i881 = 0; _i881 < _list879.size; ++_i881)",
                "+            org.apache.thrift.protocol.TList _list907 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+            struct.success = new ArrayList<OwnerResourceSummary>(_list907.size);",
                "+            OwnerResourceSummary _elem908;",
                "+            for (int _i909 = 0; _i909 < _list907.size; ++_i909)",
                "             {",
                "-              _elem880 = new OwnerResourceSummary();",
                "-              _elem880.read(iprot);",
                "-              struct.success.add(_elem880);",
                "+              _elem908 = new OwnerResourceSummary();",
                "+              _elem908.read(iprot);",
                "+              struct.success.add(_elem908);",
                "             }",
                "@@ -48261,2 +48389,611 @@ public class Nimbus {",
                "+  public static class processWorkerMetrics_args implements org.apache.thrift.TBase<processWorkerMetrics_args, processWorkerMetrics_args._Fields>, java.io.Serializable, Cloneable, Comparable<processWorkerMetrics_args>   {",
                "+    private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(\"processWorkerMetrics_args\");",
                "+",
                "+    private static final org.apache.thrift.protocol.TField METRICS_FIELD_DESC = new org.apache.thrift.protocol.TField(\"metrics\", org.apache.thrift.protocol.TType.STRUCT, (short)1);",
                "+",
                "+    private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();",
                "+    static {",
                "+      schemes.put(StandardScheme.class, new processWorkerMetrics_argsStandardSchemeFactory());",
                "+      schemes.put(TupleScheme.class, new processWorkerMetrics_argsTupleSchemeFactory());",
                "+    }",
                "+",
                "+    private WorkerMetrics metrics; // required",
                "+",
                "+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */",
                "+    public enum _Fields implements org.apache.thrift.TFieldIdEnum {",
                "+      METRICS((short)1, \"metrics\");",
                "+",
                "+      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();",
                "+",
                "+      static {",
                "+        for (_Fields field : EnumSet.allOf(_Fields.class)) {",
                "+          byName.put(field.getFieldName(), field);",
                "+        }",
                "+      }",
                "+",
                "+      /**",
                "+       * Find the _Fields constant that matches fieldId, or null if its not found.",
                "+       */",
                "+      public static _Fields findByThriftId(int fieldId) {",
                "+        switch(fieldId) {",
                "+          case 1: // METRICS",
                "+            return METRICS;",
                "+          default:",
                "+            return null;",
                "+        }",
                "+      }",
                "+",
                "+      /**",
                "+       * Find the _Fields constant that matches fieldId, throwing an exception",
                "+       * if it is not found.",
                "+       */",
                "+      public static _Fields findByThriftIdOrThrow(int fieldId) {",
                "+        _Fields fields = findByThriftId(fieldId);",
                "+        if (fields == null) throw new IllegalArgumentException(\"Field \" + fieldId + \" doesn't exist!\");",
                "+        return fields;",
                "+      }",
                "+",
                "+      /**",
                "+       * Find the _Fields constant that matches name, or null if its not found.",
                "+       */",
                "+      public static _Fields findByName(String name) {",
                "+        return byName.get(name);",
                "+      }",
                "+",
                "+      private final short _thriftId;",
                "+      private final String _fieldName;",
                "+",
                "+      _Fields(short thriftId, String fieldName) {",
                "+        _thriftId = thriftId;",
                "+        _fieldName = fieldName;",
                "+      }",
                "+",
                "+      public short getThriftFieldId() {",
                "+        return _thriftId;",
                "+      }",
                "+",
                "+      public String getFieldName() {",
                "+        return _fieldName;",
                "+      }",
                "+    }",
                "+",
                "+    // isset id assignments",
                "+    public static final Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;",
                "+    static {",
                "+      Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> tmpMap = new EnumMap<_Fields, org.apache.thrift.meta_data.FieldMetaData>(_Fields.class);",
                "+      tmpMap.put(_Fields.METRICS, new org.apache.thrift.meta_data.FieldMetaData(\"metrics\", org.apache.thrift.TFieldRequirementType.DEFAULT, ",
                "+          new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, WorkerMetrics.class)));",
                "+      metaDataMap = Collections.unmodifiableMap(tmpMap);",
                "+      org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(processWorkerMetrics_args.class, metaDataMap);",
                "+    }",
                "+",
                "+    public processWorkerMetrics_args() {",
                "+    }",
                "+",
                "+    public processWorkerMetrics_args(",
                "+      WorkerMetrics metrics)",
                "+    {",
                "+      this();",
                "+      this.metrics = metrics;",
                "+    }",
                "+",
                "+    /**",
                "+     * Performs a deep copy on <i>other</i>.",
                "+     */",
                "+    public processWorkerMetrics_args(processWorkerMetrics_args other) {",
                "+      if (other.is_set_metrics()) {",
                "+        this.metrics = new WorkerMetrics(other.metrics);",
                "+      }",
                "+    }",
                "+",
                "+    public processWorkerMetrics_args deepCopy() {",
                "+      return new processWorkerMetrics_args(this);",
                "+    }",
                "+",
                "+    @Override",
                "+    public void clear() {",
                "+      this.metrics = null;",
                "+    }",
                "+",
                "+    public WorkerMetrics get_metrics() {",
                "+      return this.metrics;",
                "+    }",
                "+",
                "+    public void set_metrics(WorkerMetrics metrics) {",
                "+      this.metrics = metrics;",
                "+    }",
                "+",
                "+    public void unset_metrics() {",
                "+      this.metrics = null;",
                "+    }",
                "+",
                "+    /** Returns true if field metrics is set (has been assigned a value) and false otherwise */",
                "+    public boolean is_set_metrics() {",
                "+      return this.metrics != null;",
                "+    }",
                "+",
                "+    public void set_metrics_isSet(boolean value) {",
                "+      if (!value) {",
                "+        this.metrics = null;",
                "+      }",
                "+    }",
                "+",
                "+    public void setFieldValue(_Fields field, Object value) {",
                "+      switch (field) {",
                "+      case METRICS:",
                "+        if (value == null) {",
                "+          unset_metrics();",
                "+        } else {",
                "+          set_metrics((WorkerMetrics)value);",
                "+        }",
                "+        break;",
                "+",
                "+      }",
                "+    }",
                "+",
                "+    public Object getFieldValue(_Fields field) {",
                "+      switch (field) {",
                "+      case METRICS:",
                "+        return get_metrics();",
                "+",
                "+      }",
                "+      throw new IllegalStateException();",
                "+    }",
                "+",
                "+    /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */",
                "+    public boolean isSet(_Fields field) {",
                "+      if (field == null) {",
                "+        throw new IllegalArgumentException();",
                "+      }",
                "+",
                "+      switch (field) {",
                "+      case METRICS:",
                "+        return is_set_metrics();",
                "+      }",
                "+      throw new IllegalStateException();",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean equals(Object that) {",
                "+      if (that == null)",
                "+        return false;",
                "+      if (that instanceof processWorkerMetrics_args)",
                "+        return this.equals((processWorkerMetrics_args)that);",
                "+      return false;",
                "+    }",
                "+",
                "+    public boolean equals(processWorkerMetrics_args that) {",
                "+      if (that == null)",
                "+        return false;",
                "+",
                "+      boolean this_present_metrics = true && this.is_set_metrics();",
                "+      boolean that_present_metrics = true && that.is_set_metrics();",
                "+      if (this_present_metrics || that_present_metrics) {",
                "+        if (!(this_present_metrics && that_present_metrics))",
                "+          return false;",
                "+        if (!this.metrics.equals(that.metrics))",
                "+          return false;",
                "+      }",
                "+",
                "+      return true;",
                "+    }",
                "+",
                "+    @Override",
                "+    public int hashCode() {",
                "+      List<Object> list = new ArrayList<Object>();",
                "+",
                "+      boolean present_metrics = true && (is_set_metrics());",
                "+      list.add(present_metrics);",
                "+      if (present_metrics)",
                "+        list.add(metrics);",
                "+",
                "+      return list.hashCode();",
                "+    }",
                "+",
                "+    @Override",
                "+    public int compareTo(processWorkerMetrics_args other) {",
                "+      if (!getClass().equals(other.getClass())) {",
                "+        return getClass().getName().compareTo(other.getClass().getName());",
                "+      }",
                "+",
                "+      int lastComparison = 0;",
                "+",
                "+      lastComparison = Boolean.valueOf(is_set_metrics()).compareTo(other.is_set_metrics());",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+      if (is_set_metrics()) {",
                "+        lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.metrics, other.metrics);",
                "+        if (lastComparison != 0) {",
                "+          return lastComparison;",
                "+        }",
                "+      }",
                "+      return 0;",
                "+    }",
                "+",
                "+    public _Fields fieldForId(int fieldId) {",
                "+      return _Fields.findByThriftId(fieldId);",
                "+    }",
                "+",
                "+    public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException {",
                "+      schemes.get(iprot.getScheme()).getScheme().read(iprot, this);",
                "+    }",
                "+",
                "+    public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException {",
                "+      schemes.get(oprot.getScheme()).getScheme().write(oprot, this);",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+      StringBuilder sb = new StringBuilder(\"processWorkerMetrics_args(\");",
                "+      boolean first = true;",
                "+",
                "+      sb.append(\"metrics:\");",
                "+      if (this.metrics == null) {",
                "+        sb.append(\"null\");",
                "+      } else {",
                "+        sb.append(this.metrics);",
                "+      }",
                "+      first = false;",
                "+      sb.append(\")\");",
                "+      return sb.toString();",
                "+    }",
                "+",
                "+    public void validate() throws org.apache.thrift.TException {",
                "+      // check for required fields",
                "+      // check for sub-struct validity",
                "+      if (metrics != null) {",
                "+        metrics.validate();",
                "+      }",
                "+    }",
                "+",
                "+    private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException {",
                "+      try {",
                "+        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));",
                "+      } catch (org.apache.thrift.TException te) {",
                "+        throw new java.io.IOException(te);",
                "+      }",
                "+    }",
                "+",
                "+    private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException {",
                "+      try {",
                "+        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));",
                "+      } catch (org.apache.thrift.TException te) {",
                "+        throw new java.io.IOException(te);",
                "+      }",
                "+    }",
                "+",
                "+    private static class processWorkerMetrics_argsStandardSchemeFactory implements SchemeFactory {",
                "+      public processWorkerMetrics_argsStandardScheme getScheme() {",
                "+        return new processWorkerMetrics_argsStandardScheme();",
                "+      }",
                "+    }",
                "+",
                "+    private static class processWorkerMetrics_argsStandardScheme extends StandardScheme<processWorkerMetrics_args> {",
                "+",
                "+      public void read(org.apache.thrift.protocol.TProtocol iprot, processWorkerMetrics_args struct) throws org.apache.thrift.TException {",
                "+        org.apache.thrift.protocol.TField schemeField;",
                "+        iprot.readStructBegin();",
                "+        while (true)",
                "+        {",
                "+          schemeField = iprot.readFieldBegin();",
                "+          if (schemeField.type == org.apache.thrift.protocol.TType.STOP) { ",
                "+            break;",
                "+          }",
                "+          switch (schemeField.id) {",
                "+            case 1: // METRICS",
                "+              if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {",
                "+                struct.metrics = new WorkerMetrics();",
                "+                struct.metrics.read(iprot);",
                "+                struct.set_metrics_isSet(true);",
                "+              } else { ",
                "+                org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+              }",
                "+              break;",
                "+            default:",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+          }",
                "+          iprot.readFieldEnd();",
                "+        }",
                "+        iprot.readStructEnd();",
                "+        struct.validate();",
                "+      }",
                "+",
                "+      public void write(org.apache.thrift.protocol.TProtocol oprot, processWorkerMetrics_args struct) throws org.apache.thrift.TException {",
                "+        struct.validate();",
                "+",
                "+        oprot.writeStructBegin(STRUCT_DESC);",
                "+        if (struct.metrics != null) {",
                "+          oprot.writeFieldBegin(METRICS_FIELD_DESC);",
                "+          struct.metrics.write(oprot);",
                "+          oprot.writeFieldEnd();",
                "+        }",
                "+        oprot.writeFieldStop();",
                "+        oprot.writeStructEnd();",
                "+      }",
                "+",
                "+    }",
                "+",
                "+    private static class processWorkerMetrics_argsTupleSchemeFactory implements SchemeFactory {",
                "+      public processWorkerMetrics_argsTupleScheme getScheme() {",
                "+        return new processWorkerMetrics_argsTupleScheme();",
                "+      }",
                "+    }",
                "+",
                "+    private static class processWorkerMetrics_argsTupleScheme extends TupleScheme<processWorkerMetrics_args> {",
                "+",
                "+      @Override",
                "+      public void write(org.apache.thrift.protocol.TProtocol prot, processWorkerMetrics_args struct) throws org.apache.thrift.TException {",
                "+        TTupleProtocol oprot = (TTupleProtocol) prot;",
                "+        BitSet optionals = new BitSet();",
                "+        if (struct.is_set_metrics()) {",
                "+          optionals.set(0);",
                "+        }",
                "+        oprot.writeBitSet(optionals, 1);",
                "+        if (struct.is_set_metrics()) {",
                "+          struct.metrics.write(oprot);",
                "+        }",
                "+      }",
                "+",
                "+      @Override",
                "+      public void read(org.apache.thrift.protocol.TProtocol prot, processWorkerMetrics_args struct) throws org.apache.thrift.TException {",
                "+        TTupleProtocol iprot = (TTupleProtocol) prot;",
                "+        BitSet incoming = iprot.readBitSet(1);",
                "+        if (incoming.get(0)) {",
                "+          struct.metrics = new WorkerMetrics();",
                "+          struct.metrics.read(iprot);",
                "+          struct.set_metrics_isSet(true);",
                "+        }",
                "+      }",
                "+    }",
                "+",
                "+  }",
                "+",
                "+  public static class processWorkerMetrics_result implements org.apache.thrift.TBase<processWorkerMetrics_result, processWorkerMetrics_result._Fields>, java.io.Serializable, Cloneable, Comparable<processWorkerMetrics_result>   {",
                "+    private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(\"processWorkerMetrics_result\");",
                "+",
                "+",
                "+    private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();",
                "+    static {",
                "+      schemes.put(StandardScheme.class, new processWorkerMetrics_resultStandardSchemeFactory());",
                "+      schemes.put(TupleScheme.class, new processWorkerMetrics_resultTupleSchemeFactory());",
                "+    }",
                "+",
                "+",
                "+    /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */",
                "+    public enum _Fields implements org.apache.thrift.TFieldIdEnum {",
                "+;",
                "+",
                "+      private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();",
                "+",
                "+      static {",
                "+        for (_Fields field : EnumSet.allOf(_Fields.class)) {",
                "+          byName.put(field.getFieldName(), field);",
                "+        }",
                "+      }",
                "+",
                "+      /**",
                "+       * Find the _Fields constant that matches fieldId, or null if its not found.",
                "+       */",
                "+      public static _Fields findByThriftId(int fieldId) {",
                "+        switch(fieldId) {",
                "+          default:",
                "+            return null;",
                "+        }",
                "+      }",
                "+",
                "+      /**",
                "+       * Find the _Fields constant that matches fieldId, throwing an exception",
                "+       * if it is not found.",
                "+       */",
                "+      public static _Fields findByThriftIdOrThrow(int fieldId) {",
                "+        _Fields fields = findByThriftId(fieldId);",
                "+        if (fields == null) throw new IllegalArgumentException(\"Field \" + fieldId + \" doesn't exist!\");",
                "+        return fields;",
                "+      }",
                "+",
                "+      /**",
                "+       * Find the _Fields constant that matches name, or null if its not found.",
                "+       */",
                "+      public static _Fields findByName(String name) {",
                "+        return byName.get(name);",
                "+      }",
                "+",
                "+      private final short _thriftId;",
                "+      private final String _fieldName;",
                "+",
                "+      _Fields(short thriftId, String fieldName) {",
                "+        _thriftId = thriftId;",
                "+        _fieldName = fieldName;",
                "+      }",
                "+",
                "+      public short getThriftFieldId() {",
                "+        return _thriftId;",
                "+      }",
                "+",
                "+      public String getFieldName() {",
                "+        return _fieldName;",
                "+      }",
                "+    }",
                "+    public static final Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;",
                "+    static {",
                "+      Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> tmpMap = new EnumMap<_Fields, org.apache.thrift.meta_data.FieldMetaData>(_Fields.class);",
                "+      metaDataMap = Collections.unmodifiableMap(tmpMap);",
                "+      org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(processWorkerMetrics_result.class, metaDataMap);",
                "+    }",
                "+",
                "+    public processWorkerMetrics_result() {",
                "+    }",
                "+",
                "+    /**",
                "+     * Performs a deep copy on <i>other</i>.",
                "+     */",
                "+    public processWorkerMetrics_result(processWorkerMetrics_result other) {",
                "+    }",
                "+",
                "+    public processWorkerMetrics_result deepCopy() {",
                "+      return new processWorkerMetrics_result(this);",
                "+    }",
                "+",
                "+    @Override",
                "+    public void clear() {",
                "+    }",
                "+",
                "+    public void setFieldValue(_Fields field, Object value) {",
                "+      switch (field) {",
                "+      }",
                "+    }",
                "+",
                "+    public Object getFieldValue(_Fields field) {",
                "+      switch (field) {",
                "+      }",
                "+      throw new IllegalStateException();",
                "+    }",
                "+",
                "+    /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */",
                "+    public boolean isSet(_Fields field) {",
                "+      if (field == null) {",
                "+        throw new IllegalArgumentException();",
                "+      }",
                "+",
                "+      switch (field) {",
                "+      }",
                "+      throw new IllegalStateException();",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean equals(Object that) {",
                "+      if (that == null)",
                "+        return false;",
                "+      if (that instanceof processWorkerMetrics_result)",
                "+        return this.equals((processWorkerMetrics_result)that);",
                "+      return false;",
                "+    }",
                "+",
                "+    public boolean equals(processWorkerMetrics_result that) {",
                "+      if (that == null)",
                "+        return false;",
                "+",
                "+      return true;",
                "+    }",
                "+",
                "+    @Override",
                "+    public int hashCode() {",
                "+      List<Object> list = new ArrayList<Object>();",
                "+",
                "+      return list.hashCode();",
                "+    }",
                "+",
                "+    @Override",
                "+    public int compareTo(processWorkerMetrics_result other) {",
                "+      if (!getClass().equals(other.getClass())) {",
                "+        return getClass().getName().compareTo(other.getClass().getName());",
                "+      }",
                "+",
                "+      int lastComparison = 0;",
                "+",
                "+      return 0;",
                "+    }",
                "+",
                "+    public _Fields fieldForId(int fieldId) {",
                "+      return _Fields.findByThriftId(fieldId);",
                "+    }",
                "+",
                "+    public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException {",
                "+      schemes.get(iprot.getScheme()).getScheme().read(iprot, this);",
                "+    }",
                "+",
                "+    public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException {",
                "+      schemes.get(oprot.getScheme()).getScheme().write(oprot, this);",
                "+      }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+      StringBuilder sb = new StringBuilder(\"processWorkerMetrics_result(\");",
                "+      boolean first = true;",
                "+",
                "+      sb.append(\")\");",
                "+      return sb.toString();",
                "+    }",
                "+",
                "+    public void validate() throws org.apache.thrift.TException {",
                "+      // check for required fields",
                "+      // check for sub-struct validity",
                "+    }",
                "+",
                "+    private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException {",
                "+      try {",
                "+        write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));",
                "+      } catch (org.apache.thrift.TException te) {",
                "+        throw new java.io.IOException(te);",
                "+      }",
                "+    }",
                "+",
                "+    private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException {",
                "+      try {",
                "+        read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));",
                "+      } catch (org.apache.thrift.TException te) {",
                "+        throw new java.io.IOException(te);",
                "+      }",
                "+    }",
                "+",
                "+    private static class processWorkerMetrics_resultStandardSchemeFactory implements SchemeFactory {",
                "+      public processWorkerMetrics_resultStandardScheme getScheme() {",
                "+        return new processWorkerMetrics_resultStandardScheme();",
                "+      }",
                "+    }",
                "+",
                "+    private static class processWorkerMetrics_resultStandardScheme extends StandardScheme<processWorkerMetrics_result> {",
                "+",
                "+      public void read(org.apache.thrift.protocol.TProtocol iprot, processWorkerMetrics_result struct) throws org.apache.thrift.TException {",
                "+        org.apache.thrift.protocol.TField schemeField;",
                "+        iprot.readStructBegin();",
                "+        while (true)",
                "+        {",
                "+          schemeField = iprot.readFieldBegin();",
                "+          if (schemeField.type == org.apache.thrift.protocol.TType.STOP) { ",
                "+            break;",
                "+          }",
                "+          switch (schemeField.id) {",
                "+            default:",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+          }",
                "+          iprot.readFieldEnd();",
                "+        }",
                "+        iprot.readStructEnd();",
                "+        struct.validate();",
                "+      }",
                "+",
                "+      public void write(org.apache.thrift.protocol.TProtocol oprot, processWorkerMetrics_result struct) throws org.apache.thrift.TException {",
                "+        struct.validate();",
                "+",
                "+        oprot.writeStructBegin(STRUCT_DESC);",
                "+        oprot.writeFieldStop();",
                "+        oprot.writeStructEnd();",
                "+      }",
                "+",
                "+    }",
                "+",
                "+    private static class processWorkerMetrics_resultTupleSchemeFactory implements SchemeFactory {",
                "+      public processWorkerMetrics_resultTupleScheme getScheme() {",
                "+        return new processWorkerMetrics_resultTupleScheme();",
                "+      }",
                "+    }",
                "+",
                "+    private static class processWorkerMetrics_resultTupleScheme extends TupleScheme<processWorkerMetrics_result> {",
                "+",
                "+      @Override",
                "+      public void write(org.apache.thrift.protocol.TProtocol prot, processWorkerMetrics_result struct) throws org.apache.thrift.TException {",
                "+        TTupleProtocol oprot = (TTupleProtocol) prot;",
                "+      }",
                "+",
                "+      @Override",
                "+      public void read(org.apache.thrift.protocol.TProtocol prot, processWorkerMetrics_result struct) throws org.apache.thrift.TException {",
                "+        TTupleProtocol iprot = (TTupleProtocol) prot;",
                "+      }",
                "+    }",
                "+",
                "+  }",
                "+",
                " }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/StormBase.java b/storm-client/src/jvm/org/apache/storm/generated/StormBase.java",
                "index b1b205c1f..733da58f3 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/StormBase.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/StormBase.java",
                "@@ -1254,11 +1254,11 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map732 = iprot.readMapBegin();",
                "-                struct.component_executors = new HashMap<String,Integer>(2*_map732.size);",
                "-                String _key733;",
                "-                int _val734;",
                "-                for (int _i735 = 0; _i735 < _map732.size; ++_i735)",
                "+                org.apache.thrift.protocol.TMap _map752 = iprot.readMapBegin();",
                "+                struct.component_executors = new HashMap<String,Integer>(2*_map752.size);",
                "+                String _key753;",
                "+                int _val754;",
                "+                for (int _i755 = 0; _i755 < _map752.size; ++_i755)",
                "                 {",
                "-                  _key733 = iprot.readString();",
                "-                  _val734 = iprot.readI32();",
                "-                  struct.component_executors.put(_key733, _val734);",
                "+                  _key753 = iprot.readString();",
                "+                  _val754 = iprot.readI32();",
                "+                  struct.component_executors.put(_key753, _val754);",
                "                 }",
                "@@ -1307,12 +1307,12 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map736 = iprot.readMapBegin();",
                "-                struct.component_debug = new HashMap<String,DebugOptions>(2*_map736.size);",
                "-                String _key737;",
                "-                DebugOptions _val738;",
                "-                for (int _i739 = 0; _i739 < _map736.size; ++_i739)",
                "+                org.apache.thrift.protocol.TMap _map756 = iprot.readMapBegin();",
                "+                struct.component_debug = new HashMap<String,DebugOptions>(2*_map756.size);",
                "+                String _key757;",
                "+                DebugOptions _val758;",
                "+                for (int _i759 = 0; _i759 < _map756.size; ++_i759)",
                "                 {",
                "-                  _key737 = iprot.readString();",
                "-                  _val738 = new DebugOptions();",
                "-                  _val738.read(iprot);",
                "-                  struct.component_debug.put(_key737, _val738);",
                "+                  _key757 = iprot.readString();",
                "+                  _val758 = new DebugOptions();",
                "+                  _val758.read(iprot);",
                "+                  struct.component_debug.put(_key757, _val758);",
                "                 }",
                "@@ -1372,6 +1372,6 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, struct.component_executors.size()));",
                "-            for (Map.Entry<String, Integer> _iter740 : struct.component_executors.entrySet())",
                "+            for (Map.Entry<String, Integer> _iter760 : struct.component_executors.entrySet())",
                "             {",
                "-              oprot.writeString(_iter740.getKey());",
                "-              oprot.writeI32(_iter740.getValue());",
                "+              oprot.writeString(_iter760.getKey());",
                "+              oprot.writeI32(_iter760.getValue());",
                "             }",
                "@@ -1413,6 +1413,6 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, struct.component_debug.size()));",
                "-            for (Map.Entry<String, DebugOptions> _iter741 : struct.component_debug.entrySet())",
                "+            for (Map.Entry<String, DebugOptions> _iter761 : struct.component_debug.entrySet())",
                "             {",
                "-              oprot.writeString(_iter741.getKey());",
                "-              _iter741.getValue().write(oprot);",
                "+              oprot.writeString(_iter761.getKey());",
                "+              _iter761.getValue().write(oprot);",
                "             }",
                "@@ -1486,6 +1486,6 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "           oprot.writeI32(struct.component_executors.size());",
                "-          for (Map.Entry<String, Integer> _iter742 : struct.component_executors.entrySet())",
                "+          for (Map.Entry<String, Integer> _iter762 : struct.component_executors.entrySet())",
                "           {",
                "-            oprot.writeString(_iter742.getKey());",
                "-            oprot.writeI32(_iter742.getValue());",
                "+            oprot.writeString(_iter762.getKey());",
                "+            oprot.writeI32(_iter762.getValue());",
                "           }",
                "@@ -1508,6 +1508,6 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "           oprot.writeI32(struct.component_debug.size());",
                "-          for (Map.Entry<String, DebugOptions> _iter743 : struct.component_debug.entrySet())",
                "+          for (Map.Entry<String, DebugOptions> _iter763 : struct.component_debug.entrySet())",
                "           {",
                "-            oprot.writeString(_iter743.getKey());",
                "-            _iter743.getValue().write(oprot);",
                "+            oprot.writeString(_iter763.getKey());",
                "+            _iter763.getValue().write(oprot);",
                "           }",
                "@@ -1535,11 +1535,11 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map744 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, iprot.readI32());",
                "-          struct.component_executors = new HashMap<String,Integer>(2*_map744.size);",
                "-          String _key745;",
                "-          int _val746;",
                "-          for (int _i747 = 0; _i747 < _map744.size; ++_i747)",
                "+          org.apache.thrift.protocol.TMap _map764 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.I32, iprot.readI32());",
                "+          struct.component_executors = new HashMap<String,Integer>(2*_map764.size);",
                "+          String _key765;",
                "+          int _val766;",
                "+          for (int _i767 = 0; _i767 < _map764.size; ++_i767)",
                "           {",
                "-            _key745 = iprot.readString();",
                "-            _val746 = iprot.readI32();",
                "-            struct.component_executors.put(_key745, _val746);",
                "+            _key765 = iprot.readString();",
                "+            _val766 = iprot.readI32();",
                "+            struct.component_executors.put(_key765, _val766);",
                "           }",
                "@@ -1567,12 +1567,12 @@ public class StormBase implements org.apache.thrift.TBase<StormBase, StormBase._",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map748 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "-          struct.component_debug = new HashMap<String,DebugOptions>(2*_map748.size);",
                "-          String _key749;",
                "-          DebugOptions _val750;",
                "-          for (int _i751 = 0; _i751 < _map748.size; ++_i751)",
                "+          org.apache.thrift.protocol.TMap _map768 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+          struct.component_debug = new HashMap<String,DebugOptions>(2*_map768.size);",
                "+          String _key769;",
                "+          DebugOptions _val770;",
                "+          for (int _i771 = 0; _i771 < _map768.size; ++_i771)",
                "           {",
                "-            _key749 = iprot.readString();",
                "-            _val750 = new DebugOptions();",
                "-            _val750.read(iprot);",
                "-            struct.component_debug.put(_key749, _val750);",
                "+            _key769 = iprot.readString();",
                "+            _val770 = new DebugOptions();",
                "+            _val770.read(iprot);",
                "+            struct.component_debug.put(_key769, _val770);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/TopologyHistoryInfo.java b/storm-client/src/jvm/org/apache/storm/generated/TopologyHistoryInfo.java",
                "index 0fc7cab68..51a76256b 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/TopologyHistoryInfo.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/TopologyHistoryInfo.java",
                "@@ -366,9 +366,9 @@ public class TopologyHistoryInfo implements org.apache.thrift.TBase<TopologyHist",
                "               {",
                "-                org.apache.thrift.protocol.TList _list842 = iprot.readListBegin();",
                "-                struct.topo_ids = new ArrayList<String>(_list842.size);",
                "-                String _elem843;",
                "-                for (int _i844 = 0; _i844 < _list842.size; ++_i844)",
                "+                org.apache.thrift.protocol.TList _list862 = iprot.readListBegin();",
                "+                struct.topo_ids = new ArrayList<String>(_list862.size);",
                "+                String _elem863;",
                "+                for (int _i864 = 0; _i864 < _list862.size; ++_i864)",
                "                 {",
                "-                  _elem843 = iprot.readString();",
                "-                  struct.topo_ids.add(_elem843);",
                "+                  _elem863 = iprot.readString();",
                "+                  struct.topo_ids.add(_elem863);",
                "                 }",
                "@@ -398,5 +398,5 @@ public class TopologyHistoryInfo implements org.apache.thrift.TBase<TopologyHist",
                "           oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, struct.topo_ids.size()));",
                "-          for (String _iter845 : struct.topo_ids)",
                "+          for (String _iter865 : struct.topo_ids)",
                "           {",
                "-            oprot.writeString(_iter845);",
                "+            oprot.writeString(_iter865);",
                "           }",
                "@@ -431,5 +431,5 @@ public class TopologyHistoryInfo implements org.apache.thrift.TBase<TopologyHist",
                "           oprot.writeI32(struct.topo_ids.size());",
                "-          for (String _iter846 : struct.topo_ids)",
                "+          for (String _iter866 : struct.topo_ids)",
                "           {",
                "-            oprot.writeString(_iter846);",
                "+            oprot.writeString(_iter866);",
                "           }",
                "@@ -445,9 +445,9 @@ public class TopologyHistoryInfo implements org.apache.thrift.TBase<TopologyHist",
                "         {",
                "-          org.apache.thrift.protocol.TList _list847 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "-          struct.topo_ids = new ArrayList<String>(_list847.size);",
                "-          String _elem848;",
                "-          for (int _i849 = 0; _i849 < _list847.size; ++_i849)",
                "+          org.apache.thrift.protocol.TList _list867 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRING, iprot.readI32());",
                "+          struct.topo_ids = new ArrayList<String>(_list867.size);",
                "+          String _elem868;",
                "+          for (int _i869 = 0; _i869 < _list867.size; ++_i869)",
                "           {",
                "-            _elem848 = iprot.readString();",
                "-            struct.topo_ids.add(_elem848);",
                "+            _elem868 = iprot.readString();",
                "+            struct.topo_ids.add(_elem868);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/WorkerMetricList.java b/storm-client/src/jvm/org/apache/storm/generated/WorkerMetricList.java",
                "new file mode 100644",
                "index 000000000..0bf95b5f6",
                "--- /dev/null",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/WorkerMetricList.java",
                "@@ -0,0 +1,466 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+/**",
                "+ * Autogenerated by Thrift Compiler (0.9.3)",
                "+ *",
                "+ * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING",
                "+ *  @generated",
                "+ */",
                "+package org.apache.storm.generated;",
                "+",
                "+import org.apache.thrift.scheme.IScheme;",
                "+import org.apache.thrift.scheme.SchemeFactory;",
                "+import org.apache.thrift.scheme.StandardScheme;",
                "+",
                "+import org.apache.thrift.scheme.TupleScheme;",
                "+import org.apache.thrift.protocol.TTupleProtocol;",
                "+import org.apache.thrift.protocol.TProtocolException;",
                "+import org.apache.thrift.EncodingUtils;",
                "+import org.apache.thrift.TException;",
                "+import org.apache.thrift.async.AsyncMethodCallback;",
                "+import org.apache.thrift.server.AbstractNonblockingServer.*;",
                "+import java.util.List;",
                "+import java.util.ArrayList;",
                "+import java.util.Map;",
                "+import java.util.HashMap;",
                "+import java.util.EnumMap;",
                "+import java.util.Set;",
                "+import java.util.HashSet;",
                "+import java.util.EnumSet;",
                "+import java.util.Collections;",
                "+import java.util.BitSet;",
                "+import java.nio.ByteBuffer;",
                "+import java.util.Arrays;",
                "+import javax.annotation.Generated;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+@SuppressWarnings({\"cast\", \"rawtypes\", \"serial\", \"unchecked\"})",
                "+@Generated(value = \"Autogenerated by Thrift Compiler (0.9.3)\")",
                "+public class WorkerMetricList implements org.apache.thrift.TBase<WorkerMetricList, WorkerMetricList._Fields>, java.io.Serializable, Cloneable, Comparable<WorkerMetricList> {",
                "+  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(\"WorkerMetricList\");",
                "+",
                "+  private static final org.apache.thrift.protocol.TField METRICS_FIELD_DESC = new org.apache.thrift.protocol.TField(\"metrics\", org.apache.thrift.protocol.TType.LIST, (short)1);",
                "+",
                "+  private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();",
                "+  static {",
                "+    schemes.put(StandardScheme.class, new WorkerMetricListStandardSchemeFactory());",
                "+    schemes.put(TupleScheme.class, new WorkerMetricListTupleSchemeFactory());",
                "+  }",
                "+",
                "+  private List<WorkerMetricPoint> metrics; // required",
                "+",
                "+  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */",
                "+  public enum _Fields implements org.apache.thrift.TFieldIdEnum {",
                "+    METRICS((short)1, \"metrics\");",
                "+",
                "+    private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();",
                "+",
                "+    static {",
                "+      for (_Fields field : EnumSet.allOf(_Fields.class)) {",
                "+        byName.put(field.getFieldName(), field);",
                "+      }",
                "+    }",
                "+",
                "+    /**",
                "+     * Find the _Fields constant that matches fieldId, or null if its not found.",
                "+     */",
                "+    public static _Fields findByThriftId(int fieldId) {",
                "+      switch(fieldId) {",
                "+        case 1: // METRICS",
                "+          return METRICS;",
                "+        default:",
                "+          return null;",
                "+      }",
                "+    }",
                "+",
                "+    /**",
                "+     * Find the _Fields constant that matches fieldId, throwing an exception",
                "+     * if it is not found.",
                "+     */",
                "+    public static _Fields findByThriftIdOrThrow(int fieldId) {",
                "+      _Fields fields = findByThriftId(fieldId);",
                "+      if (fields == null) throw new IllegalArgumentException(\"Field \" + fieldId + \" doesn't exist!\");",
                "+      return fields;",
                "+    }",
                "+",
                "+    /**",
                "+     * Find the _Fields constant that matches name, or null if its not found.",
                "+     */",
                "+    public static _Fields findByName(String name) {",
                "+      return byName.get(name);",
                "+    }",
                "+",
                "+    private final short _thriftId;",
                "+    private final String _fieldName;",
                "+",
                "+    _Fields(short thriftId, String fieldName) {",
                "+      _thriftId = thriftId;",
                "+      _fieldName = fieldName;",
                "+    }",
                "+",
                "+    public short getThriftFieldId() {",
                "+      return _thriftId;",
                "+    }",
                "+",
                "+    public String getFieldName() {",
                "+      return _fieldName;",
                "+    }",
                "+  }",
                "+",
                "+  // isset id assignments",
                "+  public static final Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;",
                "+  static {",
                "+    Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> tmpMap = new EnumMap<_Fields, org.apache.thrift.meta_data.FieldMetaData>(_Fields.class);",
                "+    tmpMap.put(_Fields.METRICS, new org.apache.thrift.meta_data.FieldMetaData(\"metrics\", org.apache.thrift.TFieldRequirementType.DEFAULT, ",
                "+        new org.apache.thrift.meta_data.ListMetaData(org.apache.thrift.protocol.TType.LIST, ",
                "+            new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, WorkerMetricPoint.class))));",
                "+    metaDataMap = Collections.unmodifiableMap(tmpMap);",
                "+    org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(WorkerMetricList.class, metaDataMap);",
                "+  }",
                "+",
                "+  public WorkerMetricList() {",
                "+  }",
                "+",
                "+  public WorkerMetricList(",
                "+    List<WorkerMetricPoint> metrics)",
                "+  {",
                "+    this();",
                "+    this.metrics = metrics;",
                "+  }",
                "+",
                "+  /**",
                "+   * Performs a deep copy on <i>other</i>.",
                "+   */",
                "+  public WorkerMetricList(WorkerMetricList other) {",
                "+    if (other.is_set_metrics()) {",
                "+      List<WorkerMetricPoint> __this__metrics = new ArrayList<WorkerMetricPoint>(other.metrics.size());",
                "+      for (WorkerMetricPoint other_element : other.metrics) {",
                "+        __this__metrics.add(new WorkerMetricPoint(other_element));",
                "+      }",
                "+      this.metrics = __this__metrics;",
                "+    }",
                "+  }",
                "+",
                "+  public WorkerMetricList deepCopy() {",
                "+    return new WorkerMetricList(this);",
                "+  }",
                "+",
                "+  @Override",
                "+  public void clear() {",
                "+    this.metrics = null;",
                "+  }",
                "+",
                "+  public int get_metrics_size() {",
                "+    return (this.metrics == null) ? 0 : this.metrics.size();",
                "+  }",
                "+",
                "+  public java.util.Iterator<WorkerMetricPoint> get_metrics_iterator() {",
                "+    return (this.metrics == null) ? null : this.metrics.iterator();",
                "+  }",
                "+",
                "+  public void add_to_metrics(WorkerMetricPoint elem) {",
                "+    if (this.metrics == null) {",
                "+      this.metrics = new ArrayList<WorkerMetricPoint>();",
                "+    }",
                "+    this.metrics.add(elem);",
                "+  }",
                "+",
                "+  public List<WorkerMetricPoint> get_metrics() {",
                "+    return this.metrics;",
                "+  }",
                "+",
                "+  public void set_metrics(List<WorkerMetricPoint> metrics) {",
                "+    this.metrics = metrics;",
                "+  }",
                "+",
                "+  public void unset_metrics() {",
                "+    this.metrics = null;",
                "+  }",
                "+",
                "+  /** Returns true if field metrics is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_metrics() {",
                "+    return this.metrics != null;",
                "+  }",
                "+",
                "+  public void set_metrics_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.metrics = null;",
                "+    }",
                "+  }",
                "+",
                "+  public void setFieldValue(_Fields field, Object value) {",
                "+    switch (field) {",
                "+    case METRICS:",
                "+      if (value == null) {",
                "+        unset_metrics();",
                "+      } else {",
                "+        set_metrics((List<WorkerMetricPoint>)value);",
                "+      }",
                "+      break;",
                "+",
                "+    }",
                "+  }",
                "+",
                "+  public Object getFieldValue(_Fields field) {",
                "+    switch (field) {",
                "+    case METRICS:",
                "+      return get_metrics();",
                "+",
                "+    }",
                "+    throw new IllegalStateException();",
                "+  }",
                "+",
                "+  /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */",
                "+  public boolean isSet(_Fields field) {",
                "+    if (field == null) {",
                "+      throw new IllegalArgumentException();",
                "+    }",
                "+",
                "+    switch (field) {",
                "+    case METRICS:",
                "+      return is_set_metrics();",
                "+    }",
                "+    throw new IllegalStateException();",
                "+  }",
                "+",
                "+  @Override",
                "+  public boolean equals(Object that) {",
                "+    if (that == null)",
                "+      return false;",
                "+    if (that instanceof WorkerMetricList)",
                "+      return this.equals((WorkerMetricList)that);",
                "+    return false;",
                "+  }",
                "+",
                "+  public boolean equals(WorkerMetricList that) {",
                "+    if (that == null)",
                "+      return false;",
                "+",
                "+    boolean this_present_metrics = true && this.is_set_metrics();",
                "+    boolean that_present_metrics = true && that.is_set_metrics();",
                "+    if (this_present_metrics || that_present_metrics) {",
                "+      if (!(this_present_metrics && that_present_metrics))",
                "+        return false;",
                "+      if (!this.metrics.equals(that.metrics))",
                "+        return false;",
                "+    }",
                "+",
                "+    return true;",
                "+  }",
                "+",
                "+  @Override",
                "+  public int hashCode() {",
                "+    List<Object> list = new ArrayList<Object>();",
                "+",
                "+    boolean present_metrics = true && (is_set_metrics());",
                "+    list.add(present_metrics);",
                "+    if (present_metrics)",
                "+      list.add(metrics);",
                "+",
                "+    return list.hashCode();",
                "+  }",
                "+",
                "+  @Override",
                "+  public int compareTo(WorkerMetricList other) {",
                "+    if (!getClass().equals(other.getClass())) {",
                "+      return getClass().getName().compareTo(other.getClass().getName());",
                "+    }",
                "+",
                "+    int lastComparison = 0;",
                "+",
                "+    lastComparison = Boolean.valueOf(is_set_metrics()).compareTo(other.is_set_metrics());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_metrics()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.metrics, other.metrics);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    return 0;",
                "+  }",
                "+",
                "+  public _Fields fieldForId(int fieldId) {",
                "+    return _Fields.findByThriftId(fieldId);",
                "+  }",
                "+",
                "+  public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException {",
                "+    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);",
                "+  }",
                "+",
                "+  public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException {",
                "+    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);",
                "+  }",
                "+",
                "+  @Override",
                "+  public String toString() {",
                "+    StringBuilder sb = new StringBuilder(\"WorkerMetricList(\");",
                "+    boolean first = true;",
                "+",
                "+    sb.append(\"metrics:\");",
                "+    if (this.metrics == null) {",
                "+      sb.append(\"null\");",
                "+    } else {",
                "+      sb.append(this.metrics);",
                "+    }",
                "+    first = false;",
                "+    sb.append(\")\");",
                "+    return sb.toString();",
                "+  }",
                "+",
                "+  public void validate() throws org.apache.thrift.TException {",
                "+    // check for required fields",
                "+    // check for sub-struct validity",
                "+  }",
                "+",
                "+  private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException {",
                "+    try {",
                "+      write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));",
                "+    } catch (org.apache.thrift.TException te) {",
                "+      throw new java.io.IOException(te);",
                "+    }",
                "+  }",
                "+",
                "+  private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException {",
                "+    try {",
                "+      read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));",
                "+    } catch (org.apache.thrift.TException te) {",
                "+      throw new java.io.IOException(te);",
                "+    }",
                "+  }",
                "+",
                "+  private static class WorkerMetricListStandardSchemeFactory implements SchemeFactory {",
                "+    public WorkerMetricListStandardScheme getScheme() {",
                "+      return new WorkerMetricListStandardScheme();",
                "+    }",
                "+  }",
                "+",
                "+  private static class WorkerMetricListStandardScheme extends StandardScheme<WorkerMetricList> {",
                "+",
                "+    public void read(org.apache.thrift.protocol.TProtocol iprot, WorkerMetricList struct) throws org.apache.thrift.TException {",
                "+      org.apache.thrift.protocol.TField schemeField;",
                "+      iprot.readStructBegin();",
                "+      while (true)",
                "+      {",
                "+        schemeField = iprot.readFieldBegin();",
                "+        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) { ",
                "+          break;",
                "+        }",
                "+        switch (schemeField.id) {",
                "+          case 1: // METRICS",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.LIST) {",
                "+              {",
                "+                org.apache.thrift.protocol.TList _list870 = iprot.readListBegin();",
                "+                struct.metrics = new ArrayList<WorkerMetricPoint>(_list870.size);",
                "+                WorkerMetricPoint _elem871;",
                "+                for (int _i872 = 0; _i872 < _list870.size; ++_i872)",
                "+                {",
                "+                  _elem871 = new WorkerMetricPoint();",
                "+                  _elem871.read(iprot);",
                "+                  struct.metrics.add(_elem871);",
                "+                }",
                "+                iprot.readListEnd();",
                "+              }",
                "+              struct.set_metrics_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          default:",
                "+            org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+        }",
                "+        iprot.readFieldEnd();",
                "+      }",
                "+      iprot.readStructEnd();",
                "+      struct.validate();",
                "+    }",
                "+",
                "+    public void write(org.apache.thrift.protocol.TProtocol oprot, WorkerMetricList struct) throws org.apache.thrift.TException {",
                "+      struct.validate();",
                "+",
                "+      oprot.writeStructBegin(STRUCT_DESC);",
                "+      if (struct.metrics != null) {",
                "+        oprot.writeFieldBegin(METRICS_FIELD_DESC);",
                "+        {",
                "+          oprot.writeListBegin(new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, struct.metrics.size()));",
                "+          for (WorkerMetricPoint _iter873 : struct.metrics)",
                "+          {",
                "+            _iter873.write(oprot);",
                "+          }",
                "+          oprot.writeListEnd();",
                "+        }",
                "+        oprot.writeFieldEnd();",
                "+      }",
                "+      oprot.writeFieldStop();",
                "+      oprot.writeStructEnd();",
                "+    }",
                "+",
                "+  }",
                "+",
                "+  private static class WorkerMetricListTupleSchemeFactory implements SchemeFactory {",
                "+    public WorkerMetricListTupleScheme getScheme() {",
                "+      return new WorkerMetricListTupleScheme();",
                "+    }",
                "+  }",
                "+",
                "+  private static class WorkerMetricListTupleScheme extends TupleScheme<WorkerMetricList> {",
                "+",
                "+    @Override",
                "+    public void write(org.apache.thrift.protocol.TProtocol prot, WorkerMetricList struct) throws org.apache.thrift.TException {",
                "+      TTupleProtocol oprot = (TTupleProtocol) prot;",
                "+      BitSet optionals = new BitSet();",
                "+      if (struct.is_set_metrics()) {",
                "+        optionals.set(0);",
                "+      }",
                "+      oprot.writeBitSet(optionals, 1);",
                "+      if (struct.is_set_metrics()) {",
                "+        {",
                "+          oprot.writeI32(struct.metrics.size());",
                "+          for (WorkerMetricPoint _iter874 : struct.metrics)",
                "+          {",
                "+            _iter874.write(oprot);",
                "+          }",
                "+        }",
                "+      }",
                "+    }",
                "+",
                "+    @Override",
                "+    public void read(org.apache.thrift.protocol.TProtocol prot, WorkerMetricList struct) throws org.apache.thrift.TException {",
                "+      TTupleProtocol iprot = (TTupleProtocol) prot;",
                "+      BitSet incoming = iprot.readBitSet(1);",
                "+      if (incoming.get(0)) {",
                "+        {",
                "+          org.apache.thrift.protocol.TList _list875 = new org.apache.thrift.protocol.TList(org.apache.thrift.protocol.TType.STRUCT, iprot.readI32());",
                "+          struct.metrics = new ArrayList<WorkerMetricPoint>(_list875.size);",
                "+          WorkerMetricPoint _elem876;",
                "+          for (int _i877 = 0; _i877 < _list875.size; ++_i877)",
                "+          {",
                "+            _elem876 = new WorkerMetricPoint();",
                "+            _elem876.read(iprot);",
                "+            struct.metrics.add(_elem876);",
                "+          }",
                "+        }",
                "+        struct.set_metrics_isSet(true);",
                "+      }",
                "+    }",
                "+  }",
                "+",
                "+}",
                "+",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/WorkerMetricPoint.java b/storm-client/src/jvm/org/apache/storm/generated/WorkerMetricPoint.java",
                "new file mode 100644",
                "index 000000000..719277a1f",
                "--- /dev/null",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/WorkerMetricPoint.java",
                "@@ -0,0 +1,903 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+/**",
                "+ * Autogenerated by Thrift Compiler (0.9.3)",
                "+ *",
                "+ * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING",
                "+ *  @generated",
                "+ */",
                "+package org.apache.storm.generated;",
                "+",
                "+import org.apache.thrift.scheme.IScheme;",
                "+import org.apache.thrift.scheme.SchemeFactory;",
                "+import org.apache.thrift.scheme.StandardScheme;",
                "+",
                "+import org.apache.thrift.scheme.TupleScheme;",
                "+import org.apache.thrift.protocol.TTupleProtocol;",
                "+import org.apache.thrift.protocol.TProtocolException;",
                "+import org.apache.thrift.EncodingUtils;",
                "+import org.apache.thrift.TException;",
                "+import org.apache.thrift.async.AsyncMethodCallback;",
                "+import org.apache.thrift.server.AbstractNonblockingServer.*;",
                "+import java.util.List;",
                "+import java.util.ArrayList;",
                "+import java.util.Map;",
                "+import java.util.HashMap;",
                "+import java.util.EnumMap;",
                "+import java.util.Set;",
                "+import java.util.HashSet;",
                "+import java.util.EnumSet;",
                "+import java.util.Collections;",
                "+import java.util.BitSet;",
                "+import java.nio.ByteBuffer;",
                "+import java.util.Arrays;",
                "+import javax.annotation.Generated;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+@SuppressWarnings({\"cast\", \"rawtypes\", \"serial\", \"unchecked\"})",
                "+@Generated(value = \"Autogenerated by Thrift Compiler (0.9.3)\")",
                "+public class WorkerMetricPoint implements org.apache.thrift.TBase<WorkerMetricPoint, WorkerMetricPoint._Fields>, java.io.Serializable, Cloneable, Comparable<WorkerMetricPoint> {",
                "+  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(\"WorkerMetricPoint\");",
                "+",
                "+  private static final org.apache.thrift.protocol.TField METRIC_NAME_FIELD_DESC = new org.apache.thrift.protocol.TField(\"metricName\", org.apache.thrift.protocol.TType.STRING, (short)1);",
                "+  private static final org.apache.thrift.protocol.TField TIMESTAMP_FIELD_DESC = new org.apache.thrift.protocol.TField(\"timestamp\", org.apache.thrift.protocol.TType.I64, (short)2);",
                "+  private static final org.apache.thrift.protocol.TField METRIC_VALUE_FIELD_DESC = new org.apache.thrift.protocol.TField(\"metricValue\", org.apache.thrift.protocol.TType.DOUBLE, (short)3);",
                "+  private static final org.apache.thrift.protocol.TField COMPONENT_ID_FIELD_DESC = new org.apache.thrift.protocol.TField(\"componentId\", org.apache.thrift.protocol.TType.STRING, (short)4);",
                "+  private static final org.apache.thrift.protocol.TField EXECUTOR_ID_FIELD_DESC = new org.apache.thrift.protocol.TField(\"executorId\", org.apache.thrift.protocol.TType.STRING, (short)5);",
                "+  private static final org.apache.thrift.protocol.TField STREAM_ID_FIELD_DESC = new org.apache.thrift.protocol.TField(\"streamId\", org.apache.thrift.protocol.TType.STRING, (short)6);",
                "+",
                "+  private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();",
                "+  static {",
                "+    schemes.put(StandardScheme.class, new WorkerMetricPointStandardSchemeFactory());",
                "+    schemes.put(TupleScheme.class, new WorkerMetricPointTupleSchemeFactory());",
                "+  }",
                "+",
                "+  private String metricName; // required",
                "+  private long timestamp; // required",
                "+  private double metricValue; // required",
                "+  private String componentId; // required",
                "+  private String executorId; // required",
                "+  private String streamId; // required",
                "+",
                "+  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */",
                "+  public enum _Fields implements org.apache.thrift.TFieldIdEnum {",
                "+    METRIC_NAME((short)1, \"metricName\"),",
                "+    TIMESTAMP((short)2, \"timestamp\"),",
                "+    METRIC_VALUE((short)3, \"metricValue\"),",
                "+    COMPONENT_ID((short)4, \"componentId\"),",
                "+    EXECUTOR_ID((short)5, \"executorId\"),",
                "+    STREAM_ID((short)6, \"streamId\");",
                "+",
                "+    private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();",
                "+",
                "+    static {",
                "+      for (_Fields field : EnumSet.allOf(_Fields.class)) {",
                "+        byName.put(field.getFieldName(), field);",
                "+      }",
                "+    }",
                "+",
                "+    /**",
                "+     * Find the _Fields constant that matches fieldId, or null if its not found.",
                "+     */",
                "+    public static _Fields findByThriftId(int fieldId) {",
                "+      switch(fieldId) {",
                "+        case 1: // METRIC_NAME",
                "+          return METRIC_NAME;",
                "+        case 2: // TIMESTAMP",
                "+          return TIMESTAMP;",
                "+        case 3: // METRIC_VALUE",
                "+          return METRIC_VALUE;",
                "+        case 4: // COMPONENT_ID",
                "+          return COMPONENT_ID;",
                "+        case 5: // EXECUTOR_ID",
                "+          return EXECUTOR_ID;",
                "+        case 6: // STREAM_ID",
                "+          return STREAM_ID;",
                "+        default:",
                "+          return null;",
                "+      }",
                "+    }",
                "+",
                "+    /**",
                "+     * Find the _Fields constant that matches fieldId, throwing an exception",
                "+     * if it is not found.",
                "+     */",
                "+    public static _Fields findByThriftIdOrThrow(int fieldId) {",
                "+      _Fields fields = findByThriftId(fieldId);",
                "+      if (fields == null) throw new IllegalArgumentException(\"Field \" + fieldId + \" doesn't exist!\");",
                "+      return fields;",
                "+    }",
                "+",
                "+    /**",
                "+     * Find the _Fields constant that matches name, or null if its not found.",
                "+     */",
                "+    public static _Fields findByName(String name) {",
                "+      return byName.get(name);",
                "+    }",
                "+",
                "+    private final short _thriftId;",
                "+    private final String _fieldName;",
                "+",
                "+    _Fields(short thriftId, String fieldName) {",
                "+      _thriftId = thriftId;",
                "+      _fieldName = fieldName;",
                "+    }",
                "+",
                "+    public short getThriftFieldId() {",
                "+      return _thriftId;",
                "+    }",
                "+",
                "+    public String getFieldName() {",
                "+      return _fieldName;",
                "+    }",
                "+  }",
                "+",
                "+  // isset id assignments",
                "+  private static final int __TIMESTAMP_ISSET_ID = 0;",
                "+  private static final int __METRICVALUE_ISSET_ID = 1;",
                "+  private byte __isset_bitfield = 0;",
                "+  public static final Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;",
                "+  static {",
                "+    Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> tmpMap = new EnumMap<_Fields, org.apache.thrift.meta_data.FieldMetaData>(_Fields.class);",
                "+    tmpMap.put(_Fields.METRIC_NAME, new org.apache.thrift.meta_data.FieldMetaData(\"metricName\", org.apache.thrift.TFieldRequirementType.REQUIRED, ",
                "+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING)));",
                "+    tmpMap.put(_Fields.TIMESTAMP, new org.apache.thrift.meta_data.FieldMetaData(\"timestamp\", org.apache.thrift.TFieldRequirementType.REQUIRED, ",
                "+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I64)));",
                "+    tmpMap.put(_Fields.METRIC_VALUE, new org.apache.thrift.meta_data.FieldMetaData(\"metricValue\", org.apache.thrift.TFieldRequirementType.REQUIRED, ",
                "+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.DOUBLE)));",
                "+    tmpMap.put(_Fields.COMPONENT_ID, new org.apache.thrift.meta_data.FieldMetaData(\"componentId\", org.apache.thrift.TFieldRequirementType.REQUIRED, ",
                "+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING)));",
                "+    tmpMap.put(_Fields.EXECUTOR_ID, new org.apache.thrift.meta_data.FieldMetaData(\"executorId\", org.apache.thrift.TFieldRequirementType.REQUIRED, ",
                "+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING)));",
                "+    tmpMap.put(_Fields.STREAM_ID, new org.apache.thrift.meta_data.FieldMetaData(\"streamId\", org.apache.thrift.TFieldRequirementType.REQUIRED, ",
                "+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING)));",
                "+    metaDataMap = Collections.unmodifiableMap(tmpMap);",
                "+    org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(WorkerMetricPoint.class, metaDataMap);",
                "+  }",
                "+",
                "+  public WorkerMetricPoint() {",
                "+  }",
                "+",
                "+  public WorkerMetricPoint(",
                "+    String metricName,",
                "+    long timestamp,",
                "+    double metricValue,",
                "+    String componentId,",
                "+    String executorId,",
                "+    String streamId)",
                "+  {",
                "+    this();",
                "+    this.metricName = metricName;",
                "+    this.timestamp = timestamp;",
                "+    set_timestamp_isSet(true);",
                "+    this.metricValue = metricValue;",
                "+    set_metricValue_isSet(true);",
                "+    this.componentId = componentId;",
                "+    this.executorId = executorId;",
                "+    this.streamId = streamId;",
                "+  }",
                "+",
                "+  /**",
                "+   * Performs a deep copy on <i>other</i>.",
                "+   */",
                "+  public WorkerMetricPoint(WorkerMetricPoint other) {",
                "+    __isset_bitfield = other.__isset_bitfield;",
                "+    if (other.is_set_metricName()) {",
                "+      this.metricName = other.metricName;",
                "+    }",
                "+    this.timestamp = other.timestamp;",
                "+    this.metricValue = other.metricValue;",
                "+    if (other.is_set_componentId()) {",
                "+      this.componentId = other.componentId;",
                "+    }",
                "+    if (other.is_set_executorId()) {",
                "+      this.executorId = other.executorId;",
                "+    }",
                "+    if (other.is_set_streamId()) {",
                "+      this.streamId = other.streamId;",
                "+    }",
                "+  }",
                "+",
                "+  public WorkerMetricPoint deepCopy() {",
                "+    return new WorkerMetricPoint(this);",
                "+  }",
                "+",
                "+  @Override",
                "+  public void clear() {",
                "+    this.metricName = null;",
                "+    set_timestamp_isSet(false);",
                "+    this.timestamp = 0;",
                "+    set_metricValue_isSet(false);",
                "+    this.metricValue = 0.0;",
                "+    this.componentId = null;",
                "+    this.executorId = null;",
                "+    this.streamId = null;",
                "+  }",
                "+",
                "+  public String get_metricName() {",
                "+    return this.metricName;",
                "+  }",
                "+",
                "+  public void set_metricName(String metricName) {",
                "+    this.metricName = metricName;",
                "+  }",
                "+",
                "+  public void unset_metricName() {",
                "+    this.metricName = null;",
                "+  }",
                "+",
                "+  /** Returns true if field metricName is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_metricName() {",
                "+    return this.metricName != null;",
                "+  }",
                "+",
                "+  public void set_metricName_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.metricName = null;",
                "+    }",
                "+  }",
                "+",
                "+  public long get_timestamp() {",
                "+    return this.timestamp;",
                "+  }",
                "+",
                "+  public void set_timestamp(long timestamp) {",
                "+    this.timestamp = timestamp;",
                "+    set_timestamp_isSet(true);",
                "+  }",
                "+",
                "+  public void unset_timestamp() {",
                "+    __isset_bitfield = EncodingUtils.clearBit(__isset_bitfield, __TIMESTAMP_ISSET_ID);",
                "+  }",
                "+",
                "+  /** Returns true if field timestamp is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_timestamp() {",
                "+    return EncodingUtils.testBit(__isset_bitfield, __TIMESTAMP_ISSET_ID);",
                "+  }",
                "+",
                "+  public void set_timestamp_isSet(boolean value) {",
                "+    __isset_bitfield = EncodingUtils.setBit(__isset_bitfield, __TIMESTAMP_ISSET_ID, value);",
                "+  }",
                "+",
                "+  public double get_metricValue() {",
                "+    return this.metricValue;",
                "+  }",
                "+",
                "+  public void set_metricValue(double metricValue) {",
                "+    this.metricValue = metricValue;",
                "+    set_metricValue_isSet(true);",
                "+  }",
                "+",
                "+  public void unset_metricValue() {",
                "+    __isset_bitfield = EncodingUtils.clearBit(__isset_bitfield, __METRICVALUE_ISSET_ID);",
                "+  }",
                "+",
                "+  /** Returns true if field metricValue is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_metricValue() {",
                "+    return EncodingUtils.testBit(__isset_bitfield, __METRICVALUE_ISSET_ID);",
                "+  }",
                "+",
                "+  public void set_metricValue_isSet(boolean value) {",
                "+    __isset_bitfield = EncodingUtils.setBit(__isset_bitfield, __METRICVALUE_ISSET_ID, value);",
                "+  }",
                "+",
                "+  public String get_componentId() {",
                "+    return this.componentId;",
                "+  }",
                "+",
                "+  public void set_componentId(String componentId) {",
                "+    this.componentId = componentId;",
                "+  }",
                "+",
                "+  public void unset_componentId() {",
                "+    this.componentId = null;",
                "+  }",
                "+",
                "+  /** Returns true if field componentId is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_componentId() {",
                "+    return this.componentId != null;",
                "+  }",
                "+",
                "+  public void set_componentId_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.componentId = null;",
                "+    }",
                "+  }",
                "+",
                "+  public String get_executorId() {",
                "+    return this.executorId;",
                "+  }",
                "+",
                "+  public void set_executorId(String executorId) {",
                "+    this.executorId = executorId;",
                "+  }",
                "+",
                "+  public void unset_executorId() {",
                "+    this.executorId = null;",
                "+  }",
                "+",
                "+  /** Returns true if field executorId is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_executorId() {",
                "+    return this.executorId != null;",
                "+  }",
                "+",
                "+  public void set_executorId_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.executorId = null;",
                "+    }",
                "+  }",
                "+",
                "+  public String get_streamId() {",
                "+    return this.streamId;",
                "+  }",
                "+",
                "+  public void set_streamId(String streamId) {",
                "+    this.streamId = streamId;",
                "+  }",
                "+",
                "+  public void unset_streamId() {",
                "+    this.streamId = null;",
                "+  }",
                "+",
                "+  /** Returns true if field streamId is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_streamId() {",
                "+    return this.streamId != null;",
                "+  }",
                "+",
                "+  public void set_streamId_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.streamId = null;",
                "+    }",
                "+  }",
                "+",
                "+  public void setFieldValue(_Fields field, Object value) {",
                "+    switch (field) {",
                "+    case METRIC_NAME:",
                "+      if (value == null) {",
                "+        unset_metricName();",
                "+      } else {",
                "+        set_metricName((String)value);",
                "+      }",
                "+      break;",
                "+",
                "+    case TIMESTAMP:",
                "+      if (value == null) {",
                "+        unset_timestamp();",
                "+      } else {",
                "+        set_timestamp((Long)value);",
                "+      }",
                "+      break;",
                "+",
                "+    case METRIC_VALUE:",
                "+      if (value == null) {",
                "+        unset_metricValue();",
                "+      } else {",
                "+        set_metricValue((Double)value);",
                "+      }",
                "+      break;",
                "+",
                "+    case COMPONENT_ID:",
                "+      if (value == null) {",
                "+        unset_componentId();",
                "+      } else {",
                "+        set_componentId((String)value);",
                "+      }",
                "+      break;",
                "+",
                "+    case EXECUTOR_ID:",
                "+      if (value == null) {",
                "+        unset_executorId();",
                "+      } else {",
                "+        set_executorId((String)value);",
                "+      }",
                "+      break;",
                "+",
                "+    case STREAM_ID:",
                "+      if (value == null) {",
                "+        unset_streamId();",
                "+      } else {",
                "+        set_streamId((String)value);",
                "+      }",
                "+      break;",
                "+",
                "+    }",
                "+  }",
                "+",
                "+  public Object getFieldValue(_Fields field) {",
                "+    switch (field) {",
                "+    case METRIC_NAME:",
                "+      return get_metricName();",
                "+",
                "+    case TIMESTAMP:",
                "+      return get_timestamp();",
                "+",
                "+    case METRIC_VALUE:",
                "+      return get_metricValue();",
                "+",
                "+    case COMPONENT_ID:",
                "+      return get_componentId();",
                "+",
                "+    case EXECUTOR_ID:",
                "+      return get_executorId();",
                "+",
                "+    case STREAM_ID:",
                "+      return get_streamId();",
                "+",
                "+    }",
                "+    throw new IllegalStateException();",
                "+  }",
                "+",
                "+  /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */",
                "+  public boolean isSet(_Fields field) {",
                "+    if (field == null) {",
                "+      throw new IllegalArgumentException();",
                "+    }",
                "+",
                "+    switch (field) {",
                "+    case METRIC_NAME:",
                "+      return is_set_metricName();",
                "+    case TIMESTAMP:",
                "+      return is_set_timestamp();",
                "+    case METRIC_VALUE:",
                "+      return is_set_metricValue();",
                "+    case COMPONENT_ID:",
                "+      return is_set_componentId();",
                "+    case EXECUTOR_ID:",
                "+      return is_set_executorId();",
                "+    case STREAM_ID:",
                "+      return is_set_streamId();",
                "+    }",
                "+    throw new IllegalStateException();",
                "+  }",
                "+",
                "+  @Override",
                "+  public boolean equals(Object that) {",
                "+    if (that == null)",
                "+      return false;",
                "+    if (that instanceof WorkerMetricPoint)",
                "+      return this.equals((WorkerMetricPoint)that);",
                "+    return false;",
                "+  }",
                "+",
                "+  public boolean equals(WorkerMetricPoint that) {",
                "+    if (that == null)",
                "+      return false;",
                "+",
                "+    boolean this_present_metricName = true && this.is_set_metricName();",
                "+    boolean that_present_metricName = true && that.is_set_metricName();",
                "+    if (this_present_metricName || that_present_metricName) {",
                "+      if (!(this_present_metricName && that_present_metricName))",
                "+        return false;",
                "+      if (!this.metricName.equals(that.metricName))",
                "+        return false;",
                "+    }",
                "+",
                "+    boolean this_present_timestamp = true;",
                "+    boolean that_present_timestamp = true;",
                "+    if (this_present_timestamp || that_present_timestamp) {",
                "+      if (!(this_present_timestamp && that_present_timestamp))",
                "+        return false;",
                "+      if (this.timestamp != that.timestamp)",
                "+        return false;",
                "+    }",
                "+",
                "+    boolean this_present_metricValue = true;",
                "+    boolean that_present_metricValue = true;",
                "+    if (this_present_metricValue || that_present_metricValue) {",
                "+      if (!(this_present_metricValue && that_present_metricValue))",
                "+        return false;",
                "+      if (this.metricValue != that.metricValue)",
                "+        return false;",
                "+    }",
                "+",
                "+    boolean this_present_componentId = true && this.is_set_componentId();",
                "+    boolean that_present_componentId = true && that.is_set_componentId();",
                "+    if (this_present_componentId || that_present_componentId) {",
                "+      if (!(this_present_componentId && that_present_componentId))",
                "+        return false;",
                "+      if (!this.componentId.equals(that.componentId))",
                "+        return false;",
                "+    }",
                "+",
                "+    boolean this_present_executorId = true && this.is_set_executorId();",
                "+    boolean that_present_executorId = true && that.is_set_executorId();",
                "+    if (this_present_executorId || that_present_executorId) {",
                "+      if (!(this_present_executorId && that_present_executorId))",
                "+        return false;",
                "+      if (!this.executorId.equals(that.executorId))",
                "+        return false;",
                "+    }",
                "+",
                "+    boolean this_present_streamId = true && this.is_set_streamId();",
                "+    boolean that_present_streamId = true && that.is_set_streamId();",
                "+    if (this_present_streamId || that_present_streamId) {",
                "+      if (!(this_present_streamId && that_present_streamId))",
                "+        return false;",
                "+      if (!this.streamId.equals(that.streamId))",
                "+        return false;",
                "+    }",
                "+",
                "+    return true;",
                "+  }",
                "+",
                "+  @Override",
                "+  public int hashCode() {",
                "+    List<Object> list = new ArrayList<Object>();",
                "+",
                "+    boolean present_metricName = true && (is_set_metricName());",
                "+    list.add(present_metricName);",
                "+    if (present_metricName)",
                "+      list.add(metricName);",
                "+",
                "+    boolean present_timestamp = true;",
                "+    list.add(present_timestamp);",
                "+    if (present_timestamp)",
                "+      list.add(timestamp);",
                "+",
                "+    boolean present_metricValue = true;",
                "+    list.add(present_metricValue);",
                "+    if (present_metricValue)",
                "+      list.add(metricValue);",
                "+",
                "+    boolean present_componentId = true && (is_set_componentId());",
                "+    list.add(present_componentId);",
                "+    if (present_componentId)",
                "+      list.add(componentId);",
                "+",
                "+    boolean present_executorId = true && (is_set_executorId());",
                "+    list.add(present_executorId);",
                "+    if (present_executorId)",
                "+      list.add(executorId);",
                "+",
                "+    boolean present_streamId = true && (is_set_streamId());",
                "+    list.add(present_streamId);",
                "+    if (present_streamId)",
                "+      list.add(streamId);",
                "+",
                "+    return list.hashCode();",
                "+  }",
                "+",
                "+  @Override",
                "+  public int compareTo(WorkerMetricPoint other) {",
                "+    if (!getClass().equals(other.getClass())) {",
                "+      return getClass().getName().compareTo(other.getClass().getName());",
                "+    }",
                "+",
                "+    int lastComparison = 0;",
                "+",
                "+    lastComparison = Boolean.valueOf(is_set_metricName()).compareTo(other.is_set_metricName());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_metricName()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.metricName, other.metricName);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    lastComparison = Boolean.valueOf(is_set_timestamp()).compareTo(other.is_set_timestamp());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_timestamp()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.timestamp, other.timestamp);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    lastComparison = Boolean.valueOf(is_set_metricValue()).compareTo(other.is_set_metricValue());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_metricValue()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.metricValue, other.metricValue);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    lastComparison = Boolean.valueOf(is_set_componentId()).compareTo(other.is_set_componentId());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_componentId()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.componentId, other.componentId);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    lastComparison = Boolean.valueOf(is_set_executorId()).compareTo(other.is_set_executorId());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_executorId()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.executorId, other.executorId);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    lastComparison = Boolean.valueOf(is_set_streamId()).compareTo(other.is_set_streamId());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_streamId()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.streamId, other.streamId);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    return 0;",
                "+  }",
                "+",
                "+  public _Fields fieldForId(int fieldId) {",
                "+    return _Fields.findByThriftId(fieldId);",
                "+  }",
                "+",
                "+  public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException {",
                "+    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);",
                "+  }",
                "+",
                "+  public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException {",
                "+    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);",
                "+  }",
                "+",
                "+  @Override",
                "+  public String toString() {",
                "+    StringBuilder sb = new StringBuilder(\"WorkerMetricPoint(\");",
                "+    boolean first = true;",
                "+",
                "+    sb.append(\"metricName:\");",
                "+    if (this.metricName == null) {",
                "+      sb.append(\"null\");",
                "+    } else {",
                "+      sb.append(this.metricName);",
                "+    }",
                "+    first = false;",
                "+    if (!first) sb.append(\", \");",
                "+    sb.append(\"timestamp:\");",
                "+    sb.append(this.timestamp);",
                "+    first = false;",
                "+    if (!first) sb.append(\", \");",
                "+    sb.append(\"metricValue:\");",
                "+    sb.append(this.metricValue);",
                "+    first = false;",
                "+    if (!first) sb.append(\", \");",
                "+    sb.append(\"componentId:\");",
                "+    if (this.componentId == null) {",
                "+      sb.append(\"null\");",
                "+    } else {",
                "+      sb.append(this.componentId);",
                "+    }",
                "+    first = false;",
                "+    if (!first) sb.append(\", \");",
                "+    sb.append(\"executorId:\");",
                "+    if (this.executorId == null) {",
                "+      sb.append(\"null\");",
                "+    } else {",
                "+      sb.append(this.executorId);",
                "+    }",
                "+    first = false;",
                "+    if (!first) sb.append(\", \");",
                "+    sb.append(\"streamId:\");",
                "+    if (this.streamId == null) {",
                "+      sb.append(\"null\");",
                "+    } else {",
                "+      sb.append(this.streamId);",
                "+    }",
                "+    first = false;",
                "+    sb.append(\")\");",
                "+    return sb.toString();",
                "+  }",
                "+",
                "+  public void validate() throws org.apache.thrift.TException {",
                "+    // check for required fields",
                "+    if (!is_set_metricName()) {",
                "+      throw new org.apache.thrift.protocol.TProtocolException(\"Required field 'metricName' is unset! Struct:\" + toString());",
                "+    }",
                "+",
                "+    if (!is_set_timestamp()) {",
                "+      throw new org.apache.thrift.protocol.TProtocolException(\"Required field 'timestamp' is unset! Struct:\" + toString());",
                "+    }",
                "+",
                "+    if (!is_set_metricValue()) {",
                "+      throw new org.apache.thrift.protocol.TProtocolException(\"Required field 'metricValue' is unset! Struct:\" + toString());",
                "+    }",
                "+",
                "+    if (!is_set_componentId()) {",
                "+      throw new org.apache.thrift.protocol.TProtocolException(\"Required field 'componentId' is unset! Struct:\" + toString());",
                "+    }",
                "+",
                "+    if (!is_set_executorId()) {",
                "+      throw new org.apache.thrift.protocol.TProtocolException(\"Required field 'executorId' is unset! Struct:\" + toString());",
                "+    }",
                "+",
                "+    if (!is_set_streamId()) {",
                "+      throw new org.apache.thrift.protocol.TProtocolException(\"Required field 'streamId' is unset! Struct:\" + toString());",
                "+    }",
                "+",
                "+    // check for sub-struct validity",
                "+  }",
                "+",
                "+  private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException {",
                "+    try {",
                "+      write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));",
                "+    } catch (org.apache.thrift.TException te) {",
                "+      throw new java.io.IOException(te);",
                "+    }",
                "+  }",
                "+",
                "+  private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException {",
                "+    try {",
                "+      // it doesn't seem like you should have to do this, but java serialization is wacky, and doesn't call the default constructor.",
                "+      __isset_bitfield = 0;",
                "+      read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));",
                "+    } catch (org.apache.thrift.TException te) {",
                "+      throw new java.io.IOException(te);",
                "+    }",
                "+  }",
                "+",
                "+  private static class WorkerMetricPointStandardSchemeFactory implements SchemeFactory {",
                "+    public WorkerMetricPointStandardScheme getScheme() {",
                "+      return new WorkerMetricPointStandardScheme();",
                "+    }",
                "+  }",
                "+",
                "+  private static class WorkerMetricPointStandardScheme extends StandardScheme<WorkerMetricPoint> {",
                "+",
                "+    public void read(org.apache.thrift.protocol.TProtocol iprot, WorkerMetricPoint struct) throws org.apache.thrift.TException {",
                "+      org.apache.thrift.protocol.TField schemeField;",
                "+      iprot.readStructBegin();",
                "+      while (true)",
                "+      {",
                "+        schemeField = iprot.readFieldBegin();",
                "+        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) { ",
                "+          break;",
                "+        }",
                "+        switch (schemeField.id) {",
                "+          case 1: // METRIC_NAME",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {",
                "+              struct.metricName = iprot.readString();",
                "+              struct.set_metricName_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          case 2: // TIMESTAMP",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.I64) {",
                "+              struct.timestamp = iprot.readI64();",
                "+              struct.set_timestamp_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          case 3: // METRIC_VALUE",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.DOUBLE) {",
                "+              struct.metricValue = iprot.readDouble();",
                "+              struct.set_metricValue_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          case 4: // COMPONENT_ID",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {",
                "+              struct.componentId = iprot.readString();",
                "+              struct.set_componentId_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          case 5: // EXECUTOR_ID",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {",
                "+              struct.executorId = iprot.readString();",
                "+              struct.set_executorId_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          case 6: // STREAM_ID",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {",
                "+              struct.streamId = iprot.readString();",
                "+              struct.set_streamId_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          default:",
                "+            org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+        }",
                "+        iprot.readFieldEnd();",
                "+      }",
                "+      iprot.readStructEnd();",
                "+      struct.validate();",
                "+    }",
                "+",
                "+    public void write(org.apache.thrift.protocol.TProtocol oprot, WorkerMetricPoint struct) throws org.apache.thrift.TException {",
                "+      struct.validate();",
                "+",
                "+      oprot.writeStructBegin(STRUCT_DESC);",
                "+      if (struct.metricName != null) {",
                "+        oprot.writeFieldBegin(METRIC_NAME_FIELD_DESC);",
                "+        oprot.writeString(struct.metricName);",
                "+        oprot.writeFieldEnd();",
                "+      }",
                "+      oprot.writeFieldBegin(TIMESTAMP_FIELD_DESC);",
                "+      oprot.writeI64(struct.timestamp);",
                "+      oprot.writeFieldEnd();",
                "+      oprot.writeFieldBegin(METRIC_VALUE_FIELD_DESC);",
                "+      oprot.writeDouble(struct.metricValue);",
                "+      oprot.writeFieldEnd();",
                "+      if (struct.componentId != null) {",
                "+        oprot.writeFieldBegin(COMPONENT_ID_FIELD_DESC);",
                "+        oprot.writeString(struct.componentId);",
                "+        oprot.writeFieldEnd();",
                "+      }",
                "+      if (struct.executorId != null) {",
                "+        oprot.writeFieldBegin(EXECUTOR_ID_FIELD_DESC);",
                "+        oprot.writeString(struct.executorId);",
                "+        oprot.writeFieldEnd();",
                "+      }",
                "+      if (struct.streamId != null) {",
                "+        oprot.writeFieldBegin(STREAM_ID_FIELD_DESC);",
                "+        oprot.writeString(struct.streamId);",
                "+        oprot.writeFieldEnd();",
                "+      }",
                "+      oprot.writeFieldStop();",
                "+      oprot.writeStructEnd();",
                "+    }",
                "+",
                "+  }",
                "+",
                "+  private static class WorkerMetricPointTupleSchemeFactory implements SchemeFactory {",
                "+    public WorkerMetricPointTupleScheme getScheme() {",
                "+      return new WorkerMetricPointTupleScheme();",
                "+    }",
                "+  }",
                "+",
                "+  private static class WorkerMetricPointTupleScheme extends TupleScheme<WorkerMetricPoint> {",
                "+",
                "+    @Override",
                "+    public void write(org.apache.thrift.protocol.TProtocol prot, WorkerMetricPoint struct) throws org.apache.thrift.TException {",
                "+      TTupleProtocol oprot = (TTupleProtocol) prot;",
                "+      oprot.writeString(struct.metricName);",
                "+      oprot.writeI64(struct.timestamp);",
                "+      oprot.writeDouble(struct.metricValue);",
                "+      oprot.writeString(struct.componentId);",
                "+      oprot.writeString(struct.executorId);",
                "+      oprot.writeString(struct.streamId);",
                "+    }",
                "+",
                "+    @Override",
                "+    public void read(org.apache.thrift.protocol.TProtocol prot, WorkerMetricPoint struct) throws org.apache.thrift.TException {",
                "+      TTupleProtocol iprot = (TTupleProtocol) prot;",
                "+      struct.metricName = iprot.readString();",
                "+      struct.set_metricName_isSet(true);",
                "+      struct.timestamp = iprot.readI64();",
                "+      struct.set_timestamp_isSet(true);",
                "+      struct.metricValue = iprot.readDouble();",
                "+      struct.set_metricValue_isSet(true);",
                "+      struct.componentId = iprot.readString();",
                "+      struct.set_componentId_isSet(true);",
                "+      struct.executorId = iprot.readString();",
                "+      struct.set_executorId_isSet(true);",
                "+      struct.streamId = iprot.readString();",
                "+      struct.set_streamId_isSet(true);",
                "+    }",
                "+  }",
                "+",
                "+}",
                "+",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/WorkerMetrics.java b/storm-client/src/jvm/org/apache/storm/generated/WorkerMetrics.java",
                "new file mode 100644",
                "index 000000000..f7813fdbe",
                "--- /dev/null",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/WorkerMetrics.java",
                "@@ -0,0 +1,712 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+/**",
                "+ * Autogenerated by Thrift Compiler (0.9.3)",
                "+ *",
                "+ * DO NOT EDIT UNLESS YOU ARE SURE THAT YOU KNOW WHAT YOU ARE DOING",
                "+ *  @generated",
                "+ */",
                "+package org.apache.storm.generated;",
                "+",
                "+import org.apache.thrift.scheme.IScheme;",
                "+import org.apache.thrift.scheme.SchemeFactory;",
                "+import org.apache.thrift.scheme.StandardScheme;",
                "+",
                "+import org.apache.thrift.scheme.TupleScheme;",
                "+import org.apache.thrift.protocol.TTupleProtocol;",
                "+import org.apache.thrift.protocol.TProtocolException;",
                "+import org.apache.thrift.EncodingUtils;",
                "+import org.apache.thrift.TException;",
                "+import org.apache.thrift.async.AsyncMethodCallback;",
                "+import org.apache.thrift.server.AbstractNonblockingServer.*;",
                "+import java.util.List;",
                "+import java.util.ArrayList;",
                "+import java.util.Map;",
                "+import java.util.HashMap;",
                "+import java.util.EnumMap;",
                "+import java.util.Set;",
                "+import java.util.HashSet;",
                "+import java.util.EnumSet;",
                "+import java.util.Collections;",
                "+import java.util.BitSet;",
                "+import java.nio.ByteBuffer;",
                "+import java.util.Arrays;",
                "+import javax.annotation.Generated;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+@SuppressWarnings({\"cast\", \"rawtypes\", \"serial\", \"unchecked\"})",
                "+@Generated(value = \"Autogenerated by Thrift Compiler (0.9.3)\")",
                "+public class WorkerMetrics implements org.apache.thrift.TBase<WorkerMetrics, WorkerMetrics._Fields>, java.io.Serializable, Cloneable, Comparable<WorkerMetrics> {",
                "+  private static final org.apache.thrift.protocol.TStruct STRUCT_DESC = new org.apache.thrift.protocol.TStruct(\"WorkerMetrics\");",
                "+",
                "+  private static final org.apache.thrift.protocol.TField TOPOLOGY_ID_FIELD_DESC = new org.apache.thrift.protocol.TField(\"topologyId\", org.apache.thrift.protocol.TType.STRING, (short)1);",
                "+  private static final org.apache.thrift.protocol.TField PORT_FIELD_DESC = new org.apache.thrift.protocol.TField(\"port\", org.apache.thrift.protocol.TType.I32, (short)2);",
                "+  private static final org.apache.thrift.protocol.TField HOSTNAME_FIELD_DESC = new org.apache.thrift.protocol.TField(\"hostname\", org.apache.thrift.protocol.TType.STRING, (short)3);",
                "+  private static final org.apache.thrift.protocol.TField METRIC_LIST_FIELD_DESC = new org.apache.thrift.protocol.TField(\"metricList\", org.apache.thrift.protocol.TType.STRUCT, (short)4);",
                "+",
                "+  private static final Map<Class<? extends IScheme>, SchemeFactory> schemes = new HashMap<Class<? extends IScheme>, SchemeFactory>();",
                "+  static {",
                "+    schemes.put(StandardScheme.class, new WorkerMetricsStandardSchemeFactory());",
                "+    schemes.put(TupleScheme.class, new WorkerMetricsTupleSchemeFactory());",
                "+  }",
                "+",
                "+  private String topologyId; // required",
                "+  private int port; // required",
                "+  private String hostname; // required",
                "+  private WorkerMetricList metricList; // required",
                "+",
                "+  /** The set of fields this struct contains, along with convenience methods for finding and manipulating them. */",
                "+  public enum _Fields implements org.apache.thrift.TFieldIdEnum {",
                "+    TOPOLOGY_ID((short)1, \"topologyId\"),",
                "+    PORT((short)2, \"port\"),",
                "+    HOSTNAME((short)3, \"hostname\"),",
                "+    METRIC_LIST((short)4, \"metricList\");",
                "+",
                "+    private static final Map<String, _Fields> byName = new HashMap<String, _Fields>();",
                "+",
                "+    static {",
                "+      for (_Fields field : EnumSet.allOf(_Fields.class)) {",
                "+        byName.put(field.getFieldName(), field);",
                "+      }",
                "+    }",
                "+",
                "+    /**",
                "+     * Find the _Fields constant that matches fieldId, or null if its not found.",
                "+     */",
                "+    public static _Fields findByThriftId(int fieldId) {",
                "+      switch(fieldId) {",
                "+        case 1: // TOPOLOGY_ID",
                "+          return TOPOLOGY_ID;",
                "+        case 2: // PORT",
                "+          return PORT;",
                "+        case 3: // HOSTNAME",
                "+          return HOSTNAME;",
                "+        case 4: // METRIC_LIST",
                "+          return METRIC_LIST;",
                "+        default:",
                "+          return null;",
                "+      }",
                "+    }",
                "+",
                "+    /**",
                "+     * Find the _Fields constant that matches fieldId, throwing an exception",
                "+     * if it is not found.",
                "+     */",
                "+    public static _Fields findByThriftIdOrThrow(int fieldId) {",
                "+      _Fields fields = findByThriftId(fieldId);",
                "+      if (fields == null) throw new IllegalArgumentException(\"Field \" + fieldId + \" doesn't exist!\");",
                "+      return fields;",
                "+    }",
                "+",
                "+    /**",
                "+     * Find the _Fields constant that matches name, or null if its not found.",
                "+     */",
                "+    public static _Fields findByName(String name) {",
                "+      return byName.get(name);",
                "+    }",
                "+",
                "+    private final short _thriftId;",
                "+    private final String _fieldName;",
                "+",
                "+    _Fields(short thriftId, String fieldName) {",
                "+      _thriftId = thriftId;",
                "+      _fieldName = fieldName;",
                "+    }",
                "+",
                "+    public short getThriftFieldId() {",
                "+      return _thriftId;",
                "+    }",
                "+",
                "+    public String getFieldName() {",
                "+      return _fieldName;",
                "+    }",
                "+  }",
                "+",
                "+  // isset id assignments",
                "+  private static final int __PORT_ISSET_ID = 0;",
                "+  private byte __isset_bitfield = 0;",
                "+  public static final Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> metaDataMap;",
                "+  static {",
                "+    Map<_Fields, org.apache.thrift.meta_data.FieldMetaData> tmpMap = new EnumMap<_Fields, org.apache.thrift.meta_data.FieldMetaData>(_Fields.class);",
                "+    tmpMap.put(_Fields.TOPOLOGY_ID, new org.apache.thrift.meta_data.FieldMetaData(\"topologyId\", org.apache.thrift.TFieldRequirementType.REQUIRED, ",
                "+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING)));",
                "+    tmpMap.put(_Fields.PORT, new org.apache.thrift.meta_data.FieldMetaData(\"port\", org.apache.thrift.TFieldRequirementType.REQUIRED, ",
                "+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.I32)));",
                "+    tmpMap.put(_Fields.HOSTNAME, new org.apache.thrift.meta_data.FieldMetaData(\"hostname\", org.apache.thrift.TFieldRequirementType.REQUIRED, ",
                "+        new org.apache.thrift.meta_data.FieldValueMetaData(org.apache.thrift.protocol.TType.STRING)));",
                "+    tmpMap.put(_Fields.METRIC_LIST, new org.apache.thrift.meta_data.FieldMetaData(\"metricList\", org.apache.thrift.TFieldRequirementType.REQUIRED, ",
                "+        new org.apache.thrift.meta_data.StructMetaData(org.apache.thrift.protocol.TType.STRUCT, WorkerMetricList.class)));",
                "+    metaDataMap = Collections.unmodifiableMap(tmpMap);",
                "+    org.apache.thrift.meta_data.FieldMetaData.addStructMetaDataMap(WorkerMetrics.class, metaDataMap);",
                "+  }",
                "+",
                "+  public WorkerMetrics() {",
                "+  }",
                "+",
                "+  public WorkerMetrics(",
                "+    String topologyId,",
                "+    int port,",
                "+    String hostname,",
                "+    WorkerMetricList metricList)",
                "+  {",
                "+    this();",
                "+    this.topologyId = topologyId;",
                "+    this.port = port;",
                "+    set_port_isSet(true);",
                "+    this.hostname = hostname;",
                "+    this.metricList = metricList;",
                "+  }",
                "+",
                "+  /**",
                "+   * Performs a deep copy on <i>other</i>.",
                "+   */",
                "+  public WorkerMetrics(WorkerMetrics other) {",
                "+    __isset_bitfield = other.__isset_bitfield;",
                "+    if (other.is_set_topologyId()) {",
                "+      this.topologyId = other.topologyId;",
                "+    }",
                "+    this.port = other.port;",
                "+    if (other.is_set_hostname()) {",
                "+      this.hostname = other.hostname;",
                "+    }",
                "+    if (other.is_set_metricList()) {",
                "+      this.metricList = new WorkerMetricList(other.metricList);",
                "+    }",
                "+  }",
                "+",
                "+  public WorkerMetrics deepCopy() {",
                "+    return new WorkerMetrics(this);",
                "+  }",
                "+",
                "+  @Override",
                "+  public void clear() {",
                "+    this.topologyId = null;",
                "+    set_port_isSet(false);",
                "+    this.port = 0;",
                "+    this.hostname = null;",
                "+    this.metricList = null;",
                "+  }",
                "+",
                "+  public String get_topologyId() {",
                "+    return this.topologyId;",
                "+  }",
                "+",
                "+  public void set_topologyId(String topologyId) {",
                "+    this.topologyId = topologyId;",
                "+  }",
                "+",
                "+  public void unset_topologyId() {",
                "+    this.topologyId = null;",
                "+  }",
                "+",
                "+  /** Returns true if field topologyId is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_topologyId() {",
                "+    return this.topologyId != null;",
                "+  }",
                "+",
                "+  public void set_topologyId_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.topologyId = null;",
                "+    }",
                "+  }",
                "+",
                "+  public int get_port() {",
                "+    return this.port;",
                "+  }",
                "+",
                "+  public void set_port(int port) {",
                "+    this.port = port;",
                "+    set_port_isSet(true);",
                "+  }",
                "+",
                "+  public void unset_port() {",
                "+    __isset_bitfield = EncodingUtils.clearBit(__isset_bitfield, __PORT_ISSET_ID);",
                "+  }",
                "+",
                "+  /** Returns true if field port is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_port() {",
                "+    return EncodingUtils.testBit(__isset_bitfield, __PORT_ISSET_ID);",
                "+  }",
                "+",
                "+  public void set_port_isSet(boolean value) {",
                "+    __isset_bitfield = EncodingUtils.setBit(__isset_bitfield, __PORT_ISSET_ID, value);",
                "+  }",
                "+",
                "+  public String get_hostname() {",
                "+    return this.hostname;",
                "+  }",
                "+",
                "+  public void set_hostname(String hostname) {",
                "+    this.hostname = hostname;",
                "+  }",
                "+",
                "+  public void unset_hostname() {",
                "+    this.hostname = null;",
                "+  }",
                "+",
                "+  /** Returns true if field hostname is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_hostname() {",
                "+    return this.hostname != null;",
                "+  }",
                "+",
                "+  public void set_hostname_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.hostname = null;",
                "+    }",
                "+  }",
                "+",
                "+  public WorkerMetricList get_metricList() {",
                "+    return this.metricList;",
                "+  }",
                "+",
                "+  public void set_metricList(WorkerMetricList metricList) {",
                "+    this.metricList = metricList;",
                "+  }",
                "+",
                "+  public void unset_metricList() {",
                "+    this.metricList = null;",
                "+  }",
                "+",
                "+  /** Returns true if field metricList is set (has been assigned a value) and false otherwise */",
                "+  public boolean is_set_metricList() {",
                "+    return this.metricList != null;",
                "+  }",
                "+",
                "+  public void set_metricList_isSet(boolean value) {",
                "+    if (!value) {",
                "+      this.metricList = null;",
                "+    }",
                "+  }",
                "+",
                "+  public void setFieldValue(_Fields field, Object value) {",
                "+    switch (field) {",
                "+    case TOPOLOGY_ID:",
                "+      if (value == null) {",
                "+        unset_topologyId();",
                "+      } else {",
                "+        set_topologyId((String)value);",
                "+      }",
                "+      break;",
                "+",
                "+    case PORT:",
                "+      if (value == null) {",
                "+        unset_port();",
                "+      } else {",
                "+        set_port((Integer)value);",
                "+      }",
                "+      break;",
                "+",
                "+    case HOSTNAME:",
                "+      if (value == null) {",
                "+        unset_hostname();",
                "+      } else {",
                "+        set_hostname((String)value);",
                "+      }",
                "+      break;",
                "+",
                "+    case METRIC_LIST:",
                "+      if (value == null) {",
                "+        unset_metricList();",
                "+      } else {",
                "+        set_metricList((WorkerMetricList)value);",
                "+      }",
                "+      break;",
                "+",
                "+    }",
                "+  }",
                "+",
                "+  public Object getFieldValue(_Fields field) {",
                "+    switch (field) {",
                "+    case TOPOLOGY_ID:",
                "+      return get_topologyId();",
                "+",
                "+    case PORT:",
                "+      return get_port();",
                "+",
                "+    case HOSTNAME:",
                "+      return get_hostname();",
                "+",
                "+    case METRIC_LIST:",
                "+      return get_metricList();",
                "+",
                "+    }",
                "+    throw new IllegalStateException();",
                "+  }",
                "+",
                "+  /** Returns true if field corresponding to fieldID is set (has been assigned a value) and false otherwise */",
                "+  public boolean isSet(_Fields field) {",
                "+    if (field == null) {",
                "+      throw new IllegalArgumentException();",
                "+    }",
                "+",
                "+    switch (field) {",
                "+    case TOPOLOGY_ID:",
                "+      return is_set_topologyId();",
                "+    case PORT:",
                "+      return is_set_port();",
                "+    case HOSTNAME:",
                "+      return is_set_hostname();",
                "+    case METRIC_LIST:",
                "+      return is_set_metricList();",
                "+    }",
                "+    throw new IllegalStateException();",
                "+  }",
                "+",
                "+  @Override",
                "+  public boolean equals(Object that) {",
                "+    if (that == null)",
                "+      return false;",
                "+    if (that instanceof WorkerMetrics)",
                "+      return this.equals((WorkerMetrics)that);",
                "+    return false;",
                "+  }",
                "+",
                "+  public boolean equals(WorkerMetrics that) {",
                "+    if (that == null)",
                "+      return false;",
                "+",
                "+    boolean this_present_topologyId = true && this.is_set_topologyId();",
                "+    boolean that_present_topologyId = true && that.is_set_topologyId();",
                "+    if (this_present_topologyId || that_present_topologyId) {",
                "+      if (!(this_present_topologyId && that_present_topologyId))",
                "+        return false;",
                "+      if (!this.topologyId.equals(that.topologyId))",
                "+        return false;",
                "+    }",
                "+",
                "+    boolean this_present_port = true;",
                "+    boolean that_present_port = true;",
                "+    if (this_present_port || that_present_port) {",
                "+      if (!(this_present_port && that_present_port))",
                "+        return false;",
                "+      if (this.port != that.port)",
                "+        return false;",
                "+    }",
                "+",
                "+    boolean this_present_hostname = true && this.is_set_hostname();",
                "+    boolean that_present_hostname = true && that.is_set_hostname();",
                "+    if (this_present_hostname || that_present_hostname) {",
                "+      if (!(this_present_hostname && that_present_hostname))",
                "+        return false;",
                "+      if (!this.hostname.equals(that.hostname))",
                "+        return false;",
                "+    }",
                "+",
                "+    boolean this_present_metricList = true && this.is_set_metricList();",
                "+    boolean that_present_metricList = true && that.is_set_metricList();",
                "+    if (this_present_metricList || that_present_metricList) {",
                "+      if (!(this_present_metricList && that_present_metricList))",
                "+        return false;",
                "+      if (!this.metricList.equals(that.metricList))",
                "+        return false;",
                "+    }",
                "+",
                "+    return true;",
                "+  }",
                "+",
                "+  @Override",
                "+  public int hashCode() {",
                "+    List<Object> list = new ArrayList<Object>();",
                "+",
                "+    boolean present_topologyId = true && (is_set_topologyId());",
                "+    list.add(present_topologyId);",
                "+    if (present_topologyId)",
                "+      list.add(topologyId);",
                "+",
                "+    boolean present_port = true;",
                "+    list.add(present_port);",
                "+    if (present_port)",
                "+      list.add(port);",
                "+",
                "+    boolean present_hostname = true && (is_set_hostname());",
                "+    list.add(present_hostname);",
                "+    if (present_hostname)",
                "+      list.add(hostname);",
                "+",
                "+    boolean present_metricList = true && (is_set_metricList());",
                "+    list.add(present_metricList);",
                "+    if (present_metricList)",
                "+      list.add(metricList);",
                "+",
                "+    return list.hashCode();",
                "+  }",
                "+",
                "+  @Override",
                "+  public int compareTo(WorkerMetrics other) {",
                "+    if (!getClass().equals(other.getClass())) {",
                "+      return getClass().getName().compareTo(other.getClass().getName());",
                "+    }",
                "+",
                "+    int lastComparison = 0;",
                "+",
                "+    lastComparison = Boolean.valueOf(is_set_topologyId()).compareTo(other.is_set_topologyId());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_topologyId()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.topologyId, other.topologyId);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    lastComparison = Boolean.valueOf(is_set_port()).compareTo(other.is_set_port());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_port()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.port, other.port);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    lastComparison = Boolean.valueOf(is_set_hostname()).compareTo(other.is_set_hostname());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_hostname()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.hostname, other.hostname);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    lastComparison = Boolean.valueOf(is_set_metricList()).compareTo(other.is_set_metricList());",
                "+    if (lastComparison != 0) {",
                "+      return lastComparison;",
                "+    }",
                "+    if (is_set_metricList()) {",
                "+      lastComparison = org.apache.thrift.TBaseHelper.compareTo(this.metricList, other.metricList);",
                "+      if (lastComparison != 0) {",
                "+        return lastComparison;",
                "+      }",
                "+    }",
                "+    return 0;",
                "+  }",
                "+",
                "+  public _Fields fieldForId(int fieldId) {",
                "+    return _Fields.findByThriftId(fieldId);",
                "+  }",
                "+",
                "+  public void read(org.apache.thrift.protocol.TProtocol iprot) throws org.apache.thrift.TException {",
                "+    schemes.get(iprot.getScheme()).getScheme().read(iprot, this);",
                "+  }",
                "+",
                "+  public void write(org.apache.thrift.protocol.TProtocol oprot) throws org.apache.thrift.TException {",
                "+    schemes.get(oprot.getScheme()).getScheme().write(oprot, this);",
                "+  }",
                "+",
                "+  @Override",
                "+  public String toString() {",
                "+    StringBuilder sb = new StringBuilder(\"WorkerMetrics(\");",
                "+    boolean first = true;",
                "+",
                "+    sb.append(\"topologyId:\");",
                "+    if (this.topologyId == null) {",
                "+      sb.append(\"null\");",
                "+    } else {",
                "+      sb.append(this.topologyId);",
                "+    }",
                "+    first = false;",
                "+    if (!first) sb.append(\", \");",
                "+    sb.append(\"port:\");",
                "+    sb.append(this.port);",
                "+    first = false;",
                "+    if (!first) sb.append(\", \");",
                "+    sb.append(\"hostname:\");",
                "+    if (this.hostname == null) {",
                "+      sb.append(\"null\");",
                "+    } else {",
                "+      sb.append(this.hostname);",
                "+    }",
                "+    first = false;",
                "+    if (!first) sb.append(\", \");",
                "+    sb.append(\"metricList:\");",
                "+    if (this.metricList == null) {",
                "+      sb.append(\"null\");",
                "+    } else {",
                "+      sb.append(this.metricList);",
                "+    }",
                "+    first = false;",
                "+    sb.append(\")\");",
                "+    return sb.toString();",
                "+  }",
                "+",
                "+  public void validate() throws org.apache.thrift.TException {",
                "+    // check for required fields",
                "+    if (!is_set_topologyId()) {",
                "+      throw new org.apache.thrift.protocol.TProtocolException(\"Required field 'topologyId' is unset! Struct:\" + toString());",
                "+    }",
                "+",
                "+    if (!is_set_port()) {",
                "+      throw new org.apache.thrift.protocol.TProtocolException(\"Required field 'port' is unset! Struct:\" + toString());",
                "+    }",
                "+",
                "+    if (!is_set_hostname()) {",
                "+      throw new org.apache.thrift.protocol.TProtocolException(\"Required field 'hostname' is unset! Struct:\" + toString());",
                "+    }",
                "+",
                "+    if (!is_set_metricList()) {",
                "+      throw new org.apache.thrift.protocol.TProtocolException(\"Required field 'metricList' is unset! Struct:\" + toString());",
                "+    }",
                "+",
                "+    // check for sub-struct validity",
                "+    if (metricList != null) {",
                "+      metricList.validate();",
                "+    }",
                "+  }",
                "+",
                "+  private void writeObject(java.io.ObjectOutputStream out) throws java.io.IOException {",
                "+    try {",
                "+      write(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(out)));",
                "+    } catch (org.apache.thrift.TException te) {",
                "+      throw new java.io.IOException(te);",
                "+    }",
                "+  }",
                "+",
                "+  private void readObject(java.io.ObjectInputStream in) throws java.io.IOException, ClassNotFoundException {",
                "+    try {",
                "+      // it doesn't seem like you should have to do this, but java serialization is wacky, and doesn't call the default constructor.",
                "+      __isset_bitfield = 0;",
                "+      read(new org.apache.thrift.protocol.TCompactProtocol(new org.apache.thrift.transport.TIOStreamTransport(in)));",
                "+    } catch (org.apache.thrift.TException te) {",
                "+      throw new java.io.IOException(te);",
                "+    }",
                "+  }",
                "+",
                "+  private static class WorkerMetricsStandardSchemeFactory implements SchemeFactory {",
                "+    public WorkerMetricsStandardScheme getScheme() {",
                "+      return new WorkerMetricsStandardScheme();",
                "+    }",
                "+  }",
                "+",
                "+  private static class WorkerMetricsStandardScheme extends StandardScheme<WorkerMetrics> {",
                "+",
                "+    public void read(org.apache.thrift.protocol.TProtocol iprot, WorkerMetrics struct) throws org.apache.thrift.TException {",
                "+      org.apache.thrift.protocol.TField schemeField;",
                "+      iprot.readStructBegin();",
                "+      while (true)",
                "+      {",
                "+        schemeField = iprot.readFieldBegin();",
                "+        if (schemeField.type == org.apache.thrift.protocol.TType.STOP) { ",
                "+          break;",
                "+        }",
                "+        switch (schemeField.id) {",
                "+          case 1: // TOPOLOGY_ID",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {",
                "+              struct.topologyId = iprot.readString();",
                "+              struct.set_topologyId_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          case 2: // PORT",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.I32) {",
                "+              struct.port = iprot.readI32();",
                "+              struct.set_port_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          case 3: // HOSTNAME",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.STRING) {",
                "+              struct.hostname = iprot.readString();",
                "+              struct.set_hostname_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          case 4: // METRIC_LIST",
                "+            if (schemeField.type == org.apache.thrift.protocol.TType.STRUCT) {",
                "+              struct.metricList = new WorkerMetricList();",
                "+              struct.metricList.read(iprot);",
                "+              struct.set_metricList_isSet(true);",
                "+            } else { ",
                "+              org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+            }",
                "+            break;",
                "+          default:",
                "+            org.apache.thrift.protocol.TProtocolUtil.skip(iprot, schemeField.type);",
                "+        }",
                "+        iprot.readFieldEnd();",
                "+      }",
                "+      iprot.readStructEnd();",
                "+      struct.validate();",
                "+    }",
                "+",
                "+    public void write(org.apache.thrift.protocol.TProtocol oprot, WorkerMetrics struct) throws org.apache.thrift.TException {",
                "+      struct.validate();",
                "+",
                "+      oprot.writeStructBegin(STRUCT_DESC);",
                "+      if (struct.topologyId != null) {",
                "+        oprot.writeFieldBegin(TOPOLOGY_ID_FIELD_DESC);",
                "+        oprot.writeString(struct.topologyId);",
                "+        oprot.writeFieldEnd();",
                "+      }",
                "+      oprot.writeFieldBegin(PORT_FIELD_DESC);",
                "+      oprot.writeI32(struct.port);",
                "+      oprot.writeFieldEnd();",
                "+      if (struct.hostname != null) {",
                "+        oprot.writeFieldBegin(HOSTNAME_FIELD_DESC);",
                "+        oprot.writeString(struct.hostname);",
                "+        oprot.writeFieldEnd();",
                "+      }",
                "+      if (struct.metricList != null) {",
                "+        oprot.writeFieldBegin(METRIC_LIST_FIELD_DESC);",
                "+        struct.metricList.write(oprot);",
                "+        oprot.writeFieldEnd();",
                "+      }",
                "+      oprot.writeFieldStop();",
                "+      oprot.writeStructEnd();",
                "+    }",
                "+",
                "+  }",
                "+",
                "+  private static class WorkerMetricsTupleSchemeFactory implements SchemeFactory {",
                "+    public WorkerMetricsTupleScheme getScheme() {",
                "+      return new WorkerMetricsTupleScheme();",
                "+    }",
                "+  }",
                "+",
                "+  private static class WorkerMetricsTupleScheme extends TupleScheme<WorkerMetrics> {",
                "+",
                "+    @Override",
                "+    public void write(org.apache.thrift.protocol.TProtocol prot, WorkerMetrics struct) throws org.apache.thrift.TException {",
                "+      TTupleProtocol oprot = (TTupleProtocol) prot;",
                "+      oprot.writeString(struct.topologyId);",
                "+      oprot.writeI32(struct.port);",
                "+      oprot.writeString(struct.hostname);",
                "+      struct.metricList.write(oprot);",
                "+    }",
                "+",
                "+    @Override",
                "+    public void read(org.apache.thrift.protocol.TProtocol prot, WorkerMetrics struct) throws org.apache.thrift.TException {",
                "+      TTupleProtocol iprot = (TTupleProtocol) prot;",
                "+      struct.topologyId = iprot.readString();",
                "+      struct.set_topologyId_isSet(true);",
                "+      struct.port = iprot.readI32();",
                "+      struct.set_port_isSet(true);",
                "+      struct.hostname = iprot.readString();",
                "+      struct.set_hostname_isSet(true);",
                "+      struct.metricList = new WorkerMetricList();",
                "+      struct.metricList.read(iprot);",
                "+      struct.set_metricList_isSet(true);",
                "+    }",
                "+  }",
                "+",
                "+}",
                "+",
                "diff --git a/storm-client/src/jvm/org/apache/storm/generated/WorkerResources.java b/storm-client/src/jvm/org/apache/storm/generated/WorkerResources.java",
                "index 56e23480f..d25b5b041 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/generated/WorkerResources.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/generated/WorkerResources.java",
                "@@ -878,11 +878,11 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map646 = iprot.readMapBegin();",
                "-                struct.resources = new HashMap<String,Double>(2*_map646.size);",
                "-                String _key647;",
                "-                double _val648;",
                "-                for (int _i649 = 0; _i649 < _map646.size; ++_i649)",
                "+                org.apache.thrift.protocol.TMap _map666 = iprot.readMapBegin();",
                "+                struct.resources = new HashMap<String,Double>(2*_map666.size);",
                "+                String _key667;",
                "+                double _val668;",
                "+                for (int _i669 = 0; _i669 < _map666.size; ++_i669)",
                "                 {",
                "-                  _key647 = iprot.readString();",
                "-                  _val648 = iprot.readDouble();",
                "-                  struct.resources.put(_key647, _val648);",
                "+                  _key667 = iprot.readString();",
                "+                  _val668 = iprot.readDouble();",
                "+                  struct.resources.put(_key667, _val668);",
                "                 }",
                "@@ -898,11 +898,11 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "               {",
                "-                org.apache.thrift.protocol.TMap _map650 = iprot.readMapBegin();",
                "-                struct.shared_resources = new HashMap<String,Double>(2*_map650.size);",
                "-                String _key651;",
                "-                double _val652;",
                "-                for (int _i653 = 0; _i653 < _map650.size; ++_i653)",
                "+                org.apache.thrift.protocol.TMap _map670 = iprot.readMapBegin();",
                "+                struct.shared_resources = new HashMap<String,Double>(2*_map670.size);",
                "+                String _key671;",
                "+                double _val672;",
                "+                for (int _i673 = 0; _i673 < _map670.size; ++_i673)",
                "                 {",
                "-                  _key651 = iprot.readString();",
                "-                  _val652 = iprot.readDouble();",
                "-                  struct.shared_resources.put(_key651, _val652);",
                "+                  _key671 = iprot.readString();",
                "+                  _val672 = iprot.readDouble();",
                "+                  struct.shared_resources.put(_key671, _val672);",
                "                 }",
                "@@ -958,6 +958,6 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, struct.resources.size()));",
                "-            for (Map.Entry<String, Double> _iter654 : struct.resources.entrySet())",
                "+            for (Map.Entry<String, Double> _iter674 : struct.resources.entrySet())",
                "             {",
                "-              oprot.writeString(_iter654.getKey());",
                "-              oprot.writeDouble(_iter654.getValue());",
                "+              oprot.writeString(_iter674.getKey());",
                "+              oprot.writeDouble(_iter674.getValue());",
                "             }",
                "@@ -973,6 +973,6 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "             oprot.writeMapBegin(new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, struct.shared_resources.size()));",
                "-            for (Map.Entry<String, Double> _iter655 : struct.shared_resources.entrySet())",
                "+            for (Map.Entry<String, Double> _iter675 : struct.shared_resources.entrySet())",
                "             {",
                "-              oprot.writeString(_iter655.getKey());",
                "-              oprot.writeDouble(_iter655.getValue());",
                "+              oprot.writeString(_iter675.getKey());",
                "+              oprot.writeDouble(_iter675.getValue());",
                "             }",
                "@@ -1041,6 +1041,6 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "           oprot.writeI32(struct.resources.size());",
                "-          for (Map.Entry<String, Double> _iter656 : struct.resources.entrySet())",
                "+          for (Map.Entry<String, Double> _iter676 : struct.resources.entrySet())",
                "           {",
                "-            oprot.writeString(_iter656.getKey());",
                "-            oprot.writeDouble(_iter656.getValue());",
                "+            oprot.writeString(_iter676.getKey());",
                "+            oprot.writeDouble(_iter676.getValue());",
                "           }",
                "@@ -1051,6 +1051,6 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "           oprot.writeI32(struct.shared_resources.size());",
                "-          for (Map.Entry<String, Double> _iter657 : struct.shared_resources.entrySet())",
                "+          for (Map.Entry<String, Double> _iter677 : struct.shared_resources.entrySet())",
                "           {",
                "-            oprot.writeString(_iter657.getKey());",
                "-            oprot.writeDouble(_iter657.getValue());",
                "+            oprot.writeString(_iter677.getKey());",
                "+            oprot.writeDouble(_iter677.getValue());",
                "           }",
                "@@ -1086,11 +1086,11 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map658 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "-          struct.resources = new HashMap<String,Double>(2*_map658.size);",
                "-          String _key659;",
                "-          double _val660;",
                "-          for (int _i661 = 0; _i661 < _map658.size; ++_i661)",
                "+          org.apache.thrift.protocol.TMap _map678 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "+          struct.resources = new HashMap<String,Double>(2*_map678.size);",
                "+          String _key679;",
                "+          double _val680;",
                "+          for (int _i681 = 0; _i681 < _map678.size; ++_i681)",
                "           {",
                "-            _key659 = iprot.readString();",
                "-            _val660 = iprot.readDouble();",
                "-            struct.resources.put(_key659, _val660);",
                "+            _key679 = iprot.readString();",
                "+            _val680 = iprot.readDouble();",
                "+            struct.resources.put(_key679, _val680);",
                "           }",
                "@@ -1101,11 +1101,11 @@ public class WorkerResources implements org.apache.thrift.TBase<WorkerResources,",
                "         {",
                "-          org.apache.thrift.protocol.TMap _map662 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "-          struct.shared_resources = new HashMap<String,Double>(2*_map662.size);",
                "-          String _key663;",
                "-          double _val664;",
                "-          for (int _i665 = 0; _i665 < _map662.size; ++_i665)",
                "+          org.apache.thrift.protocol.TMap _map682 = new org.apache.thrift.protocol.TMap(org.apache.thrift.protocol.TType.STRING, org.apache.thrift.protocol.TType.DOUBLE, iprot.readI32());",
                "+          struct.shared_resources = new HashMap<String,Double>(2*_map682.size);",
                "+          String _key683;",
                "+          double _val684;",
                "+          for (int _i685 = 0; _i685 < _map682.size; ++_i685)",
                "           {",
                "-            _key663 = iprot.readString();",
                "-            _val664 = iprot.readDouble();",
                "-            struct.shared_resources.put(_key663, _val664);",
                "+            _key683 = iprot.readString();",
                "+            _val684 = iprot.readDouble();",
                "+            struct.shared_resources.put(_key683, _val684);",
                "           }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java b/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java",
                "index dcc48540c..05247f9df 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java",
                "@@ -20,8 +20,8 @@ package org.apache.storm.security.auth.authorizer;",
                "+import java.io.IOException;",
                " import java.util.Arrays;",
                "+import java.util.Collection;",
                "+import java.util.HashSet;",
                " import java.util.Map;",
                " import java.util.Set;",
                "-import java.util.HashSet;",
                "-import java.util.Collection;",
                "-import java.io.IOException;",
                "@@ -51,3 +51,5 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "             \"getOwnerResourceSummaries\"));",
                "-    protected Set<String> supervisorCommands = new HashSet<>(Arrays.asList(\"fileDownload\"));",
                "+    protected Set<String> supervisorCommands = new HashSet<>(Arrays.asList(",
                "+            \"fileDownload\",",
                "+            \"processWorkerMetrics\"));",
                "     protected Set<String> topoReadOnlyCommands = new HashSet<>(Arrays.asList(",
                "@@ -88,4 +90,5 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "     protected IGroupMappingServiceProvider groupMappingServiceProvider;",
                "+",
                "     /**",
                "-     * Invoked once immediately after construction",
                "+     * Invoked once immediately after construction.",
                "      * @param conf Storm configuration",
                "@@ -125,3 +128,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "     /**",
                "-     * permit() method is invoked for each incoming Thrift request",
                "+     * permit() method is invoked for each incoming Thrift request.",
                "      * @param context request context includes info about",
                "@@ -140,3 +143,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "                 userGroups = groupMappingServiceProvider.getGroups(user);",
                "-            } catch(IOException e) {",
                "+            } catch (IOException e) {",
                "                 LOG.warn(\"Error while trying to fetch user groups\",e);",
                "@@ -171,3 +174,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "     private Boolean checkTopoPermission(String principal, String user, Set<String> userGroups,",
                "-                                        Map<String, Object> topoConf, String userConfigKey, String groupConfigKey){",
                "+                                        Map<String, Object> topoConf, String userConfigKey, String groupConfigKey) {",
                "         Set<String> configuredUsers = new HashSet<>();",
                "@@ -191,6 +194,7 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "     private Boolean checkUserGroupAllowed(Set<String> userGroups, Set<String> configuredGroups) {",
                "-        if(userGroups.size() > 0 && configuredGroups.size() > 0) {",
                "+        if (userGroups.size() > 0 && configuredGroups.size() > 0) {",
                "             for (String tgroup : configuredGroups) {",
                "-                if(userGroups.contains(tgroup))",
                "+                if (userGroups.contains(tgroup)) {",
                "                     return true;",
                "+                }",
                "             }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java b/storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java",
                "index 1c8015a46..1bc94ace2 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java",
                "@@ -219,3 +219,2 @@ public class ConfigUtils {",
                "     public static String absoluteStormBlobStoreDir(Map<String, Object> conf) {",
                "-        String stormHome = System.getProperty(\"storm.home\");",
                "         String blobStoreDir = (String) conf.get(Config.BLOBSTORE_DIR);",
                "@@ -227,2 +226,3 @@ public class ConfigUtils {",
                "             } else {",
                "+                String stormHome = System.getProperty(\"storm.home\");",
                "                 return (stormHome + FILE_SEPARATOR + blobStoreDir);",
                "@@ -275,3 +275,3 @@ public class ConfigUtils {",
                "         if (loginConfFile != null) {",
                "-             conf.put(\"java.security.auth.login.config\", loginConfFile);",
                "+            conf.put(\"java.security.auth.login.config\", loginConfFile);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/Utils.java b/storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "index 4ad2ee2f7..73cfc8148 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "@@ -293,2 +293,11 @@ public class Utils {",
                "     public static void addShutdownHookWithForceKillIn1Sec (Runnable func) {",
                "+        addShutdownHookWithDelayedForceKill(func, 1);",
                "+    }",
                "+",
                "+    /**",
                "+     * Adds the user supplied function as a shutdown hook for cleanup.",
                "+     * Also adds a function that sleeps for numSecs and then halts the",
                "+     * runtime to avoid any zombie process in case cleanup function hangs.",
                "+     */",
                "+    public static void addShutdownHookWithDelayedForceKill (Runnable func, int numSecs) {",
                "         Runnable sleepKill = new Runnable() {",
                "@@ -297,4 +306,5 @@ public class Utils {",
                "                 try {",
                "-                    Time.sleepSecs(1);",
                "-                    LOG.warn(\"Forceing Halt...\");",
                "+                    LOG.info(\"Halting after {} seconds\", numSecs);",
                "+                    Time.sleepSecs(numSecs);",
                "+                    LOG.warn(\"Forcing Halt...\");",
                "                     Runtime.getRuntime().halt(20);",
                "diff --git a/storm-client/src/py/storm/Nimbus-remote b/storm-client/src/py/storm/Nimbus-remote",
                "index 7b080cfc2..1ce91e462 100644",
                "--- a/storm-client/src/py/storm/Nimbus-remote",
                "+++ b/storm-client/src/py/storm/Nimbus-remote",
                "@@ -90,2 +90,3 @@ if len(sys.argv) <= 1 or sys.argv[1] == '--help':",
                "   print('   getOwnerResourceSummaries(string owner)')",
                "+  print('  void processWorkerMetrics(WorkerMetrics metrics)')",
                "   print('')",
                "@@ -422,2 +423,8 @@ elif cmd == 'getOwnerResourceSummaries':",
                "+elif cmd == 'processWorkerMetrics':",
                "+  if len(args) != 1:",
                "+    print('processWorkerMetrics requires 1 args')",
                "+    sys.exit(1)",
                "+  pp.pprint(client.processWorkerMetrics(eval(args[0]),))",
                "+",
                " else:",
                "diff --git a/storm-client/src/py/storm/Nimbus.py b/storm-client/src/py/storm/Nimbus.py",
                "index 522921b1c..5f9f324c5 100644",
                "--- a/storm-client/src/py/storm/Nimbus.py",
                "+++ b/storm-client/src/py/storm/Nimbus.py",
                "@@ -385,2 +385,9 @@ class Iface:",
                "+  def processWorkerMetrics(self, metrics):",
                "+    \"\"\"",
                "+    Parameters:",
                "+     - metrics",
                "+    \"\"\"",
                "+    pass",
                "+",
                "@@ -1969,2 +1976,31 @@ class Client(Iface):",
                "+  def processWorkerMetrics(self, metrics):",
                "+    \"\"\"",
                "+    Parameters:",
                "+     - metrics",
                "+    \"\"\"",
                "+    self.send_processWorkerMetrics(metrics)",
                "+    self.recv_processWorkerMetrics()",
                "+",
                "+  def send_processWorkerMetrics(self, metrics):",
                "+    self._oprot.writeMessageBegin('processWorkerMetrics', TMessageType.CALL, self._seqid)",
                "+    args = processWorkerMetrics_args()",
                "+    args.metrics = metrics",
                "+    args.write(self._oprot)",
                "+    self._oprot.writeMessageEnd()",
                "+    self._oprot.trans.flush()",
                "+",
                "+  def recv_processWorkerMetrics(self):",
                "+    iprot = self._iprot",
                "+    (fname, mtype, rseqid) = iprot.readMessageBegin()",
                "+    if mtype == TMessageType.EXCEPTION:",
                "+      x = TApplicationException()",
                "+      x.read(iprot)",
                "+      iprot.readMessageEnd()",
                "+      raise x",
                "+    result = processWorkerMetrics_result()",
                "+    result.read(iprot)",
                "+    iprot.readMessageEnd()",
                "+    return",
                "+",
                "@@ -2020,2 +2056,3 @@ class Processor(Iface, TProcessor):",
                "     self._processMap[\"getOwnerResourceSummaries\"] = Processor.process_getOwnerResourceSummaries",
                "+    self._processMap[\"processWorkerMetrics\"] = Processor.process_processWorkerMetrics",
                "@@ -3117,2 +3154,21 @@ class Processor(Iface, TProcessor):",
                "+  def process_processWorkerMetrics(self, seqid, iprot, oprot):",
                "+    args = processWorkerMetrics_args()",
                "+    args.read(iprot)",
                "+    iprot.readMessageEnd()",
                "+    result = processWorkerMetrics_result()",
                "+    try:",
                "+      self._handler.processWorkerMetrics(args.metrics)",
                "+      msg_type = TMessageType.REPLY",
                "+    except (TTransport.TTransportException, KeyboardInterrupt, SystemExit):",
                "+      raise",
                "+    except Exception as ex:",
                "+      msg_type = TMessageType.EXCEPTION",
                "+      logging.exception(ex)",
                "+      result = TApplicationException(TApplicationException.INTERNAL_ERROR, 'Internal error')",
                "+    oprot.writeMessageBegin(\"processWorkerMetrics\", msg_type, seqid)",
                "+    result.write(oprot)",
                "+    oprot.writeMessageEnd()",
                "+    oprot.trans.flush()",
                "+",
                "@@ -4979,7 +5035,7 @@ class getComponentPendingProfileActions_result:",
                "           self.success = []",
                "-          (_etype776, _size773) = iprot.readListBegin()",
                "-          for _i777 in xrange(_size773):",
                "-            _elem778 = ProfileRequest()",
                "-            _elem778.read(iprot)",
                "-            self.success.append(_elem778)",
                "+          (_etype801, _size798) = iprot.readListBegin()",
                "+          for _i802 in xrange(_size798):",
                "+            _elem803 = ProfileRequest()",
                "+            _elem803.read(iprot)",
                "+            self.success.append(_elem803)",
                "           iprot.readListEnd()",
                "@@ -5000,4 +5056,4 @@ class getComponentPendingProfileActions_result:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.success))",
                "-      for iter779 in self.success:",
                "-        iter779.write(oprot)",
                "+      for iter804 in self.success:",
                "+        iter804.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -10072,7 +10128,7 @@ class getOwnerResourceSummaries_result:",
                "           self.success = []",
                "-          (_etype783, _size780) = iprot.readListBegin()",
                "-          for _i784 in xrange(_size780):",
                "-            _elem785 = OwnerResourceSummary()",
                "-            _elem785.read(iprot)",
                "-            self.success.append(_elem785)",
                "+          (_etype808, _size805) = iprot.readListBegin()",
                "+          for _i809 in xrange(_size805):",
                "+            _elem810 = OwnerResourceSummary()",
                "+            _elem810.read(iprot)",
                "+            self.success.append(_elem810)",
                "           iprot.readListEnd()",
                "@@ -10099,4 +10155,4 @@ class getOwnerResourceSummaries_result:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.success))",
                "-      for iter786 in self.success:",
                "-        iter786.write(oprot)",
                "+      for iter811 in self.success:",
                "+        iter811.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -10130 +10186,113 @@ class getOwnerResourceSummaries_result:",
                "     return not (self == other)",
                "+",
                "+class processWorkerMetrics_args:",
                "+  \"\"\"",
                "+  Attributes:",
                "+   - metrics",
                "+  \"\"\"",
                "+",
                "+  thrift_spec = (",
                "+    None, # 0",
                "+    (1, TType.STRUCT, 'metrics', (WorkerMetrics, WorkerMetrics.thrift_spec), None, ), # 1",
                "+  )",
                "+",
                "+  def __init__(self, metrics=None,):",
                "+    self.metrics = metrics",
                "+",
                "+  def read(self, iprot):",
                "+    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:",
                "+      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))",
                "+      return",
                "+    iprot.readStructBegin()",
                "+    while True:",
                "+      (fname, ftype, fid) = iprot.readFieldBegin()",
                "+      if ftype == TType.STOP:",
                "+        break",
                "+      if fid == 1:",
                "+        if ftype == TType.STRUCT:",
                "+          self.metrics = WorkerMetrics()",
                "+          self.metrics.read(iprot)",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      else:",
                "+        iprot.skip(ftype)",
                "+      iprot.readFieldEnd()",
                "+    iprot.readStructEnd()",
                "+",
                "+  def write(self, oprot):",
                "+    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:",
                "+      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))",
                "+      return",
                "+    oprot.writeStructBegin('processWorkerMetrics_args')",
                "+    if self.metrics is not None:",
                "+      oprot.writeFieldBegin('metrics', TType.STRUCT, 1)",
                "+      self.metrics.write(oprot)",
                "+      oprot.writeFieldEnd()",
                "+    oprot.writeFieldStop()",
                "+    oprot.writeStructEnd()",
                "+",
                "+  def validate(self):",
                "+    return",
                "+",
                "+",
                "+  def __hash__(self):",
                "+    value = 17",
                "+    value = (value * 31) ^ hash(self.metrics)",
                "+    return value",
                "+",
                "+  def __repr__(self):",
                "+    L = ['%s=%r' % (key, value)",
                "+      for key, value in self.__dict__.iteritems()]",
                "+    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))",
                "+",
                "+  def __eq__(self, other):",
                "+    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__",
                "+",
                "+  def __ne__(self, other):",
                "+    return not (self == other)",
                "+",
                "+class processWorkerMetrics_result:",
                "+",
                "+  thrift_spec = (",
                "+  )",
                "+",
                "+  def read(self, iprot):",
                "+    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:",
                "+      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))",
                "+      return",
                "+    iprot.readStructBegin()",
                "+    while True:",
                "+      (fname, ftype, fid) = iprot.readFieldBegin()",
                "+      if ftype == TType.STOP:",
                "+        break",
                "+      else:",
                "+        iprot.skip(ftype)",
                "+      iprot.readFieldEnd()",
                "+    iprot.readStructEnd()",
                "+",
                "+  def write(self, oprot):",
                "+    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:",
                "+      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))",
                "+      return",
                "+    oprot.writeStructBegin('processWorkerMetrics_result')",
                "+    oprot.writeFieldStop()",
                "+    oprot.writeStructEnd()",
                "+",
                "+  def validate(self):",
                "+    return",
                "+",
                "+",
                "+  def __hash__(self):",
                "+    value = 17",
                "+    return value",
                "+",
                "+  def __repr__(self):",
                "+    L = ['%s=%r' % (key, value)",
                "+      for key, value in self.__dict__.iteritems()]",
                "+    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))",
                "+",
                "+  def __eq__(self, other):",
                "+    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__",
                "+",
                "+  def __ne__(self, other):",
                "+    return not (self == other)",
                "diff --git a/storm-client/src/py/storm/ttypes.py b/storm-client/src/py/storm/ttypes.py",
                "index 9e6725cb3..5d942b257 100644",
                "--- a/storm-client/src/py/storm/ttypes.py",
                "+++ b/storm-client/src/py/storm/ttypes.py",
                "@@ -9454,2 +9454,4 @@ class WorkerResources:",
                "    - shared_mem_off_heap",
                "+   - resources",
                "+   - shared_resources",
                "   \"\"\"",
                "@@ -9463,5 +9465,7 @@ class WorkerResources:",
                "     (5, TType.DOUBLE, 'shared_mem_off_heap', None, None, ), # 5",
                "+    (6, TType.MAP, 'resources', (TType.STRING,None,TType.DOUBLE,None), None, ), # 6",
                "+    (7, TType.MAP, 'shared_resources', (TType.STRING,None,TType.DOUBLE,None), None, ), # 7",
                "   )",
                "-  def __init__(self, mem_on_heap=None, mem_off_heap=None, cpu=None, shared_mem_on_heap=None, shared_mem_off_heap=None,):",
                "+  def __init__(self, mem_on_heap=None, mem_off_heap=None, cpu=None, shared_mem_on_heap=None, shared_mem_off_heap=None, resources=None, shared_resources=None,):",
                "     self.mem_on_heap = mem_on_heap",
                "@@ -9471,2 +9475,4 @@ class WorkerResources:",
                "     self.shared_mem_off_heap = shared_mem_off_heap",
                "+    self.resources = resources",
                "+    self.shared_resources = shared_resources",
                "@@ -9506,2 +9512,24 @@ class WorkerResources:",
                "           iprot.skip(ftype)",
                "+      elif fid == 6:",
                "+        if ftype == TType.MAP:",
                "+          self.resources = {}",
                "+          (_ktype596, _vtype597, _size595 ) = iprot.readMapBegin()",
                "+          for _i599 in xrange(_size595):",
                "+            _key600 = iprot.readString().decode('utf-8')",
                "+            _val601 = iprot.readDouble()",
                "+            self.resources[_key600] = _val601",
                "+          iprot.readMapEnd()",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      elif fid == 7:",
                "+        if ftype == TType.MAP:",
                "+          self.shared_resources = {}",
                "+          (_ktype603, _vtype604, _size602 ) = iprot.readMapBegin()",
                "+          for _i606 in xrange(_size602):",
                "+            _key607 = iprot.readString().decode('utf-8')",
                "+            _val608 = iprot.readDouble()",
                "+            self.shared_resources[_key607] = _val608",
                "+          iprot.readMapEnd()",
                "+        else:",
                "+          iprot.skip(ftype)",
                "       else:",
                "@@ -9536,2 +9564,18 @@ class WorkerResources:",
                "       oprot.writeFieldEnd()",
                "+    if self.resources is not None:",
                "+      oprot.writeFieldBegin('resources', TType.MAP, 6)",
                "+      oprot.writeMapBegin(TType.STRING, TType.DOUBLE, len(self.resources))",
                "+      for kiter609,viter610 in self.resources.items():",
                "+        oprot.writeString(kiter609.encode('utf-8'))",
                "+        oprot.writeDouble(viter610)",
                "+      oprot.writeMapEnd()",
                "+      oprot.writeFieldEnd()",
                "+    if self.shared_resources is not None:",
                "+      oprot.writeFieldBegin('shared_resources', TType.MAP, 7)",
                "+      oprot.writeMapBegin(TType.STRING, TType.DOUBLE, len(self.shared_resources))",
                "+      for kiter611,viter612 in self.shared_resources.items():",
                "+        oprot.writeString(kiter611.encode('utf-8'))",
                "+        oprot.writeDouble(viter612)",
                "+      oprot.writeMapEnd()",
                "+      oprot.writeFieldEnd()",
                "     oprot.writeFieldStop()",
                "@@ -9550,2 +9594,4 @@ class WorkerResources:",
                "     value = (value * 31) ^ hash(self.shared_mem_off_heap)",
                "+    value = (value * 31) ^ hash(self.resources)",
                "+    value = (value * 31) ^ hash(self.shared_resources)",
                "     return value",
                "@@ -9632,7 +9678,7 @@ class Assignment:",
                "           self.node_host = {}",
                "-          (_ktype596, _vtype597, _size595 ) = iprot.readMapBegin()",
                "-          for _i599 in xrange(_size595):",
                "-            _key600 = iprot.readString().decode('utf-8')",
                "-            _val601 = iprot.readString().decode('utf-8')",
                "-            self.node_host[_key600] = _val601",
                "+          (_ktype614, _vtype615, _size613 ) = iprot.readMapBegin()",
                "+          for _i617 in xrange(_size613):",
                "+            _key618 = iprot.readString().decode('utf-8')",
                "+            _val619 = iprot.readString().decode('utf-8')",
                "+            self.node_host[_key618] = _val619",
                "           iprot.readMapEnd()",
                "@@ -9643,13 +9689,13 @@ class Assignment:",
                "           self.executor_node_port = {}",
                "-          (_ktype603, _vtype604, _size602 ) = iprot.readMapBegin()",
                "-          for _i606 in xrange(_size602):",
                "-            _key607 = []",
                "-            (_etype612, _size609) = iprot.readListBegin()",
                "-            for _i613 in xrange(_size609):",
                "-              _elem614 = iprot.readI64()",
                "-              _key607.append(_elem614)",
                "+          (_ktype621, _vtype622, _size620 ) = iprot.readMapBegin()",
                "+          for _i624 in xrange(_size620):",
                "+            _key625 = []",
                "+            (_etype630, _size627) = iprot.readListBegin()",
                "+            for _i631 in xrange(_size627):",
                "+              _elem632 = iprot.readI64()",
                "+              _key625.append(_elem632)",
                "             iprot.readListEnd()",
                "-            _val608 = NodeInfo()",
                "-            _val608.read(iprot)",
                "-            self.executor_node_port[_key607] = _val608",
                "+            _val626 = NodeInfo()",
                "+            _val626.read(iprot)",
                "+            self.executor_node_port[_key625] = _val626",
                "           iprot.readMapEnd()",
                "@@ -9660,12 +9706,12 @@ class Assignment:",
                "           self.executor_start_time_secs = {}",
                "-          (_ktype616, _vtype617, _size615 ) = iprot.readMapBegin()",
                "-          for _i619 in xrange(_size615):",
                "-            _key620 = []",
                "-            (_etype625, _size622) = iprot.readListBegin()",
                "-            for _i626 in xrange(_size622):",
                "-              _elem627 = iprot.readI64()",
                "-              _key620.append(_elem627)",
                "+          (_ktype634, _vtype635, _size633 ) = iprot.readMapBegin()",
                "+          for _i637 in xrange(_size633):",
                "+            _key638 = []",
                "+            (_etype643, _size640) = iprot.readListBegin()",
                "+            for _i644 in xrange(_size640):",
                "+              _elem645 = iprot.readI64()",
                "+              _key638.append(_elem645)",
                "             iprot.readListEnd()",
                "-            _val621 = iprot.readI64()",
                "-            self.executor_start_time_secs[_key620] = _val621",
                "+            _val639 = iprot.readI64()",
                "+            self.executor_start_time_secs[_key638] = _val639",
                "           iprot.readMapEnd()",
                "@@ -9676,9 +9722,9 @@ class Assignment:",
                "           self.worker_resources = {}",
                "-          (_ktype629, _vtype630, _size628 ) = iprot.readMapBegin()",
                "-          for _i632 in xrange(_size628):",
                "-            _key633 = NodeInfo()",
                "-            _key633.read(iprot)",
                "-            _val634 = WorkerResources()",
                "-            _val634.read(iprot)",
                "-            self.worker_resources[_key633] = _val634",
                "+          (_ktype647, _vtype648, _size646 ) = iprot.readMapBegin()",
                "+          for _i650 in xrange(_size646):",
                "+            _key651 = NodeInfo()",
                "+            _key651.read(iprot)",
                "+            _val652 = WorkerResources()",
                "+            _val652.read(iprot)",
                "+            self.worker_resources[_key651] = _val652",
                "           iprot.readMapEnd()",
                "@@ -9689,7 +9735,7 @@ class Assignment:",
                "           self.total_shared_off_heap = {}",
                "-          (_ktype636, _vtype637, _size635 ) = iprot.readMapBegin()",
                "-          for _i639 in xrange(_size635):",
                "-            _key640 = iprot.readString().decode('utf-8')",
                "-            _val641 = iprot.readDouble()",
                "-            self.total_shared_off_heap[_key640] = _val641",
                "+          (_ktype654, _vtype655, _size653 ) = iprot.readMapBegin()",
                "+          for _i657 in xrange(_size653):",
                "+            _key658 = iprot.readString().decode('utf-8')",
                "+            _val659 = iprot.readDouble()",
                "+            self.total_shared_off_heap[_key658] = _val659",
                "           iprot.readMapEnd()",
                "@@ -9719,5 +9765,5 @@ class Assignment:",
                "       oprot.writeMapBegin(TType.STRING, TType.STRING, len(self.node_host))",
                "-      for kiter642,viter643 in self.node_host.items():",
                "-        oprot.writeString(kiter642.encode('utf-8'))",
                "-        oprot.writeString(viter643.encode('utf-8'))",
                "+      for kiter660,viter661 in self.node_host.items():",
                "+        oprot.writeString(kiter660.encode('utf-8'))",
                "+        oprot.writeString(viter661.encode('utf-8'))",
                "       oprot.writeMapEnd()",
                "@@ -9727,8 +9773,8 @@ class Assignment:",
                "       oprot.writeMapBegin(TType.LIST, TType.STRUCT, len(self.executor_node_port))",
                "-      for kiter644,viter645 in self.executor_node_port.items():",
                "-        oprot.writeListBegin(TType.I64, len(kiter644))",
                "-        for iter646 in kiter644:",
                "-          oprot.writeI64(iter646)",
                "+      for kiter662,viter663 in self.executor_node_port.items():",
                "+        oprot.writeListBegin(TType.I64, len(kiter662))",
                "+        for iter664 in kiter662:",
                "+          oprot.writeI64(iter664)",
                "         oprot.writeListEnd()",
                "-        viter645.write(oprot)",
                "+        viter663.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -9738,8 +9784,8 @@ class Assignment:",
                "       oprot.writeMapBegin(TType.LIST, TType.I64, len(self.executor_start_time_secs))",
                "-      for kiter647,viter648 in self.executor_start_time_secs.items():",
                "-        oprot.writeListBegin(TType.I64, len(kiter647))",
                "-        for iter649 in kiter647:",
                "-          oprot.writeI64(iter649)",
                "+      for kiter665,viter666 in self.executor_start_time_secs.items():",
                "+        oprot.writeListBegin(TType.I64, len(kiter665))",
                "+        for iter667 in kiter665:",
                "+          oprot.writeI64(iter667)",
                "         oprot.writeListEnd()",
                "-        oprot.writeI64(viter648)",
                "+        oprot.writeI64(viter666)",
                "       oprot.writeMapEnd()",
                "@@ -9749,5 +9795,5 @@ class Assignment:",
                "       oprot.writeMapBegin(TType.STRUCT, TType.STRUCT, len(self.worker_resources))",
                "-      for kiter650,viter651 in self.worker_resources.items():",
                "-        kiter650.write(oprot)",
                "-        viter651.write(oprot)",
                "+      for kiter668,viter669 in self.worker_resources.items():",
                "+        kiter668.write(oprot)",
                "+        viter669.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -9757,5 +9803,5 @@ class Assignment:",
                "       oprot.writeMapBegin(TType.STRING, TType.DOUBLE, len(self.total_shared_off_heap))",
                "-      for kiter652,viter653 in self.total_shared_off_heap.items():",
                "-        oprot.writeString(kiter652.encode('utf-8'))",
                "-        oprot.writeDouble(viter653)",
                "+      for kiter670,viter671 in self.total_shared_off_heap.items():",
                "+        oprot.writeString(kiter670.encode('utf-8'))",
                "+        oprot.writeDouble(viter671)",
                "       oprot.writeMapEnd()",
                "@@ -9948,7 +9994,7 @@ class StormBase:",
                "           self.component_executors = {}",
                "-          (_ktype655, _vtype656, _size654 ) = iprot.readMapBegin()",
                "-          for _i658 in xrange(_size654):",
                "-            _key659 = iprot.readString().decode('utf-8')",
                "-            _val660 = iprot.readI32()",
                "-            self.component_executors[_key659] = _val660",
                "+          (_ktype673, _vtype674, _size672 ) = iprot.readMapBegin()",
                "+          for _i676 in xrange(_size672):",
                "+            _key677 = iprot.readString().decode('utf-8')",
                "+            _val678 = iprot.readI32()",
                "+            self.component_executors[_key677] = _val678",
                "           iprot.readMapEnd()",
                "@@ -9980,8 +10026,8 @@ class StormBase:",
                "           self.component_debug = {}",
                "-          (_ktype662, _vtype663, _size661 ) = iprot.readMapBegin()",
                "-          for _i665 in xrange(_size661):",
                "-            _key666 = iprot.readString().decode('utf-8')",
                "-            _val667 = DebugOptions()",
                "-            _val667.read(iprot)",
                "-            self.component_debug[_key666] = _val667",
                "+          (_ktype680, _vtype681, _size679 ) = iprot.readMapBegin()",
                "+          for _i683 in xrange(_size679):",
                "+            _key684 = iprot.readString().decode('utf-8')",
                "+            _val685 = DebugOptions()",
                "+            _val685.read(iprot)",
                "+            self.component_debug[_key684] = _val685",
                "           iprot.readMapEnd()",
                "@@ -10024,5 +10070,5 @@ class StormBase:",
                "       oprot.writeMapBegin(TType.STRING, TType.I32, len(self.component_executors))",
                "-      for kiter668,viter669 in self.component_executors.items():",
                "-        oprot.writeString(kiter668.encode('utf-8'))",
                "-        oprot.writeI32(viter669)",
                "+      for kiter686,viter687 in self.component_executors.items():",
                "+        oprot.writeString(kiter686.encode('utf-8'))",
                "+        oprot.writeI32(viter687)",
                "       oprot.writeMapEnd()",
                "@@ -10048,5 +10094,5 @@ class StormBase:",
                "       oprot.writeMapBegin(TType.STRING, TType.STRUCT, len(self.component_debug))",
                "-      for kiter670,viter671 in self.component_debug.items():",
                "-        oprot.writeString(kiter670.encode('utf-8'))",
                "-        viter671.write(oprot)",
                "+      for kiter688,viter689 in self.component_debug.items():",
                "+        oprot.writeString(kiter688.encode('utf-8'))",
                "+        viter689.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -10140,9 +10186,9 @@ class ClusterWorkerHeartbeat:",
                "           self.executor_stats = {}",
                "-          (_ktype673, _vtype674, _size672 ) = iprot.readMapBegin()",
                "-          for _i676 in xrange(_size672):",
                "-            _key677 = ExecutorInfo()",
                "-            _key677.read(iprot)",
                "-            _val678 = ExecutorStats()",
                "-            _val678.read(iprot)",
                "-            self.executor_stats[_key677] = _val678",
                "+          (_ktype691, _vtype692, _size690 ) = iprot.readMapBegin()",
                "+          for _i694 in xrange(_size690):",
                "+            _key695 = ExecutorInfo()",
                "+            _key695.read(iprot)",
                "+            _val696 = ExecutorStats()",
                "+            _val696.read(iprot)",
                "+            self.executor_stats[_key695] = _val696",
                "           iprot.readMapEnd()",
                "@@ -10177,5 +10223,5 @@ class ClusterWorkerHeartbeat:",
                "       oprot.writeMapBegin(TType.STRUCT, TType.STRUCT, len(self.executor_stats))",
                "-      for kiter679,viter680 in self.executor_stats.items():",
                "-        kiter679.write(oprot)",
                "-        viter680.write(oprot)",
                "+      for kiter697,viter698 in self.executor_stats.items():",
                "+        kiter697.write(oprot)",
                "+        viter698.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -10332,8 +10378,8 @@ class LocalStateData:",
                "           self.serialized_parts = {}",
                "-          (_ktype682, _vtype683, _size681 ) = iprot.readMapBegin()",
                "-          for _i685 in xrange(_size681):",
                "-            _key686 = iprot.readString().decode('utf-8')",
                "-            _val687 = ThriftSerializedObject()",
                "-            _val687.read(iprot)",
                "-            self.serialized_parts[_key686] = _val687",
                "+          (_ktype700, _vtype701, _size699 ) = iprot.readMapBegin()",
                "+          for _i703 in xrange(_size699):",
                "+            _key704 = iprot.readString().decode('utf-8')",
                "+            _val705 = ThriftSerializedObject()",
                "+            _val705.read(iprot)",
                "+            self.serialized_parts[_key704] = _val705",
                "           iprot.readMapEnd()",
                "@@ -10354,5 +10400,5 @@ class LocalStateData:",
                "       oprot.writeMapBegin(TType.STRING, TType.STRUCT, len(self.serialized_parts))",
                "-      for kiter688,viter689 in self.serialized_parts.items():",
                "-        oprot.writeString(kiter688.encode('utf-8'))",
                "-        viter689.write(oprot)",
                "+      for kiter706,viter707 in self.serialized_parts.items():",
                "+        oprot.writeString(kiter706.encode('utf-8'))",
                "+        viter707.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -10427,7 +10473,7 @@ class LocalAssignment:",
                "           self.executors = []",
                "-          (_etype693, _size690) = iprot.readListBegin()",
                "-          for _i694 in xrange(_size690):",
                "-            _elem695 = ExecutorInfo()",
                "-            _elem695.read(iprot)",
                "-            self.executors.append(_elem695)",
                "+          (_etype711, _size708) = iprot.readListBegin()",
                "+          for _i712 in xrange(_size708):",
                "+            _elem713 = ExecutorInfo()",
                "+            _elem713.read(iprot)",
                "+            self.executors.append(_elem713)",
                "           iprot.readListEnd()",
                "@@ -10468,4 +10514,4 @@ class LocalAssignment:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.executors))",
                "-      for iter696 in self.executors:",
                "-        iter696.write(oprot)",
                "+      for iter714 in self.executors:",
                "+        iter714.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -10608,7 +10654,7 @@ class LSApprovedWorkers:",
                "           self.approved_workers = {}",
                "-          (_ktype698, _vtype699, _size697 ) = iprot.readMapBegin()",
                "-          for _i701 in xrange(_size697):",
                "-            _key702 = iprot.readString().decode('utf-8')",
                "-            _val703 = iprot.readI32()",
                "-            self.approved_workers[_key702] = _val703",
                "+          (_ktype716, _vtype717, _size715 ) = iprot.readMapBegin()",
                "+          for _i719 in xrange(_size715):",
                "+            _key720 = iprot.readString().decode('utf-8')",
                "+            _val721 = iprot.readI32()",
                "+            self.approved_workers[_key720] = _val721",
                "           iprot.readMapEnd()",
                "@@ -10629,5 +10675,5 @@ class LSApprovedWorkers:",
                "       oprot.writeMapBegin(TType.STRING, TType.I32, len(self.approved_workers))",
                "-      for kiter704,viter705 in self.approved_workers.items():",
                "-        oprot.writeString(kiter704.encode('utf-8'))",
                "-        oprot.writeI32(viter705)",
                "+      for kiter722,viter723 in self.approved_workers.items():",
                "+        oprot.writeString(kiter722.encode('utf-8'))",
                "+        oprot.writeI32(viter723)",
                "       oprot.writeMapEnd()",
                "@@ -10685,8 +10731,8 @@ class LSSupervisorAssignments:",
                "           self.assignments = {}",
                "-          (_ktype707, _vtype708, _size706 ) = iprot.readMapBegin()",
                "-          for _i710 in xrange(_size706):",
                "-            _key711 = iprot.readI32()",
                "-            _val712 = LocalAssignment()",
                "-            _val712.read(iprot)",
                "-            self.assignments[_key711] = _val712",
                "+          (_ktype725, _vtype726, _size724 ) = iprot.readMapBegin()",
                "+          for _i728 in xrange(_size724):",
                "+            _key729 = iprot.readI32()",
                "+            _val730 = LocalAssignment()",
                "+            _val730.read(iprot)",
                "+            self.assignments[_key729] = _val730",
                "           iprot.readMapEnd()",
                "@@ -10707,5 +10753,5 @@ class LSSupervisorAssignments:",
                "       oprot.writeMapBegin(TType.I32, TType.STRUCT, len(self.assignments))",
                "-      for kiter713,viter714 in self.assignments.items():",
                "-        oprot.writeI32(kiter713)",
                "-        viter714.write(oprot)",
                "+      for kiter731,viter732 in self.assignments.items():",
                "+        oprot.writeI32(kiter731)",
                "+        viter732.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -10782,7 +10828,7 @@ class LSWorkerHeartbeat:",
                "           self.executors = []",
                "-          (_etype718, _size715) = iprot.readListBegin()",
                "-          for _i719 in xrange(_size715):",
                "-            _elem720 = ExecutorInfo()",
                "-            _elem720.read(iprot)",
                "-            self.executors.append(_elem720)",
                "+          (_etype736, _size733) = iprot.readListBegin()",
                "+          for _i737 in xrange(_size733):",
                "+            _elem738 = ExecutorInfo()",
                "+            _elem738.read(iprot)",
                "+            self.executors.append(_elem738)",
                "           iprot.readListEnd()",
                "@@ -10816,4 +10862,4 @@ class LSWorkerHeartbeat:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.executors))",
                "-      for iter721 in self.executors:",
                "-        iter721.write(oprot)",
                "+      for iter739 in self.executors:",
                "+        iter739.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -10903,6 +10949,6 @@ class LSTopoHistory:",
                "           self.users = []",
                "-          (_etype725, _size722) = iprot.readListBegin()",
                "-          for _i726 in xrange(_size722):",
                "-            _elem727 = iprot.readString().decode('utf-8')",
                "-            self.users.append(_elem727)",
                "+          (_etype743, _size740) = iprot.readListBegin()",
                "+          for _i744 in xrange(_size740):",
                "+            _elem745 = iprot.readString().decode('utf-8')",
                "+            self.users.append(_elem745)",
                "           iprot.readListEnd()",
                "@@ -10913,6 +10959,6 @@ class LSTopoHistory:",
                "           self.groups = []",
                "-          (_etype731, _size728) = iprot.readListBegin()",
                "-          for _i732 in xrange(_size728):",
                "-            _elem733 = iprot.readString().decode('utf-8')",
                "-            self.groups.append(_elem733)",
                "+          (_etype749, _size746) = iprot.readListBegin()",
                "+          for _i750 in xrange(_size746):",
                "+            _elem751 = iprot.readString().decode('utf-8')",
                "+            self.groups.append(_elem751)",
                "           iprot.readListEnd()",
                "@@ -10941,4 +10987,4 @@ class LSTopoHistory:",
                "       oprot.writeListBegin(TType.STRING, len(self.users))",
                "-      for iter734 in self.users:",
                "-        oprot.writeString(iter734.encode('utf-8'))",
                "+      for iter752 in self.users:",
                "+        oprot.writeString(iter752.encode('utf-8'))",
                "       oprot.writeListEnd()",
                "@@ -10948,4 +10994,4 @@ class LSTopoHistory:",
                "       oprot.writeListBegin(TType.STRING, len(self.groups))",
                "-      for iter735 in self.groups:",
                "-        oprot.writeString(iter735.encode('utf-8'))",
                "+      for iter753 in self.groups:",
                "+        oprot.writeString(iter753.encode('utf-8'))",
                "       oprot.writeListEnd()",
                "@@ -11012,7 +11058,7 @@ class LSTopoHistoryList:",
                "           self.topo_history = []",
                "-          (_etype739, _size736) = iprot.readListBegin()",
                "-          for _i740 in xrange(_size736):",
                "-            _elem741 = LSTopoHistory()",
                "-            _elem741.read(iprot)",
                "-            self.topo_history.append(_elem741)",
                "+          (_etype757, _size754) = iprot.readListBegin()",
                "+          for _i758 in xrange(_size754):",
                "+            _elem759 = LSTopoHistory()",
                "+            _elem759.read(iprot)",
                "+            self.topo_history.append(_elem759)",
                "           iprot.readListEnd()",
                "@@ -11033,4 +11079,4 @@ class LSTopoHistoryList:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.topo_history))",
                "-      for iter742 in self.topo_history:",
                "-        iter742.write(oprot)",
                "+      for iter760 in self.topo_history:",
                "+        iter760.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -11369,8 +11415,8 @@ class LogConfig:",
                "           self.named_logger_level = {}",
                "-          (_ktype744, _vtype745, _size743 ) = iprot.readMapBegin()",
                "-          for _i747 in xrange(_size743):",
                "-            _key748 = iprot.readString().decode('utf-8')",
                "-            _val749 = LogLevel()",
                "-            _val749.read(iprot)",
                "-            self.named_logger_level[_key748] = _val749",
                "+          (_ktype762, _vtype763, _size761 ) = iprot.readMapBegin()",
                "+          for _i765 in xrange(_size761):",
                "+            _key766 = iprot.readString().decode('utf-8')",
                "+            _val767 = LogLevel()",
                "+            _val767.read(iprot)",
                "+            self.named_logger_level[_key766] = _val767",
                "           iprot.readMapEnd()",
                "@@ -11391,5 +11437,5 @@ class LogConfig:",
                "       oprot.writeMapBegin(TType.STRING, TType.STRUCT, len(self.named_logger_level))",
                "-      for kiter750,viter751 in self.named_logger_level.items():",
                "-        oprot.writeString(kiter750.encode('utf-8'))",
                "-        viter751.write(oprot)",
                "+      for kiter768,viter769 in self.named_logger_level.items():",
                "+        oprot.writeString(kiter768.encode('utf-8'))",
                "+        viter769.write(oprot)",
                "       oprot.writeMapEnd()",
                "@@ -11445,6 +11491,6 @@ class TopologyHistoryInfo:",
                "           self.topo_ids = []",
                "-          (_etype755, _size752) = iprot.readListBegin()",
                "-          for _i756 in xrange(_size752):",
                "-            _elem757 = iprot.readString().decode('utf-8')",
                "-            self.topo_ids.append(_elem757)",
                "+          (_etype773, _size770) = iprot.readListBegin()",
                "+          for _i774 in xrange(_size770):",
                "+            _elem775 = iprot.readString().decode('utf-8')",
                "+            self.topo_ids.append(_elem775)",
                "           iprot.readListEnd()",
                "@@ -11465,4 +11511,4 @@ class TopologyHistoryInfo:",
                "       oprot.writeListBegin(TType.STRING, len(self.topo_ids))",
                "-      for iter758 in self.topo_ids:",
                "-        oprot.writeString(iter758.encode('utf-8'))",
                "+      for iter776 in self.topo_ids:",
                "+        oprot.writeString(iter776.encode('utf-8'))",
                "       oprot.writeListEnd()",
                "@@ -11780,2 +11826,331 @@ class OwnerResourceSummary:",
                "+class WorkerMetricPoint:",
                "+  \"\"\"",
                "+  Attributes:",
                "+   - metricName",
                "+   - timestamp",
                "+   - metricValue",
                "+   - componentId",
                "+   - executorId",
                "+   - streamId",
                "+  \"\"\"",
                "+",
                "+  thrift_spec = (",
                "+    None, # 0",
                "+    (1, TType.STRING, 'metricName', None, None, ), # 1",
                "+    (2, TType.I64, 'timestamp', None, None, ), # 2",
                "+    (3, TType.DOUBLE, 'metricValue', None, None, ), # 3",
                "+    (4, TType.STRING, 'componentId', None, None, ), # 4",
                "+    (5, TType.STRING, 'executorId', None, None, ), # 5",
                "+    (6, TType.STRING, 'streamId', None, None, ), # 6",
                "+  )",
                "+",
                "+  def __init__(self, metricName=None, timestamp=None, metricValue=None, componentId=None, executorId=None, streamId=None,):",
                "+    self.metricName = metricName",
                "+    self.timestamp = timestamp",
                "+    self.metricValue = metricValue",
                "+    self.componentId = componentId",
                "+    self.executorId = executorId",
                "+    self.streamId = streamId",
                "+",
                "+  def read(self, iprot):",
                "+    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:",
                "+      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))",
                "+      return",
                "+    iprot.readStructBegin()",
                "+    while True:",
                "+      (fname, ftype, fid) = iprot.readFieldBegin()",
                "+      if ftype == TType.STOP:",
                "+        break",
                "+      if fid == 1:",
                "+        if ftype == TType.STRING:",
                "+          self.metricName = iprot.readString().decode('utf-8')",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      elif fid == 2:",
                "+        if ftype == TType.I64:",
                "+          self.timestamp = iprot.readI64()",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      elif fid == 3:",
                "+        if ftype == TType.DOUBLE:",
                "+          self.metricValue = iprot.readDouble()",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      elif fid == 4:",
                "+        if ftype == TType.STRING:",
                "+          self.componentId = iprot.readString().decode('utf-8')",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      elif fid == 5:",
                "+        if ftype == TType.STRING:",
                "+          self.executorId = iprot.readString().decode('utf-8')",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      elif fid == 6:",
                "+        if ftype == TType.STRING:",
                "+          self.streamId = iprot.readString().decode('utf-8')",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      else:",
                "+        iprot.skip(ftype)",
                "+      iprot.readFieldEnd()",
                "+    iprot.readStructEnd()",
                "+",
                "+  def write(self, oprot):",
                "+    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:",
                "+      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))",
                "+      return",
                "+    oprot.writeStructBegin('WorkerMetricPoint')",
                "+    if self.metricName is not None:",
                "+      oprot.writeFieldBegin('metricName', TType.STRING, 1)",
                "+      oprot.writeString(self.metricName.encode('utf-8'))",
                "+      oprot.writeFieldEnd()",
                "+    if self.timestamp is not None:",
                "+      oprot.writeFieldBegin('timestamp', TType.I64, 2)",
                "+      oprot.writeI64(self.timestamp)",
                "+      oprot.writeFieldEnd()",
                "+    if self.metricValue is not None:",
                "+      oprot.writeFieldBegin('metricValue', TType.DOUBLE, 3)",
                "+      oprot.writeDouble(self.metricValue)",
                "+      oprot.writeFieldEnd()",
                "+    if self.componentId is not None:",
                "+      oprot.writeFieldBegin('componentId', TType.STRING, 4)",
                "+      oprot.writeString(self.componentId.encode('utf-8'))",
                "+      oprot.writeFieldEnd()",
                "+    if self.executorId is not None:",
                "+      oprot.writeFieldBegin('executorId', TType.STRING, 5)",
                "+      oprot.writeString(self.executorId.encode('utf-8'))",
                "+      oprot.writeFieldEnd()",
                "+    if self.streamId is not None:",
                "+      oprot.writeFieldBegin('streamId', TType.STRING, 6)",
                "+      oprot.writeString(self.streamId.encode('utf-8'))",
                "+      oprot.writeFieldEnd()",
                "+    oprot.writeFieldStop()",
                "+    oprot.writeStructEnd()",
                "+",
                "+  def validate(self):",
                "+    if self.metricName is None:",
                "+      raise TProtocol.TProtocolException(message='Required field metricName is unset!')",
                "+    if self.timestamp is None:",
                "+      raise TProtocol.TProtocolException(message='Required field timestamp is unset!')",
                "+    if self.metricValue is None:",
                "+      raise TProtocol.TProtocolException(message='Required field metricValue is unset!')",
                "+    if self.componentId is None:",
                "+      raise TProtocol.TProtocolException(message='Required field componentId is unset!')",
                "+    if self.executorId is None:",
                "+      raise TProtocol.TProtocolException(message='Required field executorId is unset!')",
                "+    if self.streamId is None:",
                "+      raise TProtocol.TProtocolException(message='Required field streamId is unset!')",
                "+    return",
                "+",
                "+",
                "+  def __hash__(self):",
                "+    value = 17",
                "+    value = (value * 31) ^ hash(self.metricName)",
                "+    value = (value * 31) ^ hash(self.timestamp)",
                "+    value = (value * 31) ^ hash(self.metricValue)",
                "+    value = (value * 31) ^ hash(self.componentId)",
                "+    value = (value * 31) ^ hash(self.executorId)",
                "+    value = (value * 31) ^ hash(self.streamId)",
                "+    return value",
                "+",
                "+  def __repr__(self):",
                "+    L = ['%s=%r' % (key, value)",
                "+      for key, value in self.__dict__.iteritems()]",
                "+    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))",
                "+",
                "+  def __eq__(self, other):",
                "+    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__",
                "+",
                "+  def __ne__(self, other):",
                "+    return not (self == other)",
                "+",
                "+class WorkerMetricList:",
                "+  \"\"\"",
                "+  Attributes:",
                "+   - metrics",
                "+  \"\"\"",
                "+",
                "+  thrift_spec = (",
                "+    None, # 0",
                "+    (1, TType.LIST, 'metrics', (TType.STRUCT,(WorkerMetricPoint, WorkerMetricPoint.thrift_spec)), None, ), # 1",
                "+  )",
                "+",
                "+  def __init__(self, metrics=None,):",
                "+    self.metrics = metrics",
                "+",
                "+  def read(self, iprot):",
                "+    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:",
                "+      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))",
                "+      return",
                "+    iprot.readStructBegin()",
                "+    while True:",
                "+      (fname, ftype, fid) = iprot.readFieldBegin()",
                "+      if ftype == TType.STOP:",
                "+        break",
                "+      if fid == 1:",
                "+        if ftype == TType.LIST:",
                "+          self.metrics = []",
                "+          (_etype780, _size777) = iprot.readListBegin()",
                "+          for _i781 in xrange(_size777):",
                "+            _elem782 = WorkerMetricPoint()",
                "+            _elem782.read(iprot)",
                "+            self.metrics.append(_elem782)",
                "+          iprot.readListEnd()",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      else:",
                "+        iprot.skip(ftype)",
                "+      iprot.readFieldEnd()",
                "+    iprot.readStructEnd()",
                "+",
                "+  def write(self, oprot):",
                "+    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:",
                "+      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))",
                "+      return",
                "+    oprot.writeStructBegin('WorkerMetricList')",
                "+    if self.metrics is not None:",
                "+      oprot.writeFieldBegin('metrics', TType.LIST, 1)",
                "+      oprot.writeListBegin(TType.STRUCT, len(self.metrics))",
                "+      for iter783 in self.metrics:",
                "+        iter783.write(oprot)",
                "+      oprot.writeListEnd()",
                "+      oprot.writeFieldEnd()",
                "+    oprot.writeFieldStop()",
                "+    oprot.writeStructEnd()",
                "+",
                "+  def validate(self):",
                "+    return",
                "+",
                "+",
                "+  def __hash__(self):",
                "+    value = 17",
                "+    value = (value * 31) ^ hash(self.metrics)",
                "+    return value",
                "+",
                "+  def __repr__(self):",
                "+    L = ['%s=%r' % (key, value)",
                "+      for key, value in self.__dict__.iteritems()]",
                "+    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))",
                "+",
                "+  def __eq__(self, other):",
                "+    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__",
                "+",
                "+  def __ne__(self, other):",
                "+    return not (self == other)",
                "+",
                "+class WorkerMetrics:",
                "+  \"\"\"",
                "+  Attributes:",
                "+   - topologyId",
                "+   - port",
                "+   - hostname",
                "+   - metricList",
                "+  \"\"\"",
                "+",
                "+  thrift_spec = (",
                "+    None, # 0",
                "+    (1, TType.STRING, 'topologyId', None, None, ), # 1",
                "+    (2, TType.I32, 'port', None, None, ), # 2",
                "+    (3, TType.STRING, 'hostname', None, None, ), # 3",
                "+    (4, TType.STRUCT, 'metricList', (WorkerMetricList, WorkerMetricList.thrift_spec), None, ), # 4",
                "+  )",
                "+",
                "+  def __init__(self, topologyId=None, port=None, hostname=None, metricList=None,):",
                "+    self.topologyId = topologyId",
                "+    self.port = port",
                "+    self.hostname = hostname",
                "+    self.metricList = metricList",
                "+",
                "+  def read(self, iprot):",
                "+    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:",
                "+      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))",
                "+      return",
                "+    iprot.readStructBegin()",
                "+    while True:",
                "+      (fname, ftype, fid) = iprot.readFieldBegin()",
                "+      if ftype == TType.STOP:",
                "+        break",
                "+      if fid == 1:",
                "+        if ftype == TType.STRING:",
                "+          self.topologyId = iprot.readString().decode('utf-8')",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      elif fid == 2:",
                "+        if ftype == TType.I32:",
                "+          self.port = iprot.readI32()",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      elif fid == 3:",
                "+        if ftype == TType.STRING:",
                "+          self.hostname = iprot.readString().decode('utf-8')",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      elif fid == 4:",
                "+        if ftype == TType.STRUCT:",
                "+          self.metricList = WorkerMetricList()",
                "+          self.metricList.read(iprot)",
                "+        else:",
                "+          iprot.skip(ftype)",
                "+      else:",
                "+        iprot.skip(ftype)",
                "+      iprot.readFieldEnd()",
                "+    iprot.readStructEnd()",
                "+",
                "+  def write(self, oprot):",
                "+    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:",
                "+      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))",
                "+      return",
                "+    oprot.writeStructBegin('WorkerMetrics')",
                "+    if self.topologyId is not None:",
                "+      oprot.writeFieldBegin('topologyId', TType.STRING, 1)",
                "+      oprot.writeString(self.topologyId.encode('utf-8'))",
                "+      oprot.writeFieldEnd()",
                "+    if self.port is not None:",
                "+      oprot.writeFieldBegin('port', TType.I32, 2)",
                "+      oprot.writeI32(self.port)",
                "+      oprot.writeFieldEnd()",
                "+    if self.hostname is not None:",
                "+      oprot.writeFieldBegin('hostname', TType.STRING, 3)",
                "+      oprot.writeString(self.hostname.encode('utf-8'))",
                "+      oprot.writeFieldEnd()",
                "+    if self.metricList is not None:",
                "+      oprot.writeFieldBegin('metricList', TType.STRUCT, 4)",
                "+      self.metricList.write(oprot)",
                "+      oprot.writeFieldEnd()",
                "+    oprot.writeFieldStop()",
                "+    oprot.writeStructEnd()",
                "+",
                "+  def validate(self):",
                "+    if self.topologyId is None:",
                "+      raise TProtocol.TProtocolException(message='Required field topologyId is unset!')",
                "+    if self.port is None:",
                "+      raise TProtocol.TProtocolException(message='Required field port is unset!')",
                "+    if self.hostname is None:",
                "+      raise TProtocol.TProtocolException(message='Required field hostname is unset!')",
                "+    if self.metricList is None:",
                "+      raise TProtocol.TProtocolException(message='Required field metricList is unset!')",
                "+    return",
                "+",
                "+",
                "+  def __hash__(self):",
                "+    value = 17",
                "+    value = (value * 31) ^ hash(self.topologyId)",
                "+    value = (value * 31) ^ hash(self.port)",
                "+    value = (value * 31) ^ hash(self.hostname)",
                "+    value = (value * 31) ^ hash(self.metricList)",
                "+    return value",
                "+",
                "+  def __repr__(self):",
                "+    L = ['%s=%r' % (key, value)",
                "+      for key, value in self.__dict__.iteritems()]",
                "+    return '%s(%s)' % (self.__class__.__name__, ', '.join(L))",
                "+",
                "+  def __eq__(self, other):",
                "+    return isinstance(other, self.__class__) and self.__dict__ == other.__dict__",
                "+",
                "+  def __ne__(self, other):",
                "+    return not (self == other)",
                "+",
                " class DRPCRequest:",
                "@@ -12051,7 +12426,7 @@ class HBRecords:",
                "           self.pulses = []",
                "-          (_etype762, _size759) = iprot.readListBegin()",
                "-          for _i763 in xrange(_size759):",
                "-            _elem764 = HBPulse()",
                "-            _elem764.read(iprot)",
                "-            self.pulses.append(_elem764)",
                "+          (_etype787, _size784) = iprot.readListBegin()",
                "+          for _i788 in xrange(_size784):",
                "+            _elem789 = HBPulse()",
                "+            _elem789.read(iprot)",
                "+            self.pulses.append(_elem789)",
                "           iprot.readListEnd()",
                "@@ -12072,4 +12447,4 @@ class HBRecords:",
                "       oprot.writeListBegin(TType.STRUCT, len(self.pulses))",
                "-      for iter765 in self.pulses:",
                "-        iter765.write(oprot)",
                "+      for iter790 in self.pulses:",
                "+        iter790.write(oprot)",
                "       oprot.writeListEnd()",
                "@@ -12125,6 +12500,6 @@ class HBNodes:",
                "           self.pulseIds = []",
                "-          (_etype769, _size766) = iprot.readListBegin()",
                "-          for _i770 in xrange(_size766):",
                "-            _elem771 = iprot.readString().decode('utf-8')",
                "-            self.pulseIds.append(_elem771)",
                "+          (_etype794, _size791) = iprot.readListBegin()",
                "+          for _i795 in xrange(_size791):",
                "+            _elem796 = iprot.readString().decode('utf-8')",
                "+            self.pulseIds.append(_elem796)",
                "           iprot.readListEnd()",
                "@@ -12145,4 +12520,4 @@ class HBNodes:",
                "       oprot.writeListBegin(TType.STRING, len(self.pulseIds))",
                "-      for iter772 in self.pulseIds:",
                "-        oprot.writeString(iter772.encode('utf-8'))",
                "+      for iter797 in self.pulseIds:",
                "+        oprot.writeString(iter797.encode('utf-8'))",
                "       oprot.writeListEnd()",
                "diff --git a/storm-client/src/storm.thrift b/storm-client/src/storm.thrift",
                "index aff7507c1..c5140f3c1 100644",
                "--- a/storm-client/src/storm.thrift",
                "+++ b/storm-client/src/storm.thrift",
                "@@ -674,2 +674,22 @@ struct OwnerResourceSummary {",
                "+struct WorkerMetricPoint {",
                "+  1: required string metricName;",
                "+  2: required i64 timestamp;",
                "+  3: required double metricValue;",
                "+  4: required string componentId;",
                "+  5: required string executorId;",
                "+  6: required string streamId;",
                "+}",
                "+",
                "+struct WorkerMetricList {",
                "+  1: list<WorkerMetricPoint> metrics;",
                "+}",
                "+",
                "+struct WorkerMetrics {",
                "+  1: required string topologyId;",
                "+  2: required i32 port;",
                "+  3: required string hostname;",
                "+  4: required WorkerMetricList metricList;",
                "+}",
                "+",
                " service Nimbus {",
                "@@ -750,2 +770,3 @@ service Nimbus {",
                "   list<OwnerResourceSummary> getOwnerResourceSummaries (1: string owner) throws (1: AuthorizationException aze);",
                "+  void processWorkerMetrics(1: WorkerMetrics metrics);",
                " }",
                "@@ -838 +859,3 @@ exception HBExecutionException {",
                " }",
                "+",
                "+",
                "diff --git a/storm-server/pom.xml b/storm-server/pom.xml",
                "index 7c301a4df..cfe2d7419 100644",
                "--- a/storm-server/pom.xml",
                "+++ b/storm-server/pom.xml",
                "@@ -66,2 +66,6 @@",
                "         </dependency>",
                "+        <dependency>",
                "+            <groupId>org.rocksdb</groupId>",
                "+            <artifactId>rocksdbjni</artifactId>",
                "+        </dependency>",
                "diff --git a/storm-server/src/main/java/org/apache/storm/DaemonConfig.java b/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "index f80656222..a6881b742 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "@@ -21,2 +21,3 @@ package org.apache.storm;",
                " import static org.apache.storm.validation.ConfigValidationAnnotations.isInteger;",
                "+import static org.apache.storm.validation.ConfigValidationAnnotations.isPositiveNumber;",
                " import static org.apache.storm.validation.ConfigValidationAnnotations.isString;",
                "@@ -24,3 +25,2 @@ import static org.apache.storm.validation.ConfigValidationAnnotations.isStringLi",
                " import static org.apache.storm.validation.ConfigValidationAnnotations.isStringOrStringList;",
                "-import static org.apache.storm.validation.ConfigValidationAnnotations.isPositiveNumber;",
                " import static org.apache.storm.validation.ConfigValidationAnnotations.NotNull;",
                "@@ -35,2 +35,3 @@ import static org.apache.storm.validation.ConfigValidationAnnotations.isMapEntry",
                " import org.apache.storm.container.ResourceIsolationInterface;",
                "+import org.apache.storm.metricstore.MetricStore;",
                " import org.apache.storm.nimbus.ITopologyActionNotifierPlugin;",
                "@@ -57,3 +58,3 @@ public class DaemonConfig implements Validated {",
                "     /**",
                "-     * We check with this interval that whether the Netty channel is writable and try to write pending messages",
                "+     * We check with this interval that whether the Netty channel is writable and try to write pending messages.",
                "      */",
                "@@ -166,3 +167,3 @@ public class DaemonConfig implements Validated {",
                "      * The time to allow any given healthcheck script to run before it",
                "-     * is marked failed due to timeout",
                "+     * is marked failed due to timeout.",
                "      */",
                "@@ -811,3 +812,3 @@ public class DaemonConfig implements Validated {",
                "     @isBoolean",
                "-    public static final String STORM_TOPOLOGY_CLASSPATH_BEGINNING_ENABLED=\"storm.topology.classpath.beginning.enabled\";",
                "+    public static final String STORM_TOPOLOGY_CLASSPATH_BEGINNING_ENABLED = \"storm.topology.classpath.beginning.enabled\";",
                "@@ -907,3 +908,3 @@ public class DaemonConfig implements Validated {",
                "     /**",
                "-     * The plugin to be used for resource isolation",
                "+     * The plugin to be used for resource isolation.",
                "      */",
                "@@ -913,3 +914,3 @@ public class DaemonConfig implements Validated {",
                "     /**",
                "-     * CGroup Setting below",
                "+     * CGroup Setting below.",
                "      */",
                "@@ -917,3 +918,3 @@ public class DaemonConfig implements Validated {",
                "     /**",
                "-     * resources to to be controlled by cgroups",
                "+     * resources to to be controlled by cgroups.",
                "      */",
                "@@ -1032,2 +1033,44 @@ public class DaemonConfig implements Validated {",
                "+    /**",
                "+     * Class implementing MetricStore.",
                "+     */",
                "+    @NotNull",
                "+    @isImplementationOfClass(implementsClass = MetricStore.class)",
                "+    public static final String STORM_METRIC_STORE_CLASS = \"storm.metricstore.class\";",
                "+",
                "+    /**",
                "+     * RocksDB file location. This setting is specific to the org.apache.storm.metricstore.rocksdb.RocksDbStore",
                "+     * implementation for the storm.metricstore.class.",
                "+     */",
                "+    @isString",
                "+    public static final String STORM_ROCKSDB_LOCATION = \"storm.metricstore.rocksdb.location\";",
                "+",
                "+    /**",
                "+     * RocksDB create if missing flag. This setting is specific to the org.apache.storm.metricstore.rocksdb.RocksDbStore",
                "+     * implementation for the storm.metricstore.class.",
                "+     */",
                "+    @isBoolean",
                "+    public static final String STORM_ROCKSDB_CREATE_IF_MISSING = \"storm.metricstore.rocksdb.create_if_missing\";",
                "+",
                "+    /**",
                "+     * RocksDB metadata cache capacity. This setting is specific to the org.apache.storm.metricstore.rocksdb.RocksDbStore",
                "+     * implementation for the storm.metricstore.class.",
                "+     */",
                "+    @isInteger",
                "+    public static final String STORM_ROCKSDB_METADATA_STRING_CACHE_CAPACITY = \"storm.metricstore.rocksdb.metadata_string_cache_capacity\";",
                "+",
                "+    /**",
                "+     * RocksDB setting for length of metric retention. This setting is specific to the org.apache.storm.metricstore.rocksdb.RocksDbStore",
                "+     * implementation for the storm.metricstore.class.",
                "+     */",
                "+    @isInteger",
                "+    public static final String STORM_ROCKSDB_METRIC_RETENTION_HOURS = \"storm.metricstore.rocksdb.retention_hours\";",
                "+",
                "+    /**",
                "+     * RocksDB setting for period of metric deletion thread. This setting is specific to the",
                "+     * org.apache.storm.metricstore.rocksdb.RocksDbStore implementation for the storm.metricstore.class.",
                "+     */",
                "+    @isInteger",
                "+    public static final String STORM_ROCKSDB_METRIC_DELETION_PERIOD_HOURS = \"storm.metricstore.rocksdb.deletion_period_hours\";",
                "+",
                "     // VALIDATION ONLY CONFIGS",
                "@@ -1053,3 +1096,3 @@ public class DaemonConfig implements Validated {",
                "     /**",
                "-     * Get the cgroup resources from the conf",
                "+     * Get the cgroup resources from the conf.",
                "      *",
                "diff --git a/storm-server/src/main/java/org/apache/storm/LocalCluster.java b/storm-server/src/main/java/org/apache/storm/LocalCluster.java",
                "index de3053e5b..502f4541c 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/LocalCluster.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/LocalCluster.java",
                "@@ -79,2 +79,3 @@ import org.apache.storm.generated.TopologyInfo;",
                " import org.apache.storm.generated.TopologyPageInfo;",
                "+import org.apache.storm.generated.WorkerMetrics;",
                " import org.apache.storm.messaging.IContext;",
                "@@ -1125,3 +1126,8 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "     }",
                "-    ",
                "+",
                "+    @Override",
                "+    public void processWorkerMetrics(WorkerMetrics metrics) throws org.apache.thrift.TException {",
                "+        getNimbus().processWorkerMetrics(metrics);",
                "+    }",
                "+",
                "     public static void main(final String [] args) throws Exception {",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index afb1c2815..37141e821 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -126,2 +126,4 @@ import org.apache.storm.generated.TopologyStatus;",
                " import org.apache.storm.generated.TopologySummary;",
                "+import org.apache.storm.generated.WorkerMetricPoint;",
                "+import org.apache.storm.generated.WorkerMetrics;",
                " import org.apache.storm.generated.WorkerResources;",
                "@@ -134,2 +136,6 @@ import org.apache.storm.metric.api.IClusterMetricsConsumer;",
                " import org.apache.storm.metric.api.IClusterMetricsConsumer.ClusterInfo;",
                "+import org.apache.storm.metricstore.AggLevel;",
                "+import org.apache.storm.metricstore.Metric;",
                "+import org.apache.storm.metricstore.MetricStore;",
                "+import org.apache.storm.metricstore.MetricStoreConfig;",
                " import org.apache.storm.nimbus.DefaultTopologyValidator;",
                "@@ -232,2 +238,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "     private static final Meter shutdownCalls = StormMetricsRegistry.registerMeter(\"nimbus:num-shutdown-calls\");",
                "+    private static final Meter processWorkerMetricsCalls = StormMetricsRegistry.registerMeter(\"nimbus:process-worker-metric-calls\");",
                "     // END Metrics",
                "@@ -338,3 +345,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "-    private static final Map<TopologyStatus, Map<TopologyActions, TopologyStateTransition>> TOPO_STATE_TRANSITIONS = ",
                "+    private static final Map<TopologyStatus, Map<TopologyActions, TopologyStateTransition>> TOPO_STATE_TRANSITIONS =",
                "             new ImmutableMap.Builder<TopologyStatus, Map<TopologyActions, TopologyStateTransition>>()",
                "@@ -599,3 +606,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "      * convert {topology-id -> SchedulerAssignment} to",
                "-     *         {topology-id -> {executor [node port]}}",
                "+     *         {topology-id -> {executor [node port]}}.",
                "      * @return",
                "@@ -881,3 +888,4 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "-    private static StormTopology tryReadTopology(String topoId, TopoCache tc) throws NotAliveException, AuthorizationException, IOException {",
                "+    private static StormTopology tryReadTopology(String topoId, TopoCache tc)",
                "+            throws NotAliveException, AuthorizationException, IOException {",
                "         try {",
                "@@ -1003,6 +1011,6 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "         final ThriftServer server = new ThriftServer(conf, new Processor<>(nimbus), ThriftConnectionType.NIMBUS);",
                "-        Utils.addShutdownHookWithForceKillIn1Sec(() -> {",
                "+        Utils.addShutdownHookWithDelayedForceKill(() -> {",
                "             nimbus.shutdown();",
                "             server.stop();",
                "-        });",
                "+        }, 10);",
                "         LOG.info(\"Starting nimbus server for storm version '{}'\", STORM_VERSION);",
                "@@ -1024,2 +1032,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "     private final Map<String, Object> conf;",
                "+    private MetricStore metricsStore;",
                "     private final NavigableMap<SimpleVersion, List<String>> supervisorClasspaths;",
                "@@ -1100,2 +1109,11 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "         this.conf = conf;",
                "+",
                "+        this.metricsStore = null;",
                "+        try {",
                "+            this.metricsStore = MetricStoreConfig.configure(conf);",
                "+        } catch (Exception e) {",
                "+            // the metrics store is not critical to the operation of the cluster, allow Nimbus to come up",
                "+            LOG.error(\"Failed to initialize metric store\", e);",
                "+        }",
                "+",
                "         if (hostPortInfo == null) {",
                "@@ -2732,3 +2750,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "             // cleanup thread killing topology in b/w assignment and starting the topology",
                "-            synchronized(submitLock) {",
                "+            synchronized (submitLock) {",
                "                 assertTopoActive(topoName, false);",
                "@@ -3792,3 +3810,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "                     boolean isAllowed = userTopologies.contains(topoId);",
                "-                    for (WorkerSummary workerSummary: StatsUtil.aggWorkerStats(topoId, topoName, taskToComp, beats, ",
                "+                    for (WorkerSummary workerSummary: StatsUtil.aggWorkerStats(topoId, topoName, taskToComp, beats,",
                "                             exec2NodePort, nodeToHost, workerResources, includeSys, isAllowed, sid)) {",
                "@@ -3810,3 +3828,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "     public ComponentPageInfo getComponentPageInfo(String topoId, String componentId, String window,",
                "-            boolean includeSys) throws NotAliveException, AuthorizationException, TException {",
                "+                                                  boolean includeSys) throws NotAliveException, AuthorizationException, TException {",
                "         try {",
                "@@ -4006,4 +4024,4 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "         for (NimbusSummary nimbusSummary: nimbuses) {",
                "-            if (leader.getHost().equals(nimbusSummary.get_host()) &&",
                "-                    leader.getPort() == nimbusSummary.get_port()) {",
                "+            if (leader.getHost().equals(nimbusSummary.get_host())",
                "+                    && leader.getPort() == nimbusSummary.get_port()) {",
                "                 nimbusSummary.set_uptime_secs(Time.deltaSecs(nimbusSummary.get_uptime_secs()));",
                "@@ -4044,3 +4062,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "-            if (owner == null){",
                "+            if (owner == null) {",
                "                 // add all the owners to the map",
                "@@ -4176,2 +4194,5 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "                 zkClient.close();",
                "+            }            ",
                "+            if (metricsStore != null) {",
                "+                metricsStore.close();",
                "             }",
                "@@ -4189,2 +4210,25 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "     }",
                "+",
                "+    @Override",
                "+    public void processWorkerMetrics(WorkerMetrics metrics) throws org.apache.thrift.TException {",
                "+        processWorkerMetricsCalls.mark();",
                "+",
                "+        checkAuthorization(null, null, \"processWorkerMetrics\");",
                "+",
                "+        if (this.metricsStore == null) {",
                "+            return;",
                "+        }",
                "+",
                "+        for (WorkerMetricPoint m : metrics.get_metricList().get_metrics()) {",
                "+            try {",
                "+                Metric metric = new Metric(m.get_metricName(), m.get_timestamp(), metrics.get_topologyId(),",
                "+                        m.get_metricValue(), m.get_componentId(), m.get_executorId(), metrics.get_hostname(),",
                "+                        m.get_streamId(), metrics.get_port(), AggLevel.AGG_LEVEL_NONE);",
                "+                this.metricsStore.insert(metric);",
                "+            } catch (Exception e) {",
                "+                LOG.error(\"Failed to save metric\", e);",
                "+            }",
                "+        }",
                "+    }",
                "+",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java",
                "index ff60a4c4f..f45ce25ec 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.daemon.supervisor;",
                "@@ -41,7 +42,12 @@ import org.apache.storm.generated.LocalAssignment;",
                " import org.apache.storm.generated.ProfileRequest;",
                "+import org.apache.storm.generated.WorkerMetricPoint;",
                "+import org.apache.storm.generated.WorkerMetricList;",
                "+import org.apache.storm.generated.WorkerMetrics;",
                " import org.apache.storm.metric.StormMetricsRegistry;",
                " import org.apache.storm.utils.ConfigUtils;",
                "+import org.apache.storm.utils.LocalState;",
                "+import org.apache.storm.utils.NimbusClient;",
                " import org.apache.storm.utils.ServerConfigUtils;",
                " import org.apache.storm.utils.ServerUtils;",
                "-import org.apache.storm.utils.LocalState;",
                "+import org.apache.storm.utils.Utils;",
                " import org.slf4j.Logger;",
                "@@ -55,2 +61,7 @@ public abstract class Container implements Killable {",
                "     private static final Logger LOG = LoggerFactory.getLogger(Container.class);",
                "+    private static final String MEMORY_USED_METRIC = \"UsedMemory\";",
                "+    private static final String SYSTEM_COMPONENT_ID = \"System\";",
                "+    private static final String INVALID_EXECUTOR_ID = \"-1\";",
                "+    private static final String INVALID_STREAM_ID = \"None\";",
                "+",
                "     public static enum ContainerType {",
                "@@ -139,2 +150,3 @@ public abstract class Container implements Killable {",
                "     protected final boolean _symlinksDisabled;",
                "+    private long lastMetricProcessTime = 0L;",
                "@@ -211,3 +223,3 @@ public abstract class Container implements Killable {",
                "     /**",
                "-     * Kill a given process",
                "+     * Kill a given process.",
                "      * @param pid the id of the process to kill",
                "@@ -220,3 +232,3 @@ public abstract class Container implements Killable {",
                "     /**",
                "-     * Kill a given process",
                "+     * Kill a given process.",
                "      * @param pid the id of the process to kill",
                "@@ -261,3 +273,3 @@ public abstract class Container implements Killable {",
                "     /**",
                "-     * Is a process alive and running?",
                "+     * Is a process alive and running?.",
                "      * @param pid the PID of the running process",
                "@@ -383,3 +395,3 @@ public abstract class Container implements Killable {",
                "     /**",
                "-     * Write out the file used by the log viewer to allow/reject log access",
                "+     * Write out the file used by the log viewer to allow/reject log access.",
                "      * @param user the user this is going to run as",
                "@@ -431,3 +443,3 @@ public abstract class Container implements Killable {",
                "     /**",
                "-     * Create symlink from the containers directory/artifacts to the artifacts directory",
                "+     * Create symlink from the containers directory/artifacts to the artifacts directory.",
                "      * @throws IOException on any error",
                "@@ -695,2 +707,39 @@ public abstract class Container implements Killable {",
                "     }",
                "+",
                "+    /**",
                "+     * Send worker metrics to Nimbus.",
                "+     */",
                "+    void processMetrics() {",
                "+        try {",
                "+            if (_usedMemory.get(_port) != null) {",
                "+                // Make sure we don't process too frequently.",
                "+                long nextMetricProcessTime = this.lastMetricProcessTime + 60L * 1000L;",
                "+                long currentTimeMsec = System.currentTimeMillis();",
                "+                if (currentTimeMsec < nextMetricProcessTime) {",
                "+                    return;",
                "+                }",
                "+",
                "+                String hostname = Utils.hostname();",
                "+",
                "+                // create metric for memory",
                "+                long timestamp = System.currentTimeMillis();",
                "+                double value = _usedMemory.get(_port).memory;",
                "+                WorkerMetricPoint workerMetric = new WorkerMetricPoint(MEMORY_USED_METRIC, timestamp, value, SYSTEM_COMPONENT_ID,",
                "+                        INVALID_EXECUTOR_ID, INVALID_STREAM_ID);",
                "+",
                "+                WorkerMetricList metricList = new WorkerMetricList();",
                "+                metricList.add_to_metrics(workerMetric);",
                "+                WorkerMetrics metrics = new WorkerMetrics(_topologyId, _port, hostname, metricList);",
                "+",
                "+                try (NimbusClient client = NimbusClient.getConfiguredClient(_conf)) {",
                "+                    client.getClient().processWorkerMetrics(metrics);",
                "+                }",
                "+",
                "+                this.lastMetricProcessTime = currentTimeMsec;",
                "+            }",
                "+        } catch (Exception e) {",
                "+            LOG.error(\"Failed to process metrics\", e);",
                "+            this.lastMetricProcessTime = System.currentTimeMillis();",
                "+        }",
                "+    }",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "index cb41654a1..fe30c935f 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "@@ -938,2 +938,5 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "         }",
                "+",
                "+        dynamicState.container.processMetrics();",
                "+",
                "         Time.sleep(staticState.monitorFreqMs);",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metric/StormMetricsRegistry.java b/storm-server/src/main/java/org/apache/storm/metric/StormMetricsRegistry.java",
                "index c1e612145..28bba3e18 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/metric/StormMetricsRegistry.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/metric/StormMetricsRegistry.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.metric;",
                "@@ -55,2 +56,6 @@ public class StormMetricsRegistry {",
                "+    public static void registerProvidedGauge(final String name, Gauge gauge) {",
                "+        register(name, gauge);",
                "+    }",
                "+",
                "     public static Histogram registerHistogram(String name, Reservoir reservoir) {",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/AggLevel.java b/storm-server/src/main/java/org/apache/storm/metricstore/AggLevel.java",
                "new file mode 100644",
                "index 000000000..662a17cb0",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/AggLevel.java",
                "@@ -0,0 +1,40 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore;",
                "+",
                "+/**",
                "+ * Specifies the available timeframes for Metric aggregation.",
                "+ */",
                "+public enum AggLevel {",
                "+    AGG_LEVEL_NONE(0),",
                "+    AGG_LEVEL_1_MIN(1),",
                "+    AGG_LEVEL_10_MIN(10),",
                "+    AGG_LEVEL_60_MIN(60);",
                "+",
                "+    private final byte value;",
                "+",
                "+    AggLevel(int value) {",
                "+        this.value = (byte)value;",
                "+    }",
                "+",
                "+    public byte getValue() {",
                "+        return this.value;",
                "+    }",
                "+",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/FilterOptions.java b/storm-server/src/main/java/org/apache/storm/metricstore/FilterOptions.java",
                "new file mode 100644",
                "index 000000000..7cfbfbefd",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/FilterOptions.java",
                "@@ -0,0 +1,154 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore;",
                "+",
                "+import java.util.HashSet;",
                "+import java.util.Set;",
                "+",
                "+/**",
                "+ * FilterOptions provides a method to select various filtering options for doing a scan of the metrics database.",
                "+ */",
                "+public class FilterOptions {",
                "+    private Set<AggLevel> aggLevels = null;",
                "+    private long startTime = 0L;",
                "+    private long endTime = -1L;",
                "+    private String topologyId = null;",
                "+    private String componentId = null;",
                "+    private String metricName = null;",
                "+    private String executorId = null;",
                "+    private String hostId = null;",
                "+    private Integer port = null;",
                "+    private String streamId = null;",
                "+",
                "+    public FilterOptions() {",
                "+    }",
                "+",
                "+    public void setTopologyId(String topoId) {",
                "+        this.topologyId = topoId;",
                "+    }",
                "+",
                "+    public String getTopologyId() {",
                "+        return this.topologyId;",
                "+    }",
                "+",
                "+    public void setComponentId(String component) {",
                "+        this.componentId = component;",
                "+    }",
                "+",
                "+    public String getComponentId() {",
                "+        return this.componentId;",
                "+    }",
                "+",
                "+    public void setStartTime(Long time) {",
                "+        this.startTime = time;",
                "+    }",
                "+",
                "+    public long getStartTime() {",
                "+        return this.startTime;",
                "+    }",
                "+",
                "+    public void setEndTime(Long time) {",
                "+        this.endTime = time;",
                "+    }",
                "+",
                "+    /**",
                "+     *  Returns the end time if set, returns the current time otherwise.",
                "+     */",
                "+    public long getEndTime() {",
                "+        if (this.endTime < 0L) {",
                "+            this.endTime = System.currentTimeMillis();",
                "+        }",
                "+        return this.endTime;",
                "+    }",
                "+",
                "+    public void setMetricName(String name) {",
                "+        this.metricName = name;",
                "+    }",
                "+",
                "+    public String getMetricName() {",
                "+        return this.metricName;",
                "+    }",
                "+",
                "+    public void setExecutorId(String id) {",
                "+        this.executorId = id;",
                "+    }",
                "+",
                "+    public String getExecutorId() {",
                "+        return this.executorId;",
                "+    }",
                "+",
                "+    public void setHostId(String id) {",
                "+        this.hostId = id;",
                "+    }",
                "+",
                "+    public String getHostId() {",
                "+        return this.hostId;",
                "+    }",
                "+",
                "+    public void setPort(Integer p) {",
                "+        this.port = p;",
                "+    }",
                "+",
                "+    public Integer getPort() {",
                "+        return this.port;",
                "+    }",
                "+",
                "+    public void setStreamId(String id) {",
                "+        this.streamId = id;",
                "+    }",
                "+",
                "+    public String getStreamId() {",
                "+        return this.streamId;",
                "+    }",
                "+",
                "+    /**",
                "+     *  Add an aggregation level to search for.",
                "+     */",
                "+    public void addAggLevel(AggLevel level) {",
                "+        if (this.aggLevels == null) {",
                "+            this.aggLevels = new HashSet<>(1);",
                "+        }",
                "+        this.aggLevels.add(level);",
                "+    }",
                "+",
                "+    /**",
                "+     *  Set the aggregation levels to search for.",
                "+     */",
                "+    public void setAggLevels(Set<AggLevel> levels) throws MetricException {",
                "+        this.aggLevels = levels;",
                "+        if (this.aggLevels == null || this.aggLevels.isEmpty()) {",
                "+            throw new MetricException(\"Cannot search for empty AggLevel\");",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     *  Get the aggregation levels to search for.",
                "+     */",
                "+    public Set<AggLevel> getAggLevels() {",
                "+        if (this.aggLevels == null) {",
                "+            // assume filter choices have been made and since no selection was made, all levels are valid",
                "+            this.aggLevels = new HashSet<>(4);",
                "+            aggLevels.add(AggLevel.AGG_LEVEL_NONE);",
                "+            aggLevels.add(AggLevel.AGG_LEVEL_1_MIN);",
                "+            aggLevels.add(AggLevel.AGG_LEVEL_10_MIN);",
                "+            aggLevels.add(AggLevel.AGG_LEVEL_60_MIN);",
                "+        }",
                "+        return this.aggLevels;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/Metric.java b/storm-server/src/main/java/org/apache/storm/metricstore/Metric.java",
                "new file mode 100644",
                "index 000000000..716ced095",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/Metric.java",
                "@@ -0,0 +1,270 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore;",
                "+",
                "+import java.text.DateFormat;",
                "+import java.text.SimpleDateFormat;",
                "+import java.util.Date;",
                "+import java.util.TimeZone;",
                "+",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * Class containing metric values and all identifying fields to be stored in a MetricStore.",
                "+ */",
                "+public class Metric implements Comparable<Metric> {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(Metric.class);",
                "+",
                "+    // key fields",
                "+    private String name;",
                "+    private long timestamp;",
                "+    private String topologyId;",
                "+    private String componentId;",
                "+    private String executorId;",
                "+    private String hostname;",
                "+    private String streamId;",
                "+    private int port;",
                "+    private AggLevel aggLevel = AggLevel.AGG_LEVEL_NONE;",
                "+",
                "+    // value fields",
                "+    private double value;",
                "+    private long count = 1;",
                "+    private double min = 0.0;",
                "+    private double max = 0.0;",
                "+    private double sum = 0.0;",
                "+",
                "+",
                "+    /**",
                "+     *  Metric constructor.",
                "+     */",
                "+    public Metric(String name, Long timestamp, String topologyId, double value, String componentId, String executorId,",
                "+                  String hostname, String streamId, int port, AggLevel aggLevel) throws MetricException {",
                "+        this.name = name;",
                "+        this.timestamp = timestamp;",
                "+        this.topologyId = topologyId;",
                "+        this.componentId = componentId;",
                "+        this.executorId = executorId;",
                "+        this.hostname = hostname;",
                "+        this.streamId = streamId;",
                "+        this.port = port;",
                "+        this.setValue(value);",
                "+        setAggLevel(aggLevel);",
                "+    }",
                "+",
                "+    /**",
                "+     *  A Metric constructor with the same settings cloned from another.",
                "+     */",
                "+    public Metric(Metric o) {",
                "+        this.name = o.getMetricName();",
                "+        this.timestamp = o.getTimestamp();",
                "+        this.topologyId = o.getTopologyId();",
                "+        this.value = o.getValue();",
                "+        this.componentId = o.getComponentId();",
                "+        this.executorId = o.getExecutorId();",
                "+        this.hostname = o.getHostname();",
                "+        this.streamId = o.getStreamId();",
                "+        this.port = o.getPort();",
                "+        this.count = o.getCount();",
                "+        this.min = o.getMin();",
                "+        this.max = o.getMax();",
                "+        this.sum = o.getSum();",
                "+        this.aggLevel = o.getAggLevel();",
                "+    }",
                "+",
                "+    /**",
                "+     *  Check if a Metric matches another object.",
                "+     */",
                "+    public boolean equals(Object other) {",
                "+",
                "+        if (!(other instanceof Metric)) {",
                "+            return false;",
                "+        }",
                "+",
                "+        Metric o = (Metric) other;",
                "+",
                "+        return this == other",
                "+                || (this.name.equals(o.getMetricName())",
                "+                && this.timestamp == o.getTimestamp()",
                "+                && this.topologyId.equals(o.getTopologyId())",
                "+                && this.value == o.getValue()",
                "+                && this.componentId.equals(o.getComponentId())",
                "+                && this.executorId.equals(o.getExecutorId())",
                "+                && this.hostname.equals(o.getHostname())",
                "+                && this.streamId.equals(o.getStreamId())",
                "+                && this.port == o.getPort()",
                "+                && this.count == o.getCount()",
                "+                && this.min == o.getMin()",
                "+                && this.max == o.getMax()",
                "+                && this.sum == o.getSum()",
                "+                && this.aggLevel == o.getAggLevel());",
                "+    }",
                "+",
                "+    public AggLevel getAggLevel() {",
                "+        return this.aggLevel;",
                "+    }",
                "+",
                "+    /**",
                "+     *  Set the aggLevel.",
                "+     */",
                "+    public void setAggLevel(AggLevel aggLevel) throws MetricException {",
                "+        if (aggLevel == null) {",
                "+            throw new MetricException(\"AggLevel not set for metric\");",
                "+        }",
                "+        this.aggLevel = aggLevel;",
                "+    }",
                "+",
                "+    /**",
                "+     *  Initialize the metric value.",
                "+     */",
                "+    public void setValue(double value) {",
                "+        this.count = 1L;",
                "+        this.min = value;",
                "+        this.max = value;",
                "+        this.sum = value;",
                "+        this.value = value;",
                "+    }",
                "+",
                "+    /**",
                "+     *  Adds an additional value to the metric.",
                "+     */",
                "+    public void addValue(double value) {",
                "+        this.count += 1;",
                "+        this.min = Math.min(this.min, value);",
                "+        this.max = Math.max(this.max, value);",
                "+        this.sum += value;",
                "+        this.value = this.sum / this.count;",
                "+    }",
                "+",
                "+    public double getSum() {",
                "+        return this.sum;",
                "+    }",
                "+",
                "+    public void setSum(double sum) {",
                "+        this.sum = sum;",
                "+    }",
                "+",
                "+    public long getCount() {",
                "+        return this.count;",
                "+    }",
                "+",
                "+    public void setCount(long count) {",
                "+        this.count = count;",
                "+    }",
                "+",
                "+    public double getMin() {",
                "+        return this.min;",
                "+    }",
                "+",
                "+    public void setMin(double min) {",
                "+        this.min = min;",
                "+    }",
                "+",
                "+    public double getMax() {",
                "+        return this.max;",
                "+    }",
                "+",
                "+    public void setMax(double max) {",
                "+        this.max = max;",
                "+    }",
                "+",
                "+    public String getTopologyId() {",
                "+        return this.topologyId;",
                "+    }",
                "+",
                "+    public void setTopologyId(String topologyId) {",
                "+        this.topologyId = topologyId;",
                "+    }",
                "+",
                "+    public long getTimestamp() {",
                "+        return this.timestamp;",
                "+    }",
                "+",
                "+    public void setTimestamp(long timestamp) {",
                "+        this.timestamp = timestamp;",
                "+    }",
                "+",
                "+    public double getValue() {",
                "+        return this.value;",
                "+    }",
                "+",
                "+    public String getMetricName() {",
                "+        return this.name;",
                "+    }",
                "+",
                "+    public String getComponentId() {",
                "+        return this.componentId;",
                "+    }",
                "+",
                "+    public String getExecutorId() {",
                "+        return this.executorId;",
                "+    }",
                "+",
                "+    public String getHostname() {",
                "+        return this.hostname;",
                "+    }",
                "+",
                "+    public String getStreamId() {",
                "+        return this.streamId;",
                "+    }",
                "+",
                "+    public Integer getPort() {",
                "+        return this.port;",
                "+    }",
                "+",
                "+    @Override",
                "+    public int compareTo(Metric o) {",
                "+        long a = this.getTimestamp();",
                "+        long b = o.getTimestamp();",
                "+        return Long.compare(a, b);",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        StringBuilder sb = new StringBuilder();",
                "+        Date date = new Date(this.timestamp);",
                "+        DateFormat format = new SimpleDateFormat(\"dd/MM/yyyy HH:mm:ss.SSS\");",
                "+        format.setTimeZone(TimeZone.getTimeZone(\"UTC\"));",
                "+        sb.append(format.format(date));",
                "+        sb.append(\"|\");",
                "+        sb.append(this.topologyId);",
                "+        sb.append(\"|\");",
                "+        sb.append(aggLevel);",
                "+        sb.append(\"|\");",
                "+        sb.append(this.name);",
                "+        sb.append(\"|\");",
                "+        sb.append(this.componentId);",
                "+        sb.append(\"|\");",
                "+        sb.append(this.executorId);",
                "+        sb.append(\"|\");",
                "+        sb.append(this.hostname);",
                "+        sb.append(\"|\");",
                "+        sb.append(this.port);",
                "+        sb.append(\"|\");",
                "+        sb.append(this.streamId);",
                "+        return String.format(\"%s -- count: %d -- value: %f -- min: %f -- max: %f -- sum: %f\",",
                "+                sb.toString(),",
                "+                this.count,",
                "+                this.value,",
                "+                this.min,",
                "+                this.max,",
                "+                this.sum);",
                "+    }",
                "+}",
                "+",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/MetricException.java b/storm-server/src/main/java/org/apache/storm/metricstore/MetricException.java",
                "new file mode 100644",
                "index 000000000..e45a451e2",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/MetricException.java",
                "@@ -0,0 +1,32 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore;",
                "+",
                "+/**",
                "+ * A MetricException is used to describe an error using a MetricStore.",
                "+ */",
                "+public class MetricException extends Exception {",
                "+    public MetricException(String message) {",
                "+        super(message);",
                "+    }",
                "+",
                "+    public MetricException(String message, Throwable e) {",
                "+        super(message, e);",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/MetricStore.java b/storm-server/src/main/java/org/apache/storm/metricstore/MetricStore.java",
                "new file mode 100644",
                "index 000000000..166333bfb",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/MetricStore.java",
                "@@ -0,0 +1,74 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore;",
                "+",
                "+import java.util.Map;",
                "+",
                "+public interface MetricStore extends AutoCloseable {",
                "+",
                "+    /**",
                "+     * Create metric store instance using the configurations provided via the config map.",
                "+     *",
                "+     * @param config Storm config map",
                "+     * @throws MetricException on preparation error",
                "+     */",
                "+    void prepare(Map config) throws MetricException;",
                "+",
                "+    /**",
                "+     * Stores a metric in the store.",
                "+     *",
                "+     * @param metric  Metric to store",
                "+     * @throws MetricException on error",
                "+     */",
                "+    void insert(Metric metric) throws MetricException;",
                "+",
                "+    /**",
                "+     * Fill out the numeric values for a metric.",
                "+     *",
                "+     * @param metric  Metric to populate",
                "+     * @return   true if the metric was populated, false otherwise",
                "+     * @throws MetricException on error",
                "+     */",
                "+    boolean populateValue(Metric metric) throws MetricException;",
                "+",
                "+    /**",
                "+     * Close the metric store.",
                "+     */",
                "+    void close();",
                "+",
                "+    /**",
                "+     *  Scans all metrics in the store and returns the ones matching the specified filtering options.",
                "+     *",
                "+     * @param filter   options to filter by",
                "+     * @param scanCallback  callback for each Metric found",
                "+     * @throws MetricException  on error",
                "+     */",
                "+    void scan(FilterOptions filter, ScanCallback scanCallback) throws MetricException;",
                "+",
                "+    /**",
                "+     *  Interface used to callback metrics results from a scan.",
                "+     */",
                "+    interface ScanCallback {",
                "+        void cb(Metric metric);",
                "+    }",
                "+}",
                "+",
                "+",
                "+",
                "+",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/MetricStoreConfig.java b/storm-server/src/main/java/org/apache/storm/metricstore/MetricStoreConfig.java",
                "new file mode 100644",
                "index 000000000..2f2ad76ae",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/MetricStoreConfig.java",
                "@@ -0,0 +1,45 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore;",
                "+",
                "+import java.util.Map;",
                "+import org.apache.storm.DaemonConfig;",
                "+",
                "+",
                "+public class MetricStoreConfig {",
                "+",
                "+    /**",
                "+     * Configures metrics store to use the class specified in the conf.",
                "+     * @param conf Storm config map",
                "+     * @return MetricStore prepared store",
                "+     * @throws MetricException  on misconfiguration",
                "+     */",
                "+    public static MetricStore configure(Map conf) throws MetricException {",
                "+",
                "+        try {",
                "+            String storeClass = (String)conf.get(DaemonConfig.STORM_METRIC_STORE_CLASS);",
                "+            MetricStore store = (MetricStore) (Class.forName(storeClass)).newInstance();",
                "+            store.prepare(conf);",
                "+            return store;",
                "+        } catch (Throwable t) {",
                "+            throw new MetricException(\"Failed to create metric store\", t);",
                "+        }",
                "+    }",
                "+}",
                "+",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/KeyType.java b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/KeyType.java",
                "new file mode 100644",
                "index 000000000..a351be7df",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/KeyType.java",
                "@@ -0,0 +1,70 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore.rocksdb;",
                "+",
                "+import java.util.Collections;",
                "+import java.util.EnumSet;",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+",
                "+/**",
                "+ * Specifies all the valid types of keys and their values.",
                "+ */",
                "+public enum KeyType {",
                "+    TOPOLOGY_BLOB(0), // reserved for saving topology data",
                "+    METADATA_STRING_START(1),",
                "+    TOPOLOGY_STRING(1),",
                "+    METRIC_STRING(2),",
                "+    COMPONENT_STRING(3),",
                "+    EXEC_ID_STRING(4),",
                "+    HOST_STRING(5),",
                "+    STREAM_ID_STRING(6),",
                "+    METADATA_STRING_END(7),",
                "+    METRIC_DATA(0x80);",
                "+",
                "+    private final byte value;",
                "+    private static Map<Byte, KeyType> MAP;",
                "+",
                "+    static {",
                "+        MAP = new HashMap<>();",
                "+        for (KeyType type : EnumSet.allOf(KeyType.class)) {",
                "+            MAP.put(type.getValue(), type);",
                "+        }",
                "+        MAP = Collections.unmodifiableMap(MAP);",
                "+    }",
                "+",
                "+    KeyType(int value) {",
                "+        this.value = (byte)value;",
                "+    }",
                "+",
                "+    byte getValue() {",
                "+        return this.value;",
                "+    }",
                "+",
                "+    static KeyType getKeyType(byte value) {",
                "+        KeyType type = MAP.get(value);",
                "+        if (type == null) {",
                "+            throw new RuntimeException(\"Invalid key type \" + value);",
                "+        } else {",
                "+            return type;",
                "+        }",
                "+    }",
                "+}",
                "+",
                "+",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/MetricsCleaner.java b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/MetricsCleaner.java",
                "new file mode 100644",
                "index 000000000..6618f5db1",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/MetricsCleaner.java",
                "@@ -0,0 +1,98 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore.rocksdb;",
                "+",
                "+import com.codahale.metrics.Gauge;",
                "+import com.codahale.metrics.Meter;",
                "+import org.apache.storm.metric.StormMetricsRegistry;",
                "+import org.apache.storm.metricstore.FilterOptions;",
                "+import org.apache.storm.metricstore.MetricException;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * Class for removing expired metrics and unused metadata from the RocksDB store.",
                "+ */",
                "+public class MetricsCleaner implements Runnable, AutoCloseable {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(MetricsCleaner.class);",
                "+    private static long DEFAULT_SLEEP_MS = 4L * 60L * 60L * 1000L;",
                "+    private RocksDbStore store;",
                "+    private long retentionHours;",
                "+    private volatile boolean shutdown = false;",
                "+    private long sleepMs = DEFAULT_SLEEP_MS;",
                "+    private Meter failureMeter;",
                "+    private long purgeTimestamp = 0L;",
                "+",
                "+    MetricsCleaner(RocksDbStore store, int retentionHours, int hourlyPeriod, Meter failureMeter) {",
                "+        this.store = store;",
                "+        this.retentionHours = retentionHours;",
                "+        if (hourlyPeriod > 0) {",
                "+            this.sleepMs = hourlyPeriod * 60L * 60L * 1000L;",
                "+        }",
                "+        this.failureMeter = failureMeter;",
                "+",
                "+        Gauge<Long> gauge = new Gauge<Long>() {",
                "+            @Override",
                "+            public Long getValue() {",
                "+                return purgeTimestamp;",
                "+            }",
                "+        };",
                "+        StormMetricsRegistry.registerProvidedGauge(\"MetricsCleaner:purgeTimestamp\", gauge);",
                "+    }",
                "+",
                "+    @Override",
                "+    public void close() {",
                "+        shutdown = true;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void run() {",
                "+        while (!shutdown) {",
                "+            try {",
                "+                Thread.sleep(sleepMs);",
                "+            } catch (InterruptedException e) {",
                "+                LOG.error(\"Sleep interrupted\", e);",
                "+                continue;",
                "+            }",
                "+",
                "+            try {",
                "+                purgeMetrics();",
                "+            } catch (MetricException e) {",
                "+                LOG.error(\"Failed to purge metrics\", e);",
                "+                if (this.failureMeter != null) {",
                "+                    this.failureMeter.mark();",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    void purgeMetrics() throws MetricException {",
                "+        purgeTimestamp = System.currentTimeMillis() - this.retentionHours * 60L * 60L * 1000L;",
                "+",
                "+        LOG.info(\"Purging metrics before {}\", purgeTimestamp);",
                "+",
                "+        FilterOptions filter = new FilterOptions();",
                "+        long endTime = purgeTimestamp - 1L;",
                "+        filter.setEndTime(endTime);",
                "+        store.deleteMetrics(filter);",
                "+",
                "+        LOG.info(\"Purging metadata before \" + purgeTimestamp);",
                "+        store.deleteMetadataBefore(purgeTimestamp);",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/ReadOnlyStringMetadataCache.java b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/ReadOnlyStringMetadataCache.java",
                "new file mode 100644",
                "index 000000000..0effbc447",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/ReadOnlyStringMetadataCache.java",
                "@@ -0,0 +1,52 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore.rocksdb;",
                "+",
                "+import org.apache.http.annotation.ThreadSafe;",
                "+",
                "+/**",
                "+ * The read-only interface to a StringMetadataCache allowed to be used by any thread.",
                "+ */",
                "+@ThreadSafe",
                "+public interface ReadOnlyStringMetadataCache {",
                "+",
                "+    /**",
                "+     * Get the string metadata from the cache.",
                "+     *",
                "+     * @param s   The string to look for",
                "+     * @return   the metadata associated with the string or null if not found",
                "+     */",
                "+    StringMetadata get(String s);",
                "+",
                "+    /**",
                "+     * Returns the string matching the string Id if in the cache.",
                "+     *",
                "+     * @param stringId   The string Id to check",
                "+     * @return   the associated string if the Id is in the cache, null otherwise",
                "+     */",
                "+    String getMetadataString(Integer stringId);",
                "+",
                "+    /**",
                "+     * Determines if a string Id is contained in the cache.",
                "+     *",
                "+     * @param stringId   The string Id to check",
                "+     * @return   true if the Id is in the cache, false otherwise",
                "+     */",
                "+    boolean contains(Integer stringId);",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbKey.java b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbKey.java",
                "new file mode 100644",
                "index 000000000..786828239",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbKey.java",
                "@@ -0,0 +1,228 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore.rocksdb;",
                "+",
                "+import com.google.common.primitives.UnsignedBytes;",
                "+import java.nio.ByteBuffer;",
                "+import java.util.Collections;",
                "+import java.util.EnumSet;",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+import javax.xml.bind.DatatypeConverter;",
                "+import org.apache.storm.metricstore.AggLevel;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+",
                "+/**",
                "+ * Class representing the data used as a Key in RocksDB.  Keys can be used either for metadata or metrics.",
                "+ *",
                "+ * <P>Keys are 38 bytes in size.  The fields for a key are:",
                "+ * <pre><",
                "+ * Field             Size         Offset",
                "+ *",
                "+ * Type                 1              0      The type maps to the KeyType enum, specifying a metric or various types of metadata",
                "+ * Aggregation Level    1              1      The aggregation level for a metric (see AggLevel enum).  0 for metadata.",
                "+ * TopologyId           4              2      The metadata string Id representing a topologyId for a metric, or the unique",
                "+ *                                                   string Id for a metadata string",
                "+ * Timestamp            8              6      The timestamp for a metric, unused for metadata",
                "+ * MetricId             4             14      The metadata string Id for the metric name",
                "+ * ComponentId          4             18      The metadata string Id for the component Id",
                "+ * ExecutorId           4             22      The metadata string Id for the executor Id",
                "+ * HostId               4             26      The metadata string Id for the host Id",
                "+ * Port                 4             30      The port number",
                "+ * StreamId             4             34      The metadata string Id for the stream Id",
                "+ * </pre>",
                "+ */",
                "+public class RocksDbKey implements Comparable<RocksDbKey> {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(RocksDbKey.class);",
                "+    static final int KEY_SIZE = 38;",
                "+    private static Map<Byte, RocksDbKey> PREFIX_MAP = new HashMap<>();",
                "+    private byte[] key;",
                "+",
                "+    static {",
                "+        // pregenerate commonly used keys for scans",
                "+        for (KeyType type : EnumSet.allOf(KeyType.class)) {",
                "+            RocksDbKey key = new RocksDbKey(type, 0);",
                "+            PREFIX_MAP.put(type.getValue(), key);",
                "+        }",
                "+        PREFIX_MAP = Collections.unmodifiableMap(PREFIX_MAP);",
                "+    }",
                "+",
                "+    /**",
                "+     * Constructor for a RocksDB key for a metadata string.",
                "+     *",
                "+     * @param type  type of metadata string",
                "+     * @param metadataStringId  the string Id for the string (stored in the topologyId portion of the key)",
                "+     */",
                "+    RocksDbKey(KeyType type, int metadataStringId) {",
                "+        byte[] key = new byte[KEY_SIZE];",
                "+        ByteBuffer bb = ByteBuffer.wrap(key);",
                "+        bb.put(type.getValue());",
                "+        bb.put(AggLevel.AGG_LEVEL_NONE.getValue());",
                "+        bb.putInt(metadataStringId);",
                "+        this.key = key;",
                "+    }",
                "+",
                "+    /**",
                "+     * Constructor for a RocksDB key from raw data.",
                "+     *",
                "+     * @param raw  the key data",
                "+     */",
                "+    RocksDbKey(byte[] raw) {",
                "+        this.key = raw;",
                "+    }",
                "+",
                "+",
                "+    /**",
                "+     * Get a zeroed key of the specified type.",
                "+     *",
                "+     * @param type  the desired type",
                "+     * @return  a key of the desired type",
                "+     */",
                "+    static RocksDbKey getPrefix(KeyType type) {",
                "+        return PREFIX_MAP.get(type.getValue());",
                "+    }",
                "+",
                "+    /**",
                "+     * get the metadata string Id portion of the key for metadata keys.",
                "+     *",
                "+     * @return  the metadata string Id",
                "+     * @throws  RuntimeException  if the key is not a metadata type",
                "+     */",
                "+    int getMetadataStringId() {",
                "+        if (this.getType().getValue() < KeyType.METADATA_STRING_END.getValue()) {",
                "+            return ByteBuffer.wrap(key, 2, 4).getInt();",
                "+        } else {",
                "+            throw new RuntimeException(\"Cannot fetch metadata string for key of type \" + this.getType());",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * get the raw key bytes",
                "+     */",
                "+    byte[] getRaw() {",
                "+        return this.key;",
                "+    }",
                "+",
                "+    /**",
                "+     * get the type of key.",
                "+     *",
                "+     * @return  the type of key",
                "+     */",
                "+    KeyType getType() {",
                "+        return KeyType.getKeyType(key[0]);",
                "+    }",
                "+",
                "+    /**",
                "+     * compares to keys on a byte by byte basis.",
                "+     *",
                "+     * @return  comparison of key byte values",
                "+     */",
                "+    @Override",
                "+    public int compareTo(RocksDbKey o) {",
                "+        return UnsignedBytes.lexicographicalComparator().compare(this.getRaw(), o.getRaw());",
                "+    }",
                "+",
                "+    /**",
                "+     * gets the first possible key value for the desired key type.",
                "+     *",
                "+     * @return  the initial key",
                "+     */",
                "+    static RocksDbKey getInitialKey(KeyType type) {",
                "+        return PREFIX_MAP.get(type.getValue());",
                "+    }",
                "+",
                "+    /**",
                "+     * gets the key just larger than the last possible key value for the desired key type.",
                "+     *",
                "+     * @return  the last key",
                "+     */",
                "+    static RocksDbKey getLastKey(KeyType type) {",
                "+        byte value = (byte)(type.getValue() + 1);",
                "+        return PREFIX_MAP.get(value);",
                "+    }",
                "+",
                "+    /**",
                "+     * Creates a metric key with the desired properties.",
                "+     *",
                "+     * @return  the generated key",
                "+     */",
                "+    static RocksDbKey createMetricKey(AggLevel aggLevel, int topologyId, long metricTimestamp, int metricId,",
                "+                                      int componentId, int executorId, int hostId, int port,",
                "+                                      int streamId) {",
                "+        byte[] raw = new byte[KEY_SIZE];",
                "+        ByteBuffer bb = ByteBuffer.wrap(raw);",
                "+        bb.put(KeyType.METRIC_DATA.getValue());",
                "+        bb.put(aggLevel.getValue());",
                "+        bb.putInt(topologyId);       // offset 2",
                "+        bb.putLong(metricTimestamp); // offset 6",
                "+        bb.putInt(metricId);         // offset 14",
                "+        bb.putInt(componentId);      // offset 18",
                "+        bb.putInt(executorId);       // offset 22",
                "+        bb.putInt(hostId);           // offset 26",
                "+        bb.putInt(port);             // offset 30",
                "+        bb.putInt(streamId);         // offset 34",
                "+",
                "+        RocksDbKey key = new RocksDbKey(raw);",
                "+        return key;",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the unique string Id for a metric's topologyId.",
                "+     */",
                "+    int getTopologyId() {",
                "+        int val = ByteBuffer.wrap(key, 2, 4).getInt();",
                "+        return val;",
                "+    }",
                "+",
                "+    long getTimestamp() {",
                "+        return ByteBuffer.wrap(key, 6, 8).getLong();",
                "+    }",
                "+",
                "+    int getMetricId() {",
                "+        return ByteBuffer.wrap(key, 14, 4).getInt();",
                "+    }",
                "+",
                "+    int getComponentId() {",
                "+        return ByteBuffer.wrap(key, 18, 4).getInt();",
                "+    }",
                "+",
                "+    int getExecutorId() {",
                "+        return ByteBuffer.wrap(key, 22, 4).getInt();",
                "+    }",
                "+",
                "+    int getHostnameId() {",
                "+        return ByteBuffer.wrap(key, 26, 4).getInt();",
                "+    }",
                "+",
                "+    int getPort() {",
                "+        return ByteBuffer.wrap(key, 30, 4).getInt();",
                "+    }",
                "+",
                "+    int getStreamId() {",
                "+        return ByteBuffer.wrap(key, 34, 4).getInt();",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        return \"[0x\" + DatatypeConverter.printHexBinary(key) + \"]\";",
                "+    }",
                "+}",
                "+",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbMetricsWriter.java b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbMetricsWriter.java",
                "new file mode 100644",
                "index 000000000..a050a767f",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbMetricsWriter.java",
                "@@ -0,0 +1,320 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore.rocksdb;",
                "+",
                "+import com.codahale.metrics.Meter;",
                "+",
                "+import java.util.ArrayList;",
                "+import java.util.HashSet;",
                "+import java.util.ListIterator;",
                "+import java.util.Map;",
                "+import java.util.Set;",
                "+import java.util.TreeMap;",
                "+import java.util.concurrent.BlockingQueue;",
                "+import java.util.concurrent.ThreadLocalRandom;",
                "+",
                "+import org.apache.http.annotation.NotThreadSafe;",
                "+import org.apache.storm.metricstore.AggLevel;",
                "+import org.apache.storm.metricstore.Metric;",
                "+import org.apache.storm.metricstore.MetricException;",
                "+import org.rocksdb.FlushOptions;",
                "+import org.rocksdb.RocksDBException;",
                "+import org.rocksdb.WriteBatch;",
                "+import org.rocksdb.WriteOptions;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * Class designed to perform all metrics inserts into RocksDB.  Metrics are processed from a blocking queue.  Inserts",
                "+ * to RocksDB are done using a single thread to simplify design (such as looking up existing metric data for aggregation,",
                "+ * and fetching/evicting metadata from the cache).",
                "+ * </P>",
                "+ * A writable LRU StringMetadataCache is used to minimize looking up metadata string Ids.  As entries are added to the full cache, older",
                "+ * entries are evicted from the cache and need to be written to the database.  This happens as the handleEvictedMetadata()",
                "+ * method callback.",
                "+ * </P>",
                "+ * The following issues would need to be addressed to implement a multithreaded metrics writer:",
                "+ * <ul>",
                "+ *     <li>Generation of unique unused IDs for new metadata strings needs to be thread safe.</li>",
                "+ *     <li>Ensuring newly created metadata strings are seen by all threads.</li>",
                "+ *     <li>Maintaining a properly cached state of metadata for multiple writers.  The current LRU cache",
                "+ *     evicts data as new metadata is added.</li>",
                "+ *     <li>Processing the aggregation of a metric requires fetching and updating previous aggregates.  A multithreaded",
                "+ *     design would need to ensure two metrics were not updating an aggregated metric at the same time.</li>",
                "+ *     <li>Investigate performance of multiple threads inserting into RocksDB versus a single ordered insert.</li>",
                "+ * </ul>",
                "+ */",
                "+@NotThreadSafe",
                "+public class RocksDbMetricsWriter implements Runnable, AutoCloseable {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(RocksDbMetricsWriter.class);",
                "+    private RocksDbStore store;",
                "+    private BlockingQueue queue;",
                "+    private WritableStringMetadataCache stringMetadataCache;",
                "+    private Set<Integer> unusedIds = new HashSet<>();",
                "+    private TreeMap<RocksDbKey, RocksDbValue> insertBatch = new TreeMap<>(); // RocksDB should insert in sorted key order",
                "+    private WriteOptions writeOpts = new WriteOptions();",
                "+    private volatile boolean shutdown = false;",
                "+    private Meter failureMeter;",
                "+    private ArrayList<AggLevel> aggBuckets = new ArrayList<>();",
                "+",
                "+    /**",
                "+     * Constructor for the RocksDbMetricsWriter.",
                "+     *",
                "+     * @param store   The RocksDB store",
                "+     * @param queue   The queue to receive metrics for insertion",
                "+     */",
                "+    RocksDbMetricsWriter(RocksDbStore store, BlockingQueue queue, Meter failureMeter)  {",
                "+        this.store = store;",
                "+        this.queue = queue;",
                "+        this.failureMeter = failureMeter;",
                "+",
                "+        aggBuckets.add(AggLevel.AGG_LEVEL_1_MIN);",
                "+        aggBuckets.add(AggLevel.AGG_LEVEL_10_MIN);",
                "+        aggBuckets.add(AggLevel.AGG_LEVEL_60_MIN);",
                "+    }",
                "+",
                "+    /**",
                "+     * Init routine called once the Metadata cache has been created.",
                "+     *",
                "+     * @throws MetricException  on cache error",
                "+     */",
                "+    void init() throws MetricException {",
                "+        this.stringMetadataCache = StringMetadataCache.getWritableStringMetadataCache();",
                "+    }",
                "+",
                "+    /**",
                "+     * Run routine to wait for metrics on a queue and insert into RocksDB.",
                "+     */",
                "+    @Override",
                "+    public void run() {",
                "+        while (!shutdown) {",
                "+            try {",
                "+                Metric m = (Metric) queue.take();",
                "+                processInsert(m);",
                "+            } catch (Exception e) {",
                "+                LOG.error(\"Failed to insert metric\", e);",
                "+                if (this.failureMeter != null) {",
                "+                    this.failureMeter.mark();",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Performs the actual metric insert, and aggregates over all bucket times.",
                "+     *",
                "+     * @param metric  Metric to store",
                "+     * @throws MetricException  if database write fails",
                "+     */",
                "+    private void processInsert(Metric metric) throws MetricException {",
                "+",
                "+        // convert all strings to numeric Ids for the metric key and add to the metadata cache",
                "+        long metricTimestamp = metric.getTimestamp();",
                "+        Integer topologyId = storeMetadataString(KeyType.TOPOLOGY_STRING, metric.getTopologyId(), metricTimestamp);",
                "+        Integer metricId = storeMetadataString(KeyType.METRIC_STRING, metric.getMetricName(), metricTimestamp);",
                "+        Integer componentId = storeMetadataString(KeyType.COMPONENT_STRING, metric.getComponentId(), metricTimestamp);",
                "+        Integer executorId = storeMetadataString(KeyType.EXEC_ID_STRING, metric.getExecutorId(), metricTimestamp);",
                "+        Integer hostId = storeMetadataString(KeyType.HOST_STRING, metric.getHostname(), metricTimestamp);",
                "+        Integer streamId = storeMetadataString(KeyType.STREAM_ID_STRING, metric.getStreamId(), metricTimestamp);",
                "+",
                "+        RocksDbKey key = RocksDbKey.createMetricKey(AggLevel.AGG_LEVEL_NONE, topologyId, metric.getTimestamp(), metricId,",
                "+                componentId, executorId, hostId, metric.getPort(), streamId);",
                "+",
                "+        // save metric key/value to be batched",
                "+        RocksDbValue value = new RocksDbValue(metric);",
                "+        insertBatch.put(key, value);",
                "+",
                "+        // Aggregate matching metrics over bucket timeframes.",
                "+        // We'll process starting with the longest bucket.  If the metric for this does not exist, we don't have to",
                "+        // search for the remaining bucket metrics.",
                "+        ListIterator li = aggBuckets.listIterator(aggBuckets.size());",
                "+        boolean populate = true;",
                "+        while (li.hasPrevious()) {",
                "+            AggLevel bucket = (AggLevel)li.previous();",
                "+            Metric aggMetric = new Metric(metric);",
                "+            aggMetric.setAggLevel(bucket);",
                "+",
                "+            long msToBucket = 1000L * 60L * bucket.getValue();",
                "+            long roundedToBucket = msToBucket * (metric.getTimestamp() / msToBucket);",
                "+            aggMetric.setTimestamp(roundedToBucket);",
                "+",
                "+            RocksDbKey aggKey = RocksDbKey.createMetricKey(bucket, topologyId, aggMetric.getTimestamp(), metricId,",
                "+                    componentId, executorId, hostId, aggMetric.getPort(), streamId);",
                "+",
                "+            if (populate) {",
                "+                // retrieve any existing aggregation matching this one and update the values",
                "+                if (store.populateFromKey(aggKey, aggMetric)) {",
                "+                    aggMetric.addValue(metric.getValue());",
                "+                } else {",
                "+                    // aggregating metric did not exist, don't look for further ones with smaller timestamps",
                "+                    populate = false;",
                "+                }",
                "+            }",
                "+",
                "+            // save metric key/value to be batched",
                "+            RocksDbValue aggVal = new RocksDbValue(aggMetric);",
                "+            insertBatch.put(aggKey, aggVal);",
                "+        }",
                "+",
                "+        processBatchInsert(insertBatch);",
                "+",
                "+        insertBatch.clear();",
                "+    }",
                "+",
                "+    // converts a metadata string into a unique integer.  Updates the timestamp of the string",
                "+    // so we can track when it was last used for later deletion on database cleanup.",
                "+    private int storeMetadataString(KeyType type, String s, long metricTimestamp) throws MetricException {",
                "+        if (s == null) {",
                "+            throw new MetricException(\"No string for metric metadata string type \" + type);",
                "+        }",
                "+",
                "+        // attempt to find it in the string cache",
                "+        StringMetadata stringMetadata = stringMetadataCache.get(s);",
                "+        if (stringMetadata != null) {",
                "+            // make sure the timestamp on the metadata has the latest time",
                "+            stringMetadata.update(metricTimestamp, type);",
                "+            return stringMetadata.getStringId();",
                "+        }",
                "+",
                "+        // attempt to find the string in the database",
                "+        stringMetadata = store.rocksDbGetStringMetadata(type, s);",
                "+        if (stringMetadata != null) {",
                "+            // update to the latest timestamp and add to the string cache",
                "+            stringMetadata.update(metricTimestamp, type);",
                "+            stringMetadataCache.put(s, stringMetadata, false);",
                "+            return stringMetadata.getStringId();",
                "+        }",
                "+",
                "+        // string does not exist, create using an unique string id and add to cache",
                "+        if (LOG.isDebugEnabled()) {",
                "+            LOG.debug(type + \".\" + s + \" does not exist in cache or database\");",
                "+        }",
                "+        int stringId = getUniqueMetadataStringId();",
                "+        stringMetadata = new StringMetadata(type, stringId, metricTimestamp);",
                "+        stringMetadataCache.put(s, stringMetadata, true);",
                "+",
                "+        return stringMetadata.getStringId();",
                "+    }",
                "+",
                "+    // get a currently unused unique string id",
                "+    private int getUniqueMetadataStringId() throws MetricException {",
                "+        generateUniqueStringIds();",
                "+        int id = unusedIds.iterator().next();",
                "+        unusedIds.remove(id);",
                "+        return  id;",
                "+    }",
                "+",
                "+    // guarantees a list of unused string Ids exists.  Once the list is empty, creates a new list",
                "+    // by generating a list of random numbers and removing the ones that already are in use.",
                "+    private void generateUniqueStringIds() throws MetricException {",
                "+        int attempts = 0;",
                "+        while (unusedIds.isEmpty()) {",
                "+            attempts++;",
                "+            if (attempts > 100) {",
                "+                String message = \"Failed to generate unique ids\";",
                "+                LOG.error(message);",
                "+                throw new MetricException(message);",
                "+            }",
                "+            for (int i = 0; i < 600; i++) {",
                "+                int n = ThreadLocalRandom.current().nextInt();",
                "+                if (n == RocksDbStore.INVALID_METADATA_STRING_ID) {",
                "+                    continue;",
                "+                }",
                "+                // remove any entries in the cache",
                "+                if (stringMetadataCache.contains(n)) {",
                "+                    continue;",
                "+                }",
                "+                unusedIds.add(n);",
                "+            }",
                "+            // now scan all metadata and remove any matching string Ids from this list",
                "+            RocksDbKey firstPrefix = RocksDbKey.getPrefix(KeyType.METADATA_STRING_START);",
                "+            RocksDbKey lastPrefix = RocksDbKey.getPrefix(KeyType.METADATA_STRING_END);",
                "+            store.scanRange(firstPrefix, lastPrefix, (key, value) -> {",
                "+                unusedIds.remove(key.getMetadataStringId());",
                "+                return true; // process all metadata",
                "+            });",
                "+        }",
                "+    }",
                "+",
                "+    // writes multiple metric values into the database as a batch operation.  The tree map keeps the keys sorted",
                "+    // for faster insertion to RocksDB.",
                "+    private void processBatchInsert(TreeMap<RocksDbKey, RocksDbValue> batchMap) throws MetricException {",
                "+        try (WriteBatch writeBatch = new WriteBatch()) {",
                "+            // take the batched metric data and write to the database",
                "+            for (RocksDbKey k : batchMap.keySet()) {",
                "+                RocksDbValue v = batchMap.get(k);",
                "+                writeBatch.put(k.getRaw(), v.getRaw());",
                "+            }",
                "+            store.db.write(writeOpts, writeBatch);",
                "+        } catch (Exception e) {",
                "+            String message = \"Failed to store data to RocksDB\";",
                "+            LOG.error(message, e);",
                "+            throw new MetricException(message, e);",
                "+        }",
                "+    }",
                "+",
                "+    // evicted metadata needs to be stored immediately.  Metadata lookups count on it being in the cache",
                "+    // or database.",
                "+    void handleEvictedMetadata(RocksDbKey key, RocksDbValue val) {",
                "+        try {",
                "+            store.db.put(key.getRaw(), val.getRaw());",
                "+        } catch (Exception e) {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "+    boolean isShutdown() {",
                "+        return this.shutdown;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void close() {",
                "+        this.shutdown = true;",
                "+",
                "+        // get all metadata from the cache to put into the database",
                "+        TreeMap<RocksDbKey, RocksDbValue> batchMap = new TreeMap<>();  // use a new map to prevent threading issues with writer thread",
                "+        for (Map.Entry entry : stringMetadataCache.entrySet()) {",
                "+            String metadataString = (String)entry.getKey();",
                "+            StringMetadata val = (StringMetadata)entry.getValue();",
                "+            RocksDbValue rval = new RocksDbValue(val.getLastTimestamp(), metadataString);",
                "+",
                "+            for (KeyType type : val.getMetadataTypes()) {   // save the metadata for all types of strings it matches",
                "+                RocksDbKey rkey = new RocksDbKey(type, val.getStringId());",
                "+                batchMap.put(rkey, rval);",
                "+            }",
                "+        }",
                "+",
                "+        try {",
                "+            processBatchInsert(batchMap);",
                "+        } catch (MetricException e) {",
                "+            LOG.error(\"Failed to insert all metadata\", e);",
                "+        }",
                "+",
                "+        // flush db to disk",
                "+        try (FlushOptions flushOps = new FlushOptions()) {",
                "+            flushOps.setWaitForFlush(true);",
                "+            store.db.flush(flushOps);",
                "+        } catch (RocksDBException e) {",
                "+            LOG.error(\"Failed ot flush RocksDB\", e);",
                "+            if (this.failureMeter != null) {",
                "+                this.failureMeter.mark();",
                "+            }",
                "+        }",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbStore.java b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbStore.java",
                "new file mode 100644",
                "index 000000000..2f44aff9f",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbStore.java",
                "@@ -0,0 +1,639 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore.rocksdb;",
                "+",
                "+import com.codahale.metrics.Meter;",
                "+import java.io.File;",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+import java.util.concurrent.BlockingQueue;",
                "+import java.util.concurrent.LinkedBlockingQueue;",
                "+import java.util.concurrent.atomic.AtomicReference;",
                "+",
                "+import org.apache.storm.DaemonConfig;",
                "+import org.apache.storm.metric.StormMetricsRegistry;",
                "+import org.apache.storm.metricstore.AggLevel;",
                "+import org.apache.storm.metricstore.FilterOptions;",
                "+import org.apache.storm.metricstore.Metric;",
                "+import org.apache.storm.metricstore.MetricException;",
                "+import org.apache.storm.metricstore.MetricStore;",
                "+import org.apache.storm.utils.ObjectReader;",
                "+import org.rocksdb.BlockBasedTableConfig;",
                "+import org.rocksdb.IndexType;",
                "+import org.rocksdb.Options;",
                "+import org.rocksdb.ReadOptions;",
                "+import org.rocksdb.RocksDB;",
                "+import org.rocksdb.RocksDBException;",
                "+import org.rocksdb.RocksIterator;",
                "+import org.rocksdb.WriteBatch;",
                "+import org.rocksdb.WriteOptions;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+",
                "+public class RocksDbStore implements MetricStore, AutoCloseable {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(RocksDbStore.class);",
                "+    private static final int MAX_QUEUE_CAPACITY = 4000;",
                "+    static final int INVALID_METADATA_STRING_ID = 0;",
                "+    RocksDB db;",
                "+    private ReadOnlyStringMetadataCache readOnlyStringMetadataCache = null;",
                "+    private BlockingQueue queue = new LinkedBlockingQueue(MAX_QUEUE_CAPACITY);",
                "+    private RocksDbMetricsWriter metricsWriter = null;",
                "+    private MetricsCleaner metricsCleaner = null;",
                "+    private Meter failureMeter = null;",
                "+",
                "+    interface RocksDbScanCallback {",
                "+        boolean cb(RocksDbKey key, RocksDbValue val);  // return false to stop scan",
                "+    }",
                "+",
                "+    /**",
                "+     * Create metric store instance using the configurations provided via the config map.",
                "+     *",
                "+     * @param config Storm config map",
                "+     * @throws MetricException on preparation error",
                "+     */",
                "+    public void prepare(Map config) throws MetricException {",
                "+        validateConfig(config);",
                "+",
                "+        this.failureMeter = StormMetricsRegistry.registerMeter(\"RocksDB:metric-failures\");",
                "+",
                "+        RocksDB.loadLibrary();",
                "+        boolean createIfMissing = ObjectReader.getBoolean(config.get(DaemonConfig.STORM_ROCKSDB_CREATE_IF_MISSING), false);",
                "+",
                "+        try (Options options = new Options().setCreateIfMissing(createIfMissing)) {",
                "+            // use the hash index for prefix searches",
                "+            BlockBasedTableConfig tfc = new BlockBasedTableConfig();",
                "+            tfc.setIndexType(IndexType.kHashSearch);",
                "+            options.setTableFormatConfig(tfc);",
                "+            options.useCappedPrefixExtractor(RocksDbKey.KEY_SIZE);",
                "+",
                "+            String path = getRocksDbAbsoluteDir(config);",
                "+            LOG.info(\"Opening RocksDB from {}\", path);",
                "+            db = RocksDB.open(options, path);",
                "+        } catch (RocksDBException e) {",
                "+            String message = \"Error opening RockDB database\";",
                "+            LOG.error(message, e);",
                "+            throw new MetricException(message, e);",
                "+        }",
                "+",
                "+        // create thread to delete old metrics and metadata",
                "+        Integer retentionHours = Integer.parseInt(config.get(DaemonConfig.STORM_ROCKSDB_METRIC_RETENTION_HOURS).toString());",
                "+        Integer deletionPeriod = 0;",
                "+        if (config.containsKey(DaemonConfig.STORM_ROCKSDB_METRIC_DELETION_PERIOD_HOURS)) {",
                "+            deletionPeriod = Integer.parseInt(config.get(DaemonConfig.STORM_ROCKSDB_METRIC_DELETION_PERIOD_HOURS).toString());",
                "+        }",
                "+        metricsCleaner = new MetricsCleaner(this, retentionHours, deletionPeriod, failureMeter);",
                "+",
                "+        // create thread to process insertion of all metrics",
                "+        metricsWriter = new RocksDbMetricsWriter(this, this.queue, this.failureMeter);",
                "+",
                "+        int cacheCapacity = Integer.parseInt(config.get(DaemonConfig.STORM_ROCKSDB_METADATA_STRING_CACHE_CAPACITY).toString());",
                "+        StringMetadataCache.init(metricsWriter, cacheCapacity);",
                "+        readOnlyStringMetadataCache = StringMetadataCache.getReadOnlyStringMetadataCache();",
                "+        metricsWriter.init(); // init the writer once the cache is setup",
                "+",
                "+        // start threads after metadata cache created",
                "+        Thread thread = new Thread(metricsCleaner, \"RocksDbMetricsCleaner\");",
                "+        thread.setDaemon(true);",
                "+        thread.start();",
                "+",
                "+        thread = new Thread(metricsWriter, \"RocksDbMetricsWriter\");",
                "+        thread.setDaemon(true);",
                "+        thread.start();",
                "+    }",
                "+",
                "+    /**",
                "+     * Implements configuration validation of Metrics Store, validates storm configuration for Metrics Store.",
                "+     *",
                "+     * @param config Storm config to specify which store type, location of store and creation policy",
                "+     * @throws MetricException if there is a missing required configuration or if the store does not exist but",
                "+     *                         the config specifies not to create the store",
                "+     */",
                "+    private void validateConfig(Map config) throws MetricException {",
                "+        if (!(config.containsKey(DaemonConfig.STORM_ROCKSDB_LOCATION))) {",
                "+            throw new MetricException(\"Not a vaild RocksDB configuration - Missing store location \" + DaemonConfig.STORM_ROCKSDB_LOCATION);",
                "+        }",
                "+",
                "+        if (!(config.containsKey(DaemonConfig.STORM_ROCKSDB_CREATE_IF_MISSING))) {",
                "+            throw new MetricException(\"Not a vaild RocksDB configuration - Does not specify creation policy \"",
                "+                    + DaemonConfig.STORM_ROCKSDB_CREATE_IF_MISSING);",
                "+        }",
                "+",
                "+        // validate path defined",
                "+        String storePath = getRocksDbAbsoluteDir(config);",
                "+",
                "+        boolean createIfMissing = ObjectReader.getBoolean(config.get(DaemonConfig.STORM_ROCKSDB_CREATE_IF_MISSING), false);",
                "+        if (!createIfMissing) {",
                "+            if (!(new File(storePath).exists())) {",
                "+                throw new MetricException(\"Configuration specifies not to create a store but no store currently exists at \" + storePath);",
                "+            }",
                "+        }",
                "+",
                "+        if (!(config.containsKey(DaemonConfig.STORM_ROCKSDB_METADATA_STRING_CACHE_CAPACITY))) {",
                "+            throw new MetricException(\"Not a valid RocksDB configuration - Missing metadata string cache size \"",
                "+                    + DaemonConfig.STORM_ROCKSDB_METADATA_STRING_CACHE_CAPACITY);",
                "+        }",
                "+",
                "+        if (!config.containsKey(DaemonConfig.STORM_ROCKSDB_METRIC_RETENTION_HOURS)) {",
                "+            throw new MetricException(\"Not a valid RocksDB configuration - Missing metric retention \"",
                "+                    + DaemonConfig.STORM_ROCKSDB_METRIC_RETENTION_HOURS);",
                "+        }",
                "+    }",
                "+",
                "+    private String getRocksDbAbsoluteDir(Map conf) throws MetricException {",
                "+        String storePath = (String)conf.get(DaemonConfig.STORM_ROCKSDB_LOCATION);",
                "+        if (storePath == null) {",
                "+            throw new MetricException(\"Not a vaild RocksDB configuration - Missing store location \" + DaemonConfig.STORM_ROCKSDB_LOCATION);",
                "+        } else {",
                "+            if (new File(storePath).isAbsolute()) {",
                "+                return storePath;",
                "+            } else {",
                "+                String stormHome = System.getProperty(\"storm.home\");",
                "+                if (stormHome == null) {",
                "+                    throw new MetricException(\"storm.home not set\");",
                "+                }",
                "+                return (stormHome + File.separator + storePath);",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Stores metrics in the store.",
                "+     *",
                "+     * @param metric  Metric to store",
                "+     * @throws MetricException  if database write fails",
                "+     */",
                "+    public void insert(Metric metric) throws MetricException {",
                "+        try {",
                "+            // don't bother blocking on a full queue, just drop metrics in case we can't keep up",
                "+            if (queue.remainingCapacity() <= 0) {",
                "+                LOG.info(\"Metrics q full, dropping metric\");",
                "+                return;",
                "+            }",
                "+            queue.put(metric);",
                "+        } catch (Exception e) {",
                "+            String message = \"Failed to insert metric\";",
                "+            LOG.error(message, e);",
                "+            if (this.failureMeter != null) {",
                "+                this.failureMeter.mark();",
                "+            }",
                "+            throw new MetricException(message, e);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Fill out the numeric values for a metric.",
                "+     *",
                "+     * @param metric  Metric to populate",
                "+     * @return   true if the metric was populated, false otherwise",
                "+     * @throws MetricException  if read from database fails",
                "+     */",
                "+    @Override",
                "+    public boolean populateValue(Metric metric) throws MetricException {",
                "+        Map<String, Integer> localLookupCache = new HashMap<>(6);",
                "+",
                "+        int topologyId = lookupMetadataString(KeyType.TOPOLOGY_STRING, metric.getTopologyId(), localLookupCache);",
                "+        if (INVALID_METADATA_STRING_ID == topologyId) {",
                "+            return false;",
                "+        }",
                "+        int metricId = lookupMetadataString(KeyType.METRIC_STRING, metric.getMetricName(), localLookupCache);",
                "+        if (INVALID_METADATA_STRING_ID == metricId) {",
                "+            return false;",
                "+        }",
                "+        int componentId = lookupMetadataString(KeyType.COMPONENT_STRING, metric.getComponentId(), localLookupCache);",
                "+        if (INVALID_METADATA_STRING_ID == componentId) {",
                "+            return false;",
                "+        }",
                "+        int executorId = lookupMetadataString(KeyType.EXEC_ID_STRING, metric.getExecutorId(), localLookupCache);",
                "+        if (INVALID_METADATA_STRING_ID == executorId) {",
                "+            return false;",
                "+        }",
                "+        int hostId = lookupMetadataString(KeyType.HOST_STRING, metric.getHostname(), localLookupCache);",
                "+        if (INVALID_METADATA_STRING_ID == hostId) {",
                "+            return false;",
                "+        }",
                "+        int streamId = lookupMetadataString(KeyType.STREAM_ID_STRING, metric.getStreamId(), localLookupCache);",
                "+        if (INVALID_METADATA_STRING_ID == streamId) {",
                "+            return false;",
                "+        }",
                "+",
                "+        RocksDbKey key = RocksDbKey.createMetricKey(metric.getAggLevel(), topologyId, metric.getTimestamp(), metricId,",
                "+                componentId, executorId, hostId, metric.getPort(), streamId);",
                "+",
                "+        return populateFromKey(key, metric);",
                "+    }",
                "+",
                "+    // populate metric values using the provided key",
                "+    boolean populateFromKey(RocksDbKey key, Metric metric) throws MetricException {",
                "+        try {",
                "+            byte[] value = db.get(key.getRaw());",
                "+            if (value == null) {",
                "+                return false;",
                "+            }",
                "+            RocksDbValue rdbValue = new RocksDbValue(value);",
                "+            rdbValue.populateMetric(metric);",
                "+        } catch (Exception e) {",
                "+            String message = \"Failed to populate metric\";",
                "+            LOG.error(message, e);",
                "+            if (this.failureMeter != null) {",
                "+                this.failureMeter.mark();",
                "+            }",
                "+            throw new MetricException(message, e);",
                "+        }",
                "+        return true;",
                "+    }",
                "+",
                "+    // attempts to lookup the unique Id for a string that may not exist yet.  Returns INVALID_METADATA_STRING_ID",
                "+    // if it does not exist.",
                "+    private int lookupMetadataString(KeyType type, String s, Map<String, Integer> lookupCache) throws MetricException {",
                "+        if (s == null) {",
                "+            if (this.failureMeter != null) {",
                "+                this.failureMeter.mark();",
                "+            }",
                "+            throw new MetricException(\"No string for metric metadata string type \" + type);",
                "+        }",
                "+",
                "+        // attempt to find it in the string cache, this will update the LRU",
                "+        StringMetadata stringMetadata = readOnlyStringMetadataCache.get(s);",
                "+        if (stringMetadata != null) {",
                "+            return stringMetadata.getStringId();",
                "+        }",
                "+",
                "+        // attempt to find it in callers cache",
                "+        Integer id = lookupCache.get(s);",
                "+        if (id != null) {",
                "+            return id;",
                "+        }",
                "+",
                "+        // attempt to find the string in the database",
                "+        stringMetadata = rocksDbGetStringMetadata(type, s);",
                "+        if (stringMetadata != null) {",
                "+            id = stringMetadata.getStringId();",
                "+",
                "+            // add to the callers cache.  We can't add it to the stringMetadataCache, since that could cause an eviction",
                "+            // database write, which we want to only occur from the inserting DB thread.",
                "+            lookupCache.put(s, id);",
                "+",
                "+            return id;",
                "+        }",
                "+",
                "+        // string does not exist",
                "+        return INVALID_METADATA_STRING_ID;",
                "+    }",
                "+",
                "+    // scans the database to look for a metadata string and returns the metadata info",
                "+    StringMetadata rocksDbGetStringMetadata(KeyType type, String s) {",
                "+        RocksDbKey firstKey = RocksDbKey.getInitialKey(type);",
                "+        RocksDbKey lastKey = RocksDbKey.getLastKey(type);",
                "+        final AtomicReference<StringMetadata> reference = new AtomicReference<>();",
                "+        scanRange(firstKey, lastKey, (key, value) -> {",
                "+            if (s.equals(value.getMetdataString())) {",
                "+                reference.set(value.getStringMetadata(key));",
                "+                return false;",
                "+            } else {",
                "+                return true;  // haven't found string, keep searching",
                "+            }",
                "+        });",
                "+        return reference.get();",
                "+    }",
                "+",
                "+    // scans from key start to the key before end, calling back until callback indicates not to process further",
                "+    void scanRange(RocksDbKey start, RocksDbKey end, RocksDbScanCallback fn) {",
                "+        try (ReadOptions ro = new ReadOptions()) {",
                "+            ro.setTotalOrderSeek(true);",
                "+            RocksIterator iterator = db.newIterator(ro);",
                "+            for (iterator.seek(start.getRaw()); iterator.isValid(); iterator.next()) {",
                "+                RocksDbKey key = new RocksDbKey(iterator.key());",
                "+                if (key.compareTo(end) >= 0) { // past limit, quit",
                "+                    return;",
                "+                }",
                "+",
                "+                RocksDbValue val = new RocksDbValue(iterator.value());",
                "+                if (!fn.cb(key, val)) {",
                "+                    // if cb returns false, we are done with this section of rows",
                "+                    return;",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Shutdown the store.",
                "+     */",
                "+    @Override",
                "+    public void close() {",
                "+        metricsWriter.close();",
                "+        metricsCleaner.close();",
                "+    }",
                "+",
                "+    /**",
                "+     *  Scans all metrics in the store and returns the ones matching the specified filtering options.",
                "+     *  Callback returns Metric class results.",
                "+     *",
                "+     * @param filter   options to filter by",
                "+     * @param scanCallback  callback for each Metric found",
                "+     * @throws MetricException  on error",
                "+     */",
                "+    public void scan(FilterOptions filter, ScanCallback scanCallback) throws MetricException {",
                "+        scanInternal(filter, scanCallback, null);",
                "+    }",
                "+",
                "+    /**",
                "+     *  Scans all metrics in the store and returns the ones matching the specified filtering options.",
                "+     *  Callback returns raw key/value data.",
                "+     *",
                "+     * @param filter   options to filter by",
                "+     * @param rawCallback  callback for each Metric found",
                "+     * @throws MetricException  on error",
                "+     */",
                "+    private void scanRaw(FilterOptions filter, RocksDbScanCallback rawCallback) throws MetricException {",
                "+        scanInternal(filter, null, rawCallback);",
                "+    }",
                "+",
                "+    // perform a scan given filter options, and return results in either Metric or raw data.",
                "+    private void scanInternal(FilterOptions filter, ScanCallback scanCallback, RocksDbScanCallback rawCallback) throws MetricException {",
                "+",
                "+        Map<String, Integer> stringToIdCache = new HashMap<>();",
                "+        Map<Integer, String> idToStringCache = new HashMap<>();",
                "+",
                "+        int startTopologyId = 0;",
                "+        int endTopologyId = 0xFFFFFFFF;",
                "+        String filterTopologyId = filter.getTopologyId();",
                "+        if (filterTopologyId != null) {",
                "+            int topologyId = lookupMetadataString(KeyType.TOPOLOGY_STRING, filterTopologyId, stringToIdCache);",
                "+            if (INVALID_METADATA_STRING_ID == topologyId) {",
                "+                return;  // string does not exist in database",
                "+            }",
                "+            startTopologyId = topologyId;",
                "+            endTopologyId = topologyId;",
                "+        }",
                "+",
                "+        long startTime = filter.getStartTime();",
                "+        long endTime = filter.getEndTime();",
                "+",
                "+        int startMetricId = 0;",
                "+        int endMetricId = 0xFFFFFFFF;",
                "+        String filterMetricName = filter.getMetricName();",
                "+        if (filterMetricName != null) {",
                "+            int metricId = lookupMetadataString(KeyType.METRIC_STRING, filterMetricName, stringToIdCache);",
                "+            if (INVALID_METADATA_STRING_ID == metricId) {",
                "+                return;  // string does not exist in database",
                "+            }",
                "+            startMetricId = metricId;",
                "+            endMetricId = metricId;",
                "+        }",
                "+",
                "+        int startComponentId = 0;",
                "+        int endComponentId = 0xFFFFFFFF;",
                "+        String filterComponentId = filter.getComponentId();",
                "+        if (filterComponentId != null) {",
                "+            int componentId = lookupMetadataString(KeyType.COMPONENT_STRING, filterComponentId, stringToIdCache);",
                "+            if (INVALID_METADATA_STRING_ID == componentId) {",
                "+                return;  // string does not exist in database",
                "+            }",
                "+            startComponentId = componentId;",
                "+            endComponentId = componentId;",
                "+        }",
                "+",
                "+        int startExecutorId = 0;",
                "+        int endExecutorId = 0xFFFFFFFF;",
                "+        String filterExecutorName = filter.getExecutorId();",
                "+        if (filterExecutorName != null) {",
                "+            int executorId = lookupMetadataString(KeyType.EXEC_ID_STRING, filterExecutorName, stringToIdCache);",
                "+            if (INVALID_METADATA_STRING_ID == executorId) {",
                "+                return;  // string does not exist in database",
                "+            }",
                "+            startExecutorId = executorId;",
                "+            endExecutorId = executorId;",
                "+        }",
                "+",
                "+        int startHostId = 0;",
                "+        int endHostId = 0xFFFFFFFF;",
                "+        String filterHostId = filter.getHostId();",
                "+        if (filterHostId != null) {",
                "+            int hostId = lookupMetadataString(KeyType.HOST_STRING, filterHostId, stringToIdCache);",
                "+            if (INVALID_METADATA_STRING_ID == hostId) {",
                "+                return;  // string does not exist in database",
                "+            }",
                "+            startHostId = hostId;",
                "+            endHostId = hostId;",
                "+        }",
                "+",
                "+        int startPort = 0;",
                "+        int endPort = 0xFFFFFFFF;",
                "+        Integer filterPort = filter.getPort();",
                "+        if (filterPort != null) {",
                "+            startPort = filterPort;",
                "+            endPort = filterPort;",
                "+        }",
                "+",
                "+        int startStreamId = 0;",
                "+        int endStreamId = 0xFFFFFFFF;",
                "+        String filterStreamId = filter.getStreamId();",
                "+        if (filterStreamId != null) {",
                "+            int streamId = lookupMetadataString(KeyType.HOST_STRING, filterStreamId, stringToIdCache);",
                "+            if (INVALID_METADATA_STRING_ID == streamId) {",
                "+                return;  // string does not exist in database",
                "+            }",
                "+            startStreamId = streamId;",
                "+            endStreamId = streamId;",
                "+        }",
                "+",
                "+        ReadOptions ro = new ReadOptions();",
                "+        ro.setTotalOrderSeek(true);",
                "+",
                "+        for (AggLevel aggLevel : filter.getAggLevels()) {",
                "+",
                "+            RocksDbKey startKey = RocksDbKey.createMetricKey(aggLevel, startTopologyId, startTime, startMetricId,",
                "+                    startComponentId, startExecutorId, startHostId, startPort, startStreamId);",
                "+            RocksDbKey endKey = RocksDbKey.createMetricKey(aggLevel, endTopologyId, endTime, endMetricId,",
                "+                    endComponentId, endExecutorId, endHostId, endPort, endStreamId);",
                "+",
                "+            RocksIterator iterator = db.newIterator(ro);",
                "+            for (iterator.seek(startKey.getRaw()); iterator.isValid(); iterator.next()) {",
                "+                RocksDbKey key = new RocksDbKey(iterator.key());",
                "+",
                "+                if (key.compareTo(endKey) > 0) { // past limit, quit",
                "+                    break;",
                "+                }",
                "+",
                "+                if (startTopologyId != 0 && key.getTopologyId() != startTopologyId) {",
                "+                    continue;",
                "+                }",
                "+",
                "+                long timestamp = key.getTimestamp();",
                "+                if (timestamp < startTime || timestamp > endTime) {",
                "+                    continue;",
                "+                }",
                "+",
                "+                if (startMetricId != 0 && key.getMetricId() != startMetricId) {",
                "+                    continue;",
                "+                }",
                "+",
                "+                if (startComponentId != 0 && key.getComponentId() != startComponentId) {",
                "+                    continue;",
                "+                }",
                "+",
                "+                if (startExecutorId != 0 && key.getExecutorId() != startExecutorId) {",
                "+                    continue;",
                "+                }",
                "+",
                "+                if (startHostId != 0 && key.getHostnameId() != startHostId) {",
                "+                    continue;",
                "+                }",
                "+",
                "+                if (startPort != 0 && key.getPort() != startPort) {",
                "+                    continue;",
                "+                }",
                "+",
                "+                if (startStreamId != 0 && key.getStreamId() != startStreamId) {",
                "+                    continue;",
                "+                }",
                "+",
                "+                RocksDbValue val = new RocksDbValue(iterator.value());",
                "+",
                "+                if (scanCallback != null) {",
                "+                    try {",
                "+                        // populate a metric",
                "+                        String metricName = metadataIdToString(KeyType.METRIC_STRING, key.getMetricId(), idToStringCache);",
                "+                        String topologyId = metadataIdToString(KeyType.TOPOLOGY_STRING, key.getTopologyId(), idToStringCache);",
                "+                        String componentId = metadataIdToString(KeyType.COMPONENT_STRING, key.getComponentId(), idToStringCache);",
                "+                        String executorId = metadataIdToString(KeyType.EXEC_ID_STRING, key.getExecutorId(), idToStringCache);",
                "+                        String hostname = metadataIdToString(KeyType.HOST_STRING, key.getHostnameId(), idToStringCache);",
                "+                        String streamId = metadataIdToString(KeyType.STREAM_ID_STRING, key.getStreamId(), idToStringCache);",
                "+",
                "+                        Metric metric = new Metric(metricName, timestamp, topologyId, 0.0, componentId, executorId, hostname,",
                "+                                streamId, key.getPort(), aggLevel);",
                "+",
                "+                        val.populateMetric(metric);",
                "+",
                "+                        // callback to caller",
                "+                        scanCallback.cb(metric);",
                "+                    } catch (MetricException e) {",
                "+                        LOG.warn(\"Failed to report found metric: {}\", e.getMessage());",
                "+                    }",
                "+                } else {",
                "+                    if (!rawCallback.cb(key, val)) {",
                "+                        return;",
                "+                    }",
                "+                }",
                "+            }",
                "+            iterator.close();",
                "+        }",
                "+        ro.close();",
                "+    }",
                "+",
                "+    // Finds the metadata string that matches the string Id and type provided.  The string should exist, as it is",
                "+    // referenced from a metric.",
                "+    private String metadataIdToString(KeyType type, int id, Map<Integer, String> lookupCache) throws MetricException {",
                "+        String s = readOnlyStringMetadataCache.getMetadataString(id);",
                "+        if (s != null) {",
                "+            return s;",
                "+        }",
                "+        s = lookupCache.get(id);",
                "+        if (s != null) {",
                "+            return s;",
                "+        }",
                "+        // get from DB and add to lookup cache",
                "+        RocksDbKey key = new RocksDbKey(type, id);",
                "+        try {",
                "+            byte[] value = db.get(key.getRaw());",
                "+            if (value == null) {",
                "+                throw new MetricException(\"Failed to find metadata string for id \" + id + \" of type \" + type);",
                "+            }",
                "+            RocksDbValue rdbValue = new RocksDbValue(value);",
                "+            s = rdbValue.getMetdataString();",
                "+            lookupCache.put(id, s);",
                "+            return s;",
                "+        }  catch (RocksDBException e) {",
                "+            if (this.failureMeter != null) {",
                "+                this.failureMeter.mark();",
                "+            }",
                "+            throw new MetricException(\"Failed to get from RocksDb\", e);",
                "+        }",
                "+    }",
                "+",
                "+    // deletes metrics matching the filter options",
                "+    void deleteMetrics(FilterOptions filter) throws MetricException {",
                "+        try (WriteBatch writeBatch = new WriteBatch();",
                "+             WriteOptions writeOps = new WriteOptions()) {",
                "+",
                "+            scanRaw(filter, (RocksDbKey key, RocksDbValue value) -> {",
                "+                writeBatch.remove(key.getRaw());",
                "+                return true;",
                "+            });",
                "+",
                "+            if (writeBatch.count() > 0) {",
                "+                LOG.info(\"Deleting {} metrics\", writeBatch.count());",
                "+                try {",
                "+                    db.write(writeOps, writeBatch);",
                "+                } catch (Exception e) {",
                "+                    String message = \"Failed delete metrics\";",
                "+                    LOG.error(message, e);",
                "+                    if (this.failureMeter != null) {",
                "+                        this.failureMeter.mark();",
                "+                    }",
                "+                    throw new MetricException(message, e);",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    // deletes metadata strings before the provided timestamp",
                "+    void deleteMetadataBefore(long firstValidTimestamp) throws MetricException {",
                "+        if (firstValidTimestamp < 1L) {",
                "+            if (this.failureMeter != null) {",
                "+                this.failureMeter.mark();",
                "+            }",
                "+            throw new MetricException(\"Invalid timestamp for deleting metadata: \" + firstValidTimestamp);",
                "+        }",
                "+",
                "+        try (WriteBatch writeBatch = new WriteBatch();",
                "+             WriteOptions writeOps = new WriteOptions()) {",
                "+",
                "+            // search all metadata strings",
                "+            RocksDbKey topologyMetadataPrefix = RocksDbKey.getPrefix(KeyType.METADATA_STRING_START);",
                "+            RocksDbKey lastPrefix = RocksDbKey.getPrefix(KeyType.METADATA_STRING_END);",
                "+            scanRange(topologyMetadataPrefix, lastPrefix, (key, value) -> {",
                "+                // we'll assume the metadata was recently used if still in the cache.",
                "+                if (!readOnlyStringMetadataCache.contains(key.getMetadataStringId())) {",
                "+                    if (value.getLastTimestamp() < firstValidTimestamp) {",
                "+                        writeBatch.remove(key.getRaw());",
                "+                    }",
                "+                }",
                "+                return true;",
                "+            });",
                "+",
                "+            if (writeBatch.count() > 0) {",
                "+                LOG.info(\"Deleting {} metadata strings\", writeBatch.count());",
                "+                try {",
                "+                    db.write(writeOps, writeBatch);",
                "+                } catch (Exception e) {",
                "+                    String message = \"Failed delete metadata strings\";",
                "+                    LOG.error(message, e);",
                "+                    if (this.failureMeter != null) {",
                "+                        this.failureMeter.mark();",
                "+                    }",
                "+                    throw new MetricException(message, e);",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+}",
                "+",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbValue.java b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbValue.java",
                "new file mode 100644",
                "index 000000000..58b2c76c4",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbValue.java",
                "@@ -0,0 +1,144 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore.rocksdb;",
                "+",
                "+import java.nio.ByteBuffer;",
                "+import org.apache.storm.metricstore.Metric;",
                "+",
                "+",
                "+/**",
                "+ * Class representing the data used as a Value in RocksDB.  Values can be used either for metadata or metrics.",
                "+ *",
                "+ * <p>Formats for Metadata String values are:",
                "+ *",
                "+ * <pre>",
                "+ * Field             Size         Offset",
                "+ *",
                "+ * Version              1              0      The current metadata version - allows migrating if the format changes in the future",
                "+ * Timestamp            8              1      The time when the metadata was last used by a metric.  Allows deleting of old metadata.",
                "+ * Metadata String    any              9      The metadata string",
                "+ *</pre>",
                "+ *",
                "+ * <p>Formats for Metric values are:",
                "+ *",
                "+ * <pre>",
                "+ * Field             Size         Offset",
                "+ *",
                "+ * Version              1              0      The current metric version - allows migrating if the format changes in the future",
                "+ * Value                8              1      The metric value",
                "+ * Count                8              9      The metric count",
                "+ * Min                  8             17      The minimum metric value",
                "+ * Max                  8             25      The maximum metric value",
                "+ * Sum                  8             33      The sum of the metric values",
                "+ * </pre>",
                "+ */",
                "+",
                "+class RocksDbValue {",
                "+    private static int METRIC_VALUE_SIZE = 41;",
                "+    private byte[] value;",
                "+    private static final byte CURRENT_METADATA_VERSION = 0;",
                "+    private static final byte CURRENT_METRIC_VERSION = 0;",
                "+    private static int MIN_METADATA_VALUE_SIZE = 9;",
                "+",
                "+    /**",
                "+     * Constructor from raw data.",
                "+     *",
                "+     * @param value  the raw bytes representing the key",
                "+     */",
                "+    RocksDbValue(byte[] value) {",
                "+        this.value = value;",
                "+    }",
                "+",
                "+    /**",
                "+     * Constructor for a metadata string.",
                "+     *",
                "+     * @param lastTimestamp  the last timestamp when the string was used",
                "+     * @param metadataString   the metadata string",
                "+     */",
                "+    RocksDbValue(long lastTimestamp, String metadataString) {",
                "+        this.value = new byte[MIN_METADATA_VALUE_SIZE + metadataString.length()];",
                "+        ByteBuffer bb = ByteBuffer.wrap(value);",
                "+        bb.put(CURRENT_METADATA_VERSION);",
                "+        bb.putLong(lastTimestamp);",
                "+        bb.put(metadataString.getBytes());",
                "+    }",
                "+",
                "+    /**",
                "+     * Constructor for a metric.",
                "+     *",
                "+     * @param m  the metric to create a value from",
                "+     */",
                "+    RocksDbValue(Metric m) {",
                "+        this.value = new byte[METRIC_VALUE_SIZE];",
                "+        ByteBuffer bb = ByteBuffer.wrap(value);",
                "+        bb.put(CURRENT_METRIC_VERSION);",
                "+        bb.putDouble(m.getValue());",
                "+        bb.putLong(m.getCount());",
                "+        bb.putDouble(m.getMin());",
                "+        bb.putDouble(m.getMax());",
                "+        bb.putDouble(m.getSum());",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the metadata string portion of the value.  Assumes the value is metadata.",
                "+     *",
                "+     * @return   the metadata string",
                "+     */",
                "+    String getMetdataString() {",
                "+        if (this.value.length < MIN_METADATA_VALUE_SIZE) {",
                "+            throw new RuntimeException(\"RocksDB value is too small to be a metadata string!\");",
                "+        }",
                "+        return new String(this.value, 9, this.value.length - 9);",
                "+    }",
                "+",
                "+    /**",
                "+     * Gets StringMetadata associated with the key/value pair.",
                "+     */",
                "+    StringMetadata getStringMetadata(RocksDbKey key) {",
                "+        return new StringMetadata(key.getType(), key.getMetadataStringId(), this.getLastTimestamp());",
                "+    }",
                "+",
                "+    /**",
                "+     * Gets the last time a metadata string was used.",
                "+     */",
                "+    long getLastTimestamp() {",
                "+        return ByteBuffer.wrap(value, 1, 8).getLong();",
                "+    }",
                "+",
                "+    /**",
                "+     * get the raw value bytes",
                "+     */",
                "+    byte[] getRaw() {",
                "+        return this.value;",
                "+    }",
                "+",
                "+    /**",
                "+     * populate metric values from the raw data.",
                "+     */",
                "+    void populateMetric(Metric metric) {",
                "+        ByteBuffer bb = ByteBuffer.wrap(this.value, 0, METRIC_VALUE_SIZE);",
                "+        bb.get();  // version",
                "+        metric.setValue(bb.getDouble());",
                "+        metric.setCount(bb.getLong());",
                "+        metric.setMin(bb.getDouble());",
                "+        metric.setMax(bb.getDouble());",
                "+        metric.setSum(bb.getDouble());",
                "+    }",
                "+",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/StringMetadata.java b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/StringMetadata.java",
                "new file mode 100644",
                "index 000000000..6f54a5883",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/StringMetadata.java",
                "@@ -0,0 +1,78 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore.rocksdb;",
                "+",
                "+import java.util.ArrayList;",
                "+import java.util.List;",
                "+",
                "+/**",
                "+ * Class that contains the information associated with a metadata string that remains cached in memory.",
                "+ */",
                "+class StringMetadata {",
                "+    private List<KeyType> types = new ArrayList<>(1);  // its possible a string is used by multiple types of metadata strings",
                "+    private int stringId;",
                "+    private long lastTimestamp;",
                "+",
                "+    /**",
                "+     * Constructor for StringMetadata.",
                "+     *",
                "+     * @param metadataType   the type of metadata string",
                "+     * @param stringId       the unique id for the metadata string",
                "+     * @param lastTimestamp   the timestamp when the metric used the metadata string",
                "+     */",
                "+    StringMetadata(KeyType metadataType, Integer stringId, Long lastTimestamp) {",
                "+        this.types.add(metadataType);",
                "+        this.stringId = stringId;",
                "+        this.lastTimestamp = lastTimestamp;",
                "+    }",
                "+",
                "+    int getStringId() {",
                "+        return this.stringId;",
                "+    }",
                "+",
                "+    long getLastTimestamp() {",
                "+        return this.lastTimestamp;",
                "+    }",
                "+",
                "+    List<KeyType> getMetadataTypes() {",
                "+        return this.types;",
                "+    }",
                "+",
                "+    private void addKeyType(KeyType type) {",
                "+        if (!this.types.contains(type)) {",
                "+            this.types.add(type);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Updates the timestamp of when a metadata string was last used.  Adds the type of the string if it is a new",
                "+     * type.",
                "+     *",
                "+     * @param metricTimestamp   the timestamp of the metric using the metadata string",
                "+     * @param type    the type of metadata string for the metric",
                "+     */",
                "+    void update(Long metricTimestamp, KeyType type) {",
                "+        if (metricTimestamp > this.lastTimestamp) {",
                "+            this.lastTimestamp = metricTimestamp;",
                "+        }",
                "+        addKeyType(type);",
                "+    }",
                "+",
                "+}",
                "+",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/StringMetadataCache.java b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/StringMetadataCache.java",
                "new file mode 100644",
                "index 000000000..7ce843548",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/StringMetadataCache.java",
                "@@ -0,0 +1,202 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore.rocksdb;",
                "+",
                "+import java.util.Collections;",
                "+import java.util.Map;",
                "+import java.util.Set;",
                "+import java.util.concurrent.ConcurrentHashMap;",
                "+import org.apache.storm.metricstore.MetricException;",
                "+import org.apache.storm.utils.LruMap;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * Class to create a use a cache that stores Metadata string information in memory.  It allows searching for a",
                "+ * Metadata string's unique id, or looking up the string by the unique id.  The StringMetadata is stored in an",
                "+ * LRU map.  When an entry is added to the cache, an older entry may be evicted, which then needs to be",
                "+ * immediately stored to the database to provide a consistent view of all the metadata strings.",
                "+ *",
                "+ * <p>All write operations adding metadata to RocksDB are done by a single thread (a RocksDbMetricsWriter),",
                "+ * but multiple threads can read values from the cache. To clarify which permissions are accessible by various",
                "+ * threads, the ReadOnlyStringMetadataCache and WritableStringMetadataCache are provided to be used.",
                "+ */",
                "+",
                "+public class StringMetadataCache implements LruMap.CacheEvictionCallback<String, StringMetadata>,",
                "+        WritableStringMetadataCache, ReadOnlyStringMetadataCache {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(StringMetadataCache.class);",
                "+    private Map<String, StringMetadata> lruStringCache;",
                "+    private Map<Integer, String> hashToString = new ConcurrentHashMap<>();",
                "+    private RocksDbMetricsWriter dbWriter;",
                "+    private static StringMetadataCache instance = null;",
                "+",
                "+    /**",
                "+     * Initializes the cache instance.",
                "+     *",
                "+     * @param dbWriter   the RocksDB writer instance to handle writing evicted cache data",
                "+     * @param capacity   the number of StringMetadata instances to hold in memory",
                "+     * @throws MetricException   if creating multiple cache instances",
                "+     */",
                "+    static void init(RocksDbMetricsWriter dbWriter, int capacity) throws MetricException {",
                "+        if (instance == null) {",
                "+            instance = new StringMetadataCache(dbWriter, capacity);",
                "+        } else {",
                "+            throw new MetricException(\"StringMetadataCache already created\");",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Provides the WritableStringMetadataCache interface to the cache instance.",
                "+     *",
                "+     * @throws MetricException  if the cache instance was not created",
                "+     */",
                "+    static WritableStringMetadataCache getWritableStringMetadataCache() throws MetricException {",
                "+        if (instance != null) {",
                "+            return instance;",
                "+        } else {",
                "+            throw new MetricException(\"StringMetadataCache was not initialized\");",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Provides the ReadOnlyStringMetadataCache interface to the cache instance.",
                "+     *",
                "+     * @throws MetricException  if the cache instance was not created",
                "+     */",
                "+    static ReadOnlyStringMetadataCache getReadOnlyStringMetadataCache() throws MetricException {",
                "+        if (instance != null) {",
                "+            return instance;",
                "+        } else {",
                "+            throw new MetricException(\"StringMetadataCache was not initialized\");",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Constructor to create a cache.",
                "+     *",
                "+     * @param dbWriter   The rocks db writer instance the cache should use when evicting data",
                "+     * @param capacity  The cache size",
                "+     */",
                "+    private StringMetadataCache(RocksDbMetricsWriter dbWriter, int capacity) {",
                "+        lruStringCache = Collections.synchronizedMap(new LruMap<>(capacity, this));",
                "+        this.dbWriter = dbWriter;",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the string metadata from the cache.",
                "+     *",
                "+     * @param s   The string to look for",
                "+     * @return   the metadata associated with the string or null if not found",
                "+     */",
                "+    public StringMetadata get(String s) {",
                "+        return lruStringCache.get(s);",
                "+    }",
                "+",
                "+    /**",
                "+     * Add the string metadata to the cache.",
                "+     *",
                "+     * NOTE: this can cause data to be evicted from the cache when full.  When this occurs, the evictionCallback() method",
                "+     * is called to store the metadata back into the RocksDB database.",
                "+     *",
                "+     * This method is only exposed to the WritableStringMetadataCache interface.",
                "+     *",
                "+     * @param s   The string to add",
                "+     * @param stringMetadata  The string's metadata",
                "+     * @param newEntry   Indicates the metadata is being used for the first time and should be written to RocksDB immediately",
                "+     * @throws MetricException   when evicted data fails to save to the database or when the database is shutdown",
                "+     */",
                "+    public void put(String s, StringMetadata stringMetadata, boolean newEntry) throws MetricException {",
                "+        if (dbWriter.isShutdown()) {",
                "+            // another thread could be writing out the metadata cache to the database.",
                "+            throw new MetricException(\"Shutting down\");",
                "+        }",
                "+        try {",
                "+            if (newEntry) {",
                "+                writeMetadataToDisk(s, stringMetadata);",
                "+            }",
                "+            lruStringCache.put(s, stringMetadata);",
                "+            hashToString.put(stringMetadata.getStringId(), s);",
                "+        } catch (Exception e) { // catch any runtime exceptions caused by eviction",
                "+            throw new MetricException(\"Failed to save string in metadata cache\", e);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Callback when data is about to be removed from the cache.  This method then",
                "+     * immediately writes the metadata to RocksDB.",
                "+     *",
                "+     * @param key   The evicted string",
                "+     * @param val  The evicted string's metadata",
                "+     * @throws RuntimeException   when evicted data fails to save to the database",
                "+     */",
                "+    public void evictionCallback(String key, StringMetadata val) {",
                "+        writeMetadataToDisk(key, val);",
                "+    }",
                "+",
                "+    private void writeMetadataToDisk(String key, StringMetadata val) {",
                "+        if (LOG.isDebugEnabled()) {",
                "+            LOG.debug(\"Writing {} to RocksDB\", key);",
                "+        }",
                "+        // remove reverse lookup from map",
                "+        hashToString.remove(val.getStringId());",
                "+",
                "+        // save the evicted key/value to the database immediately",
                "+        RocksDbValue rval = new RocksDbValue(val.getLastTimestamp(), key);",
                "+",
                "+        for (KeyType type : val.getMetadataTypes()) { // save the metadata for all types of strings it matches",
                "+            RocksDbKey rkey = new RocksDbKey(type, val.getStringId());",
                "+            dbWriter.handleEvictedMetadata(rkey, rval);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Determines if a string Id is contained in the cache.",
                "+     *",
                "+     * @param stringId   The string Id to check",
                "+     * @return   true if the Id is in the cache, false otherwise",
                "+     */",
                "+    public boolean contains(Integer stringId) {",
                "+        return hashToString.containsKey(stringId);",
                "+    }",
                "+",
                "+    /**",
                "+     * Returns the string matching the string Id if in the cache.",
                "+     *",
                "+     * @param stringId   The string Id to check",
                "+     * @return   the associated string if the Id is in the cache, null otherwise",
                "+     */",
                "+    public String getMetadataString(Integer stringId) {",
                "+        return hashToString.get(stringId);",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the map of the cache contents.  Provided to allow writing the data to RocksDB on shutdown.",
                "+     *",
                "+     * @return   the string metadata map entrySet",
                "+     */",
                "+    public Set<Map.Entry<String, StringMetadata>> entrySet() {",
                "+        return lruStringCache.entrySet();",
                "+    }",
                "+",
                "+    static void cleanUp() {",
                "+        instance = null;",
                "+    }",
                "+",
                "+}",
                "+",
                "diff --git a/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/WritableStringMetadataCache.java b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/WritableStringMetadataCache.java",
                "new file mode 100644",
                "index 000000000..2d4165f09",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/WritableStringMetadataCache.java",
                "@@ -0,0 +1,54 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing,",
                "+ * software distributed under the License is distributed on an",
                "+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY",
                "+ * KIND, either express or implied.  See the License for the",
                "+ * specific language governing permissions and limitations",
                "+ * under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.metricstore.rocksdb;",
                "+",
                "+import java.util.Map;",
                "+import java.util.Set;",
                "+",
                "+import org.apache.http.annotation.NotThreadSafe;",
                "+import org.apache.storm.metricstore.MetricException;",
                "+",
                "+/**",
                "+ * The writable interface to a StringMetadataCache intended to be used by a single RocksDBMetricwWriter instance.",
                "+ */",
                "+@NotThreadSafe",
                "+public interface WritableStringMetadataCache extends ReadOnlyStringMetadataCache {",
                "+",
                "+    /**",
                "+     * Add the string metadata to the cache.",
                "+     *",
                "+     * * NOTE: this can cause data to be evicted from the cache when full.  When this occurs, the evictionCallback() method",
                "+     * is called to store the metadata back into the RocksDB database.",
                "+     *",
                "+     * This method is only exposed to the WritableStringMetadataCache interface.",
                "+     *",
                "+     * @param s   The string to add",
                "+     * @param stringMetadata  The string's metadata",
                "+     * @param newEntry   Indicates the metadata is being used for the first time and should be written to RocksDB immediately",
                "+     * @throws MetricException   when evicted data fails to save to the database or when the database is shutdown",
                "+     */",
                "+    void put(String s, StringMetadata stringMetadata, boolean newEntry) throws MetricException;",
                "+",
                "+    /**",
                "+     * Get the map of the cache contents.  Provided to allow writing the data to RocksDB on shutdown.",
                "+     *",
                "+     * @return   the string metadata map entrySet",
                "+     */",
                "+    Set<Map.Entry<String, StringMetadata>> entrySet();",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/utils/LruMap.java b/storm-server/src/main/java/org/apache/storm/utils/LruMap.java",
                "new file mode 100644",
                "index 000000000..3ed5d06f9",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/utils/LruMap.java",
                "@@ -0,0 +1,56 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.utils;",
                "+",
                "+import java.util.LinkedHashMap;",
                "+import java.util.Map;",
                "+",
                "+public class LruMap<A, B> extends LinkedHashMap<A, B> {",
                "+    private int maxSize;",
                "+    private CacheEvictionCallback evCb = null;",
                "+",
                "+    public LruMap(int maxSize) {",
                "+        super(maxSize + 1, 1.0f, true);",
                "+        this.maxSize = maxSize;",
                "+    }",
                "+",
                "+    /**",
                "+     * Creates an LRU map that will call back before data is removed from the map.",
                "+     *",
                "+     * @param maxSize   max capacity for the map",
                "+     * @param evictionCallback   callback to be called before removing data",
                "+     */",
                "+    public LruMap(int maxSize, CacheEvictionCallback evictionCallback) {",
                "+        this(maxSize);",
                "+        this.evCb = evictionCallback;",
                "+    }",
                "+    ",
                "+    @Override",
                "+    protected boolean removeEldestEntry(final Map.Entry<A, B> eldest) {",
                "+        boolean evict = size() > this.maxSize;",
                "+        if (evict && this.evCb != null) {",
                "+            this.evCb.evictionCallback(eldest.getKey(), eldest.getValue());",
                "+        }",
                "+        return evict;",
                "+    }",
                "+",
                "+    public interface CacheEvictionCallback<K, V> {",
                "+        void evictionCallback(K key, V val);",
                "+    }",
                "+}"
            ],
            "changed_files": [
                "conf/defaults.yaml",
                "pom.xml",
                "storm-client/src/jvm/org/apache/storm/generated/Assignment.java",
                "storm-client/src/jvm/org/apache/storm/generated/ClusterWorkerHeartbeat.java",
                "storm-client/src/jvm/org/apache/storm/generated/HBNodes.java",
                "storm-client/src/jvm/org/apache/storm/generated/HBRecords.java",
                "storm-client/src/jvm/org/apache/storm/generated/LSApprovedWorkers.java",
                "storm-client/src/jvm/org/apache/storm/generated/LSSupervisorAssignments.java",
                "storm-client/src/jvm/org/apache/storm/generated/LSTopoHistory.java",
                "storm-client/src/jvm/org/apache/storm/generated/LSTopoHistoryList.java",
                "storm-client/src/jvm/org/apache/storm/generated/LSWorkerHeartbeat.java",
                "storm-client/src/jvm/org/apache/storm/generated/LocalAssignment.java",
                "storm-client/src/jvm/org/apache/storm/generated/LocalStateData.java",
                "storm-client/src/jvm/org/apache/storm/generated/LogConfig.java",
                "storm-client/src/jvm/org/apache/storm/generated/Nimbus.java",
                "storm-client/src/jvm/org/apache/storm/generated/StormBase.java",
                "storm-client/src/jvm/org/apache/storm/generated/TopologyHistoryInfo.java",
                "storm-client/src/jvm/org/apache/storm/generated/WorkerMetricList.java",
                "storm-client/src/jvm/org/apache/storm/generated/WorkerMetricPoint.java",
                "storm-client/src/jvm/org/apache/storm/generated/WorkerMetrics.java",
                "storm-client/src/jvm/org/apache/storm/generated/WorkerResources.java",
                "storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java",
                "storm-client/src/jvm/org/apache/storm/utils/ConfigUtils.java",
                "storm-client/src/jvm/org/apache/storm/utils/Utils.java",
                "storm-client/src/py/storm/Nimbus-remote",
                "storm-client/src/py/storm/Nimbus.py",
                "storm-client/src/py/storm/ttypes.py",
                "storm-client/src/storm.thrift",
                "storm-server/pom.xml",
                "storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "storm-server/src/main/java/org/apache/storm/LocalCluster.java",
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "storm-server/src/main/java/org/apache/storm/metric/StormMetricsRegistry.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/AggLevel.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/FilterOptions.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/Metric.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/MetricException.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/MetricStore.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/MetricStoreConfig.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/KeyType.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/MetricsCleaner.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/ReadOnlyStringMetadataCache.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbKey.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbMetricsWriter.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbStore.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/RocksDbValue.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/StringMetadata.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/StringMetadataCache.java",
                "storm-server/src/main/java/org/apache/storm/metricstore/rocksdb/WritableStringMetadataCache.java",
                "storm-server/src/main/java/org/apache/storm/utils/LruMap.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2887": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2887",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "ed0548e8b61a8ea67007d906953406a264b15c99",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516826821,
            "hunks": 5,
            "message": "STORM-2910:  Override local nimbus client by default",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/LocalCluster.java b/storm-server/src/main/java/org/apache/storm/LocalCluster.java",
                "index 502f4541c..20a46a3ca 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/LocalCluster.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/LocalCluster.java",
                "@@ -370,2 +370,3 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "     private final SimulatedTime time;",
                "+    private final NimbusClient.LocalOverride nimbusOverride;",
                "@@ -478,2 +479,9 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "             }",
                "+            if (thriftServer == null) {",
                "+                //We don't want to override the client if there is a thrift server up and running, or we would not test any",
                "+                // Of the actual thrift code",
                "+                this.nimbusOverride = new NimbusClient.LocalOverride(this);",
                "+            } else {",
                "+                this.nimbusOverride = null;",
                "+            }",
                "             success = true;",
                "@@ -660,2 +668,5 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "     public synchronized void close() throws Exception {",
                "+        if (nimbusOverride != null) {",
                "+            nimbusOverride.close();",
                "+        }",
                "         if (nimbus != null) {",
                "@@ -1100,2 +1111,3 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "      * Run c with a local mode cluster overriding the NimbusClient and DRPCClient calls.",
                "+     * NOTE local mode override happens by default now unless netty is turned on for the local cluster.",
                "      * @param c the callable to run in this mode",
                "@@ -1108,3 +1120,2 @@ public class LocalCluster implements ILocalClusterTrackedTopologyAware, Iface {",
                "         try (LocalCluster local = new LocalCluster();",
                "-                NimbusClient.LocalOverride nimbusOverride = new NimbusClient.LocalOverride(local);",
                "                 LocalDRPC drpc = new LocalDRPC();"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/LocalCluster.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2910": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2910",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "4fd02caabe2415481c4a0c52d51638a022730f73",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513206351,
            "hunks": 14,
            "message": "STORM-2872: Fix for wouldFit and rebalance as part of GenericResourceAwareScheduling changes",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "index 39afa9ff4..9ab0c9365 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "@@ -470,3 +470,3 @@ public class Cluster implements ISchedulingState {",
                "         NormalizedResourceRequest requestedResources = td.getTotalResources(exec);",
                "-        if (!resourcesAvailable.couldHoldIgnoringMemory(requestedResources)) {",
                "+        if (!resourcesAvailable.couldHoldIgnoringSharedMemory(requestedResources)) {",
                "             return false;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "index f332da5ff..eea38cff9 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "@@ -20,4 +20,2 @@ package org.apache.storm.scheduler.resource;",
                "-import static org.apache.storm.Constants.*;",
                "-",
                " import com.google.common.annotations.VisibleForTesting;",
                "@@ -46,7 +44,7 @@ public abstract class NormalizedResources {",
                "         Map<String, String> tmp = new HashMap<>();",
                "-        tmp.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, COMMON_CPU_RESOURCE_NAME);",
                "-        tmp.put(Config.SUPERVISOR_CPU_CAPACITY, COMMON_CPU_RESOURCE_NAME);",
                "-        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "-        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "-        tmp.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, COMMON_TOTAL_MEMORY_RESOURCE_NAME);",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, Constants.COMMON_CPU_RESOURCE_NAME);",
                "+        tmp.put(Config.SUPERVISOR_CPU_CAPACITY, Constants.COMMON_CPU_RESOURCE_NAME);",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "+        tmp.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME);",
                "         RESOURCE_NAME_MAPPING = Collections.unmodifiableMap(tmp);",
                "@@ -58,6 +56,6 @@ public abstract class NormalizedResources {",
                "             //We are going to skip over CPU and Memory, because they are captured elsewhere",
                "-            if (!COMMON_CPU_RESOURCE_NAME.equals(key)",
                "-                && !COMMON_TOTAL_MEMORY_RESOURCE_NAME.equals(key)",
                "-                && !COMMON_OFFHEAP_MEMORY_RESOURCE_NAME.equals(key)",
                "-                && !COMMON_ONHEAP_MEMORY_RESOURCE_NAME.equals(key)) {",
                "+            if (!Constants.COMMON_CPU_RESOURCE_NAME.equals(key)",
                "+                && !Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME.equals(key)",
                "+                && !Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME.equals(key)",
                "+                && !Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME.equals(key)) {",
                "                 resourceNames.computeIfAbsent(key, (k) -> counter.getAndIncrement());",
                "@@ -233,3 +231,3 @@ public abstract class NormalizedResources {",
                "      */",
                "-    public boolean couldHoldIgnoringMemory(NormalizedResources other) {",
                "+    public boolean couldHoldIgnoringSharedMemory(NormalizedResources other) {",
                "         if (this.cpu < other.getTotalCpu()) {",
                "@@ -243,2 +241,6 @@ public abstract class NormalizedResources {",
                "         }",
                "+",
                "+        if (this.getTotalMemoryMb() < other.getTotalMemoryMb()) {",
                "+            return false;",
                "+        }",
                "         return true;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "index 83ee4cb6d..db9dbfabb 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "@@ -24,2 +24,3 @@ import java.util.Map;",
                " import org.apache.storm.Config;",
                "+import org.apache.storm.Constants;",
                " import org.apache.storm.generated.Bolt;",
                "@@ -34,2 +35,4 @@ import org.slf4j.LoggerFactory;",
                "+import static org.apache.storm.scheduler.resource.NormalizedResources.normalizedResourceMap;",
                "+",
                " public class ResourceUtils {",
                "@@ -94,3 +97,3 @@ public class ResourceUtils {",
                "                     ComponentCommon spoutCommon = spoutSpec.get_common();",
                "-                    Map<String, Double> resourcesUpdate = resourceUpdatesMap.get(spoutName);",
                "+                    Map<String, Double> resourcesUpdate = normalizedResourceMap(resourceUpdatesMap.get(spoutName));",
                "                     String newJsonConf = getJsonWithUpdatedResources(spoutCommon.get_json_conf(), resourcesUpdate);",
                "@@ -107,5 +110,5 @@ public class ResourceUtils {",
                "-                if(resourceUpdatesMap.containsKey(boltName)) {",
                "+                if (resourceUpdatesMap.containsKey(boltName)) {",
                "                     ComponentCommon boltCommon = boltObj.get_common();",
                "-                    Map<String, Double> resourcesUpdate = resourceUpdatesMap.get(boltName);",
                "+                    Map<String, Double> resourcesUpdate = normalizedResourceMap(resourceUpdatesMap.get(boltName));",
                "                     String newJsonConf = getJsonWithUpdatedResources(boltCommon.get_json_conf(), resourceUpdatesMap.get(boltName));",
                "@@ -126,2 +129,11 @@ public class ResourceUtils {",
                "+    public static String getCorrespondingLegacyResourceName(String normalizedResourceName) {",
                "+        for(Map.Entry<String, String> entry : NormalizedResources.RESOURCE_NAME_MAPPING.entrySet()) {",
                "+            if (entry.getValue().equals(normalizedResourceName)) {",
                "+                return entry.getKey();",
                "+            }",
                "+        }",
                "+        return normalizedResourceName;",
                "+    }",
                "+",
                "     public static String getJsonWithUpdatedResources(String jsonConf, Map<String, Double> resourceUpdates) {",
                "@@ -132,14 +144,17 @@ public class ResourceUtils {",
                "-            if (resourceUpdates.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB)) {",
                "-                Double topoMemOnHeap = resourceUpdates.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB);",
                "-                jsonObject.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, topoMemOnHeap);",
                "-            }",
                "-            if (resourceUpdates.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB)) {",
                "-                Double topoMemOffHeap = resourceUpdates.get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB);",
                "-                jsonObject.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, topoMemOffHeap);",
                "-            }",
                "-            if (resourceUpdates.containsKey(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT)) {",
                "-                Double topoCPU = resourceUpdates.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT);",
                "-                jsonObject.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, topoCPU);",
                "+            Map<String, Double> componentResourceMap =",
                "+                    (Map<String, Double>) jsonObject.getOrDefault(",
                "+                            Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<String, Double>()",
                "+                    );",
                "+",
                "+            for (Map.Entry<String, Double> resourceUpdateEntry : resourceUpdates.entrySet()) {",
                "+                if (NormalizedResources.RESOURCE_NAME_MAPPING.containsValue(resourceUpdateEntry.getKey())) {",
                "+                    // if there will be legacy values they will be in the outer conf",
                "+                    jsonObject.remove(getCorrespondingLegacyResourceName(resourceUpdateEntry.getKey()));",
                "+                    componentResourceMap.remove(getCorrespondingLegacyResourceName(resourceUpdateEntry.getKey()));",
                "+                }",
                "+                componentResourceMap.put(resourceUpdateEntry.getKey(), resourceUpdateEntry.getValue());",
                "             }",
                "+            jsonObject.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, componentResourceMap);",
                "+",
                "             return jsonObject.toJSONString();"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2872": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2872",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "a5406a61b8d65d54662078f5b2dfe96afe3ed319",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512057886,
            "hunks": 11,
            "message": "STORM-2837: ConstraintSolverStrategy",
            "diff": [
                "diff --git a/storm-client/src/jvm/org/apache/storm/Config.java b/storm-client/src/jvm/org/apache/storm/Config.java",
                "index 9a41cc643..480094ad1 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/Config.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/Config.java",
                "@@ -19,2 +19,3 @@ package org.apache.storm;",
                "+import java.util.Arrays;",
                " import org.apache.storm.metric.IEventLogger;",
                "@@ -356,2 +357,30 @@ public class Config extends HashMap<String, Object> {",
                "+    /**",
                "+     * Declare scheduling constraints for a topology used by the constraint solver strategy.",
                "+     * A List of pairs (also a list) of components that cannot coexist in the same worker.",
                "+     */",
                "+    @CustomValidator(validatorClass = ListOfListOfStringValidator.class)",
                "+    public static final String TOPOLOGY_RAS_CONSTRAINTS = \"topology.ras.constraints\";",
                "+",
                "+    /**",
                "+     * Array of components that scheduler should try to place on separate hosts when using the constraint solver strategy or the",
                "+     * multi-tenant scheduler.",
                "+     */",
                "+    @isStringList",
                "+    public static final String TOPOLOGY_SPREAD_COMPONENTS = \"topology.spread.components\";",
                "+",
                "+    /**",
                "+     * The maximum number of states that will be searched looking for a solution in the constraint solver strategy.",
                "+     */",
                "+    @isInteger",
                "+    @isPositiveNumber",
                "+    public static final String TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH = \"topology.ras.constraint.max.state.search\";",
                "+",
                "+    /**",
                "+     * The maximum number of seconds to spend scheduling a topology using the constraint solver.  Null means no limit.",
                "+     */",
                "+    @isInteger",
                "+    @isPositiveNumber",
                "+    public static final String TOPOLOGY_RAS_CONSTRAINT_MAX_TIME_SECS = \"topology.ras.constraint.max.time.secs\";",
                "+",
                "     /**",
                "@@ -779,8 +808,2 @@ public class Config extends HashMap<String, Object> {",
                "-    /**",
                "-     * Array of components that scheduler should try to place on separate hosts.",
                "-     */",
                "-    @isStringList",
                "-    public static final String TOPOLOGY_SPREAD_COMPONENTS = \"topology.spread.components\";",
                "-",
                "     /**",
                "@@ -1940,4 +1963,4 @@ public class Config extends HashMap<String, Object> {",
                "     /**",
                "-     * set the max heap size allow per worker for this topology",
                "-     * @param size",
                "+     * Set the max heap size allow per worker for this topology.",
                "+     * @param size the maximum heap size for a worker.",
                "      */",
                "@@ -1950,3 +1973,30 @@ public class Config extends HashMap<String, Object> {",
                "     /**",
                "-     * set the priority for a topology",
                "+     * Declares executors of component1 cannot be on the same worker as executors of component2.",
                "+     * This function is additive.",
                "+     * Thus a user can setTopologyComponentWorkerConstraints(\"A\", \"B\")",
                "+     * and then setTopologyComponentWorkerConstraints(\"B\", \"C\")",
                "+     * Which means executors form component A cannot be on the same worker with executors of component B",
                "+     * and executors of Component B cannot be on workers with executors of component C",
                "+     * @param component1 a component that should not coexist with component2",
                "+     * @param component2 a component that should not coexist with component1",
                "+     */",
                "+    public void setTopologyComponentWorkerConstraints(String component1, String component2) {",
                "+        if (component1 != null && component2 != null) {",
                "+            List<String> constraintPair = Arrays.asList(component1, component2);",
                "+            List<List<String>> constraints = (List<List<String>>)computeIfAbsent(Config.TOPOLOGY_RAS_CONSTRAINTS,",
                "+                (k) -> new ArrayList<>(1));",
                "+            constraints.add(constraintPair);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Sets the maximum number of states that will be searched in the constraint solver strategy.",
                "+     * @param numStates maximum number of stats to search.",
                "+     */",
                "+    public void setTopologyConstraintsMaxStateSearch(int numStates) {",
                "+        this.put(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH, numStates);",
                "+    }",
                "+",
                "+    /**",
                "+     * Set the priority for a topology.",
                "      * @param priority",
                "diff --git a/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java b/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "index adc08ccc3..7db9aa5ed 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "@@ -607,2 +607,33 @@ public class ConfigValidation {",
                "     /**",
                "+     * Validates a list of a list of Strings.",
                "+     */",
                "+    public static class ListOfListOfStringValidator extends Validator {",
                "+",
                "+        @Override",
                "+        public void validateField(String name, Object o) throws IllegalArgumentException {",
                "+            if (o == null) {",
                "+                return;",
                "+            }",
                "+            if (o instanceof List) {",
                "+                for (Object entry1 : (List) o) {",
                "+                    if (entry1 instanceof List) {",
                "+                        for (Object entry2 : (List) entry1) {",
                "+                            if (!(entry2 instanceof String)) {",
                "+                                throw new IllegalArgumentException(",
                "+                                    \"Field \" + name + \" must be an Iterable containing only List of List of Strings\");",
                "+                            }",
                "+                        }",
                "+                    } else {",
                "+                        throw new IllegalArgumentException(",
                "+                            \"Field \" + name + \" must be an Iterable containing only List of List of Strings\");",
                "+                    }",
                "+                }",
                "+            } else {",
                "+                throw new IllegalArgumentException(",
                "+                    \"Field \" + name + \" must be an Iterable containing only List of List of Strings\");",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    /*",
                "      * Methods for validating confs",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "index c311897b5..39afa9ff4 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "@@ -36,2 +36,3 @@ import org.apache.storm.generated.WorkerResources;",
                " import org.apache.storm.networktopography.DNSToSwitchMapping;",
                "+import org.apache.storm.networktopography.DefaultRackDNSToSwitchMapping;",
                " import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                "@@ -162,17 +163,17 @@ public class Cluster implements ISchedulingState {",
                "             String clazz = (String) conf.get(Config.STORM_NETWORK_TOPOGRAPHY_PLUGIN);",
                "-            if (clazz != null && !clazz.isEmpty()) {",
                "-                DNSToSwitchMapping topographyMapper =",
                "-                    (DNSToSwitchMapping) ReflectionUtils.newInstance(clazz);",
                "-",
                "-                Map<String, String> resolvedSuperVisors = topographyMapper.resolve(supervisorHostNames);",
                "-                for (Map.Entry<String, String> entry : resolvedSuperVisors.entrySet()) {",
                "-                    String hostName = entry.getKey();",
                "-                    String rack = entry.getValue();",
                "-                    List<String> nodesForRack = this.networkTopography.get(rack);",
                "-                    if (nodesForRack == null) {",
                "-                        nodesForRack = new ArrayList<String>();",
                "-                        this.networkTopography.put(rack, nodesForRack);",
                "-                    }",
                "-                    nodesForRack.add(hostName);",
                "+            if (clazz == null || clazz.isEmpty()) {",
                "+                clazz = DefaultRackDNSToSwitchMapping.class.getName();",
                "+            }",
                "+            DNSToSwitchMapping topographyMapper = ReflectionUtils.newInstance(clazz);",
                "+",
                "+            Map<String, String> resolvedSuperVisors = topographyMapper.resolve(supervisorHostNames);",
                "+            for (Map.Entry<String, String> entry : resolvedSuperVisors.entrySet()) {",
                "+                String hostName = entry.getKey();",
                "+                String rack = entry.getValue();",
                "+                List<String> nodesForRack = this.networkTopography.get(rack);",
                "+                if (nodesForRack == null) {",
                "+                    nodesForRack = new ArrayList<>();",
                "+                    this.networkTopography.put(rack, nodesForRack);",
                "                 }",
                "+                nodesForRack.add(hostName);",
                "             }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ConstraintSolverStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ConstraintSolverStrategy.java",
                "new file mode 100644",
                "index 000000000..f790050cd",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ConstraintSolverStrategy.java",
                "@@ -0,0 +1,587 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * <p>",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ * <p>",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource.strategies.scheduling;",
                "+",
                "+import com.google.common.annotations.VisibleForTesting;",
                "+import java.util.ArrayList;",
                "+import java.util.Collection;",
                "+import java.util.Comparator;",
                "+import java.util.HashMap;",
                "+import java.util.HashSet;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import java.util.NavigableMap;",
                "+import java.util.Set;",
                "+import java.util.TreeMap;",
                "+import java.util.TreeSet;",
                "+import java.util.stream.Collectors;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.scheduler.Cluster;",
                "+import org.apache.storm.scheduler.ExecutorDetails;",
                "+import org.apache.storm.scheduler.SchedulerAssignment;",
                "+import org.apache.storm.scheduler.TopologyDetails;",
                "+import org.apache.storm.scheduler.WorkerSlot;",
                "+import org.apache.storm.scheduler.resource.RAS_Node;",
                "+import org.apache.storm.scheduler.resource.RAS_Nodes;",
                "+import org.apache.storm.scheduler.resource.SchedulingResult;",
                "+import org.apache.storm.scheduler.resource.SchedulingStatus;",
                "+import org.apache.storm.utils.ObjectReader;",
                "+import org.apache.storm.utils.Time;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+public class ConstraintSolverStrategy extends BaseResourceAwareStrategy {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(ConstraintSolverStrategy.class);",
                "+",
                "+    protected static class SolverResult {",
                "+        private final int statesSearched;",
                "+        private final boolean success;",
                "+        private final long timeTakenMillis;",
                "+        private final int backtracked;",
                "+",
                "+        public SolverResult(SearcherState state, boolean success) {",
                "+            this.statesSearched = state.getStatesSearched();",
                "+            this.success = success;",
                "+            timeTakenMillis = Time.currentTimeMillis() - state.startTimeMillis;",
                "+            backtracked = state.numBacktrack;",
                "+        }",
                "+",
                "+        public SchedulingResult asSchedulingResult() {",
                "+            if (success) {",
                "+                return SchedulingResult.success(\"Fully Scheduled by ConstraintSolverStrategy (\" + statesSearched",
                "+                    + \" states traversed in \" + timeTakenMillis + \"ms, backtracked \" + backtracked + \" times)\");",
                "+            }",
                "+            return SchedulingResult.failure(SchedulingStatus.FAIL_NOT_ENOUGH_RESOURCES,",
                "+                \"Cannot find scheduling that satisfies all constraints (\" + statesSearched",
                "+                    + \" states traversed in \" + timeTakenMillis + \"ms, backtracked \" + backtracked + \" times)\");",
                "+        }",
                "+    }",
                "+",
                "+    protected static class SearcherState {",
                "+        // Metrics",
                "+        // How many states searched so far.",
                "+        private int statesSearched = 0;",
                "+        // Number of times we had to backtrack.",
                "+        private int numBacktrack = 0;",
                "+        final long startTimeMillis;",
                "+        private final long maxEndTimeMs;",
                "+",
                "+        // Current state",
                "+        // The current executor we are trying to schedule",
                "+        private int execIndex = 0;",
                "+        // A map of the worker to the components in the worker to be able to enforce constraints.",
                "+        private final Map<WorkerSlot, Set<String>> workerCompAssignment;",
                "+        private final boolean[] okToRemoveFromWorker;",
                "+        // for the currently tested assignment a Map of the node to the components on it to be able to enforce constraints",
                "+        private final Map<RAS_Node, Set<String>> nodeCompAssignment;",
                "+        private final boolean[] okToRemoveFromNode;",
                "+",
                "+        // Static State",
                "+        // The list of all executors (preferably sorted to make assignments simpler).",
                "+        private final List<ExecutorDetails> execs;",
                "+        //The maximum number of state to search before stopping.",
                "+        private final int maxStatesSearched;",
                "+        //The topology we are scheduling",
                "+        private final TopologyDetails td;",
                "+",
                "+        private SearcherState(Map<WorkerSlot, Set<String>> workerCompAssignment, Map<RAS_Node, Set<String>> nodeCompAssignment,",
                "+                              int maxStatesSearched, long maxTimeMs, List<ExecutorDetails> execs, TopologyDetails td) {",
                "+            assert !execs.isEmpty();",
                "+            assert execs != null;",
                "+",
                "+            this.workerCompAssignment = workerCompAssignment;",
                "+            this.nodeCompAssignment = nodeCompAssignment;",
                "+            this.maxStatesSearched = maxStatesSearched;",
                "+            this.execs = execs;",
                "+            okToRemoveFromWorker = new boolean[execs.size()];",
                "+            okToRemoveFromNode = new boolean[execs.size()];",
                "+            this.td = td;",
                "+            startTimeMillis = Time.currentTimeMillis();",
                "+            if (maxTimeMs <= 0) {",
                "+                maxEndTimeMs = Long.MAX_VALUE;",
                "+            } else {",
                "+                maxEndTimeMs = startTimeMillis + maxTimeMs;",
                "+            }",
                "+        }",
                "+",
                "+        public void incStatesSearched() {",
                "+            statesSearched++;",
                "+            if (LOG.isDebugEnabled() && statesSearched % 1_000 == 0) {",
                "+                LOG.debug(\"States Searched: {}\", statesSearched);",
                "+                LOG.debug(\"backtrack: {}\", numBacktrack);",
                "+            }",
                "+        }",
                "+",
                "+        public int getStatesSearched() {",
                "+            return statesSearched;",
                "+        }",
                "+",
                "+        public boolean areSearchLimitsExceeded() {",
                "+            return statesSearched > maxStatesSearched || Time.currentTimeMillis() > maxEndTimeMs;",
                "+        }",
                "+",
                "+        public SearcherState nextExecutor() {",
                "+            execIndex++;",
                "+            if (execIndex >= execs.size()) {",
                "+                throw new IllegalStateException(\"Internal Error: exceeded the exec limit \" + execIndex + \" >= \" + execs.size());",
                "+            }",
                "+            return this;",
                "+        }",
                "+",
                "+        public boolean areAllExecsScheduled() {",
                "+            return execIndex == execs.size() - 1;",
                "+        }",
                "+",
                "+        public ExecutorDetails currentExec() {",
                "+            return execs.get(execIndex);",
                "+        }",
                "+",
                "+        public void tryToSchedule(Map<ExecutorDetails, String> execToComp, RAS_Node node, WorkerSlot workerSlot) {",
                "+            ExecutorDetails exec = currentExec();",
                "+            String comp = execToComp.get(exec);",
                "+            LOG.trace(\"Trying assignment of {} {} to {}\", exec, comp, workerSlot);",
                "+            //It is possible that this component is already scheduled on this node or worker.  If so when we backtrack we cannot remove it",
                "+            okToRemoveFromWorker[execIndex] = workerCompAssignment.computeIfAbsent(workerSlot, (k) -> new HashSet<>()).add(comp);",
                "+            okToRemoveFromNode[execIndex] = nodeCompAssignment.computeIfAbsent(node, (k) -> new HashSet<>()).add(comp);",
                "+            node.assignSingleExecutor(workerSlot, exec, td);",
                "+        }",
                "+",
                "+        public void backtrack(Map<ExecutorDetails, String> execToComp, RAS_Node node, WorkerSlot workerSlot) {",
                "+            execIndex--;",
                "+            if (execIndex < 0) {",
                "+                throw new IllegalStateException(\"Internal Error: exec index became negative\");",
                "+            }",
                "+            numBacktrack++;",
                "+            ExecutorDetails exec = currentExec();",
                "+            String comp = execToComp.get(exec);",
                "+            LOG.trace(\"Backtracking {} {} from {}\", exec, comp, workerSlot);",
                "+            if (okToRemoveFromWorker[execIndex]) {",
                "+                workerCompAssignment.get(workerSlot).remove(comp);",
                "+                okToRemoveFromWorker[execIndex] = false;",
                "+            }",
                "+            if (okToRemoveFromNode[execIndex]) {",
                "+                nodeCompAssignment.get(node).remove(comp);",
                "+                okToRemoveFromNode[execIndex] = false;",
                "+            }",
                "+            node.freeSingleExecutor(exec, td);",
                "+        }",
                "+    }",
                "+",
                "+    private Map<String, RAS_Node> nodes;",
                "+    private Map<ExecutorDetails, String> execToComp;",
                "+    private Map<String, Set<ExecutorDetails>> compToExecs;",
                "+    private List<String> favoredNodes;",
                "+    private List<String> unFavoredNodes;",
                "+",
                "+    //constraints and spreads",
                "+    private Map<String, Map<String, Integer>> constraintMatrix;",
                "+    private HashSet<String> spreadComps = new HashSet<>();",
                "+",
                "+    //hard coded max number of states to search",
                "+    public static final int MAX_STATE_SEARCH = 100_000;",
                "+    public static final int DEFAULT_STATE_SEARCH = 10_000;",
                "+",
                "+    @Override",
                "+    public SchedulingResult schedule(Cluster cluster, TopologyDetails td) {",
                "+        prepare(cluster);",
                "+        LOG.debug(\"Scheduling {}\", td.getId());",
                "+        nodes = RAS_Nodes.getAllNodesFrom(cluster);",
                "+        Map<WorkerSlot, Set<String>> workerCompAssignment = new HashMap<>();",
                "+        Map<RAS_Node, Set<String>> nodeCompAssignment = new HashMap<>();",
                "+        //set max number of states to search",
                "+        final int maxStateSearch = Math.min(MAX_STATE_SEARCH,",
                "+            ObjectReader.getInt(td.getConf().get(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_STATE_SEARCH), DEFAULT_STATE_SEARCH));",
                "+",
                "+        final long maxTimeMs =",
                "+            ObjectReader.getInt(td.getConf().get(Config.TOPOLOGY_RAS_CONSTRAINT_MAX_TIME_SECS), -1).intValue() * 1000L;",
                "+",
                "+        favoredNodes = (List<String>) td.getConf().get(Config.TOPOLOGY_SCHEDULER_FAVORED_NODES);",
                "+        unFavoredNodes = (List<String>) td.getConf().get(Config.TOPOLOGY_SCHEDULER_UNFAVORED_NODES);",
                "+",
                "+        //get mapping of execs to components",
                "+        execToComp = td.getExecutorToComponent();",
                "+        //get mapping of components to executors",
                "+        compToExecs = getCompToExecs(execToComp);",
                "+",
                "+        //get topology constraints",
                "+        constraintMatrix = getConstraintMap(td, compToExecs.keySet());",
                "+",
                "+        //get spread components",
                "+        spreadComps = getSpreadComps(td);",
                "+",
                "+        //get a sorted list of unassigned executors based on number of constraints",
                "+        Set<ExecutorDetails> unassignedExecutors = new HashSet<>(cluster.getUnassignedExecutors(td));",
                "+        List<ExecutorDetails> sortedExecs = getSortedExecs(spreadComps, constraintMatrix, compToExecs).stream()",
                "+            .filter(unassignedExecutors::contains)",
                "+            .collect(Collectors.toList());",
                "+",
                "+        //populate with existing assignments",
                "+        SchedulerAssignment existingAssignment = cluster.getAssignmentById(td.getId());",
                "+        if (existingAssignment != null) {",
                "+            existingAssignment.getExecutorToSlot().forEach((exec, ws) -> {",
                "+                String compId = execToComp.get(exec);",
                "+                RAS_Node node = nodes.get(ws.getNodeId());",
                "+                //populate node to component Assignments",
                "+                nodeCompAssignment.computeIfAbsent(node, (k) -> new HashSet<>()).add(compId);",
                "+                //populate worker to comp assignments",
                "+                workerCompAssignment.computeIfAbsent(ws, (k) -> new HashSet<>()).add(compId);",
                "+            });",
                "+        }",
                "+",
                "+        //early detection/early fail",
                "+        if (!checkSchedulingFeasibility()) {",
                "+            //Scheduling Status set to FAIL_OTHER so no eviction policy will be attempted to make space for this topology",
                "+            return SchedulingResult.failure(SchedulingStatus.FAIL_OTHER, \"Scheduling not feasible!\");",
                "+        }",
                "+        return backtrackSearch(new SearcherState(workerCompAssignment, nodeCompAssignment, maxStateSearch, maxTimeMs, sortedExecs, td))",
                "+            .asSchedulingResult();",
                "+    }",
                "+",
                "+    private boolean checkSchedulingFeasibility() {",
                "+        for (String comp : spreadComps) {",
                "+            int numExecs = compToExecs.get(comp).size();",
                "+            if (numExecs > nodes.size()) {",
                "+                LOG.error(\"Unsatisfiable constraint: Component: {} marked as spread has {} executors which is larger \"",
                "+                    + \"than number of nodes: {}\", comp, numExecs, nodes.size());",
                "+                return false;",
                "+            }",
                "+        }",
                "+        if (execToComp.size() >= MAX_STATE_SEARCH) {",
                "+            LOG.error(\"Number of executors is greater than the maximum number of states allowed to be searched.  \"",
                "+                + \"# of executors: {} Max states to search: {}\", execToComp.size(), MAX_STATE_SEARCH);",
                "+            return false;",
                "+        }",
                "+        return true;",
                "+    }",
                "+    ",
                "+    @Override",
                "+    protected TreeSet<ObjectResources> sortObjectResources(",
                "+        final AllResources allResources, ExecutorDetails exec, TopologyDetails topologyDetails,",
                "+        final ExistingScheduleFunc existingScheduleFunc) {",
                "+        return GenericResourceAwareStrategy.sortObjectResourcesImpl(allResources, exec, topologyDetails, existingScheduleFunc);",
                "+    }",
                "+",
                "+    // Backtracking algorithm does not take into account the ordering of executors in worker to reduce traversal space",
                "+    @VisibleForTesting",
                "+    protected SolverResult backtrackSearch(SearcherState state) {",
                "+        state.incStatesSearched();",
                "+        if (state.areSearchLimitsExceeded()) {",
                "+            LOG.warn(\"Limits Exceeded\");",
                "+            return new SolverResult(state, false);",
                "+        }",
                "+",
                "+        ExecutorDetails exec = state.currentExec();",
                "+        List<ObjectResources> sortedNodes = sortAllNodes(state.td, exec, favoredNodes, unFavoredNodes);",
                "+",
                "+        for (ObjectResources nodeResources: sortedNodes) {",
                "+            RAS_Node node = nodes.get(nodeResources.id);",
                "+            for (WorkerSlot workerSlot : node.getSlotsAvailbleTo(state.td)) {",
                "+                if (isExecAssignmentToWorkerValid(workerSlot, state)) {",
                "+                    state.tryToSchedule(execToComp, node, workerSlot);",
                "+",
                "+                    if (state.areAllExecsScheduled()) {",
                "+                        //Everything is scheduled correctly, so no need to search any more.",
                "+                        return new SolverResult(state, true);",
                "+                    }",
                "+",
                "+                    SolverResult results = backtrackSearch(state.nextExecutor());",
                "+                    if (results.success) {",
                "+                        //We found a good result we are done.",
                "+                        return results;",
                "+                    }",
                "+",
                "+                    if (state.areSearchLimitsExceeded()) {",
                "+                        //No need to search more it is not going to help.",
                "+                        return new SolverResult(state, false);",
                "+                    }",
                "+",
                "+                    //backtracking (If we ever get here there really isn't a lot of hope that we will find a scheduling)",
                "+                    state.backtrack(execToComp, node, workerSlot);",
                "+                }",
                "+            }",
                "+        }",
                "+        //Tried all of the slots and none of them worked.",
                "+        return new SolverResult(state, false);",
                "+    }",
                "+",
                "+    /**",
                "+     * Check if any constraints are violated if exec is scheduled on worker.",
                "+     * @return true if scheduling exec on worker does not violate any constraints, returns false if it does",
                "+     */",
                "+    public boolean isExecAssignmentToWorkerValid(WorkerSlot worker, SearcherState state) {",
                "+        final ExecutorDetails exec = state.currentExec();",
                "+        //check resources",
                "+        RAS_Node node = nodes.get(worker.getNodeId());",
                "+        if (!node.wouldFit(worker, exec, state.td)) {",
                "+            LOG.trace(\"{} would not fit in resources available on {}\", exec, worker);",
                "+            return false;",
                "+        }",
                "+",
                "+        //check if exec can be on worker based on user defined component exclusions",
                "+        String execComp = execToComp.get(exec);",
                "+        Set<String> components = state.workerCompAssignment.get(worker);",
                "+        if (components != null) {",
                "+            Map<String, Integer> subMatrix = constraintMatrix.get(execComp);",
                "+            for (String comp : components) {",
                "+                if (subMatrix.get(comp) != 0) {",
                "+                    LOG.trace(\"{} found {} constraint violation {} on {}\", exec, execComp, comp, worker);",
                "+                    return false;",
                "+                }",
                "+            }",
                "+        }",
                "+",
                "+        //check if exec satisfy spread",
                "+        if (spreadComps.contains(execComp)) {",
                "+            if (state.nodeCompAssignment.computeIfAbsent(node, (k) -> new HashSet<>()).contains(execComp)) {",
                "+                LOG.trace(\"{} Found spread violation {} on node {}\", exec, execComp, node.getId());",
                "+                return false;",
                "+            }",
                "+        }",
                "+        return true;",
                "+    }",
                "+",
                "+    static Map<String, Map<String, Integer>> getConstraintMap(TopologyDetails topo, Set<String> comps) {",
                "+        Map<String, Map<String, Integer>> matrix = new HashMap<>();",
                "+        for (String comp : comps) {",
                "+            matrix.put(comp, new HashMap<>());",
                "+            for (String comp2 : comps) {",
                "+                matrix.get(comp).put(comp2, 0);",
                "+            }",
                "+        }",
                "+        List<List<String>> constraints = (List<List<String>>) topo.getConf().get(Config.TOPOLOGY_RAS_CONSTRAINTS);",
                "+        if (constraints != null) {",
                "+            for (List<String> constraintPair : constraints) {",
                "+                String comp1 = constraintPair.get(0);",
                "+                String comp2 = constraintPair.get(1);",
                "+                if (!matrix.containsKey(comp1)) {",
                "+                    LOG.warn(\"Comp: {} declared in constraints is not valid!\", comp1);",
                "+                    continue;",
                "+                }",
                "+                if (!matrix.containsKey(comp2)) {",
                "+                    LOG.warn(\"Comp: {} declared in constraints is not valid!\", comp2);",
                "+                    continue;",
                "+                }",
                "+                matrix.get(comp1).put(comp2, 1);",
                "+                matrix.get(comp2).put(comp1, 1);",
                "+            }",
                "+        }",
                "+        return matrix;",
                "+    }",
                "+",
                "+    /**",
                "+     * Determines if a scheduling is valid and all constraints are satisfied.",
                "+     */",
                "+    @VisibleForTesting",
                "+    public static boolean validateSolution(Cluster cluster, TopologyDetails td) {",
                "+        return checkSpreadSchedulingValid(cluster, td)",
                "+            && checkConstraintsSatisfied(cluster, td)",
                "+            && checkResourcesCorrect(cluster, td);",
                "+    }",
                "+",
                "+    /**",
                "+     * Check if constraints are satisfied.",
                "+     */",
                "+    private static boolean checkConstraintsSatisfied(Cluster cluster, TopologyDetails topo) {",
                "+        LOG.info(\"Checking constraints...\");",
                "+        Map<ExecutorDetails, WorkerSlot> result = cluster.getAssignmentById(topo.getId()).getExecutorToSlot();",
                "+        Map<ExecutorDetails, String> execToComp = topo.getExecutorToComponent();",
                "+        //get topology constraints",
                "+        Map<String, Map<String, Integer>> constraintMatrix = getConstraintMap(topo, new HashSet<>(topo.getExecutorToComponent().values()));",
                "+",
                "+        Map<WorkerSlot, Set<String>> workerCompMap = new HashMap<>();",
                "+        result.forEach((exec, worker) -> {",
                "+            String comp = execToComp.get(exec);",
                "+            workerCompMap.computeIfAbsent(worker, (k) -> new HashSet<>()).add(comp);",
                "+        });",
                "+        for (Map.Entry<WorkerSlot, Set<String>> entry : workerCompMap.entrySet()) {",
                "+            Set<String> comps = entry.getValue();",
                "+            for (String comp1 : comps) {",
                "+                for (String comp2: comps) {",
                "+                    if (!comp1.equals(comp2) && constraintMatrix.get(comp1).get(comp2) != 0) {",
                "+                        LOG.error(\"Incorrect Scheduling: worker exclusion for Component {} and {} not satisfied on WorkerSlot: {}\",",
                "+                            comp1, comp2, entry.getKey());",
                "+                        return false;",
                "+                    }",
                "+                }",
                "+            }",
                "+        }",
                "+        return true;",
                "+    }",
                "+",
                "+    private static Map<WorkerSlot, RAS_Node> workerToNodes(Cluster cluster) {",
                "+        Map<WorkerSlot, RAS_Node> workerToNodes = new HashMap<>();",
                "+        for (RAS_Node node: RAS_Nodes.getAllNodesFrom(cluster).values()) {",
                "+            for (WorkerSlot s : node.getUsedSlots()) {",
                "+                workerToNodes.put(s, node);",
                "+            }",
                "+        }",
                "+        return workerToNodes;",
                "+    }",
                "+",
                "+    private static boolean checkSpreadSchedulingValid(Cluster cluster, TopologyDetails topo) {",
                "+        LOG.info(\"Checking for a valid scheduling...\");",
                "+        Map<ExecutorDetails, WorkerSlot> result = cluster.getAssignmentById(topo.getId()).getExecutorToSlot();",
                "+        Map<ExecutorDetails, String> execToComp = topo.getExecutorToComponent();",
                "+        Map<WorkerSlot, HashSet<ExecutorDetails>> workerExecMap = new HashMap<>();",
                "+        Map<WorkerSlot, HashSet<String>> workerCompMap = new HashMap<>();",
                "+        Map<RAS_Node, HashSet<String>> nodeCompMap = new HashMap<>();",
                "+        Map<WorkerSlot, RAS_Node> workerToNodes = workerToNodes(cluster);",
                "+        boolean ret = true;",
                "+",
                "+        HashSet<String> spreadComps = getSpreadComps(topo);",
                "+        for (Map.Entry<ExecutorDetails, WorkerSlot> entry : result.entrySet()) {",
                "+            ExecutorDetails exec = entry.getKey();",
                "+            WorkerSlot worker = entry.getValue();",
                "+            RAS_Node node = workerToNodes.get(worker);",
                "+",
                "+            if (workerExecMap.computeIfAbsent(worker, (k) -> new HashSet<>()).contains(exec)) {",
                "+                LOG.error(\"Incorrect Scheduling: Found duplicate in scheduling\");",
                "+                return false;",
                "+            }",
                "+            workerExecMap.get(worker).add(exec);",
                "+            String comp = execToComp.get(exec);",
                "+            workerCompMap.computeIfAbsent(worker, (k) -> new HashSet<>()).add(comp);",
                "+            if (spreadComps.contains(comp)) {",
                "+                if (nodeCompMap.computeIfAbsent(node, (k) -> new HashSet<>()).contains(comp)) {",
                "+                    LOG.error(\"Incorrect Scheduling: Spread for Component: {} {} on node {} not satisfied {}\",",
                "+                        comp, exec, node.getId(), nodeCompMap.get(node));",
                "+                    ret = false;",
                "+                }",
                "+            }",
                "+            nodeCompMap.computeIfAbsent(node, (k) -> new HashSet<>()).add(comp);",
                "+        }",
                "+        return ret;",
                "+    }",
                "+",
                "+    /**",
                "+     * Check if resource constraints satisfied.",
                "+     */",
                "+    private static boolean checkResourcesCorrect(Cluster cluster, TopologyDetails topo) {",
                "+        LOG.info(\"Checking Resources...\");",
                "+        Map<ExecutorDetails, WorkerSlot> result = cluster.getAssignmentById(topo.getId()).getExecutorToSlot();",
                "+        Map<RAS_Node, Collection<ExecutorDetails>> nodeToExecs = new HashMap<>();",
                "+        Map<ExecutorDetails, WorkerSlot> mergedExecToWorker = new HashMap<>();",
                "+        Map<String, RAS_Node> nodes = RAS_Nodes.getAllNodesFrom(cluster);",
                "+        //merge with existing assignments",
                "+        if (cluster.getAssignmentById(topo.getId()) != null",
                "+                && cluster.getAssignmentById(topo.getId()).getExecutorToSlot() != null) {",
                "+            mergedExecToWorker.putAll(cluster.getAssignmentById(topo.getId()).getExecutorToSlot());",
                "+        }",
                "+        mergedExecToWorker.putAll(result);",
                "+",
                "+        for (Map.Entry<ExecutorDetails, WorkerSlot> entry : mergedExecToWorker.entrySet()) {",
                "+            ExecutorDetails exec = entry.getKey();",
                "+            WorkerSlot worker = entry.getValue();",
                "+            RAS_Node node = nodes.get(worker.getNodeId());",
                "+",
                "+            if (node.getAvailableMemoryResources() < 0.0 && node.getAvailableCpuResources() < 0.0) {",
                "+                LOG.error(\"Incorrect Scheduling: found node with negative available resources\");",
                "+                return false;",
                "+            }",
                "+            nodeToExecs.computeIfAbsent(node, (k) -> new HashSet<>()).add(exec);",
                "+        }",
                "+",
                "+        for (Map.Entry<RAS_Node, Collection<ExecutorDetails>> entry : nodeToExecs.entrySet()) {",
                "+            RAS_Node node = entry.getKey();",
                "+            Collection<ExecutorDetails> execs = entry.getValue();",
                "+            double cpuUsed = 0.0;",
                "+            double memoryUsed = 0.0;",
                "+            for (ExecutorDetails exec : execs) {",
                "+                cpuUsed += topo.getTotalCpuReqTask(exec);",
                "+                memoryUsed += topo.getTotalMemReqTask(exec);",
                "+            }",
                "+            if (node.getAvailableCpuResources() != (node.getTotalCpuResources() - cpuUsed)) {",
                "+                LOG.error(\"Incorrect Scheduling: node {} has consumed incorrect amount of cpu. Expected: {}\"",
                "+                        + \" Actual: {} Executors scheduled on node: {}\",",
                "+                        node.getId(), (node.getTotalCpuResources() - cpuUsed), node.getAvailableCpuResources(), execs);",
                "+                return false;",
                "+            }",
                "+            if (node.getAvailableMemoryResources() != (node.getTotalMemoryResources() - memoryUsed)) {",
                "+                LOG.error(\"Incorrect Scheduling: node {} has consumed incorrect amount of memory. Expected: {}\"",
                "+                        + \" Actual: {} Executors scheduled on node: {}\",",
                "+                        node.getId(), (node.getTotalMemoryResources() - memoryUsed), node.getAvailableMemoryResources(), execs);",
                "+                return false;",
                "+            }",
                "+        }",
                "+        return true;",
                "+    }",
                "+",
                "+    private Map<String, Set<ExecutorDetails>> getCompToExecs(Map<ExecutorDetails, String> executorToComp) {",
                "+        Map<String, Set<ExecutorDetails>> retMap = new HashMap<>();",
                "+        executorToComp.forEach((exec, comp) -> retMap.computeIfAbsent(comp, (k) -> new HashSet<>()).add(exec));",
                "+        return retMap;",
                "+    }",
                "+",
                "+    private ArrayList<ExecutorDetails> getSortedExecs(HashSet<String> spreadComps, Map<String, Map<String, Integer>> constraintMatrix,",
                "+                                                      Map<String, Set<ExecutorDetails>> compToExecs) {",
                "+        ArrayList<ExecutorDetails> retList = new ArrayList<>();",
                "+        //find number of constraints per component",
                "+        //Key->Comp Value-># of constraints",
                "+        Map<String, Integer> compConstraintCountMap = new HashMap<>();",
                "+        constraintMatrix.forEach((comp, subMatrix) -> {",
                "+            int count = subMatrix.values().stream().mapToInt(Number::intValue).sum();",
                "+            //check component is declared for spreading",
                "+            if (spreadComps.contains(comp)) {",
                "+                count++;",
                "+            }",
                "+            compConstraintCountMap.put(comp, count);",
                "+        });",
                "+        //Sort comps by number of constraints",
                "+        NavigableMap<String, Integer> sortedCompConstraintCountMap = sortByValues(compConstraintCountMap);",
                "+        //sort executors based on component constraints",
                "+        for (String comp : sortedCompConstraintCountMap.keySet()) {",
                "+            retList.addAll(compToExecs.get(comp));",
                "+        }",
                "+        return retList;",
                "+    }",
                "+",
                "+    private static HashSet<String> getSpreadComps(TopologyDetails topo) {",
                "+        HashSet<String> retSet = new HashSet<>();",
                "+        List<String> spread = (List<String>) topo.getConf().get(Config.TOPOLOGY_SPREAD_COMPONENTS);",
                "+        if (spread != null) {",
                "+            Set<String> comps = topo.getComponents().keySet();",
                "+            for (String comp : spread) {",
                "+                if (comps.contains(comp)) {",
                "+                    retSet.add(comp);",
                "+                } else {",
                "+                    LOG.warn(\"Comp {} declared for spread not valid\", comp);",
                "+                }",
                "+            }",
                "+        }",
                "+        return retSet;",
                "+    }",
                "+",
                "+    /**",
                "+     * Used to sort a Map by the values.",
                "+     */",
                "+    @VisibleForTesting",
                "+    public <K extends Comparable<K>, V extends Comparable<V>> NavigableMap<K, V> sortByValues(final Map<K, V> map) {",
                "+        Comparator<K> valueComparator = (k1, k2) -> {",
                "+            int compare = map.get(k2).compareTo(map.get(k1));",
                "+            if (compare == 0) {",
                "+                return k2.compareTo(k1);",
                "+            } else {",
                "+                return compare;",
                "+            }",
                "+        };",
                "+        NavigableMap<K, V> sortedByValues = new TreeMap<>(valueComparator);",
                "+        sortedByValues.putAll(map);",
                "+        return sortedByValues;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "index 893e2898e..108dc497a 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "@@ -131,3 +131,11 @@ public class GenericResourceAwareStrategy extends BaseResourceAwareStrategy impl",
                "             final ExistingScheduleFunc existingScheduleFunc) {",
                "+        return sortObjectResourcesImpl(allResources, exec, topologyDetails, existingScheduleFunc);",
                "+    }",
                "+    /**",
                "+     * Implementation of the sortObjectResources method so other strategies can reuse it.",
                "+     */",
                "+    public static TreeSet<ObjectResources> sortObjectResourcesImpl(",
                "+        final AllResources allResources, ExecutorDetails exec, TopologyDetails topologyDetails,",
                "+        final ExistingScheduleFunc existingScheduleFunc) {",
                "         AllResources affinityBasedAllResources = new AllResources(allResources);"
            ],
            "changed_files": [
                "storm-client/src/jvm/org/apache/storm/Config.java",
                "storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ConstraintSolverStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2837": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2837",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "48c2fda867aa1d1123f6ba9c9f623ae80c9df280",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516895360,
            "hunks": 21,
            "message": "STORM-2910: have metrics reported in the background",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java",
                "index f45ce25ec..a06e44c3d 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java",
                "@@ -52,2 +52,3 @@ import org.apache.storm.utils.ServerUtils;",
                " import org.apache.storm.utils.Utils;",
                "+import org.apache.thrift.TException;",
                " import org.slf4j.Logger;",
                "@@ -711,3 +712,3 @@ public abstract class Container implements Killable {",
                "      */",
                "-    void processMetrics() {",
                "+    void processMetrics(OnlyLatestExecutor<Integer> exec) {",
                "         try {",
                "@@ -727,3 +728,3 @@ public abstract class Container implements Killable {",
                "                 WorkerMetricPoint workerMetric = new WorkerMetricPoint(MEMORY_USED_METRIC, timestamp, value, SYSTEM_COMPONENT_ID,",
                "-                        INVALID_EXECUTOR_ID, INVALID_STREAM_ID);",
                "+                    INVALID_EXECUTOR_ID, INVALID_STREAM_ID);",
                "@@ -733,7 +734,9 @@ public abstract class Container implements Killable {",
                "-                try (NimbusClient client = NimbusClient.getConfiguredClient(_conf)) {",
                "-                    client.getClient().processWorkerMetrics(metrics);",
                "-                }",
                "-",
                "-                this.lastMetricProcessTime = currentTimeMsec;",
                "+                exec.execute(_port, () -> {",
                "+                    try (NimbusClient client = NimbusClient.getConfiguredClient(_conf)) {",
                "+                        client.getClient().processWorkerMetrics(metrics);",
                "+                    } catch (Exception e) {",
                "+                        LOG.error(\"Failed to process metrics\", e);",
                "+                    }",
                "+                });",
                "             }",
                "@@ -741,2 +744,3 @@ public abstract class Container implements Killable {",
                "             LOG.error(\"Failed to process metrics\", e);",
                "+        } finally {",
                "             this.lastMetricProcessTime = System.currentTimeMillis();",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/OnlyLatestExecutor.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/OnlyLatestExecutor.java",
                "new file mode 100644",
                "index 000000000..bd73766b7",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/OnlyLatestExecutor.java",
                "@@ -0,0 +1,55 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.daemon.supervisor;",
                "+",
                "+import java.util.concurrent.ConcurrentHashMap;",
                "+import java.util.concurrent.ConcurrentMap;",
                "+import java.util.concurrent.Executor;",
                "+",
                "+/**",
                "+ * This allows you to submit a Runnable with a key.  If the previous submission for that key has not yet run,",
                "+ * it will be replaced with the latest one.",
                "+ */",
                "+public class OnlyLatestExecutor<K> {",
                "+    private final Executor exec;",
                "+    private final ConcurrentMap<K, Runnable> latest;",
                "+",
                "+    public OnlyLatestExecutor(Executor exec) {",
                "+        this.exec = exec;",
                "+        latest = new ConcurrentHashMap<>();",
                "+    }",
                "+",
                "+    /**",
                "+     * Run something in the future, but replace it with the latest if it is taking too long",
                "+     * @param key what to use to dedupe things.",
                "+     * @param r what you want to run.",
                "+     */",
                "+    public void execute(final K key, Runnable r) {",
                "+        Runnable old = latest.put(key, r);",
                "+        if (old == null) {",
                "+            //It was not there before so we need to run it.",
                "+            exec.execute(() -> {",
                "+                Runnable run = latest.remove(key);",
                "+                if (run != null) {",
                "+                    run.run();;",
                "+                }",
                "+            });",
                "+        }",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "index 884efcb0e..5d8bb3361 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "@@ -67,3 +67,4 @@ public class ReadClusterState implements Runnable, AutoCloseable {",
                "     private final AtomicReference<Map<Long, LocalAssignment>> cachedAssignments;",
                "-    ",
                "+    private final OnlyLatestExecutor<Integer> metricsExec;",
                "+",
                "     public ReadClusterState(Supervisor supervisor) throws Exception {",
                "@@ -79,2 +80,3 @@ public class ReadClusterState implements Runnable, AutoCloseable {",
                "         this.cachedAssignments = supervisor.getCurrAssignment();",
                "+        this.metricsExec = new OnlyLatestExecutor<>(supervisor.getHeartbeatExecutor());",
                "@@ -110,3 +112,3 @@ public class ReadClusterState implements Runnable, AutoCloseable {",
                "         return new Slot(localizer, superConf, launcher, host, port,",
                "-                localState, stormClusterState, iSuper, cachedAssignments);",
                "+                localState, stormClusterState, iSuper, cachedAssignments, metricsExec);",
                "     }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "index fe30c935f..670029114 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "@@ -97,8 +97,10 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "         public final BlobChangingCallback changingCallback;",
                "-        ",
                "+        public final OnlyLatestExecutor<Integer> metricsExec;",
                "+",
                "         StaticState(AsyncLocalizer localizer, long hbTimeoutMs, long firstHbTimeoutMs,",
                "-                long killSleepMs, long monitorFreqMs,",
                "-                ContainerLauncher containerLauncher, String host, int port,",
                "-                ISupervisor iSupervisor, LocalState localState,",
                "-                BlobChangingCallback changingCallback) {",
                "+                    long killSleepMs, long monitorFreqMs,",
                "+                    ContainerLauncher containerLauncher, String host, int port,",
                "+                    ISupervisor iSupervisor, LocalState localState,",
                "+                    BlobChangingCallback changingCallback,",
                "+                    OnlyLatestExecutor<Integer> metricsExec) {",
                "             this.localizer = localizer;",
                "@@ -114,2 +116,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "             this.changingCallback = changingCallback;",
                "+            this.metricsExec = metricsExec;",
                "         }",
                "@@ -939,3 +942,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "-        dynamicState.container.processMetrics();",
                "+        dynamicState.container.processMetrics(staticState.metricsExec);",
                "@@ -973,10 +976,13 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "     private final AtomicReference<Map<Long, LocalAssignment>> cachedCurrentAssignments;",
                "-    ",
                "+    private final OnlyLatestExecutor<Integer> metricsExec;",
                "+",
                "     public Slot(AsyncLocalizer localizer, Map<String, Object> conf,",
                "-            ContainerLauncher containerLauncher, String host,",
                "-            int port, LocalState localState,",
                "-            IStormClusterState clusterState,",
                "-            ISupervisor iSupervisor,",
                "-            AtomicReference<Map<Long, LocalAssignment>> cachedCurrentAssignments) throws Exception {",
                "+                ContainerLauncher containerLauncher, String host,",
                "+                int port, LocalState localState,",
                "+                IStormClusterState clusterState,",
                "+                ISupervisor iSupervisor,",
                "+                AtomicReference<Map<Long, LocalAssignment>> cachedCurrentAssignments,",
                "+                OnlyLatestExecutor<Integer> metricsExec) throws Exception {",
                "         super(\"SLOT_\"+port);",
                "+        this.metricsExec = metricsExec;",
                "@@ -1026,3 +1032,4 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "             localState,",
                "-            this);",
                "+            this,",
                "+            metricsExec);",
                "         this.newAssignment.set(dynamicState.newAssignment);",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java",
                "index ee5a55c7b..147a8aa36 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java",
                "@@ -29,2 +29,4 @@ import java.util.Map;",
                " import java.util.concurrent.Callable;",
                "+import java.util.concurrent.ExecutorService;",
                "+import java.util.concurrent.Executors;",
                " import java.util.concurrent.atomic.AtomicReference;",
                "@@ -76,2 +78,6 @@ public class Supervisor implements DaemonCommon, AutoCloseable {",
                "     private final StormTimer eventTimer;",
                "+    //Right now this is only used for sending metrics to nimbus,",
                "+    // but we may want to combine it with the heartbeatTimer at some point",
                "+    // to really make this work well.",
                "+    private final ExecutorService heartbeatExecutor;",
                "     private final AsyncLocalizer asyncLocalizer;",
                "@@ -91,2 +97,3 @@ public class Supervisor implements DaemonCommon, AutoCloseable {",
                "         this.sharedContext = sharedContext;",
                "+        this.heartbeatExecutor = Executors.newFixedThreadPool(1);",
                "@@ -127,3 +134,10 @@ public class Supervisor implements DaemonCommon, AutoCloseable {",
                "     }",
                "-    ",
                "+",
                "+    /**",
                "+     * Get the executor service that is supposed to be used for heart-beats.",
                "+     */",
                "+    public ExecutorService getHeartbeatExecutor() {",
                "+        return heartbeatExecutor;",
                "+    }",
                "+",
                "     public String getId() {"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/OnlyLatestExecutor.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2910": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2910",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "cfb7d263d2c5f4b48f106f930ed654b0b539674d",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509765466,
            "hunks": 252,
            "message": "STORM-2796: Implement support for static factory methods * add test for factory args * improve handling of non-primitive numbers * merge/fix patch from @roshannaik * Address checkstyle errors. Squashed by Jungtaek Lim <kabhwan@gmail.com> This closes #2409",
            "diff": [
                "diff --git a/flux/flux-core/pom.xml b/flux/flux-core/pom.xml",
                "index 7df517b82..acdf805e6 100644",
                "--- a/flux/flux-core/pom.xml",
                "+++ b/flux/flux-core/pom.xml",
                "@@ -114,3 +114,3 @@",
                "                 <configuration>",
                "-                    <maxAllowedViolations>285</maxAllowedViolations>",
                "+                    <maxAllowedViolations>0</maxAllowedViolations>",
                "                 </configuration>",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/Flux.java b/flux/flux-core/src/main/java/org/apache/storm/flux/Flux.java",
                "index 1d38adac3..e88f4db96 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/Flux.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/Flux.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux;",
                "@@ -48,3 +49,2 @@ import org.slf4j.LoggerFactory;",
                "  * Flux entry point.",
                "- *",
                "  */",
                "@@ -65,2 +65,7 @@ public class Flux {",
                "+    /**",
                "+     * Flux main entry point.",
                "+     * @param args command line arguments",
                "+     * @throws Exception if parsing/topology creation fails",
                "+     */",
                "     public static void main(String[] args) throws Exception {",
                "@@ -74,7 +79,7 @@ public class Flux {",
                "-        options.addOption(option(1, \"s\", OPTION_SLEEP, \"ms\", \"When running locally, the amount of time to sleep (in ms.) \" +",
                "-                \"before killing the topology and shutting down the local cluster.\"));",
                "+        options.addOption(option(1, \"s\", OPTION_SLEEP, \"ms\", \"When running locally, the amount of time to sleep (in ms.) \"",
                "+                + \"before killing the topology and shutting down the local cluster.\"));",
                "-        options.addOption(option(0, \"d\", OPTION_DRY_RUN, \"Do not run or deploy the topology. Just build, validate, \" +",
                "-                \"and print information about the topology.\"));",
                "+        options.addOption(option(0, \"d\", OPTION_DRY_RUN, \"Do not run or deploy the topology. Just build, validate, \"",
                "+                + \"and print information about the topology.\"));",
                "@@ -86,11 +91,11 @@ public class Flux {",
                "-        options.addOption(option(1, \"z\", OPTION_ZOOKEEPER, \"host:port\", \"When running in local mode, use the ZooKeeper at the \" +",
                "-                \"specified <host>:<port> instead of the in-process ZooKeeper. (requires Storm 0.9.3 or later)\"));",
                "+        options.addOption(option(1, \"z\", OPTION_ZOOKEEPER, \"host:port\", \"When running in local mode, use the ZooKeeper at the \"",
                "+                + \"specified <host>:<port> instead of the in-process ZooKeeper. (requires Storm 0.9.3 or later)\"));",
                "-        options.addOption(option(1, \"f\", OPTION_FILTER, \"file\", \"Perform property substitution. Use the specified file \" +",
                "-                \"as a source of properties, and replace keys identified with {$[property name]} with the value defined \" +",
                "-                \"in the properties file.\"));",
                "+        options.addOption(option(1, \"f\", OPTION_FILTER, \"file\", \"Perform property substitution. Use the specified file \"",
                "+                + \"as a source of properties, and replace keys identified with {$[property name]} with the value defined \"",
                "+                + \"in the properties file.\"));",
                "-        options.addOption(option(0, \"e\", OPTION_ENV_FILTER, \"Perform environment variable substitution. Replace keys\" +",
                "-                \"identified with `${ENV-[NAME]}` will be replaced with the corresponding `NAME` environment value\"));",
                "+        options.addOption(option(0, \"e\", OPTION_ENV_FILTER, \"Perform environment variable substitution. Replace keys\"",
                "+                + \"identified with `${ENV-[NAME]}` will be replaced with the corresponding `NAME` environment value\"));",
                "@@ -106,7 +111,7 @@ public class Flux {",
                "-    private static Option option(int argCount, String shortName, String longName, String description){",
                "-       return option(argCount, shortName, longName, longName, description);",
                "+    private static Option option(int argCount, String shortName, String longName, String description) {",
                "+        return option(argCount, shortName, longName, longName, description);",
                "     }",
                "-    private static Option option(int argCount, String shortName, String longName, String argName, String description){",
                "+    private static Option option(int argCount, String shortName, String longName, String argName, String description) {",
                "         Option option = OptionBuilder.hasArgs(argCount)",
                "@@ -121,9 +126,9 @@ public class Flux {",
                "         HelpFormatter formatter = new HelpFormatter();",
                "-        formatter.printHelp(\"storm jar <my_topology_uber_jar.jar> \" +",
                "-                Flux.class.getName() +",
                "-                \" [options] <topology-config.yaml>\", options);",
                "+        formatter.printHelp(\"storm jar <my_topology_uber_jar.jar> \"",
                "+                + Flux.class.getName()",
                "+                + \" [options] <topology-config.yaml>\", options);",
                "     }",
                "-    private static void runCli(CommandLine cmd)throws Exception {",
                "-        if(!cmd.hasOption(OPTION_NO_SPLASH)) {",
                "+    private static void runCli(CommandLine cmd) throws Exception {",
                "+        if (!cmd.hasOption(OPTION_NO_SPLASH)) {",
                "             printSplash();",
                "@@ -134,3 +139,3 @@ public class Flux {",
                "         TopologyDef topologyDef = null;",
                "-        String filePath = (String)cmd.getArgList().get(0);",
                "+        String filePath = (String) cmd.getArgList().get(0);",
                "@@ -138,3 +143,3 @@ public class Flux {",
                "         String filterProps = null;",
                "-        if(cmd.hasOption(OPTION_FILTER)){",
                "+        if (cmd.hasOption(OPTION_FILTER)) {",
                "             filterProps = cmd.getOptionValue(OPTION_FILTER);",
                "@@ -144,3 +149,3 @@ public class Flux {",
                "         boolean envFilter = cmd.hasOption(OPTION_ENV_FILTER);",
                "-        if(cmd.hasOption(OPTION_RESOURCE)){",
                "+        if (cmd.hasOption(OPTION_RESOURCE)) {",
                "             printf(\"Parsing classpath resource: %s\", filePath);",
                "@@ -160,3 +165,3 @@ public class Flux {",
                "-        if(!cmd.hasOption(OPTION_NO_DETAIL)){",
                "+        if (!cmd.hasOption(OPTION_NO_DETAIL)) {",
                "             printTopologyInfo(context);",
                "@@ -164,3 +169,3 @@ public class Flux {",
                "-        if(!cmd.hasOption(OPTION_DRY_RUN)) {",
                "+        if (!cmd.hasOption(OPTION_DRY_RUN)) {",
                "             if (cmd.hasOption(OPTION_REMOTE)) {",
                "@@ -169,3 +174,3 @@ public class Flux {",
                "                 SubmitOptions submitOptions = null;",
                "-                if(cmd.hasOption(OPTION_INACTIVE)){",
                "+                if (cmd.hasOption(OPTION_INACTIVE)) {",
                "                     LOG.info(\"Deploying topology in an INACTIVE state...\");",
                "@@ -184,5 +189,5 @@ public class Flux {",
                "-    static void printTopologyInfo(ExecutionContext ctx){",
                "+    static void printTopologyInfo(ExecutionContext ctx) {",
                "         TopologyDef t = ctx.getTopologyDef();",
                "-        if(t.isDslTopology()) {",
                "+        if (t.isDslTopology()) {",
                "             print(\"---------- TOPOLOGY DETAILS ----------\");",
                "@@ -208,3 +213,3 @@ public class Flux {",
                "     // save a little typing",
                "-    private static void printf(String format, Object... args){",
                "+    private static void printf(String format, Object... args) {",
                "         print(String.format(format, args));",
                "@@ -212,3 +217,3 @@ public class Flux {",
                "-    private static void print(String string){",
                "+    private static void print(String string) {",
                "         System.out.println(string);",
                "@@ -219,3 +224,3 @@ public class Flux {",
                "         InputStream is = Flux.class.getResourceAsStream(\"/splash.txt\");",
                "-        if(is != null){",
                "+        if (is != null) {",
                "             InputStreamReader isr = new InputStreamReader(is, \"UTF-8\");",
                "@@ -223,3 +228,3 @@ public class Flux {",
                "             String line = null;",
                "-            while((line = br.readLine()) != null){",
                "+            while ((line = br.readLine()) != null) {",
                "                 System.out.println(line);",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/FluxBuilder.java b/flux/flux-core/src/main/java/org/apache/storm/flux/FluxBuilder.java",
                "index 1758a3792..20bde1839 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/FluxBuilder.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/FluxBuilder.java",
                "@@ -17,9 +17,39 @@",
                "  */",
                "+",
                " package org.apache.storm.flux;",
                "+import java.lang.reflect.Array;",
                "+import java.lang.reflect.Constructor;",
                "+import java.lang.reflect.Field;",
                "+import java.lang.reflect.InvocationTargetException;",
                "+import java.lang.reflect.Method;",
                "+import java.util.ArrayList;",
                "+import java.util.Collection;",
                "+import java.util.HashMap;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+",
                " import org.apache.storm.Config;",
                "-import org.apache.storm.flux.model.*;",
                "+import org.apache.storm.flux.model.BeanDef;",
                "+import org.apache.storm.flux.model.BeanListReference;",
                "+import org.apache.storm.flux.model.BeanReference;",
                "+import org.apache.storm.flux.model.BoltDef;",
                "+import org.apache.storm.flux.model.ConfigMethodDef;",
                "+import org.apache.storm.flux.model.ExecutionContext;",
                "+import org.apache.storm.flux.model.GroupingDef;",
                "+import org.apache.storm.flux.model.ObjectDef;",
                "+import org.apache.storm.flux.model.PropertyDef;",
                "+import org.apache.storm.flux.model.SpoutDef;",
                "+import org.apache.storm.flux.model.StreamDef;",
                "+import org.apache.storm.flux.model.TopologyDef;",
                " import org.apache.storm.generated.StormTopology;",
                " import org.apache.storm.grouping.CustomStreamGrouping;",
                "-import org.apache.storm.topology.*;",
                "+import org.apache.storm.topology.BoltDeclarer;",
                "+import org.apache.storm.topology.IBasicBolt;",
                "+import org.apache.storm.topology.IRichBolt;",
                "+import org.apache.storm.topology.IRichSpout;",
                "+import org.apache.storm.topology.IStatefulBolt;",
                "+import org.apache.storm.topology.IWindowedBolt;",
                "+import org.apache.storm.topology.SpoutDeclarer;",
                "+import org.apache.storm.topology.TopologyBuilder;",
                " import org.apache.storm.tuple.Fields;",
                "@@ -29,5 +59,2 @@ import org.slf4j.LoggerFactory;",
                "-import java.lang.reflect.*;",
                "-import java.util.*;",
                "-",
                " public class FluxBuilder {",
                "@@ -35,7 +62,7 @@ public class FluxBuilder {",
                "+",
                "     /**",
                "      * Given a topology definition, return a populated `org.apache.storm.Config` instance.",
                "-     *",
                "-     * @param topologyDef",
                "-     * @return",
                "+     * @param topologyDef topology definition",
                "+     * @return a Storm Config object",
                "      */",
                "@@ -50,10 +77,10 @@ public class FluxBuilder {",
                "      * Given a topology definition, return a Storm topology that can be run either locally or remotely.",
                "-     *",
                "-     * @param context",
                "-     * @return",
                "-     * @throws IllegalAccessException",
                "-     * @throws InstantiationException",
                "-     * @throws ClassNotFoundException",
                "-     * @throws NoSuchMethodException",
                "-     * @throws InvocationTargetException",
                "+     * @param context execution context",
                "+     * @return A runable Storm topology",
                "+     * @throws IllegalAccessException if security policy disallows operation",
                "+     * @throws InstantiationException if a class can't be instantiated",
                "+     * @throws ClassNotFoundException if a class can't be found",
                "+     * @throws NoSuchMethodException if a method can't be found",
                "+     * @throws InvocationTargetException if method invocation fails",
                "+     * @throws NoSuchFieldException if a referenced field does not exist",
                "      */",
                "@@ -65,5 +92,5 @@ public class FluxBuilder {",
                "-        if(!topologyDef.validate()){",
                "-            throw new IllegalArgumentException(\"Invalid topology config. Spouts, bolts and streams cannot be \" +",
                "-                    \"defined in the same configuration as a topologySource.\");",
                "+        if (!topologyDef.validate()) {",
                "+            throw new IllegalArgumentException(\"Invalid topology config. Spouts, bolts and streams cannot be \"",
                "+                    + \"defined in the same configuration as a topologySource.\");",
                "         }",
                "@@ -75,3 +102,3 @@ public class FluxBuilder {",
                "-        if(topologyDef.isDslTopology()) {",
                "+        if (topologyDef.isDslTopology()) {",
                "             // This is a DSL (YAML, etc.) topology...",
                "@@ -107,5 +134,5 @@ public class FluxBuilder {",
                "      * @param topologySource object to inspect for the specified method",
                "-     * @param methodName name of the method to look for",
                "-     * @return",
                "-     * @throws NoSuchMethodException",
                "+     * @param methodName     name of the method to look for",
                "+     * @return a Method object that returns a storm topology",
                "+     * @throws NoSuchMethodException if no such method exists",
                "      */",
                "@@ -113,9 +140,9 @@ public class FluxBuilder {",
                "         Class clazz = topologySource.getClass();",
                "-        Method[] methods =  clazz.getMethods();",
                "+        Method[] methods = clazz.getMethods();",
                "         ArrayList<Method> candidates = new ArrayList<Method>();",
                "-        for(Method method : methods){",
                "-            if(!method.getName().equals(methodName)){",
                "+        for (Method method : methods) {",
                "+            if (!method.getName().equals(methodName)) {",
                "                 continue;",
                "             }",
                "-            if(!method.getReturnType().equals(StormTopology.class)){",
                "+            if (!method.getReturnType().equals(StormTopology.class)) {",
                "                 continue;",
                "@@ -123,6 +150,6 @@ public class FluxBuilder {",
                "             Class[] paramTypes = method.getParameterTypes();",
                "-            if(paramTypes.length != 1){",
                "+            if (paramTypes.length != 1) {",
                "                 continue;",
                "             }",
                "-            if(paramTypes[0].isAssignableFrom(Map.class) || paramTypes[0].isAssignableFrom(Config.class)){",
                "+            if (paramTypes[0].isAssignableFrom(Map.class) || paramTypes[0].isAssignableFrom(Config.class)) {",
                "                 candidates.add(method);",
                "@@ -131,5 +158,5 @@ public class FluxBuilder {",
                "-        if(candidates.size() == 0){",
                "+        if (candidates.size() == 0) {",
                "             throw new IllegalArgumentException(\"Unable to find method '\" + methodName + \"' method in class: \" + clazz.getName());",
                "-        } else if (candidates.size() > 1){",
                "+        } else if (candidates.size() > 1) {",
                "             LOG.warn(\"Found multiple candidate methods in class '\" + clazz.getName() + \"'. Using the first one found\");",
                "@@ -141,4 +168,5 @@ public class FluxBuilder {",
                "     /**",
                "-     * @param context",
                "-     * @param builder",
                "+     * Builds stream definitions.",
                "+     * @param context context",
                "+     * @param builder builder",
                "      */",
                "@@ -154,3 +182,3 @@ public class FluxBuilder {",
                "             if (boltObj instanceof IRichBolt) {",
                "-                if(declarer == null) {",
                "+                if (declarer == null) {",
                "                     declarer = builder.setBolt(stream.getTo(),",
                "@@ -161,3 +189,3 @@ public class FluxBuilder {",
                "             } else if (boltObj instanceof IBasicBolt) {",
                "-                if(declarer == null) {",
                "+                if (declarer == null) {",
                "                     declarer = builder.setBolt(",
                "@@ -169,3 +197,3 @@ public class FluxBuilder {",
                "             } else if (boltObj instanceof IWindowedBolt) {",
                "-                if(declarer == null) {",
                "+                if (declarer == null) {",
                "                     declarer = builder.setBolt(",
                "@@ -177,3 +205,3 @@ public class FluxBuilder {",
                "             } else if (boltObj instanceof IStatefulBolt) {",
                "-                if(declarer == null) {",
                "+                if (declarer == null) {",
                "                     declarer = builder.setBolt(",
                "@@ -185,4 +213,4 @@ public class FluxBuilder {",
                "             } else {",
                "-                throw new IllegalArgumentException(\"Class does not appear to be a bolt: \" +",
                "-                        boltObj.getClass().getName());",
                "+                throw new IllegalArgumentException(\"Class does not appear to be a bolt: \"",
                "+                        + boltObj.getClass().getName());",
                "             }",
                "@@ -290,3 +318,3 @@ public class FluxBuilder {",
                "         LOG.debug(\"Checking arguments for references.\");",
                "-        List<Object> cArgs = new ArrayList<Object>();",
                "+        List<Object> constructorArgs = new ArrayList<Object>();",
                "         // resolve references",
                "@@ -294,3 +322,3 @@ public class FluxBuilder {",
                "             if (arg instanceof BeanReference) {",
                "-                cArgs.add(context.getComponent(((BeanReference) arg).getId()));",
                "+                constructorArgs.add(context.getComponent(((BeanReference) arg).getId()));",
                "             } else if (arg instanceof BeanListReference) {",
                "@@ -303,8 +331,8 @@ public class FluxBuilder {",
                "                 LOG.debug(\"BeanListReference resolved as {}\", components);",
                "-                cArgs.add(components);",
                "+                constructorArgs.add(components);",
                "             } else {",
                "-                cArgs.add(arg);",
                "+                constructorArgs.add(arg);",
                "             }",
                "         }",
                "-        return cArgs;",
                "+        return constructorArgs;",
                "     }",
                "@@ -317,10 +345,10 @@ public class FluxBuilder {",
                "             LOG.debug(\"Found constructor arguments in definition: \" + def.getConstructorArgs().getClass().getName());",
                "-            List<Object> cArgs = def.getConstructorArgs();",
                "-            if(def.hasReferences()){",
                "-                cArgs = resolveReferences(cArgs, context);",
                "+            List<Object> constructorArgs = def.getConstructorArgs();",
                "+            if (def.hasReferences()) {",
                "+                constructorArgs = resolveReferences(constructorArgs, context);",
                "             }",
                "-            Constructor con = findCompatibleConstructor(cArgs, clazz);",
                "+            Constructor con = findCompatibleConstructor(constructorArgs, clazz);",
                "             if (con != null) {",
                "                 LOG.debug(\"Found something seemingly compatible, attempting invocation...\");",
                "-                obj = con.newInstance(getArgsWithListCoercian(cArgs, con.getParameterTypes()));",
                "+                obj = con.newInstance(getArgsWithListCoercian(constructorArgs, con.getParameterTypes()));",
                "             } else {",
                "@@ -328,5 +356,25 @@ public class FluxBuilder {",
                "                         clazz.getName(),",
                "-                        cArgs);",
                "+                        constructorArgs);",
                "+                throw new IllegalArgumentException(msg);",
                "+            }",
                "+        } else if (def.hasFactory()) {",
                "+            Method method = null;",
                "+            List<Object> methodArgs = new ArrayList<>(); // empty if no factoryArgs",
                "+            if (def.hasFactoryArgs()) {",
                "+                methodArgs = def.getFactoryArgs();",
                "+                if (def.hasReferences()) {",
                "+                    methodArgs = resolveReferences(methodArgs, context);",
                "+                }",
                "+            }",
                "+            method = findCompatibleMethod(methodArgs, clazz, def.getFactory());",
                "+            if (method != null) {",
                "+                obj = method.invoke(null, getArgsWithListCoercian(methodArgs, method.getParameterTypes()));",
                "+            } else {",
                "+                String msg = String.format(\"Couldn't find a suitable static method '%s' for class '%s' with arguments '%s'.\",",
                "+                        def.getFactory(),",
                "+                        clazz.getName(),",
                "+                        methodArgs);",
                "                 throw new IllegalArgumentException(msg);",
                "             }",
                "+",
                "         } else {",
                "@@ -347,3 +395,3 @@ public class FluxBuilder {",
                "         Method getTopology = findGetTopologyMethod(topologySource, methodName);",
                "-        if(getTopology.getParameterTypes()[0].equals(Config.class)){",
                "+        if (getTopology.getParameterTypes()[0].equals(Config.class)) {",
                "             Config config = new Config();",
                "@@ -360,3 +408,3 @@ public class FluxBuilder {",
                "         Object grouping = buildObject(def, context);",
                "-        return (CustomStreamGrouping)grouping;",
                "+        return (CustomStreamGrouping) grouping;",
                "     }",
                "@@ -369,5 +417,5 @@ public class FluxBuilder {",
                "             IllegalAccessException, InvocationTargetException, InstantiationException, NoSuchFieldException {",
                "-        Collection<BeanDef> cDefs = context.getTopologyDef().getComponents();",
                "-        if (cDefs != null) {",
                "-            for (BeanDef bean : cDefs) {",
                "+        Collection<BeanDef> beanDefs = context.getTopologyDef().getComponents();",
                "+        if (beanDefs != null) {",
                "+            for (BeanDef bean : beanDefs) {",
                "                 Object obj = buildObject(bean, context);",
                "@@ -409,3 +457,3 @@ public class FluxBuilder {",
                "             IllegalAccessException, InstantiationException, NoSuchMethodException, InvocationTargetException, NoSuchFieldException {",
                "-        return (IRichSpout)buildObject(def, context);",
                "+        return (IRichSpout) buildObject(def, context);",
                "     }",
                "@@ -427,3 +475,2 @@ public class FluxBuilder {",
                "      * Given a list of constructor arguments, and a target class, attempt to find a suitable constructor.",
                "-     *",
                "      */",
                "@@ -458,2 +505,10 @@ public class FluxBuilder {",
                "+    /**",
                "+     * Invokes configuration methods on an class instance.",
                "+     * @param bean the bean/component definition",
                "+     * @param instance the class instance being operated on",
                "+     * @param context execution context",
                "+     * @throws InvocationTargetException if method invocation fails",
                "+     * @throws IllegalAccessException if security policy prefents invocation",
                "+     */",
                "     public static void invokeConfigMethods(ObjectDef bean, Object instance, ExecutionContext context)",
                "@@ -462,3 +517,3 @@ public class FluxBuilder {",
                "         List<ConfigMethodDef> methodDefs = bean.getConfigMethods();",
                "-        if(methodDefs == null || methodDefs.size() == 0){",
                "+        if (methodDefs == null || methodDefs.size() == 0) {",
                "             return;",
                "@@ -466,8 +521,8 @@ public class FluxBuilder {",
                "         Class clazz = instance.getClass();",
                "-        for(ConfigMethodDef methodDef : methodDefs){",
                "+        for (ConfigMethodDef methodDef : methodDefs) {",
                "             List<Object> args = methodDef.getArgs();",
                "-            if (args == null){",
                "+            if (args == null) {",
                "                 args = new ArrayList();",
                "             }",
                "-            if(methodDef.hasReferences()){",
                "+            if (methodDef.hasReferences()) {",
                "                 args = resolveReferences(args, context);",
                "@@ -476,3 +531,3 @@ public class FluxBuilder {",
                "             Method method = findCompatibleMethod(args, clazz, methodName);",
                "-            if(method != null) {",
                "+            if (method != null) {",
                "                 Object[] methodArgs = getArgsWithListCoercian(args, method.getParameterTypes());",
                "@@ -487,3 +542,3 @@ public class FluxBuilder {",
                "-    private static Method findCompatibleMethod(List<Object> args, Class target, String methodName){",
                "+    private static Method findCompatibleMethod(List<Object> args, Class target, String methodName) {",
                "         Method retval = null;",
                "@@ -499,3 +554,3 @@ public class FluxBuilder {",
                "                 boolean invokable = false;",
                "-                if (args.size() == 0){",
                "+                if (args.size() == 0) {",
                "                     // it's a method with zero args",
                "@@ -515,5 +570,5 @@ public class FluxBuilder {",
                "         if (eligibleCount > 1) {",
                "-            LOG.warn(\"Found multiple invokable methods for class {}, method {}, given arguments {}. \" +",
                "-                            \"Using the last one found.\",",
                "-                            new Object[]{target, methodName, args});",
                "+            LOG.warn(\"Found multiple invokable methods for class {}, method {}, given arguments {}. \"",
                "+                    + \"Using the last one found.\",",
                "+                    new Object[]{target, methodName, args});",
                "         }",
                "@@ -528,3 +583,2 @@ public class FluxBuilder {",
                "     private static Object[] getArgsWithListCoercian(List<Object> args, Class[] parameterTypes) {",
                "-//        Class[] parameterTypes = constructor.getParameterTypes();",
                "         if (parameterTypes.length != args.size()) {",
                "@@ -552,5 +606,5 @@ public class FluxBuilder {",
                "             }",
                "-            if (isPrimitiveBoolean(paramType) && Boolean.class.isAssignableFrom(objectType)){",
                "+            if (isPrimitiveBoolean(paramType) && Boolean.class.isAssignableFrom(objectType)) {",
                "                 LOG.debug(\"Its a primitive boolean.\");",
                "-                Boolean bool = (Boolean)args.get(i);",
                "+                Boolean bool = (Boolean) args.get(i);",
                "                 constructorParams[i] = bool.booleanValue();",
                "@@ -558,16 +612,17 @@ public class FluxBuilder {",
                "             }",
                "-            if(isPrimitiveNumber(paramType) && Number.class.isAssignableFrom(objectType)){",
                "-                LOG.debug(\"Its a primitive number.\");",
                "-                Number num = (Number)args.get(i);",
                "-                if(paramType == Float.TYPE){",
                "+            if ((isPrimitiveNumber(paramType) || Number.class.isAssignableFrom(paramType))",
                "+                    && Number.class.isAssignableFrom(objectType)) {",
                "+                LOG.debug(\"Its a number.\");",
                "+                Number num = (Number) args.get(i);",
                "+                if (paramType == Float.TYPE || paramType == Float.class) {",
                "                     constructorParams[i] = num.floatValue();",
                "-                } else if (paramType == Double.TYPE) {",
                "+                } else if (paramType == Double.TYPE || paramType == Double.class) {",
                "                     constructorParams[i] = num.doubleValue();",
                "-                } else if (paramType == Long.TYPE) {",
                "+                } else if (paramType == Long.TYPE || paramType == Long.class) {",
                "                     constructorParams[i] = num.longValue();",
                "-                } else if (paramType == Integer.TYPE) {",
                "+                } else if (paramType == Integer.TYPE || paramType == Integer.class) {",
                "                     constructorParams[i] = num.intValue();",
                "-                } else if (paramType == Short.TYPE) {",
                "+                } else if (paramType == Short.TYPE || paramType == Short.class) {",
                "                     constructorParams[i] = num.shortValue();",
                "-                } else if (paramType == Byte.TYPE) {",
                "+                } else if (paramType == Byte.TYPE || paramType == Byte.class) {",
                "                     constructorParams[i] = num.byteValue();",
                "@@ -580,5 +635,5 @@ public class FluxBuilder {",
                "             // enum conversion",
                "-            if(paramType.isEnum() && objectType.equals(String.class)){",
                "+            if (paramType.isEnum() && objectType.equals(String.class)) {",
                "                 LOG.debug(\"Yes, will convert a String to enum\");",
                "-                constructorParams[i] = Enum.valueOf(paramType, (String)args.get(i));",
                "+                constructorParams[i] = Enum.valueOf(paramType, (String) args.get(i));",
                "                 continue;",
                "@@ -611,5 +666,5 @@ public class FluxBuilder {",
                "      *",
                "-     * @param args",
                "-     * @param parameterTypes",
                "-     * @return",
                "+     * @param args arguments",
                "+     * @param parameterTypes parameter types",
                "+     * @return true if parameter types and args are compatible",
                "      */",
                "@@ -634,7 +689,8 @@ public class FluxBuilder {",
                "                 LOG.debug(\"Yes, assignment is possible.\");",
                "-            } else if (isPrimitiveBoolean(paramType) && Boolean.class.isAssignableFrom(objectType)){",
                "+            } else if (isPrimitiveBoolean(paramType) && Boolean.class.isAssignableFrom(objectType)) {",
                "                 LOG.debug(\"Yes, assignment is possible.\");",
                "-            } else if(isPrimitiveNumber(paramType) && Number.class.isAssignableFrom(objectType)){",
                "+            } else if (isPrimitiveNumber(paramType) || Number.class.isAssignableFrom(paramType)",
                "+                    && Number.class.isAssignableFrom(objectType)) {",
                "                 LOG.debug(\"Yes, assignment is possible.\");",
                "-            } else if(paramType.isEnum() && objectType.equals(String.class)){",
                "+            } else if (paramType.isEnum() && objectType.equals(String.class)) {",
                "                 LOG.debug(\"Yes, will convert a String to enum\");",
                "@@ -651,3 +707,3 @@ public class FluxBuilder {",
                "-    public static boolean isPrimitiveNumber(Class clazz){",
                "+    public static boolean isPrimitiveNumber(Class clazz) {",
                "         return clazz.isPrimitive() && !clazz.equals(boolean.class);",
                "@@ -655,3 +711,3 @@ public class FluxBuilder {",
                "-    public static boolean isPrimitiveBoolean(Class clazz){",
                "+    public static boolean isPrimitiveBoolean(Class clazz) {",
                "         return clazz.isPrimitive() && clazz.equals(boolean.class);",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/api/TopologySource.java b/flux/flux-core/src/main/java/org/apache/storm/flux/api/TopologySource.java",
                "index 277785468..1944c3836 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/api/TopologySource.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/api/TopologySource.java",
                "@@ -17,4 +17,6 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.api;",
                "+import java.util.Map;",
                "@@ -22,7 +24,5 @@ import org.apache.storm.generated.StormTopology;",
                "-import java.util.Map;",
                "-",
                " /**",
                "  * Marker interface for objects that can produce `StormTopology` objects.",
                "- *",
                "+ * <p/>",
                "  * If a `topology-source` class implements the `getTopology()` method, Flux will",
                "@@ -30,3 +30,3 @@ import java.util.Map;",
                "  * similar method that produces a `StormTopology` instance.",
                "- *",
                "+ * <p/>",
                "  * Note that it is not strictly necessary for a class to implement this interface.",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanDef.java",
                "index f0247ede2..199f4d23b 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanDef.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanListReference.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanListReference.java",
                "index 652210c56..8b9fa3ef4 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanListReference.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanListReference.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "@@ -27,5 +28,5 @@ public class BeanListReference {",
                "-    public BeanListReference(){}",
                "+    public BeanListReference() {}",
                "-    public BeanListReference(List<String> ids){",
                "+    public BeanListReference(List<String> ids) {",
                "         this.ids = ids;",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanReference.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanReference.java",
                "index bd236f1cd..f2f7cf94e 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanReference.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanReference.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "@@ -25,5 +26,5 @@ public class BeanReference {",
                "-    public BeanReference(){}",
                "+    public BeanReference() {}",
                "-    public BeanReference(String id){",
                "+    public BeanReference(String id) {",
                "         this.id = id;",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/BoltDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/BoltDef.java",
                "index 362abf120..696949223 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/BoltDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/BoltDef.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/ConfigMethodDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/ConfigMethodDef.java",
                "index 69cabc382..d10059849 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/ConfigMethodDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/ConfigMethodDef.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "@@ -41,2 +42,6 @@ public class ConfigMethodDef {",
                "+    /**",
                "+     * Set the method arguments.",
                "+     * @param args method parameters",
                "+     */",
                "     public void setArgs(List<Object> args) {",
                "@@ -44,6 +49,6 @@ public class ConfigMethodDef {",
                "         List<Object> newVal = new ArrayList<Object>();",
                "-        for(Object obj : args){",
                "-            if(obj instanceof LinkedHashMap){",
                "+        for (Object obj : args) {",
                "+            if (obj instanceof LinkedHashMap) {",
                "                 Map map = (Map)obj;",
                "-                if(map.containsKey(\"ref\") && map.size() == 1){",
                "+                if (map.containsKey(\"ref\") && map.size() == 1) {",
                "                     newVal.add(new BeanReference((String)map.get(\"ref\")));",
                "@@ -63,3 +68,3 @@ public class ConfigMethodDef {",
                "-    public boolean hasReferences(){",
                "+    public boolean hasReferences() {",
                "         return this.hasReferences;",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/ExecutionContext.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/ExecutionContext.java",
                "index 1520006f0..ec0529e83 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/ExecutionContext.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/ExecutionContext.java",
                "@@ -17,7 +17,4 @@",
                "  */",
                "-package org.apache.storm.flux.model;",
                "-import org.apache.storm.Config;",
                "-import org.apache.storm.task.IBolt;",
                "-import org.apache.storm.topology.IRichSpout;",
                "+package org.apache.storm.flux.model;",
                "@@ -27,2 +24,6 @@ import java.util.Map;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.task.IBolt;",
                "+import org.apache.storm.topology.IRichSpout;",
                "+",
                " /**",
                "@@ -47,3 +48,3 @@ public class ExecutionContext {",
                "-    public ExecutionContext(TopologyDef topologyDef, Config config){",
                "+    public ExecutionContext(TopologyDef topologyDef, Config config) {",
                "         this.topologyDef = topologyDef;",
                "@@ -52,3 +53,3 @@ public class ExecutionContext {",
                "-    public TopologyDef getTopologyDef(){",
                "+    public TopologyDef getTopologyDef() {",
                "         return this.topologyDef;",
                "@@ -56,3 +57,3 @@ public class ExecutionContext {",
                "-    public void addSpout(String id, IRichSpout spout){",
                "+    public void addSpout(String id, IRichSpout spout) {",
                "         this.spoutMap.put(id, spout);",
                "@@ -60,3 +61,3 @@ public class ExecutionContext {",
                "-    public void addBolt(String id, Object bolt){",
                "+    public void addBolt(String id, Object bolt) {",
                "         this.boltMap.put(id, bolt);",
                "@@ -64,3 +65,3 @@ public class ExecutionContext {",
                "-    public Object getBolt(String id){",
                "+    public Object getBolt(String id) {",
                "         return this.boltMap.get(id);",
                "@@ -68,3 +69,3 @@ public class ExecutionContext {",
                "-    public void addComponent(String id, Object value){",
                "+    public void addComponent(String id, Object value) {",
                "         this.componentMap.put(id, value);",
                "@@ -72,3 +73,3 @@ public class ExecutionContext {",
                "-    public Object getComponent(String id){",
                "+    public Object getComponent(String id) {",
                "         return this.componentMap.get(id);",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/GroupingDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/GroupingDef.java",
                "index e4fac8e38..f191e3c62 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/GroupingDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/GroupingDef.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "@@ -27,3 +28,3 @@ public class GroupingDef {",
                "     /**",
                "-     * Types of stream groupings Storm allows",
                "+     * Types of stream groupings Storm allows.",
                "      */",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/IncludeDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/IncludeDef.java",
                "index 23fd9d2ff..52e9ac3d6 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/IncludeDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/IncludeDef.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "@@ -21,3 +22,3 @@ package org.apache.storm.flux.model;",
                "  * Represents an include. Includes can be either a file or a classpath resource.",
                "- *",
                "+ *<p/>",
                "  * If an include is marked as `override=true` then existing properties will be replaced.",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/ObjectDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/ObjectDef.java",
                "index 04a7e8a7e..170ee4f3d 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/ObjectDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/ObjectDef.java",
                "@@ -17,5 +17,4 @@",
                "  */",
                "-package org.apache.storm.flux.model;",
                "-import org.apache.storm.Config;",
                "+package org.apache.storm.flux.model;",
                "@@ -36,2 +35,4 @@ public class ObjectDef {",
                "     private List<ConfigMethodDef> configMethods;",
                "+    private String factory;",
                "+    private List<Object> factoryArgs;",
                "@@ -49,2 +50,6 @@ public class ObjectDef {",
                "+    /**",
                "+     * Sets the arguments for the constructor and checks for references.",
                "+     * @param constructorArgs Constructor arguments",
                "+     */",
                "     public void setConstructorArgs(List<Object> constructorArgs) {",
                "@@ -52,6 +57,6 @@ public class ObjectDef {",
                "         List<Object> newVal = new ArrayList<Object>();",
                "-        for(Object obj : constructorArgs){",
                "-            if(obj instanceof LinkedHashMap){",
                "+        for (Object obj : constructorArgs) {",
                "+            if (obj instanceof LinkedHashMap) {",
                "                 Map map = (Map)obj;",
                "-                if(map.containsKey(\"ref\") && map.size() == 1) {",
                "+                if (map.containsKey(\"ref\") && map.size() == 1) {",
                "                     newVal.add(new BeanReference((String) map.get(\"ref\")));",
                "@@ -71,3 +76,3 @@ public class ObjectDef {",
                "-    public boolean hasConstructorArgs(){",
                "+    public boolean hasConstructorArgs() {",
                "         return this.constructorArgs != null && this.constructorArgs.size() > 0;",
                "@@ -75,3 +80,3 @@ public class ObjectDef {",
                "-    public boolean hasReferences(){",
                "+    public boolean hasReferences() {",
                "         return this.hasReferences;",
                "@@ -94,2 +99,47 @@ public class ObjectDef {",
                "     }",
                "+",
                "+    public boolean hasFactory() {",
                "+        return this.factory != null && !this.factory.isEmpty();",
                "+    }",
                "+",
                "+    public boolean hasFactoryArgs() {",
                "+        return this.factoryArgs != null && this.factoryArgs.size() > 0;",
                "+    }",
                "+",
                "+    public String getFactory() {",
                "+        return this.factory;",
                "+    }",
                "+",
                "+    public void setFactory(String factory) {",
                "+        this.factory = factory;",
                "+    }",
                "+",
                "+    public List<Object> getFactoryArgs() {",
                "+        return this.factoryArgs;",
                "+    }",
                "+",
                "+    /**",
                "+     * Sets factory method arguments and checks for references.",
                "+     * @param factoryArgs factory method arguments",
                "+     */",
                "+    public void setFactoryArgs(List<Object> factoryArgs) {",
                "+        List<Object> newVal = new ArrayList<Object>();",
                "+        for (Object obj : factoryArgs) {",
                "+            if (obj instanceof LinkedHashMap) {",
                "+                Map map = (Map)obj;",
                "+                if (map.containsKey(\"ref\") && map.size() == 1) {",
                "+                    newVal.add(new BeanReference((String) map.get(\"ref\")));",
                "+                    this.hasReferences = true;",
                "+                } else if (map.containsKey(\"reflist\") && map.size() == 1) {",
                "+                    newVal.add(new BeanListReference((List<String>) map.get(\"reflist\")));",
                "+                    this.hasReferences = true;",
                "+                } else {",
                "+                    newVal.add(obj);",
                "+                }",
                "+            } else {",
                "+                newVal.add(obj);",
                "+            }",
                "+        }",
                "+        this.factoryArgs = newVal;",
                "+    }",
                " }",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/PropertyDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/PropertyDef.java",
                "index f3d7704ae..3debba979 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/PropertyDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/PropertyDef.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "@@ -36,4 +37,9 @@ public class PropertyDef {",
                "+    /**",
                "+     * Sets the value of this property. Throws IllegalArgumentException if a reference has",
                "+     * already been set.",
                "+     * @param value property value",
                "+     */",
                "     public void setValue(Object value) {",
                "-        if(this.ref != null){",
                "+        if (this.ref != null) {",
                "             throw new IllegalStateException(\"A property can only have a value OR a reference, not both.\");",
                "@@ -47,4 +53,9 @@ public class PropertyDef {",
                "+    /**",
                "+     * Sets the value of this property to a reference. Throws IllegalArgumentException if a value has",
                "+     * already been set.",
                "+     * @param ref property reference",
                "+     */",
                "     public void setRef(String ref) {",
                "-        if(this.value != null){",
                "+        if (this.value != null) {",
                "             throw new IllegalStateException(\"A property can only have a value OR a reference, not both.\");",
                "@@ -54,3 +65,3 @@ public class PropertyDef {",
                "-    public boolean isReference(){",
                "+    public boolean isReference() {",
                "         return this.ref != null;",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/SpoutDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/SpoutDef.java",
                "index 277c60177..df17b2a6d 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/SpoutDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/SpoutDef.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/StreamDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/StreamDef.java",
                "index da80f1c64..2a1b87cf4 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/StreamDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/StreamDef.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "@@ -21,3 +22,3 @@ package org.apache.storm.flux.model;",
                "  * Represents a stream of tuples from one Storm component (Spout or Bolt) to another (an edge in the topology DAG).",
                "- *",
                "+ * <p/>",
                "  * Required fields are `from` and `to`, which define the source and destination, and the stream `grouping`.",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/TopologyDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/TopologyDef.java",
                "index 86614f19e..a0b4a0329 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/TopologyDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/TopologyDef.java",
                "@@ -17,4 +17,11 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "+import java.util.ArrayList;",
                "+import java.util.HashMap;",
                "+import java.util.LinkedHashMap;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+",
                " import org.slf4j.Logger;",
                "@@ -22,7 +29,5 @@ import org.slf4j.LoggerFactory;",
                "-import java.util.*;",
                "-",
                " /**",
                "  * Bean represenation of a topology.",
                "- *",
                "+ * <p/>",
                "  * It consists of the following:",
                "@@ -60,4 +65,9 @@ public class TopologyDef {",
                "-    public void setName(String name, boolean override){",
                "-        if(this.name == null || override){",
                "+    /**",
                "+     * Sets the name of the topology.",
                "+     * @param name topology name",
                "+     * @param override whether to override if already set",
                "+     */",
                "+    public void setName(String name, boolean override) {",
                "+        if (this.name == null || override) {",
                "             this.name = name;",
                "@@ -68,2 +78,6 @@ public class TopologyDef {",
                "+    /**",
                "+     * Returns all spout definitions.",
                "+     * @return spout definitions.",
                "+     */",
                "     public List<SpoutDef> getSpouts() {",
                "@@ -74,5 +88,9 @@ public class TopologyDef {",
                "+    /**",
                "+     * Set spout definitions.",
                "+     * @param spouts spout definitions",
                "+     */",
                "     public void setSpouts(List<SpoutDef> spouts) {",
                "         this.spoutMap = new LinkedHashMap<String, SpoutDef>();",
                "-        for(SpoutDef spout : spouts){",
                "+        for (SpoutDef spout : spouts) {",
                "             this.spoutMap.put(spout.getId(), spout);",
                "@@ -81,2 +99,6 @@ public class TopologyDef {",
                "+    /**",
                "+     * Returns bolt definitions.",
                "+     * @return bolt definitions",
                "+     */",
                "     public List<BoltDef> getBolts() {",
                "@@ -87,5 +109,9 @@ public class TopologyDef {",
                "+    /**",
                "+     * Sets bolt definitions.",
                "+     * @param bolts bolt definitions",
                "+     */",
                "     public void setBolts(List<BoltDef> bolts) {",
                "-        this.boltMap = new LinkedHashMap<String, BoltDef>();",
                "-        for(BoltDef bolt : bolts){",
                "+        this.boltMap = new LinkedHashMap<>();",
                "+        for (BoltDef bolt : bolts) {",
                "             this.boltMap.put(bolt.getId(), bolt);",
                "@@ -110,2 +136,6 @@ public class TopologyDef {",
                "+    /**",
                "+     * Returns a list of all component definitions.",
                "+     * @return components",
                "+     */",
                "     public List<BeanDef> getComponents() {",
                "@@ -116,5 +146,9 @@ public class TopologyDef {",
                "+    /**",
                "+     * Sets the list of component definitions.",
                "+     * @param components components definitions",
                "+     */",
                "     public void setComponents(List<BeanDef> components) {",
                "-        this.componentMap = new LinkedHashMap<String, BeanDef>();",
                "-        for(BeanDef component : components){",
                "+        this.componentMap = new LinkedHashMap<>();",
                "+        for (BeanDef component : components) {",
                "             this.componentMap.put(component.getId(), component);",
                "@@ -132,3 +166,3 @@ public class TopologyDef {",
                "     // utility methods",
                "-    public int parallelismForBolt(String boltId){",
                "+    public int parallelismForBolt(String boltId) {",
                "         return this.boltMap.get(boltId).getParallelism();",
                "@@ -136,3 +170,3 @@ public class TopologyDef {",
                "-    public BoltDef getBoltDef(String id){",
                "+    public BoltDef getBoltDef(String id) {",
                "         return this.boltMap.get(id);",
                "@@ -140,3 +174,3 @@ public class TopologyDef {",
                "-    public SpoutDef getSpoutDef(String id){",
                "+    public SpoutDef getSpoutDef(String id) {",
                "         return this.spoutMap.get(id);",
                "@@ -144,3 +178,3 @@ public class TopologyDef {",
                "-    public BeanDef getComponent(String id){",
                "+    public BeanDef getComponent(String id) {",
                "         return this.componentMap.get(id);",
                "@@ -148,7 +182,12 @@ public class TopologyDef {",
                "-    // used by includes implementation",
                "-    public void addAllBolts(List<BoltDef> bolts, boolean override){",
                "-        for(BoltDef bolt : bolts){",
                "+    /**",
                "+     * Adds a list of bolt definitions. Optionally overriding existing definitions",
                "+     * if one with the same ID already exists.",
                "+     * @param bolts bolt definitions",
                "+     * @param override whether or not to override existing definitions",
                "+     */",
                "+    public void addAllBolts(List<BoltDef> bolts, boolean override) {",
                "+        for (BoltDef bolt : bolts) {",
                "             String id = bolt.getId();",
                "-            if(this.boltMap.get(id) == null || override) {",
                "+            if (this.boltMap.get(id) == null || override) {",
                "                 this.boltMap.put(bolt.getId(), bolt);",
                "@@ -160,6 +199,12 @@ public class TopologyDef {",
                "-    public void addAllSpouts(List<SpoutDef> spouts, boolean override){",
                "-        for(SpoutDef spout : spouts){",
                "+    /**",
                "+     * Adds a list of spout definitions. Optionally overriding existing definitions",
                "+     * if one with the same ID already exists.",
                "+     * @param spouts spout definitions",
                "+     * @param override whether or not to override existing definitions",
                "+     */",
                "+    public void addAllSpouts(List<SpoutDef> spouts, boolean override) {",
                "+        for (SpoutDef spout : spouts) {",
                "             String id = spout.getId();",
                "-            if(this.spoutMap.get(id) == null || override) {",
                "+            if (this.spoutMap.get(id) == null || override) {",
                "                 this.spoutMap.put(spout.getId(), spout);",
                "@@ -171,6 +216,12 @@ public class TopologyDef {",
                "+    /**",
                "+     * Adds a list of component definitions. Optionally overriding existing definitions",
                "+     * if one with the same ID already exists.",
                "+     * @param components component definitions",
                "+     * @param override whether or not to override existing definitions",
                "+     */",
                "     public void addAllComponents(List<BeanDef> components, boolean override) {",
                "-        for(BeanDef bean : components){",
                "+        for (BeanDef bean : components) {",
                "             String id = bean.getId();",
                "-            if(this.componentMap.get(id) == null || override) {",
                "+            if (this.componentMap.get(id) == null || override) {",
                "                 this.componentMap.put(bean.getId(), bean);",
                "@@ -182,2 +233,8 @@ public class TopologyDef {",
                "+    /**",
                "+     * Adds a list of stream definitions. Optionally overriding existing definitions",
                "+     * if one with the same ID already exists.",
                "+     * @param streams stream definitions",
                "+     * @param override whether or not to override existing definitions (currently ignored)",
                "+     */",
                "     public void addAllStreams(List<StreamDef> streams, boolean override) {",
                "@@ -196,3 +253,3 @@ public class TopologyDef {",
                "-    public boolean isDslTopology(){",
                "+    public boolean isDslTopology() {",
                "         return this.topologySource == null;",
                "@@ -201,3 +258,7 @@ public class TopologyDef {",
                "-    public boolean validate(){",
                "+    /**",
                "+     * Determines is this represents a valid Topology.",
                "+     * @return true if valid",
                "+     */",
                "+    public boolean validate() {",
                "         boolean hasSpouts = this.spoutMap != null && this.spoutMap.size() > 0;",
                "@@ -205,3 +266,2 @@ public class TopologyDef {",
                "         boolean hasStreams = this.streams != null && this.streams.size() > 0;",
                "-        boolean hasSpoutsBoltsStreams = hasStreams && hasBolts && hasSpouts;",
                "         // you cant define a topologySource and a DSL topology at the same time...",
                "@@ -210,3 +270,3 @@ public class TopologyDef {",
                "         }",
                "-        if(isDslTopology() && (hasSpouts && hasBolts && hasStreams)) {",
                "+        if (isDslTopology() && (hasSpouts && hasBolts && hasStreams)) {",
                "             return true;",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/TopologySourceDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/TopologySourceDef.java",
                "index d6a2f57cc..17904a030 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/TopologySourceDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/TopologySourceDef.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "@@ -24,3 +25,3 @@ public class TopologySourceDef extends ObjectDef {",
                "-    public TopologySourceDef(){",
                "+    public TopologySourceDef() {",
                "         this.methodName = DEFAULT_METHOD_NAME;",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/model/VertexDef.java b/flux/flux-core/src/main/java/org/apache/storm/flux/model/VertexDef.java",
                "index 865181930..cd09d052a 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/model/VertexDef.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/model/VertexDef.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.model;",
                "@@ -20,3 +21,3 @@ package org.apache.storm.flux.model;",
                " /**",
                "- * Abstract parent class of component definitions",
                "+ * Abstract parent class of component definitions.",
                "  * (spouts/bolts)",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java b/flux/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java",
                "index 1bb018a93..cc23fb88f 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java",
                "@@ -17,4 +17,12 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.parser;",
                "+import java.io.ByteArrayOutputStream;",
                "+import java.io.FileInputStream;",
                "+import java.io.IOException;",
                "+import java.io.InputStream;",
                "+import java.util.Map;",
                "+import java.util.Properties;",
                "+",
                " import org.apache.storm.flux.model.BoltDef;",
                "@@ -29,9 +37,5 @@ import org.yaml.snakeyaml.constructor.Constructor;",
                "-import java.io.ByteArrayOutputStream;",
                "-import java.io.FileInputStream;",
                "-import java.io.IOException;",
                "-import java.io.InputStream;",
                "-import java.util.Map;",
                "-import java.util.Properties;",
                "-",
                "+/**",
                "+ * Static utility methods for parsing flux YAML.",
                "+ */",
                " public class FluxParser {",
                "@@ -39,8 +43,20 @@ public class FluxParser {",
                "-    private FluxParser(){}",
                "+    private FluxParser() {",
                "+    }",
                "     // TODO refactor input stream processing (see parseResource() method).",
                "+",
                "+    /**",
                "+     * Parse a flux topology definition.",
                "+     * @param inputFile source YAML file",
                "+     * @param dumpYaml if true, dump the parsed YAML to stdout",
                "+     * @param processIncludes whether or not to process includes",
                "+     * @param propertiesFile properties file for variable substitution",
                "+     * @param envSub whether or not to perform environment variable substitution",
                "+     * @return resulting topologuy definition",
                "+     * @throws IOException if there is a problem reading file(s)",
                "+     */",
                "     public static TopologyDef parseFile(String inputFile, boolean dumpYaml, boolean processIncludes,",
                "-    \tString propertiesFile, boolean envSub) throws IOException {",
                "-   ",
                "+                                        String propertiesFile, boolean envSub) throws IOException {",
                "+",
                "         FileInputStream in = new FileInputStream(inputFile);",
                "@@ -48,3 +64,3 @@ public class FluxParser {",
                "         in.close();",
                "-        ",
                "+",
                "         return topology;",
                "@@ -52,5 +68,15 @@ public class FluxParser {",
                "+    /**",
                "+     * Parse a flux topology definition from a classpath resource..",
                "+     * @param resource YAML resource",
                "+     * @param dumpYaml if true, dump the parsed YAML to stdout",
                "+     * @param processIncludes whether or not to process includes",
                "+     * @param propertiesFile properties file for variable substitution",
                "+     * @param envSub whether or not to perform environment variable substitution",
                "+     * @return resulting topologuy definition",
                "+     * @throws IOException if there is a problem reading file(s)",
                "+     */",
                "     public static TopologyDef parseResource(String resource, boolean dumpYaml, boolean processIncludes,",
                "-    \tString propertiesFile, boolean envSub) throws IOException {",
                "-        ",
                "+                                            String propertiesFile, boolean envSub) throws IOException {",
                "+",
                "         InputStream in = FluxParser.class.getResourceAsStream(resource);",
                "@@ -58,27 +84,37 @@ public class FluxParser {",
                "         in.close();",
                "-        ",
                "+",
                "         return topology;",
                "     }",
                "-    ",
                "+",
                "+    /**",
                "+     * Parse a flux topology definition.",
                "+     * @param inputStream InputStream representation of YAML file",
                "+     * @param dumpYaml if true, dump the parsed YAML to stdout",
                "+     * @param processIncludes whether or not to process includes",
                "+     * @param propertiesFile properties file for variable substitution",
                "+     * @param envSub whether or not to perform environment variable substitution",
                "+     * @return resulting topologuy definition",
                "+     * @throws IOException if there is a problem reading file(s)",
                "+     */",
                "     public static TopologyDef parseInputStream(InputStream inputStream, boolean dumpYaml, boolean processIncludes,",
                "-    \tString propertiesFile, boolean envSub) throws IOException {",
                "-\t\t",
                "-\tYaml yaml = yaml();",
                "-    \t",
                "-\tif (inputStream == null) {",
                "-\t\tLOG.error(\"Unable to load input stream\");",
                "-\t\tSystem.exit(1);",
                "-\t}",
                "-\t\t",
                "-\tTopologyDef topology = loadYaml(yaml, inputStream, propertiesFile, envSub);",
                "-\t\t",
                "-\tif (dumpYaml) {",
                "-\t\tdumpYaml(topology, yaml);",
                "-\t}",
                "-\t",
                "-\tif (processIncludes) {",
                "-\t\treturn processIncludes(yaml, topology, propertiesFile, envSub);",
                "-\t} else {",
                "-\t\treturn topology;",
                "-\t}",
                "+                                               String propertiesFile, boolean envSub) throws IOException {",
                "+",
                "+        Yaml yaml = yaml();",
                "+",
                "+        if (inputStream == null) {",
                "+            LOG.error(\"Unable to load input stream\");",
                "+            System.exit(1);",
                "+        }",
                "+",
                "+        TopologyDef topology = loadYaml(yaml, inputStream, propertiesFile, envSub);",
                "+",
                "+        if (dumpYaml) {",
                "+            dumpYaml(topology, yaml);",
                "+        }",
                "+",
                "+        if (processIncludes) {",
                "+            return processIncludes(yaml, topology, propertiesFile, envSub);",
                "+        } else {",
                "+            return topology;",
                "+        }",
                "     }",
                "@@ -89,3 +125,3 @@ public class FluxParser {",
                "         int b = -1;",
                "-        while((b = in.read()) != -1){",
                "+        while ((b = in.read()) != -1) {",
                "             bos.write(b);",
                "@@ -96,3 +132,3 @@ public class FluxParser {",
                "         // properties file substitution",
                "-        if(propsFile != null){",
                "+        if (propsFile != null) {",
                "             LOG.info(\"Performing property substitution.\");",
                "@@ -110,6 +146,6 @@ public class FluxParser {",
                "         // environment variable substitution",
                "-        if(envSubstitution){",
                "+        if (envSubstitution) {",
                "             LOG.info(\"Performing environment variable substitution...\");",
                "             Map<String, String> envs = System.getenv();",
                "-            for(String key : envs.keySet()){",
                "+            for (String key : envs.keySet()) {",
                "                 str = str.replace(\"${ENV-\" + key + \"}\", envs.get(key));",
                "@@ -119,6 +155,6 @@ public class FluxParser {",
                "         }",
                "-        return (TopologyDef)yaml.load(str);",
                "+        return (TopologyDef) yaml.load(str);",
                "     }",
                "-    private static void dumpYaml(TopologyDef topology, Yaml yaml){",
                "+    private static void dumpYaml(TopologyDef topology, Yaml yaml) {",
                "         System.out.println(\"Configuration (interpreted): \\n\" + yaml.dump(topology));",
                "@@ -126,5 +162,3 @@ public class FluxParser {",
                "-    private static Yaml yaml(){",
                "-        Constructor constructor = new Constructor(TopologyDef.class);",
                "-",
                "+    private static Yaml yaml() {",
                "         TypeDescription topologyDescription = new TypeDescription(TopologyDef.class);",
                "@@ -133,5 +167,7 @@ public class FluxParser {",
                "         topologyDescription.putListPropertyType(\"includes\", IncludeDef.class);",
                "+",
                "+        Constructor constructor = new Constructor(TopologyDef.class);",
                "         constructor.addTypeDescription(topologyDescription);",
                "-        Yaml  yaml = new Yaml(constructor);",
                "+        Yaml yaml = new Yaml(constructor);",
                "         return yaml;",
                "@@ -140,4 +176,4 @@ public class FluxParser {",
                "     /**",
                "-     *",
                "-     * @param yaml the yaml parser for parsing the include file(s)",
                "+     * Process includes contained within a yaml file.",
                "+     * @param yaml        the yaml parser for parsing the include file(s)",
                "      * @param topologyDef the topology definition containing (possibly zero) includes",
                "@@ -148,4 +184,4 @@ public class FluxParser {",
                "         //TODO support multiple levels of includes",
                "-        if(topologyDef.getIncludes() != null) {",
                "-            for (IncludeDef include : topologyDef.getIncludes()){",
                "+        if (topologyDef.getIncludes() != null) {",
                "+            for (IncludeDef include : topologyDef.getIncludes()) {",
                "                 TopologyDef includeTopologyDef = null;",
                "@@ -162,3 +198,3 @@ public class FluxParser {",
                "                 // name",
                "-                if(includeTopologyDef.getName() != null){",
                "+                if (includeTopologyDef.getName() != null) {",
                "                     topologyDef.setName(includeTopologyDef.getName(), override);",
                "@@ -167,3 +203,3 @@ public class FluxParser {",
                "                 // config",
                "-                if(includeTopologyDef.getConfig() != null) {",
                "+                if (includeTopologyDef.getConfig() != null) {",
                "                     //TODO move this logic to the model class",
                "@@ -171,10 +207,9 @@ public class FluxParser {",
                "                     Map<String, Object> includeConfig = includeTopologyDef.getConfig();",
                "-                    if(override) {",
                "+                    if (override) {",
                "                         config.putAll(includeTopologyDef.getConfig());",
                "                     } else {",
                "-                        for(String key : includeConfig.keySet()){",
                "-                            if(config.containsKey(key)){",
                "+                        for (String key : includeConfig.keySet()) {",
                "+                            if (config.containsKey(key)) {",
                "                                 LOG.warn(\"Ignoring attempt to set topology config property '{}' with override == false\", key);",
                "-                            }",
                "-                            else {",
                "+                            } else {",
                "                                 config.put(key, includeConfig.get(key));",
                "@@ -186,3 +221,3 @@ public class FluxParser {",
                "                 //component overrides",
                "-                if(includeTopologyDef.getComponents() != null){",
                "+                if (includeTopologyDef.getComponents() != null) {",
                "                     topologyDef.addAllComponents(includeTopologyDef.getComponents(), override);",
                "@@ -190,3 +225,3 @@ public class FluxParser {",
                "                 //bolt overrides",
                "-                if(includeTopologyDef.getBolts() != null){",
                "+                if (includeTopologyDef.getBolts() != null) {",
                "                     topologyDef.addAllBolts(includeTopologyDef.getBolts(), override);",
                "@@ -194,3 +229,3 @@ public class FluxParser {",
                "                 //spout overrides",
                "-                if(includeTopologyDef.getSpouts() != null) {",
                "+                if (includeTopologyDef.getSpouts() != null) {",
                "                     topologyDef.addAllSpouts(includeTopologyDef.getSpouts(), override);",
                "@@ -199,3 +234,3 @@ public class FluxParser {",
                "                 //TODO streams should be uniquely identifiable",
                "-                if(includeTopologyDef.getStreams() != null) {",
                "+                if (includeTopologyDef.getStreams() != null) {",
                "                     topologyDef.addAllStreams(includeTopologyDef.getStreams(), override);",
                "diff --git a/flux/flux-examples/pom.xml b/flux/flux-examples/pom.xml",
                "index 3a15f2e01..a9d9c1e4b 100644",
                "--- a/flux/flux-examples/pom.xml",
                "+++ b/flux/flux-examples/pom.xml",
                "@@ -152,3 +152,3 @@",
                "                 <configuration>",
                "-                    <maxAllowedViolations>1</maxAllowedViolations>",
                "+                    <maxAllowedViolations>0</maxAllowedViolations>",
                "                 </configuration>",
                "diff --git a/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/StatefulWordCounter.java b/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/StatefulWordCounter.java",
                "index cc3ef38fc..f1f4a6cd6 100644",
                "--- a/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/StatefulWordCounter.java",
                "+++ b/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/StatefulWordCounter.java",
                "@@ -17,4 +17,7 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.examples;",
                "+import java.util.Map;",
                "+",
                " import org.apache.storm.state.KeyValueState;",
                "@@ -22,4 +25,4 @@ import org.apache.storm.task.OutputCollector;",
                " import org.apache.storm.task.TopologyContext;",
                "-import org.apache.storm.topology.base.BaseStatefulBolt;",
                " import org.apache.storm.topology.OutputFieldsDeclarer;",
                "+import org.apache.storm.topology.base.BaseStatefulBolt;",
                " import org.apache.storm.tuple.Fields;",
                "@@ -28,4 +31,2 @@ import org.apache.storm.tuple.Values;",
                "-import java.util.Map;",
                "-",
                " public class StatefulWordCounter extends BaseStatefulBolt<KeyValueState<String, Long>> {",
                "diff --git a/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestPrintBolt.java b/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestPrintBolt.java",
                "index 137e35489..2d28c7f2c 100644",
                "--- a/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestPrintBolt.java",
                "+++ b/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestPrintBolt.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.examples;",
                "@@ -25,3 +26,3 @@ import org.apache.storm.tuple.Tuple;",
                " /**",
                "- * Prints the tuples to stdout",
                "+ * Prints the tuples to stdout.",
                "  */",
                "diff --git a/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestWindowBolt.java b/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestWindowBolt.java",
                "index f8c140bdb..57826b547 100644",
                "--- a/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestWindowBolt.java",
                "+++ b/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestWindowBolt.java",
                "@@ -17,4 +17,7 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.examples;",
                "+import java.util.Map;",
                "+",
                " import org.apache.storm.task.OutputCollector;",
                "@@ -27,4 +30,2 @@ import org.apache.storm.windowing.TupleWindow;",
                "-import java.util.Map;",
                "-",
                " public class TestWindowBolt extends BaseWindowedBolt {",
                "diff --git a/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCountClient.java b/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCountClient.java",
                "index eb4fb7a76..6e732ae37 100644",
                "--- a/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCountClient.java",
                "+++ b/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCountClient.java",
                "@@ -17,4 +17,8 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.examples;",
                "+import java.io.FileInputStream;",
                "+import java.util.Properties;",
                "+",
                " import org.apache.hadoop.conf.Configuration;",
                "@@ -26,11 +30,8 @@ import org.apache.hadoop.hbase.util.Bytes;",
                "-import java.io.FileInputStream;",
                "-import java.util.Properties;",
                "-",
                " /**",
                "  * Connects to the 'WordCount' HBase table and prints counts for each word.",
                "- *",
                "+ * <p/>",
                "  * Assumes you have run (or are running) the YAML topology definition in",
                "  * <code>simple_hbase.yaml</code>",
                "- *",
                "+ * <p/>",
                "  * You will also need to modify `src/main/resources/hbase-site.xml`",
                "@@ -42,5 +43,10 @@ public class WordCountClient {",
                "+    /**",
                "+     * Entry point for WordCountClient.",
                "+     * @param args command line arguments",
                "+     * @throws Exception if an unexpected error occurs",
                "+     */",
                "     public static void main(String[] args) throws Exception {",
                "         Configuration config = HBaseConfiguration.create();",
                "-        if(args.length == 1){",
                "+        if (args.length == 1) {",
                "             Properties props = new Properties();",
                "@@ -48,3 +54,3 @@ public class WordCountClient {",
                "             System.out.println(\"HBase configuration:\");",
                "-            for(Object key : props.keySet()) {",
                "+            for (Object key : props.keySet()) {",
                "                 System.out.println(key + \"=\" + props.get(key));",
                "diff --git a/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCounter.java b/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCounter.java",
                "index 94f65babe..f15af9526 100644",
                "--- a/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCounter.java",
                "+++ b/flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCounter.java",
                "@@ -17,4 +17,9 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.examples;",
                "+import static org.apache.storm.utils.Utils.tuple;",
                "+",
                "+import java.util.Map;",
                "+",
                " import org.apache.storm.task.TopologyContext;",
                "@@ -28,6 +33,2 @@ import org.slf4j.LoggerFactory;",
                "-import java.util.Map;",
                "-",
                "-import static org.apache.storm.utils.Utils.tuple;",
                "-",
                " /**",
                "@@ -35,3 +36,3 @@ import static org.apache.storm.utils.Utils.tuple;",
                "  * found in the incoming tuple as \"word\", with a \"count\" of `1`.",
                "- *",
                "+ * <p/>",
                "  * In this case, the downstream HBase bolt handles the counting, so a value",
                "diff --git a/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/FluxShellBolt.java b/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/FluxShellBolt.java",
                "index 05b8e7a3f..8878107e2 100644",
                "--- a/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/FluxShellBolt.java",
                "+++ b/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/FluxShellBolt.java",
                "@@ -17,8 +17,4 @@",
                "  */",
                "-package org.apache.storm.flux.wrappers.bolts;",
                "-import org.apache.storm.task.ShellBolt;",
                "-import org.apache.storm.topology.IRichBolt;",
                "-import org.apache.storm.topology.OutputFieldsDeclarer;",
                "-import org.apache.storm.tuple.Fields;",
                "+package org.apache.storm.flux.wrappers.bolts;",
                "@@ -29,2 +25,7 @@ import java.util.Map;",
                "+import org.apache.storm.task.ShellBolt;",
                "+import org.apache.storm.topology.IRichBolt;",
                "+import org.apache.storm.topology.OutputFieldsDeclarer;",
                "+import org.apache.storm.tuple.Fields;",
                "+",
                " /**",
                "@@ -34,3 +35,3 @@ import java.util.Map;",
                "  */",
                "-public class FluxShellBolt extends ShellBolt implements IRichBolt{",
                "+public class FluxShellBolt extends ShellBolt implements IRichBolt {",
                "     private Map<String, String[]> outputFields;",
                "@@ -39,6 +40,6 @@ public class FluxShellBolt extends ShellBolt implements IRichBolt{",
                "     /**",
                "-     * Create a ShellBolt with command line arguments",
                "+     * Create a ShellBolt with command line arguments.",
                "      * @param command Command line arguments for the bolt",
                "      */",
                "-    public FluxShellBolt(String[] command){",
                "+    public FluxShellBolt(String[] command) {",
                "         super(command);",
                "@@ -49,3 +50,2 @@ public class FluxShellBolt extends ShellBolt implements IRichBolt{",
                "      * Create a ShellBolt with command line arguments and output fields",
                "-     * ",
                "      * Keep this constructor for backward compatibility.",
                "@@ -55,3 +55,3 @@ public class FluxShellBolt extends ShellBolt implements IRichBolt{",
                "      */",
                "-    public FluxShellBolt(String[] command, String[] outputFields){",
                "+    public FluxShellBolt(String[] command, String[] outputFields) {",
                "         this(command);",
                "@@ -62,3 +62,3 @@ public class FluxShellBolt extends ShellBolt implements IRichBolt{",
                "      * Add configuration for this bolt. This method is called from YAML file:",
                "-     *",
                "+     * <p/>",
                "      * ```",
                "@@ -75,4 +75,4 @@ public class FluxShellBolt extends ShellBolt implements IRichBolt{",
                "      *",
                "-     * @param key",
                "-     * @param value",
                "+     * @param key config key",
                "+     * @param value config value",
                "      */",
                "@@ -87,3 +87,3 @@ public class FluxShellBolt extends ShellBolt implements IRichBolt{",
                "      * Add configuration for this bolt. This method is called from YAML file:",
                "-     *",
                "+     * <p/>",
                "      * ```",
                "@@ -102,4 +102,4 @@ public class FluxShellBolt extends ShellBolt implements IRichBolt{",
                "      *",
                "-     * @param key",
                "-     * @param values",
                "+     * @param key config key",
                "+     * @param values config values",
                "      */",
                "@@ -114,3 +114,3 @@ public class FluxShellBolt extends ShellBolt implements IRichBolt{",
                "      * Set default stream outputFields, this method is called from YAML file:",
                "-     * ",
                "+     * <p/>",
                "      * ```",
                "@@ -135,3 +135,3 @@ public class FluxShellBolt extends ShellBolt implements IRichBolt{",
                "      * Set custom *named* stream outputFields, this method is called from YAML file:",
                "-     * ",
                "+     * <p/>",
                "      * ```",
                "@@ -162,3 +162,3 @@ public class FluxShellBolt extends ShellBolt implements IRichBolt{",
                "             String[] value = (String[])entryTuple.getValue();",
                "-            if(key.equals(\"default\")) {",
                "+            if (key.equals(\"default\")) {",
                "                 declarer.declare(new Fields(value));",
                "diff --git a/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/LogInfoBolt.java b/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/LogInfoBolt.java",
                "index 5f0e84b72..3e9979117 100644",
                "--- a/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/LogInfoBolt.java",
                "+++ b/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/LogInfoBolt.java",
                "@@ -36,3 +36,3 @@ public class LogInfoBolt extends BaseBasicBolt {",
                "     public void execute(Tuple tuple, BasicOutputCollector basicOutputCollector) {",
                "-       LOG.info(\"{}\", tuple);",
                "+        LOG.info(\"{}\", tuple);",
                "     }",
                "diff --git a/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/spouts/FluxShellSpout.java b/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/spouts/FluxShellSpout.java",
                "index 5fd378dd6..51ad2c8b5 100644",
                "--- a/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/spouts/FluxShellSpout.java",
                "+++ b/flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/spouts/FluxShellSpout.java",
                "@@ -17,4 +17,10 @@",
                "  */",
                "+",
                " package org.apache.storm.flux.wrappers.spouts;",
                "+import java.util.HashMap;",
                "+import java.util.Iterator;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+",
                " import org.apache.storm.spout.ShellSpout;",
                "@@ -24,6 +30,2 @@ import org.apache.storm.tuple.Fields;",
                "-import java.util.List;",
                "-import java.util.Map;",
                "-import java.util.HashMap;",
                "-import java.util.Iterator;",
                "@@ -39,6 +41,6 @@ public class FluxShellSpout extends ShellSpout implements IRichSpout {",
                "     /**",
                "-     * Create a ShellSpout with command line arguments",
                "+     * Create a ShellSpout with command line arguments.",
                "      * @param command Command line arguments for the bolt",
                "      */",
                "-    public FluxShellSpout(String[] command){",
                "+    public FluxShellSpout(String[] command) {",
                "         super(command);",
                "@@ -49,3 +51,3 @@ public class FluxShellSpout extends ShellSpout implements IRichSpout {",
                "      * Create a ShellSpout with command line arguments and output fields",
                "-     * ",
                "+     * <p/>",
                "      * Keep this constructor for backward compatibility.",
                "@@ -55,3 +57,3 @@ public class FluxShellSpout extends ShellSpout implements IRichSpout {",
                "      */",
                "-    public FluxShellSpout(String[] args, String[] outputFields){",
                "+    public FluxShellSpout(String[] args, String[] outputFields) {",
                "         this(args);",
                "@@ -62,3 +64,3 @@ public class FluxShellSpout extends ShellSpout implements IRichSpout {",
                "      * Add configuration for this spout. This method is called from YAML file:",
                "-     *",
                "+     * <p></p>",
                "      * ```",
                "@@ -75,4 +77,4 @@ public class FluxShellSpout extends ShellSpout implements IRichSpout {",
                "      *",
                "-     * @param key",
                "-     * @param value",
                "+     * @param key config key",
                "+     * @param value config value",
                "      */",
                "@@ -87,3 +89,3 @@ public class FluxShellSpout extends ShellSpout implements IRichSpout {",
                "      * Add configuration for this spout. This method is called from YAML file:",
                "-     *",
                "+     * <p/>",
                "      * ```",
                "@@ -102,4 +104,4 @@ public class FluxShellSpout extends ShellSpout implements IRichSpout {",
                "      *",
                "-     * @param key",
                "-     * @param values",
                "+     * @param key config key",
                "+     * @param values config values",
                "      */",
                "@@ -114,3 +116,3 @@ public class FluxShellSpout extends ShellSpout implements IRichSpout {",
                "      * Set default stream outputFields, this method is called from YAML file:",
                "-     * ",
                "+     * <p/>",
                "      * ```",
                "@@ -135,3 +137,3 @@ public class FluxShellSpout extends ShellSpout implements IRichSpout {",
                "      * Set custom *named* stream outputFields, this method is called from YAML file:",
                "-     * ",
                "+     * <p/>",
                "      * ```",
                "@@ -162,3 +164,3 @@ public class FluxShellSpout extends ShellSpout implements IRichSpout {",
                "             String[] value = (String[])entryTuple.getValue();",
                "-            if(key.equals(\"default\")) {",
                "+            if (key.equals(\"default\")) {",
                "                 declarer.declare(new Fields(value));",
                "diff --git a/flux/pom.xml b/flux/pom.xml",
                "index 4e04fa887..834cfa7b2 100644",
                "--- a/flux/pom.xml",
                "+++ b/flux/pom.xml",
                "@@ -43,2 +43,3 @@",
                "         <module>flux-core</module>",
                "+        <module>flux-examples</module>",
                "     </modules>"
            ],
            "changed_files": [
                "flux/flux-core/pom.xml",
                "flux/flux-core/src/main/java/org/apache/storm/flux/Flux.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/FluxBuilder.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/api/TopologySource.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanListReference.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/BeanReference.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/BoltDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/ConfigMethodDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/ExecutionContext.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/GroupingDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/IncludeDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/ObjectDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/PropertyDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/SpoutDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/StreamDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/TopologyDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/TopologySourceDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/model/VertexDef.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java",
                "flux/flux-examples/pom.xml",
                "flux/flux-examples/src/main/java/org/apache/storm/flux/examples/StatefulWordCounter.java",
                "flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestPrintBolt.java",
                "flux/flux-examples/src/main/java/org/apache/storm/flux/examples/TestWindowBolt.java",
                "flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCountClient.java",
                "flux/flux-examples/src/main/java/org/apache/storm/flux/examples/WordCounter.java",
                "flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/FluxShellBolt.java",
                "flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/bolts/LogInfoBolt.java",
                "flux/flux-wrappers/src/main/java/org/apache/storm/flux/wrappers/spouts/FluxShellSpout.java",
                "flux/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2796": ""
            },
            "ghissue_refs": {
                "2409": ""
            },
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "b48421e71258b670062d280efd1084bb52738e9c"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2796",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2409",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "864188f1cfb18aedccf9e235e18de3b059ebdc80",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510264487,
            "hunks": 12,
            "message": "[STORM-2806] Give users an option to disable the login cache",
            "diff": [
                "diff --git a/storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java b/storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java",
                "index 642be7622..b6571baac 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java",
                "@@ -55,14 +55,16 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {",
                "     private static Map <LoginCacheKey, Login> loginCache = new ConcurrentHashMap<>();",
                "+    private static final String DISABLE_LOGIN_CACHE = \"disableLoginCache\";",
                "     private class LoginCacheKey {",
                "-        private String _keyString = null;",
                "+        private String keyString = null;",
                "-        public LoginCacheKey(Configuration conf, String login_context) throws IOException {",
                "-            if (conf == null) {",
                "-                throw new IllegalArgumentException(\"Configuration should not be null\");",
                "-            }",
                "-            SortedMap<String, ?> authConf = AuthUtils.pullConfig(conf, login_context);",
                "-            if (authConf!=null) {",
                "+        public LoginCacheKey(SortedMap<String, ?> authConf) throws IOException {",
                "+            if (authConf != null) {",
                "                 StringBuilder stringBuilder = new StringBuilder();",
                "                 for (String configKey: authConf.keySet()) {",
                "+                    //DISABLE_LOGIN_CACHE indicates whether or not to use the LoginCache.",
                "+                    //So we exclude it from the keyString",
                "+                    if (configKey.equals(DISABLE_LOGIN_CACHE)) {",
                "+                        continue;",
                "+                    }",
                "                     String configValue = (String) authConf.get(configKey);",
                "@@ -71,5 +73,5 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {",
                "                 }",
                "-                _keyString = stringBuilder.toString();",
                "+                keyString = stringBuilder.toString();",
                "             } else {",
                "-                throw new RuntimeException(\"Error in parsing the kerberos login Configuration, returned null\");",
                "+                throw new IllegalArgumentException(\"Configuration should not be null\");",
                "             }",
                "@@ -79,3 +81,3 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {",
                "         public int hashCode() {",
                "-            return _keyString.hashCode();",
                "+            return keyString.hashCode();",
                "         }",
                "@@ -84,3 +86,3 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {",
                "         public boolean equals(Object obj) {",
                "-            return (obj instanceof LoginCacheKey) && _keyString.equals(((LoginCacheKey)obj)._keyString);",
                "+            return (obj instanceof LoginCacheKey) && keyString.equals(((LoginCacheKey)obj).keyString);",
                "         }",
                "@@ -89,3 +91,3 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {",
                "         public String toString() {",
                "-            return (_keyString);",
                "+            return (keyString);",
                "         }",
                "@@ -137,25 +139,51 @@ public class KerberosSaslTransportPlugin extends SaslTransportPlugin {",
                "+    private Login mkLogin() throws IOException {",
                "+        try {",
                "+            //create an authentication callback handler",
                "+            ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);",
                "+            //specify a configuration object to be used",
                "+            Configuration.setConfiguration(login_conf);",
                "+            //now login",
                "+            Login login = new Login(AuthUtils.LOGIN_CONTEXT_CLIENT, client_callback_handler);",
                "+            login.startThreadIfNeeded();",
                "+            return login;",
                "+        } catch (LoginException ex) {",
                "+            LOG.error(\"Server failed to login in principal:\" + ex, ex);",
                "+            throw new RuntimeException(ex);",
                "+        }",
                "+    }",
                "+",
                "     @Override",
                "     public TTransport connect(TTransport transport, String serverHost, String asUser) throws TTransportException, IOException {",
                "-        //create an authentication callback handler",
                "-        ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);",
                "-        ",
                "         //login our user",
                "-        LoginCacheKey key = new LoginCacheKey(login_conf, AuthUtils.LOGIN_CONTEXT_CLIENT);",
                "-        Login login = loginCache.get(key);",
                "-        if (login == null) {",
                "-            LOG.debug(\"Kerberos Login was not found in the Login Cache, attempting to contact the Kerberos Server\");",
                "-            synchronized (loginCache) {",
                "-                login = loginCache.get(key);",
                "-                if (login == null) {",
                "-                    try {",
                "-                        //specify a configuration object to be used",
                "-                        Configuration.setConfiguration(login_conf);",
                "-                        //now login",
                "-                        login = new Login(AuthUtils.LOGIN_CONTEXT_CLIENT, client_callback_handler);",
                "-                        login.startThreadIfNeeded();",
                "+        SortedMap<String, ?> authConf = AuthUtils.pullConfig(login_conf, AuthUtils.LOGIN_CONTEXT_CLIENT);",
                "+        if (authConf == null) {",
                "+            throw new RuntimeException(\"Error in parsing the kerberos login Configuration, returned null\");",
                "+        }",
                "+",
                "+        boolean disableLoginCache = false;",
                "+        if (authConf.containsKey(DISABLE_LOGIN_CACHE)) {",
                "+            disableLoginCache = Boolean.valueOf((String) authConf.get(DISABLE_LOGIN_CACHE));",
                "+        }",
                "+",
                "+        Login login;",
                "+        LoginCacheKey key = new LoginCacheKey(authConf);",
                "+        if (disableLoginCache) {",
                "+            LOG.debug(\"Kerberos Login Cache is disabled, attempting to contact the Kerberos Server\");",
                "+            login = mkLogin();",
                "+            //this is to prevent the potential bug that",
                "+            //if the Login Cache is (1) enabled, and then (2) disabled and then (3) enabled again,",
                "+            //and if the LoginCacheKey remains unchanged, (3) will use the Login cache from (1), which could be wrong,",
                "+            //because the TGT cache (as well as the principle) could have been changed during (2)",
                "+            loginCache.remove(key);",
                "+        } else {",
                "+            LOG.debug(\"Trying to get the Kerberos Login from the Login Cache\");",
                "+            login = loginCache.get(key);",
                "+            if (login == null) {",
                "+                synchronized (loginCache) {",
                "+                    login = loginCache.get(key);",
                "+                    if (login == null) {",
                "+                        LOG.debug(\"Kerberos Login was not found in the Login Cache, attempting to contact the Kerberos Server\");",
                "+                        login = mkLogin();",
                "                         loginCache.put(key, login);",
                "-                    } catch (LoginException ex) {",
                "-                        LOG.error(\"Server failed to login in principal:\" + ex, ex);",
                "-                        throw new RuntimeException(ex);",
                "                     }"
            ],
            "changed_files": [
                "storm-client/src/jvm/org/apache/storm/security/auth/kerberos/KerberosSaslTransportPlugin.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2806": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: user",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2806",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8a407dd894d85555f95bee4b5886607031fd6a27",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514630592,
            "hunks": 2,
            "message": "STORM-2874: Minor refactoring of backpressure test code",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java b/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java",
                "index f3b5a66cc..3c3ae6faa 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java",
                "@@ -23,3 +23,3 @@ import org.slf4j.LoggerFactory;",
                "-public class WorkerBackpressureThread extends Thread {",
                "+public final class WorkerBackpressureThread extends Thread {",
                "@@ -68,11 +68,13 @@ public class WorkerBackpressureThread extends Thread {",
                "     }",
                "-}",
                "+   ",
                "+    private static class BackpressureUncaughtExceptionHandler implements Thread.UncaughtExceptionHandler {",
                "+",
                "+        private static final Logger LOG = LoggerFactory.getLogger(BackpressureUncaughtExceptionHandler.class);",
                "-class BackpressureUncaughtExceptionHandler implements Thread.UncaughtExceptionHandler {",
                "-    private static final Logger LOG = LoggerFactory.getLogger(BackpressureUncaughtExceptionHandler.class);",
                "-    @Override",
                "-    public void uncaughtException(Thread t, Throwable e) {",
                "-        // note that exception that happens during connecting to ZK has been ignored in the callback implementation",
                "-        LOG.error(\"Received error or exception in WorkerBackpressureThread.. terminating the worker...\", e);",
                "-        Runtime.getRuntime().exit(1);",
                "+        @Override",
                "+        public void uncaughtException(Thread t, Throwable e) {",
                "+            // note that exception that happens during connecting to ZK has been ignored in the callback implementation",
                "+            LOG.error(\"Received error or exception in WorkerBackpressureThread.. terminating the worker...\", e);",
                "+            Runtime.getRuntime().exit(1);",
                "+        }",
                "     }"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/utils/WorkerBackpressureThread.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2874": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: code",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2874",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "73e5f6746cab33b5dcb616b8bad3c0bb00d796f4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515018294,
            "hunks": 0,
            "message": "Merge branch 'YSTORM-4457-II' of https://github.com/govind-menon/storm into STORM-2872 STORM-2872: TestRebalance can be flaky This closes #2491",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "YSTORM-4457": "",
                "STORM-2872": ""
            },
            "ghissue_refs": {
                "2491": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: YSTORM-4457, STORM-2872",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2491",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7cdde90d703c61a7908ffc038a86a7ec60509ff5",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512733484,
            "hunks": 2,
            "message": "STORM-2850: Make ManualPartitionSubscription call  rebalance listener on revoke hook before assigning new partitions to the consumer",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/ManualPartitionSubscription.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/ManualPartitionSubscription.java",
                "index 6bc4bea1e..0a2840aa7 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/ManualPartitionSubscription.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/ManualPartitionSubscription.java",
                "@@ -58,3 +58,2 @@ public class ManualPartitionSubscription extends Subscription {",
                "         if (!newAssignment.equals(currentAssignment)) {",
                "-            consumer.assign(newAssignment);",
                "             if (currentAssignment != null) {",
                "@@ -63,2 +62,3 @@ public class ManualPartitionSubscription extends Subscription {",
                "             currentAssignment = newAssignment;",
                "+            consumer.assign(newAssignment);",
                "             listener.onPartitionsAssigned(newAssignment);"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/ManualPartitionSubscription.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2850": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "c71f4c351511b8683b7b28528259774a7f11c3de"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2850",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6207d320c3f79185f5e8e19458d73fbf1aa10f72",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508967558,
            "hunks": 0,
            "message": "Merge branch 'STORM-2786' of https://github.com/revans2/incubator-storm into STORM-2786 STORM-2786: Turn ticks back on for ackers This closes #2382",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2786": ""
            },
            "ghissue_refs": {
                "2382": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2786",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2382",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "956ba1c16b46a92e00db0be4b026dc731ec6621f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517429586,
            "hunks": 1,
            "message": "STORM-2918 Update Netty version",
            "diff": [
                "diff --git a/pom.xml b/pom.xml",
                "index ccace428a..28e9b8db7 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -265,3 +265,3 @@",
                "         <auto-service.version>1.0-rc3</auto-service.version>",
                "-        <netty.version>3.9.0.Final</netty.version>",
                "+        <netty.version>3.9.9.Final</netty.version>",
                "         <sysout-over-slf4j.version>1.0.2</sysout-over-slf4j.version>"
            ],
            "changed_files": [
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2918": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "c863d9a8461d4ee77d556d9145d8168a3f9cee73"
                ],
                [
                    "no-tag",
                    "4df41444ba3a507cb757d59795262f83c74bbca7"
                ],
                [
                    "no-tag",
                    "4df94e944b7f192e4e6ca4b3e96f8e9c77787d51"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2918",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e8dd1f7e2df5e034a20573b89cb89e9f3529e373",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515445375,
            "hunks": 0,
            "message": "Merge branch 'STORM-2859' of https://github.com/srdo/storm into STORM-2859 STORM-2859: Fix a number of issues with NormalizedResources when resource totals are zero This closes #2485",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2859": ""
            },
            "ghissue_refs": {
                "2485": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2859",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2485",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "cca93d2a1ad503e5d2630fb4d1a88778df359c35",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1497116302,
            "hunks": 17,
            "message": "STORM-2549: Fix broken enforcement mechanism for maxUncommittedOffsets in storm-kafka-client spout",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index b181c43c3..3364fb080 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -93,5 +93,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     // Records that have been polled and are queued to be emitted in the nextTuple() call. One record is emitted per nextTuple()",
                "-    private transient Iterator<ConsumerRecord<K, V>> waitingToEmit;",
                "-    // Number of offsets that have been polled and emitted but not yet been committed. Not used if auto commit mode is enabled.",
                "-    private transient long numUncommittedOffsets;",
                "+    private transient Iterator<ConsumerRecord<K, V>> waitingToEmit;                         ",
                "     // Triggers when a subscription should be refreshed",
                "@@ -117,3 +115,2 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         this.collector = collector;",
                "-        numUncommittedOffsets = 0;",
                "@@ -229,2 +226,7 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             if (initialized) {             ",
                "+             ",
                "+                if (refreshSubscriptionTimer.isExpiredResetOnTrue()) {",
                "+                    kafkaSpoutConfig.getSubscription().refreshAssignment();",
                "+                }",
                "+",
                "                 if (commit()) {",
                "@@ -233,5 +235,6 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-                if (poll()) {",
                "+                Set<TopicPartition> pollablePartitions = poll();",
                "+                if (!pollablePartitions.isEmpty()) {",
                "                     try {",
                "-                        setWaitingToEmit(pollKafkaBroker());",
                "+                        setWaitingToEmit(pollKafkaBroker(pollablePartitions));",
                "                     } catch (RetriableException e) {",
                "@@ -262,23 +265,34 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    private boolean poll() {",
                "+    private Set<TopicPartition> poll() {",
                "         final int maxUncommittedOffsets = kafkaSpoutConfig.getMaxUncommittedOffsets();",
                "-        final int readyMessageCount = retryService.readyMessageCount();",
                "-        final boolean poll = !waitingToEmit()",
                "-            // Check that the number of uncommitted, non-retriable tuples is less than the maxUncommittedOffsets limit.",
                "-            // Accounting for retriable tuples in this way still guarantees that the limit is followed on a per partition basis,",
                "-            // and prevents locking up the spout when there are too many retriable tuples",
                "-            && (numUncommittedOffsets - readyMessageCount < maxUncommittedOffsets || !isAtLeastOnceProcessing());",
                "-",
                "-        if (!poll) {",
                "-            if (waitingToEmit()) {",
                "-                LOG.debug(\"Not polling. Tuples waiting to be emitted.\"",
                "-                    + \" [{}] uncommitted offsets across all topic partitions\", numUncommittedOffsets);",
                "-            }",
                "-",
                "-            if (numUncommittedOffsets >= maxUncommittedOffsets && isAtLeastOnceProcessing()) {",
                "-                LOG.debug(\"Not polling. [{}] uncommitted offsets across all topic partitions has reached the threshold of [{}]\",",
                "-                    numUncommittedOffsets, maxUncommittedOffsets);",
                "+        ",
                "+        if (waitingToEmit()) {",
                "+            LOG.debug(\"Not polling. Tuples waiting to be emitted.\");",
                "+            return Collections.emptySet();",
                "+        }",
                "+        Set<TopicPartition> assignment = kafkaConsumer.assignment();",
                "+        if (!isAtLeastOnceProcessing()) {",
                "+            return assignment;",
                "+        }",
                "+        Map<TopicPartition, Long> earliestRetriableOffsets = retryService.earliestRetriableOffsets();",
                "+        Set<TopicPartition> pollablePartitions = new HashSet<>();",
                "+        for (TopicPartition tp : assignment) {",
                "+            OffsetManager offsetManager = offsetManagers.get(tp);",
                "+            int numUncommittedOffsets = offsetManager.getNumUncommittedOffsets();",
                "+            if (numUncommittedOffsets < maxUncommittedOffsets) {",
                "+                //Allow poll if the partition is not at the maxUncommittedOffsets limit",
                "+                pollablePartitions.add(tp);",
                "+            } else {",
                "+                long offsetAtLimit = offsetManager.getNthUncommittedOffsetAfterCommittedOffset(maxUncommittedOffsets);",
                "+                Long earliestRetriableOffset = earliestRetriableOffsets.get(tp);",
                "+                if (earliestRetriableOffset != null && earliestRetriableOffset <= offsetAtLimit) {",
                "+                    //Allow poll if there are retriable tuples within the maxUncommittedOffsets limit",
                "+                    pollablePartitions.add(tp);",
                "+                } else {",
                "+                    LOG.debug(\"Not polling on partition [{}]. It has [{}] uncommitted offsets, which exceeds the limit of [{}]. \", tp,",
                "+                        numUncommittedOffsets, maxUncommittedOffsets);",
                "+                }",
                "             }",
                "         }",
                "-        return poll;",
                "+        return pollablePartitions;",
                "     }",
                "@@ -298,19 +312,23 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     // ======== poll =========",
                "-    private ConsumerRecords<K, V> pollKafkaBroker() {",
                "-        doSeekRetriableTopicPartitions();",
                "-        if (refreshSubscriptionTimer.isExpiredResetOnTrue()) {",
                "-            kafkaSpoutConfig.getSubscription().refreshAssignment();",
                "-        }",
                "-        final ConsumerRecords<K, V> consumerRecords = kafkaConsumer.poll(kafkaSpoutConfig.getPollTimeoutMs());",
                "-        final int numPolledRecords = consumerRecords.count();",
                "-        LOG.debug(\"Polled [{}] records from Kafka. [{}] uncommitted offsets across all topic partitions\",",
                "-            numPolledRecords, numUncommittedOffsets);",
                "-        if (kafkaSpoutConfig.getProcessingGuarantee() == KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE) {",
                "-            //Commit polled records immediately to ensure delivery is at-most-once.",
                "-            kafkaConsumer.commitSync();",
                "+    private ConsumerRecords<K, V> pollKafkaBroker(Set<TopicPartition> pollablePartitions) {",
                "+        doSeekRetriableTopicPartitions(pollablePartitions);",
                "+        Set<TopicPartition> pausedPartitions = new HashSet<>(kafkaConsumer.assignment());",
                "+        pausedPartitions.removeIf(pollablePartitions::contains);",
                "+        try {",
                "+            kafkaConsumer.pause(pausedPartitions);",
                "+            final ConsumerRecords<K, V> consumerRecords = kafkaConsumer.poll(kafkaSpoutConfig.getPollTimeoutMs());",
                "+            final int numPolledRecords = consumerRecords.count();",
                "+            LOG.debug(\"Polled [{}] records from Kafka.\",",
                "+                numPolledRecords);",
                "+            if (kafkaSpoutConfig.getProcessingGuarantee() == KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE) {",
                "+                //Commit polled records immediately to ensure delivery is at-most-once.",
                "+                kafkaConsumer.commitSync();",
                "+            }",
                "+            return consumerRecords;",
                "+        } finally {",
                "+            kafkaConsumer.resume(pausedPartitions);",
                "         }",
                "-        return consumerRecords;",
                "     }",
                "-    private void doSeekRetriableTopicPartitions() {",
                "+    private void doSeekRetriableTopicPartitions(Set<TopicPartition> pollablePartitions) {",
                "         final Map<TopicPartition, Long> retriableTopicPartitions = retryService.earliestRetriableOffsets();",
                "@@ -318,4 +336,6 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         for (Entry<TopicPartition, Long> retriableTopicPartitionAndOffset : retriableTopicPartitions.entrySet()) {",
                "-            //Seek directly to the earliest retriable message for each retriable topic partition",
                "-            kafkaConsumer.seek(retriableTopicPartitionAndOffset.getKey(), retriableTopicPartitionAndOffset.getValue());",
                "+            if (pollablePartitions.contains(retriableTopicPartitionAndOffset.getKey())) {",
                "+                //Seek directly to the earliest retriable message for each retriable topic partition",
                "+                kafkaConsumer.seek(retriableTopicPartitionAndOffset.getKey(), retriableTopicPartitionAndOffset.getValue());",
                "+            }",
                "         }",
                "@@ -369,4 +389,2 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                             retryService.remove(msgId);",
                "-                        } else {            //New tuple, hence increment the uncommitted offset counter",
                "-                            numUncommittedOffsets++;",
                "                         }",
                "@@ -431,6 +449,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 final OffsetManager offsetManager = offsetManagers.get(tp);",
                "-                long numCommittedOffsets = offsetManager.commit(tpOffset.getValue());",
                "-                numUncommittedOffsets -= numCommittedOffsets;",
                "-                LOG.debug(\"[{}] uncommitted offsets across all topic partitions\",",
                "-                    numUncommittedOffsets);",
                "+                offsetManager.commit(tpOffset.getValue());",
                "+                LOG.debug(\"[{}] uncommitted offsets for partition [{}] after commit\", offsetManager.getNumUncommittedOffsets(), tp);",
                "             }",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "index 6a693fe24..d5fceb467 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "@@ -255,7 +255,8 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         /**",
                "-         * Defines the max number of polled offsets (records) that can be pending commit, before another poll can take place. Once this",
                "-         * limit is reached, no more offsets (records) can be polled until the next successful commit(s) sets the number of pending offsets",
                "-         * below the threshold. The default is {@link #DEFAULT_MAX_UNCOMMITTED_OFFSETS}. Note that this limit can in some cases be exceeded,",
                "-         * but no partition will exceed this limit by more than maxPollRecords - 1.",
                "-         *",
                "+         * Defines the max number of polled offsets (records) that can be pending commit, before another poll can take place.",
                "+         * Once this limit is reached, no more offsets (records) can be polled until the next successful commit(s) sets the number",
                "+         * of pending offsets below the threshold. The default is {@link #DEFAULT_MAX_UNCOMMITTED_OFFSETS}.",
                "+         * This limit is per partition and may in some cases be exceeded,",
                "+         * but each partition cannot exceed this limit by more than maxPollRecords - 1.",
                "+         * ",
                "          * <p>This setting only has an effect if the configured {@link ProcessingGuarantee} is {@link ProcessingGuarantee#AT_LEAST_ONCE}.",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "index 075c4dd63..e7711b0bb 100755",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "@@ -22,2 +22,3 @@ import java.util.Iterator;",
                " import java.util.NavigableSet;",
                "+import java.util.NoSuchElementException;",
                " import java.util.TreeSet;",
                "@@ -67,2 +68,23 @@ public class OffsetManager {",
                "     }",
                "+    ",
                "+    public int getNumUncommittedOffsets() {",
                "+        return this.emittedOffsets.size();",
                "+    }",
                "+    ",
                "+    /**",
                "+     * Gets the offset of the nth emitted message after the committed offset. ",
                "+     * Example: If the committed offset is 0 and offsets 1, 2, 8, 10 have been emitted,",
                "+     * getNthUncommittedOffsetAfterCommittedOffset(3) returns 8.",
                "+     * ",
                "+     * @param index The index of the message to get the offset for",
                "+     * @return The offset",
                "+     * @throws NoSuchElementException if the index is out of range",
                "+     */",
                "+    public long getNthUncommittedOffsetAfterCommittedOffset(int index) {",
                "+        Iterator<Long> offsetIter = emittedOffsets.iterator();",
                "+        for (int i = 0; i < index - 1; i++) {",
                "+            offsetIter.next();",
                "+        }",
                "+        return offsetIter.next();",
                "+    }"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2549": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "07ecff1202bff8661fe3fd4b27a22649d33113b4"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2549",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "ef6fa09c948d5a22bc27318794214ea5b89ed70d",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508274652,
            "hunks": 13,
            "message": "STORM-2706: Upgrade to Curator 4.0.0",
            "diff": [
                "diff --git a/external/storm-eventhubs/pom.xml b/external/storm-eventhubs/pom.xml",
                "index 0809b6eaa..e13f3059c 100755",
                "--- a/external/storm-eventhubs/pom.xml",
                "+++ b/external/storm-eventhubs/pom.xml",
                "@@ -69,13 +69,2 @@",
                "             <artifactId>curator-framework</artifactId>",
                "-            <version>${curator.version}</version>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "diff --git a/external/storm-kafka-monitor/pom.xml b/external/storm-kafka-monitor/pom.xml",
                "index c66e701c9..76e7c8715 100644",
                "--- a/external/storm-kafka-monitor/pom.xml",
                "+++ b/external/storm-kafka-monitor/pom.xml",
                "@@ -59,8 +59,2 @@",
                "             <artifactId>curator-framework</artifactId>",
                "-            <exclusions>",
                "-               <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "diff --git a/external/storm-kafka/pom.xml b/external/storm-kafka/pom.xml",
                "index f9cd57e6a..3fa532ce7 100644",
                "--- a/external/storm-kafka/pom.xml",
                "+++ b/external/storm-kafka/pom.xml",
                "@@ -79,13 +79,2 @@",
                "             <artifactId>curator-framework</artifactId>",
                "-            <version>${curator.version}</version>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "@@ -94,9 +83,2 @@",
                "             <artifactId>curator-recipes</artifactId>",
                "-            <version>${curator.version}</version>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "             <scope>test</scope>",
                "@@ -106,13 +88,2 @@",
                "             <artifactId>curator-test</artifactId>",
                "-            <version>${curator.version}</version>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.testng</groupId>",
                "-                    <artifactId>testng</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "             <scope>test</scope>",
                "diff --git a/external/storm-kinesis/pom.xml b/external/storm-kinesis/pom.xml",
                "index f97a3a372..88c67d234 100644",
                "--- a/external/storm-kinesis/pom.xml",
                "+++ b/external/storm-kinesis/pom.xml",
                "@@ -47,13 +47,2 @@",
                "             <artifactId>curator-framework</artifactId>",
                "-            <version>${curator.version}</version>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "diff --git a/pom.xml b/pom.xml",
                "index 21ec2bc64..3b0d39b88 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -231,3 +231,4 @@",
                "         <clj-time.version>0.8.0</clj-time.version>",
                "-        <curator.version>2.12.0</curator.version>",
                "+        <curator.version>4.0.0</curator.version>",
                "+        <curator-test.version>2.12.0</curator-test.version>",
                "         <json-simple.version>1.1</json-simple.version>",
                "@@ -703,6 +704,2 @@",
                "                 <exclusions>",
                "-                    <exclusion>",
                "-                        <groupId>log4j</groupId>",
                "-                        <artifactId>log4j</artifactId>",
                "-                    </exclusion>",
                "                     <exclusion>",
                "@@ -717,12 +714,2 @@",
                "                 <version>${curator.version}</version>",
                "-                <exclusions>",
                "-                    <exclusion>",
                "-                        <groupId>log4j</groupId>",
                "-                        <artifactId>log4j</artifactId>",
                "-                    </exclusion>",
                "-                    <exclusion>",
                "-                        <groupId>org.slf4j</groupId>",
                "-                        <artifactId>slf4j-log4j12</artifactId>",
                "-                    </exclusion>",
                "-                </exclusions>",
                "             </dependency>",
                "@@ -736,3 +723,6 @@",
                "                 <artifactId>curator-test</artifactId>",
                "-                <version>${curator.version}</version>",
                "+                <!-- curator-test is not compatible with Zookeeper 3.4.x.",
                "+                Curator works around this by using an older curator-test jar.",
                "+                See https://github.com/apache/curator/tree/6ba4de36d4e8b2b65d45c005a6a92dd85c3c497f/curator-test-zk34-->",
                "+                <version>${curator-test.version}</version>",
                "                 <scope>test</scope>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index 0497bdc46..3d714c0c4 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -190,12 +190,2 @@",
                "             <scope>compile</scope>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "@@ -204,12 +194,2 @@",
                "             <artifactId>curator-recipes</artifactId>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "@@ -218,12 +198,2 @@",
                "             <artifactId>curator-test</artifactId>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>"
            ],
            "changed_files": [
                "external/storm-eventhubs/pom.xml",
                "external/storm-kafka-monitor/pom.xml",
                "external/storm-kafka/pom.xml",
                "external/storm-kinesis/pom.xml",
                "pom.xml",
                "storm-core/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2706": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "7f6d7063df1690f267399a6d581fd2b782712e04"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2706",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "9b2b2dbd087aea6f5081442338c90e24f9799e4f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513783131,
            "hunks": 20,
            "message": "STORM-2867: Add consumer lag metrics to KafkaSpout",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 9fe654f71..c52309ecf 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -41,2 +41,3 @@ import java.util.Set;",
                " import java.util.concurrent.TimeUnit;",
                "+import java.util.function.Supplier;",
                " import java.util.stream.Collectors;",
                "@@ -58,2 +59,3 @@ import org.apache.storm.kafka.spout.internal.OffsetManager;",
                " import org.apache.storm.kafka.spout.internal.Timer;",
                "+import org.apache.storm.kafka.spout.metrics.KafkaOffsetMetric;",
                " import org.apache.storm.spout.SpoutOutputCollector;",
                "@@ -106,2 +108,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private transient String commitMetadata;",
                "+    private transient KafkaOffsetMetric kafkaOffsetMetric;",
                "@@ -144,2 +147,5 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         tupleListener.open(conf, context);",
                "+        if (canRegisterMetrics()) {",
                "+            registerMetric();",
                "+        }",
                "@@ -148,2 +154,18 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "+    private void registerMetric() {",
                "+        LOG.info(\"Registering Spout Metrics\");",
                "+        kafkaOffsetMetric = new KafkaOffsetMetric(() -> offsetManagers, () -> kafkaConsumer);",
                "+        context.registerMetric(\"kafkaOffset\", kafkaOffsetMetric, kafkaSpoutConfig.getMetricsTimeBucketSizeInSecs());",
                "+    }",
                "+",
                "+    private boolean canRegisterMetrics() {",
                "+        try {",
                "+            KafkaConsumer.class.getDeclaredMethod(\"beginningOffsets\", Collection.class);",
                "+        } catch (NoSuchMethodException e) {",
                "+            LOG.warn(\"Minimum required kafka-clients library version to enable metrics is 0.10.1.0. Disabling spout metrics.\");",
                "+            return false;",
                "+        }",
                "+        return true;",
                "+    }",
                "+",
                "     private void setCommitMetadata(TopologyContext context) {",
                "@@ -709,2 +731,7 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     }",
                "+",
                "+    @VisibleForTesting",
                "+    KafkaOffsetMetric getKafkaOffsetMetric() {",
                "+        return  kafkaOffsetMetric;",
                "+    }",
                " }",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "index 1c99f24ef..a063790b2 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "@@ -24,3 +24,2 @@ import java.util.List;",
                " import java.util.Map;",
                "-import java.util.Objects;",
                " import java.util.Properties;",
                "@@ -72,2 +71,5 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "+    public static final int DEFAULT_METRICS_TIME_BUCKET_SIZE_SECONDS = 60;",
                "+",
                "+",
                "     // Kafka consumer configuration",
                "@@ -88,2 +90,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "     private final boolean tupleTrackingEnforced;",
                "+    private final int metricsTimeBucketSizeInSecs;",
                "@@ -109,2 +112,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         this.tupleTrackingEnforced = builder.tupleTrackingEnforced;",
                "+        this.metricsTimeBucketSizeInSecs = builder.metricsTimeBucketSizeInSecs;",
                "     }",
                "@@ -179,2 +183,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         private boolean tupleTrackingEnforced = false;",
                "+        private int metricsTimeBucketSizeInSecs = DEFAULT_METRICS_TIME_BUCKET_SIZE_SECONDS;",
                "@@ -397,2 +402,11 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "+        /**",
                "+         * The time period that metrics data in bucketed into.",
                "+         * @param metricsTimeBucketSizeInSecs time in seconds",
                "+         */",
                "+        public Builder<K, V> setMetricsTimeBucketSizeInSecs(int metricsTimeBucketSizeInSecs) {",
                "+            this.metricsTimeBucketSizeInSecs = metricsTimeBucketSizeInSecs;",
                "+            return this;",
                "+        }",
                "+",
                "         public KafkaSpoutConfig<K, V> build() {",
                "@@ -535,2 +549,6 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "+    public int getMetricsTimeBucketSizeInSecs() {",
                "+        return metricsTimeBucketSizeInSecs;",
                "+    }",
                "+",
                "     @Override",
                "@@ -547,2 +565,4 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "             + \", tupleListener=\" + tupleListener",
                "+            + \", processingGuarantee=\" + processingGuarantee",
                "+            + \", metricsTimeBucketSizeInSecs=\" + metricsTimeBucketSizeInSecs",
                "             + '}';",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "index ec6f2a1b7..c9f9541a7 100755",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "@@ -48,2 +48,3 @@ public class OffsetManager {",
                "     private boolean committed;",
                "+    private long latestEmittedOffset;",
                "@@ -65,3 +66,4 @@ public class OffsetManager {",
                "     public void addToEmitMsgs(long offset) {",
                "-        this.emittedOffsets.add(offset);                  // O(Log N)",
                "+        this.emittedOffsets.add(offset);  // O(Log N)",
                "+        this.latestEmittedOffset = Math.max(latestEmittedOffset, offset);",
                "     }",
                "@@ -217,2 +219,10 @@ public class OffsetManager {",
                "+    public long getLatestEmittedOffset() {",
                "+        return latestEmittedOffset;",
                "+    }",
                "+",
                "+    public long getCommittedOffset() {",
                "+        return committedOffset;",
                "+    }",
                "+",
                "     @Override",
                "@@ -224,2 +234,3 @@ public class OffsetManager {",
                "             + \", ackedMsgs=\" + ackedMsgs",
                "+            + \", latestEmittedOffset=\" + latestEmittedOffset",
                "             + '}';",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/metrics/KafkaOffsetMetric.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/metrics/KafkaOffsetMetric.java",
                "new file mode 100644",
                "index 000000000..d6ed209e6",
                "--- /dev/null",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/metrics/KafkaOffsetMetric.java",
                "@@ -0,0 +1,141 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.kafka.spout.metrics;",
                "+",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+import java.util.Set;",
                "+import java.util.function.Supplier;",
                "+",
                "+import org.apache.kafka.clients.consumer.KafkaConsumer;",
                "+import org.apache.kafka.common.TopicPartition;",
                "+import org.apache.storm.kafka.spout.internal.OffsetManager;",
                "+import org.apache.storm.metric.api.IMetric;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * This class is used compute the partition and topic level offset metrics.",
                "+ * <p>",
                "+ * Partition level metrics are:",
                "+ * topicName/partition_{number}/earliestTimeOffset //gives beginning offset of the partition",
                "+ * topicName/partition_{number}/latestTimeOffset //gives end offset of the partition",
                "+ * topicName/partition_{number}/latestEmittedOffset //gives latest emitted offset of the partition from the spout",
                "+ * topicName/partition_{number}/latestCompletedOffset //gives latest committed offset of the partition from the spout",
                "+ * topicName/partition_{number}/spoutLag // the delta between the latest Offset and latestCompletedOffset",
                "+ * topicName/partition_{number}/recordsInPartition // total number of records in the partition",
                "+ * </p>",
                "+ * <p>",
                "+ * Topic level metrics are:",
                "+ * topicName/totalEarliestTimeOffset //gives the total beginning offset of all the associated partitions of this spout",
                "+ * topicName/totalLatestTimeOffset //gives the total end offset of all the associated partitions of this spout",
                "+ * topicName/totalLatestEmittedOffset //gives the total latest emitted offset of all the associated partitions of this spout",
                "+ * topicName/totalLatestCompletedOffset //gives the total latest committed offset of all the associated partitions of this spout",
                "+ * topicName/spoutLag // total spout lag of all the associated partitions of this spout",
                "+ * topicName/totalRecordsInPartitions //total number of records in all the associated partitions of this spout",
                "+ * </p>",
                "+ */",
                "+public class KafkaOffsetMetric implements IMetric {",
                "+",
                "+    private static final Logger LOG = LoggerFactory.getLogger(KafkaOffsetMetric.class);",
                "+    private final Supplier<Map<TopicPartition, OffsetManager>> offsetManagerSupplier;",
                "+    private final Supplier<KafkaConsumer> consumerSupplier;",
                "+",
                "+    public KafkaOffsetMetric(Supplier<Map<TopicPartition, OffsetManager>> offsetManagerSupplier, Supplier<KafkaConsumer> consumerSupplier) {",
                "+        this.offsetManagerSupplier = offsetManagerSupplier;",
                "+        this.consumerSupplier = consumerSupplier;",
                "+    }",
                "+",
                "+    @Override",
                "+    public Object getValueAndReset() {",
                "+",
                "+        Map<TopicPartition, OffsetManager> offsetManagers = offsetManagerSupplier.get();",
                "+        KafkaConsumer kafkaConsumer = consumerSupplier.get();",
                "+",
                "+        if (offsetManagers == null || offsetManagers.isEmpty() || kafkaConsumer == null) {",
                "+            LOG.debug(\"Metrics Tick: offsetManagers or kafkaConsumer is null.\");",
                "+            return null;",
                "+        }",
                "+",
                "+        Map<String,TopicMetrics> topicMetricsMap = new HashMap<>();",
                "+        Set<TopicPartition> topicPartitions = offsetManagers.keySet();",
                "+",
                "+        Map<TopicPartition, Long> beginningOffsets = kafkaConsumer.beginningOffsets(topicPartitions);",
                "+        Map<TopicPartition, Long> endOffsets = kafkaConsumer.endOffsets(topicPartitions);",
                "+        //map to hold partition level and topic level metrics",
                "+        Map<String, Long> result = new HashMap<>();",
                "+",
                "+        for (Map.Entry<TopicPartition, OffsetManager> entry : offsetManagers.entrySet()) {",
                "+            TopicPartition topicPartition = entry.getKey();",
                "+            OffsetManager offsetManager = entry.getValue();",
                "+",
                "+            long latestTimeOffset = endOffsets.get(topicPartition);",
                "+            long earliestTimeOffset = beginningOffsets.get(topicPartition);",
                "+",
                "+            long latestEmittedOffset = offsetManager.getLatestEmittedOffset();",
                "+            long latestCompletedOffset = offsetManager.getCommittedOffset();",
                "+            long spoutLag = latestTimeOffset - latestCompletedOffset;",
                "+            long recordsInPartition =  latestTimeOffset - earliestTimeOffset;",
                "+",
                "+            String metricPath = topicPartition.topic()  + \"/partition_\" + topicPartition.partition();",
                "+            result.put(metricPath + \"/\" + \"spoutLag\", spoutLag);",
                "+            result.put(metricPath + \"/\" + \"earliestTimeOffset\", earliestTimeOffset);",
                "+            result.put(metricPath + \"/\" + \"latestTimeOffset\", latestTimeOffset);",
                "+            result.put(metricPath + \"/\" + \"latestEmittedOffset\", latestEmittedOffset);",
                "+            result.put(metricPath + \"/\" + \"latestCompletedOffset\", latestCompletedOffset);",
                "+            result.put(metricPath + \"/\" + \"recordsInPartition\", recordsInPartition);",
                "+",
                "+            TopicMetrics topicMetrics = topicMetricsMap.get(topicPartition.topic());",
                "+            if (topicMetrics == null) {",
                "+                topicMetrics = new TopicMetrics();",
                "+                topicMetricsMap.put(topicPartition.topic(), topicMetrics);",
                "+            }",
                "+",
                "+            topicMetrics.totalSpoutLag += spoutLag;",
                "+            topicMetrics.totalEarliestTimeOffset += earliestTimeOffset;",
                "+            topicMetrics.totalLatestTimeOffset += latestTimeOffset;",
                "+            topicMetrics.totalLatestEmittedOffset += latestEmittedOffset;",
                "+            topicMetrics.totalLatestCompletedOffset += latestCompletedOffset;",
                "+            topicMetrics.totalRecordsInPartitions += recordsInPartition;",
                "+        }",
                "+",
                "+        for (Map.Entry<String, TopicMetrics> e : topicMetricsMap.entrySet()) {",
                "+            String topic = e.getKey();",
                "+            TopicMetrics topicMetrics = e.getValue();",
                "+            result.put(topic + \"/\" + \"totalSpoutLag\", topicMetrics.totalSpoutLag);",
                "+            result.put(topic + \"/\" + \"totalEarliestTimeOffset\", topicMetrics.totalEarliestTimeOffset);",
                "+            result.put(topic + \"/\" + \"totalLatestTimeOffset\", topicMetrics.totalLatestTimeOffset);",
                "+            result.put(topic + \"/\" + \"totalLatestEmittedOffset\", topicMetrics.totalLatestEmittedOffset);",
                "+            result.put(topic + \"/\" + \"totalLatestCompletedOffset\", topicMetrics.totalLatestCompletedOffset);",
                "+            result.put(topic + \"/\" + \"totalRecordsInPartitions\", topicMetrics.totalRecordsInPartitions);",
                "+        }",
                "+",
                "+        LOG.debug(\"Metrics Tick: value : {}\", result);",
                "+        return result;",
                "+    }",
                "+",
                "+    private class TopicMetrics {",
                "+        long totalSpoutLag = 0;",
                "+        long totalEarliestTimeOffset = 0;",
                "+        long totalLatestTimeOffset = 0;",
                "+        long totalLatestEmittedOffset = 0;",
                "+        long totalLatestCompletedOffset = 0;",
                "+        long totalRecordsInPartitions = 0;",
                "+    }",
                "+}",
                "diff --git a/pom.xml b/pom.xml",
                "index 543c0d057..65b1cca84 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -306,3 +306,3 @@",
                "         <!-- kafka version used by new storm-kafka-client spout code -->",
                "-        <storm.kafka.client.version>0.10.0.0</storm.kafka.client.version>",
                "+        <storm.kafka.client.version>0.10.1.0</storm.kafka.client.version>"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/metrics/KafkaOffsetMetric.java",
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2867": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "587eae4ff5e8f82d7d179542845108f2df6fdaf2"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2867",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "667ffee1580d77469c5eaadd7b5651972a316fc5",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508733329,
            "hunks": 2,
            "message": "STORM-2784: storm-kafka-client KafkaTupleListener method onPartitionsReassigned() should be called after initialization is complete",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 68bce11a3..5022862c7 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -167,4 +167,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-            tupleListener.onPartitionsReassigned(partitions);",
                "             initialize(partitions);",
                "+            tupleListener.onPartitionsReassigned(partitions);",
                "         }"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2784": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "472a0ad79701158bce8a27bf794d5687a537048d"
                ],
                [
                    "no-tag",
                    "ae827dac08fa67997c5eb8e6c23828f190de2851"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2784",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "9d52435c5b5378bc4a618731ef5ecf573f07294a",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511187155,
            "hunks": 0,
            "message": "Merge branch 'STORM-2805' of https://github.com/revans2/incubator-storm into STORM-2805 STORM-2805: Clean up confs in TopologyBuilders This closes #2419",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2805": ""
            },
            "ghissue_refs": {
                "2419": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2805",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2419",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "90e1385264104028ef720df4ee89d4f194a5e233",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507909520,
            "hunks": 5,
            "message": "Merge branch 'STORM-2771' of https://github.com/revans2/incubator-storm into STORM-2771 STORM-2771: By default don't run any tests as integration tests This closes #2368",
            "diff": [
                "diff --git a/pom.xml b/pom.xml",
                "index f53171b74..21ec2bc64 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -283,2 +283,3 @@",
                "         <java.unit.test.include>**/Test*.java, **/*Test.java, **/*TestCase.java</java.unit.test.include>    <!--maven surefire plugin default test list-->",
                "+        <java.integration.test.include>no.tests</java.integration.test.include>",
                "         <!-- by default the clojure test set are all clojure tests that are not integration tests. This property is overridden in the profiles -->",
                "@@ -555,3 +556,2 @@",
                "                 <java.integration.test.include>**/*.java</java.integration.test.include>",
                "-                <java.integration.test.group>org.apache.storm.testing.IntegrationTest</java.integration.test.group>",
                "                 <clojure.test.set>*.*</clojure.test.set>",
                "@@ -565,3 +565,2 @@",
                "                 <java.integration.test.include>**/*.java</java.integration.test.include>",
                "-                <java.integration.test.group>org.apache.storm.testing.IntegrationTest</java.integration.test.group>",
                "                 <!--Clojure-->",
                "@@ -1034,2 +1033,3 @@",
                "                     <configuration>",
                "+                        <redirectTestOutputToFile>true</redirectTestOutputToFile>",
                "                         <includes>",
                "@@ -1037,3 +1037,3 @@",
                "                         </includes>",
                "-                        <groups>${java.integration.test.group}</groups>  <!--set in integration-test the profile-->",
                "+                        <groups>org.apache.storm.testing.IntegrationTest</groups>",
                "                         <argLine>-Xmx1536m</argLine>"
            ],
            "changed_files": [
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2771": ""
            },
            "ghissue_refs": {
                "2368": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2771",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2368",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5753913436b72fb3f1844c25ea559d4bc02cff30",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513180051,
            "hunks": 15,
            "message": "STORM-2858: Fix worker-launcher build by erroring out if asprintf fails to allocate memory. Replace make-maven-plugin with a build shell script.",
            "diff": [
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index 7c6f310fa..02d60c96f 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -33,2 +33,3 @@",
                "         <worker-launcher.conf.dir>/etc/storm</worker-launcher.conf.dir>",
                "+        <worker-launcher.build.dir>${project.build.directory}/native/worker-launcher</worker-launcher.build.dir>",
                "         <worker-launcher.additional_cflags />",
                "@@ -146,3 +147,3 @@",
                "         </dependency>",
                "-        ",
                "+",
                "         <!--java-->",
                "@@ -345,3 +346,3 @@",
                "         <sourceDirectory>src/jvm</sourceDirectory>",
                "-         <testSourceDirectory>test/jvm</testSourceDirectory>",
                "+        <testSourceDirectory>test/jvm</testSourceDirectory>",
                "         <resources>",
                "@@ -1030,3 +1031,3 @@",
                "                         <artifactId>exec-maven-plugin</artifactId>",
                "-                        <version>1.2.1</version>",
                "+                        <version>1.6.0</version>",
                "                         <executions>",
                "@@ -1035,4 +1036,2 @@",
                "                                 <goals><goal>exec</goal></goals>",
                "-                            </execution>",
                "-                        </executions>",
                "                         <configuration>",
                "@@ -1044,8 +1043,3 @@",
                "                         </configuration>",
                "-                    </plugin>",
                "-                    <plugin>",
                "-                        <groupId>org.codehaus.mojo</groupId>",
                "-                        <artifactId>make-maven-plugin</artifactId>",
                "-                        <version>1.0-beta-1</version>",
                "-                        <executions>",
                "+                            </execution>",
                "                             <execution>",
                "@@ -1054,6 +1048,11 @@",
                "                                 <goals>",
                "-                                    <goal>autoreconf</goal>",
                "-                                    <goal>configure</goal>",
                "-                                    <goal>make-install</goal>",
                "+                                    <goal>exec</goal>",
                "                                 </goals>",
                "+                                <configuration>",
                "+                                    <executable>sh</executable>",
                "+                                    <arguments>",
                "+                                        <argument>compile-worker-launcher.sh</argument>",
                "+                                    </arguments>",
                "+                                    <workingDirectory>${worker-launcher.build.dir}</workingDirectory>",
                "+                                </configuration>",
                "                             </execution>",
                "@@ -1063,27 +1062,39 @@",
                "                                 <goals>",
                "-                                    <goal>test</goal>",
                "+                                    <goal>exec</goal>",
                "                                 </goals>",
                "-                            </execution>",
                "-                        </executions>",
                "                         <configuration>",
                "-                            <!-- autoreconf settings -->",
                "-                            <workDir>${project.build.directory}/native/worker-launcher</workDir>",
                "+                                    <executable>make</executable>",
                "                             <arguments>",
                "-                                <argument>-i</argument>",
                "+                                        <argument>check</argument>",
                "                             </arguments>",
                "-",
                "-                            <!-- configure settings -->",
                "-                            <configureEnvironment>",
                "-                                <property>",
                "-                                    <name>CFLAGS</name>",
                "-                                    <value>-DEXEC_CONF_DIR=${worker-launcher.conf.dir} ${worker-launcher.additional_cflags}</value>",
                "-                                </property>",
                "-                            </configureEnvironment>",
                "-                            <configureWorkDir>${project.build.directory}/native/worker-launcher</configureWorkDir>",
                "-                            <prefix>/usr/local</prefix>",
                "-",
                "-                            <!-- configure & make settings -->",
                "-                            <destDir>${project.build.directory}/native/target</destDir>",
                "-",
                "+                                    <workingDirectory>${worker-launcher.build.dir}</workingDirectory>",
                "                         </configuration>",
                "+                            </execution>",
                "+                        </executions>",
                "+                    </plugin>",
                "+                    <plugin>",
                "+                        <groupId>org.apache.maven.plugins</groupId>",
                "+                        <artifactId>maven-resources-plugin</artifactId>",
                "+                        <version>3.0.2</version>",
                "+                        <executions>",
                "+                            <execution>",
                "+                                <id>copy-build-scripts</id>",
                "+                                <phase>process-resources</phase>",
                "+                                <goals>",
                "+                                    <goal>copy-resources</goal>",
                "+                                </goals>",
                "+                                <configuration>",
                "+                                    <resources>",
                "+                                        <resource>",
                "+                                            <directory>src/resources</directory>",
                "+                                            <includes>",
                "+                                                <include>compile-worker-launcher.sh</include>",
                "+                                            </includes>",
                "+                                            <filtering>true</filtering>",
                "+                                        </resource>",
                "+                                    </resources>",
                "+                                    <outputDirectory>${worker-launcher.build.dir}</outputDirectory>",
                "+                                </configuration>",
                "+                            </execution>",
                "+                        </executions>",
                "                     </plugin>",
                "diff --git a/storm-core/src/resources/compile-worker-launcher.sh b/storm-core/src/resources/compile-worker-launcher.sh",
                "new file mode 100644",
                "index 000000000..dffe2854a",
                "--- /dev/null",
                "+++ b/storm-core/src/resources/compile-worker-launcher.sh",
                "@@ -0,0 +1,21 @@",
                "+# Licensed to the Apache Software Foundation (ASF) under one",
                "+# or more contributor license agreements.  See the NOTICE file",
                "+# distributed with this work for additional information",
                "+# regarding copyright ownership.  The ASF licenses this file",
                "+# to you under the Apache License, Version 2.0 (the",
                "+# \"License\"); you may not use this file except in compliance",
                "+# with the License.  You may obtain a copy of the License at",
                "+#",
                "+#     http://www.apache.org/licenses/LICENSE-2.0",
                "+#",
                "+# Unless required by applicable law or agreed to in writing, software",
                "+# distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+# See the License for the specific language governing permissions and",
                "+# limitations under the License.",
                "+",
                "+autoreconf -i",
                "+export CFLAGS=-DEXEC_CONF_DIR=${worker-launcher.conf.dir} ${worker-launcher.additional_cflags}",
                "+./configure --prefix=/usr/local",
                "+export DESTDIR=${project.build.directory}/native/target",
                "+make install",
                "\\ No newline at end of file"
            ],
            "changed_files": [
                "storm-core/pom.xml",
                "storm-core/src/resources/compile-worker-launcher.sh"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2858": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "b8ec387066df47bfac4a1bb0364e592f41801aa0"
                ],
                [
                    "no-tag",
                    "6d3ef144360daafc16ad4cd9c9a8b264bb95cee3"
                ],
                [
                    "no-tag",
                    "0862911cd5e5442e8d1954736555d2a06b5ebc21"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2858",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "609a4a78f2a762f784648a6a57af060abd089d7c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516772415,
            "hunks": 1,
            "message": "STORM-2906 Pick HBase delegation token properly while handling HBase auth via delegation token * log to warn if there's more than one HBase auth tokens",
            "diff": [
                "diff --git a/external/storm-hbase/src/main/java/org/apache/storm/hbase/common/Utils.java b/external/storm-hbase/src/main/java/org/apache/storm/hbase/common/Utils.java",
                "index f56f9e090..981d4ff46 100644",
                "--- a/external/storm-hbase/src/main/java/org/apache/storm/hbase/common/Utils.java",
                "+++ b/external/storm-hbase/src/main/java/org/apache/storm/hbase/common/Utils.java",
                "@@ -59,8 +59,13 @@ public class Utils {",
                "                     // use UGI from token",
                "-                    LOG.debug(\"Found HBASE_AUTH_TOKEN - using the token to replace current user.\");",
                "+                    if (!foundHBaseAuthToken) {",
                "+                        LOG.debug(\"Found HBASE_AUTH_TOKEN - using the token to replace current user.\");",
                "-                    ugi = token.decodeIdentifier().getUser();",
                "-                    ugi.addToken(token);",
                "+                        ugi = token.decodeIdentifier().getUser();",
                "+                        ugi.addToken(token);",
                "-                    foundHBaseAuthToken = true;",
                "+                        foundHBaseAuthToken = true;",
                "+                    } else {",
                "+                        LOG.warn(\"Found multiple HBASE_AUTH_TOKEN - will use already found token. \" +",
                "+                                \"Please enable DEBUG log level to track delegation tokens.\");",
                "+                    }",
                "                 }"
            ],
            "changed_files": [
                "external/storm-hbase/src/main/java/org/apache/storm/hbase/common/Utils.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2906": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "12a03e3899b2c4c4377d2db26d95f8fbc22458f6"
                ],
                [
                    "no-tag",
                    "d291f39c9aa6e29184a2712797e956b42cae3293"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2906",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "c8947c2fede62036c20472c9e0335ef90a06b536",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508167479,
            "hunks": 0,
            "message": "Merge branch 'STORM-2686-redesign' of https://github.com/Ethanlm/storm into STORM-2686 STORM-2686: Add locality awareness to LoadAwareShuffleGrouping This closes #2366",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2686": ""
            },
            "ghissue_refs": {
                "2366": "[STORM-2686] Add Locality Aware Shuffle Grouping #2270"
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2686",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2366",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "a3899b75a79781602fa58b90de6c8aa784af5332",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513569578,
            "hunks": 62,
            "message": "STORM-2844: KafkaSpout Throws IllegalStateException After Committing to Kafka When First Poll Strategy Set to EARLIEST      - Check if commits to Kafka were committed by this topology to correctly enforce FirstPollOffsetStrategy      - Cache if OffsetManager has committed to avoid JSON deserialization for every tuple and speedup lookups      - CommitMetadata includes topology id, and task id and threadName      - Update FirstPollOffsetStrategy javadoc      - Refactor and add unit tests",
            "diff": [
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index 816f5dd11..5ee1a6118 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -77,2 +77,10 @@",
                "         </dependency>",
                "+        <dependency>",
                "+            <groupId>com.fasterxml.jackson.core</groupId>",
                "+            <artifactId>jackson-databind</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>com.google.guava</groupId>",
                "+            <artifactId>guava</artifactId>",
                "+        </dependency>",
                "         <!--test dependencies -->",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 373630eee..b7e313689 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -25,2 +25,7 @@ import static org.apache.storm.kafka.spout.KafkaSpoutConfig.FirstPollOffsetStrat",
                "+import com.fasterxml.jackson.core.JsonProcessingException;",
                "+import com.fasterxml.jackson.databind.ObjectMapper;",
                "+import com.google.common.annotations.VisibleForTesting;",
                "+",
                "+import java.io.IOException;",
                " import java.util.Collection;",
                "@@ -37,2 +42,3 @@ import java.util.concurrent.TimeUnit;",
                " import java.util.stream.Collectors;",
                "+",
                " import org.apache.commons.lang.Validate;",
                "@@ -47,2 +53,3 @@ import org.apache.kafka.common.errors.RetriableException;",
                " import org.apache.storm.kafka.spout.KafkaSpoutConfig.FirstPollOffsetStrategy;",
                "+import org.apache.storm.kafka.spout.internal.CommitMetadata;",
                " import org.apache.storm.kafka.spout.internal.KafkaConsumerFactory;",
                "@@ -65,2 +72,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private static final Logger LOG = LoggerFactory.getLogger(KafkaSpout.class);",
                "+    private static final ObjectMapper JSON_MAPPER = new ObjectMapper();",
                "@@ -96,2 +104,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private transient TopologyContext context;",
                "+    // Metadata information to commit to Kafka. It is unique per spout per topology.",
                "+    private transient String commitMetadata;",
                "@@ -101,3 +111,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    //This constructor is here for testing",
                "+    @VisibleForTesting",
                "     KafkaSpout(KafkaSpoutConfig<K, V> kafkaSpoutConfig, KafkaConsumerFactory<K, V> kafkaConsumerFactory) {",
                "@@ -131,2 +141,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         waitingToEmit = Collections.emptyListIterator();",
                "+        setCommitMetadata(context);",
                "@@ -137,2 +148,12 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "+    private void setCommitMetadata(TopologyContext context) {",
                "+        try {",
                "+            commitMetadata = JSON_MAPPER.writeValueAsString(new CommitMetadata(",
                "+                context.getStormId(), context.getThisTaskId(), Thread.currentThread().getName()));",
                "+        } catch (JsonProcessingException e) {",
                "+            LOG.error(\"Failed to create Kafka commit metadata due to JSON serialization error\",e);",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "     private boolean isAtLeastOnceProcessing() {",
                "@@ -145,3 +166,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         private Collection<TopicPartition> previousAssignment = new HashSet<>();",
                "-",
                "+        ",
                "         @Override",
                "@@ -169,3 +190,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             if (isAtLeastOnceProcessing()) {",
                "-                // remove from acked all partitions that are no longer assigned to this spout",
                "+                // remove offsetManagers for all partitions that are no longer assigned to this spout",
                "                 offsetManagers.keySet().retainAll(partitions);",
                "@@ -182,10 +203,10 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             newPartitions.removeAll(previousAssignment);",
                "-            for (TopicPartition tp : newPartitions) {",
                "-                final OffsetAndMetadata committedOffset = kafkaConsumer.committed(tp);",
                "-                final long fetchOffset = doSeek(tp, committedOffset);",
                "-                LOG.debug(\"Set consumer position to [{}] for topic-partition [{}], based on strategy [{}] and committed offset [{}]\",",
                "-                    fetchOffset, tp, firstPollOffsetStrategy, committedOffset);",
                "+            for (TopicPartition newTp : newPartitions) {",
                "+                final OffsetAndMetadata committedOffset = kafkaConsumer.committed(newTp);",
                "+                final long fetchOffset = doSeek(newTp, committedOffset);",
                "+                LOG.debug(\"Set consumer position to [{}] for topic-partition [{}] with [{}] and committed offset [{}]\",",
                "+                    fetchOffset, newTp, firstPollOffsetStrategy, committedOffset);",
                "                 // If this partition was previously assigned to this spout, leave the acked offsets as they were to resume where it left off",
                "-                if (isAtLeastOnceProcessing() && !offsetManagers.containsKey(tp)) {",
                "-                    offsetManagers.put(tp, new OffsetManager(tp, fetchOffset));",
                "+                if (isAtLeastOnceProcessing() && !offsetManagers.containsKey(newTp)) {",
                "+                    offsetManagers.put(newTp, new OffsetManager(newTp, fetchOffset));",
                "                 }",
                "@@ -198,20 +219,58 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "          */",
                "-        private long doSeek(TopicPartition tp, OffsetAndMetadata committedOffset) {",
                "-            if (committedOffset != null) {             // offset was committed for this TopicPartition",
                "-                if (firstPollOffsetStrategy.equals(EARLIEST)) {",
                "-                    kafkaConsumer.seekToBeginning(Collections.singleton(tp));",
                "-                } else if (firstPollOffsetStrategy.equals(LATEST)) {",
                "-                    kafkaConsumer.seekToEnd(Collections.singleton(tp));",
                "+        private long doSeek(TopicPartition newTp, OffsetAndMetadata committedOffset) {",
                "+            LOG.trace(\"Seeking offset for topic-partition [{}] with [{}] and committed offset [{}]\",",
                "+                newTp, firstPollOffsetStrategy, committedOffset);",
                "+",
                "+            if (committedOffset != null) {",
                "+                // offset was previously committed for this consumer group and topic-partition, either by this or another topology.",
                "+                if (isOffsetCommittedByThisTopology(newTp, committedOffset)) {",
                "+                    // Another KafkaSpout instance (of this topology) already committed, therefore FirstPollOffsetStrategy does not apply.",
                "+                    kafkaConsumer.seek(newTp, committedOffset.offset());",
                "                 } else {",
                "-                    // By default polling starts at the last committed offset, i.e. the first offset that was not marked as processed.",
                "-                    kafkaConsumer.seek(tp, committedOffset.offset());",
                "+                    // offset was not committed by this topology, therefore FirstPollOffsetStrategy applies",
                "+                    // (only when the topology is first deployed).",
                "+                    if (firstPollOffsetStrategy.equals(EARLIEST)) {",
                "+                        kafkaConsumer.seekToBeginning(Collections.singleton(newTp));",
                "+                    } else if (firstPollOffsetStrategy.equals(LATEST)) {",
                "+                        kafkaConsumer.seekToEnd(Collections.singleton(newTp));",
                "+                    } else {",
                "+                        // Resume polling at the last committed offset, i.e. the first offset that is not marked as processed.",
                "+                        kafkaConsumer.seek(newTp, committedOffset.offset());",
                "+                    }",
                "                 }",
                "-            } else {    // no commits have ever been done, so start at the beginning or end depending on the strategy",
                "+            } else {",
                "+                // no offset commits have ever been done for this consumer group and topic-partition,",
                "+                // so start at the beginning or end depending on FirstPollOffsetStrategy",
                "                 if (firstPollOffsetStrategy.equals(EARLIEST) || firstPollOffsetStrategy.equals(UNCOMMITTED_EARLIEST)) {",
                "-                    kafkaConsumer.seekToBeginning(Collections.singleton(tp));",
                "+                    kafkaConsumer.seekToBeginning(Collections.singleton(newTp));",
                "                 } else if (firstPollOffsetStrategy.equals(LATEST) || firstPollOffsetStrategy.equals(UNCOMMITTED_LATEST)) {",
                "-                    kafkaConsumer.seekToEnd(Collections.singleton(tp));",
                "+                    kafkaConsumer.seekToEnd(Collections.singleton(newTp));",
                "                 }",
                "             }",
                "-            return kafkaConsumer.position(tp);",
                "+            return kafkaConsumer.position(newTp);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Checks If {@link OffsetAndMetadata} was committed by a {@link KafkaSpout} instance in this topology.",
                "+     * This info is used to decide if {@link FirstPollOffsetStrategy} should be applied",
                "+     *",
                "+     * @param tp topic-partition",
                "+     * @param committedOffset {@link OffsetAndMetadata} info committed to Kafka",
                "+     * @return true if this topology committed this {@link OffsetAndMetadata}, false otherwise",
                "+     */",
                "+    private boolean isOffsetCommittedByThisTopology(TopicPartition tp, OffsetAndMetadata committedOffset) {",
                "+        try {",
                "+            if (offsetManagers.containsKey(tp) && offsetManagers.get(tp).hasCommitted()) {",
                "+                return true;",
                "+            }",
                "+",
                "+            final CommitMetadata committedMetadata = JSON_MAPPER.readValue(committedOffset.metadata(), CommitMetadata.class);",
                "+            return committedMetadata.getTopologyId().equals(context.getStormId());",
                "+        } catch (IOException e) {",
                "+            LOG.warn(\"Failed to deserialize [{}]. Error likely occurred because the last commit \"",
                "+                + \"for this topic-partition was done using an earlier version of Storm. \"",
                "+                + \"Defaulting to behavior compatible with earlier version\", committedOffset);",
                "+            LOG.trace(\"\",e);",
                "+            return false;",
                "         }",
                "@@ -261,3 +320,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         }",
                "-",
                "+        ",
                "         Set<TopicPartition> assignment = kafkaConsumer.assignment();",
                "@@ -266,3 +325,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         }",
                "-",
                "+        ",
                "         Map<TopicPartition, Long> earliestRetriableOffsets = retryService.earliestRetriableOffsets();",
                "@@ -313,3 +372,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             final int numPolledRecords = consumerRecords.count();",
                "-            LOG.debug(\"Polled [{}] records from Kafka.\",",
                "+            LOG.debug(\"Polled [{}] records from Kafka\",",
                "                 numPolledRecords);",
                "@@ -384,3 +443,7 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         } else {",
                "-            if (kafkaConsumer.committed(tp) != null && (kafkaConsumer.committed(tp).offset() > kafkaConsumer.position(tp))) {",
                "+            final OffsetAndMetadata committedOffset = kafkaConsumer.committed(tp);",
                "+            if (committedOffset != null && isOffsetCommittedByThisTopology(tp, committedOffset)",
                "+                && committedOffset.offset() > kafkaConsumer.position(tp)) {",
                "+                // Ensures that after a topology with this id is started, the consumer fetch",
                "+                // position never falls behind the committed offset (STORM-2844)",
                "                 throw new IllegalStateException(\"Attempting to emit a message that has already been committed.\");",
                "@@ -439,3 +502,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         for (Map.Entry<TopicPartition, OffsetManager> tpOffset : assignedOffsetManagers.entrySet()) {",
                "-            final OffsetAndMetadata nextCommitOffset = tpOffset.getValue().findNextCommitOffset();",
                "+            final OffsetAndMetadata nextCommitOffset = tpOffset.getValue().findNextCommitOffset(commitMetadata);",
                "             if (nextCommitOffset != null) {",
                "@@ -469,3 +532,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 }",
                "-",
                "+                ",
                "                 final OffsetManager offsetManager = assignedOffsetManagers.get(tp);",
                "@@ -617,3 +680,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     }",
                "-",
                "+    ",
                "     private static class PollablePartitionsInfo {",
                "@@ -623,3 +686,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         private final Map<TopicPartition, Long> pollableEarliestRetriableOffsets;",
                "-",
                "+        ",
                "         public PollablePartitionsInfo(Set<TopicPartition> pollablePartitions, Map<TopicPartition, Long> earliestRetriableOffsets) {",
                "@@ -630,3 +693,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         }",
                "-",
                "+        ",
                "         public boolean shouldPoll() {",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "index 6c792ab6a..1c99f24ef 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "@@ -112,18 +112,17 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "     /**",
                "-     * The offset used by the Kafka spout in the first poll to Kafka broker. The choice of this parameter will affect the number of consumer",
                "-     * records returned in the first poll. By default this parameter is set to UNCOMMITTED_EARLIEST. <br/><br/>",
                "-     * The allowed values are EARLIEST, LATEST, UNCOMMITTED_EARLIEST, UNCOMMITTED_LATEST. <br/>",
                "+     * Defines how the {@link KafkaSpout} seeks the offset to be used in the first poll to Kafka upon topology deployment.",
                "+     * By default this parameter is set to UNCOMMITTED_EARLIEST. If the strategy is set to:",
                "+     * <br/>",
                "      * <ul>",
                "-     * <li>EARLIEST means that the kafka spout polls records starting in the first offset of the partition, regardless of previous",
                "-     * commits</li>",
                "-     * <li>LATEST means that the kafka spout polls records with offsets greater than the last offset in the partition, regardless of",
                "-     * previous commits</li>",
                "-     * <li>UNCOMMITTED_EARLIEST means that the kafka spout polls records from the last committed offset, if any. If no offset has been",
                "-     * committed, it behaves as EARLIEST.</li>",
                "-     * <li>UNCOMMITTED_LATEST means that the kafka spout polls records from the last committed offset, if any. If no offset has been",
                "-     * committed, it behaves as LATEST.</li>",
                "+     * <li>EARLIEST - the kafka spout polls records starting in the first offset of the partition, regardless",
                "+     * of previous commits. This setting only takes effect on topology deployment.</li>",
                "+     * <li>LATEST - the kafka spout polls records with offsets greater than the last offset in the partition,",
                "+     * regardless of previous commits. This setting only takes effect on topology deployment.</li>",
                "+     * <li>UNCOMMITTED_EARLIEST - the kafka spout polls records from the last committed offset, if any. If no offset has been",
                "+     * committed it behaves as EARLIEST.</li>",
                "+     * <li>UNCOMMITTED_LATEST - the kafka spout polls records from the last committed offset, if any. If no offset has been",
                "+     * committed it behaves as LATEST.</li>",
                "      * </ul>",
                "-     *",
                "      */",
                "-    public static enum FirstPollOffsetStrategy {",
                "+    public enum FirstPollOffsetStrategy {",
                "         EARLIEST,",
                "@@ -131,3 +130,8 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         UNCOMMITTED_EARLIEST,",
                "-        UNCOMMITTED_LATEST",
                "+        UNCOMMITTED_LATEST;",
                "+",
                "+        @Override",
                "+        public String toString() {",
                "+            return \"FirstPollOffsetStrategy{\" + super.toString() + \"}\";",
                "+        }",
                "     }",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutMessageId.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutMessageId.java",
                "index b802a52b8..1626fee00 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutMessageId.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutMessageId.java",
                "@@ -32,3 +32,3 @@ public class KafkaSpoutMessageId implements Serializable {",
                "      */",
                "-    private boolean emitted;   ",
                "+    private boolean emitted;",
                "@@ -90,16 +90,2 @@ public class KafkaSpoutMessageId implements Serializable {",
                "-    /**",
                "-     * Gets metadata for this message which may be committed to Kafka.",
                "-     * @param currThread The calling thread",
                "-     * @return The metadata",
                "-     */",
                "-    public String getMetadata(Thread currThread) {",
                "-        return \"{\"",
                "-                + \"topic-partition=\" + topicPart",
                "-                + \", offset=\" + offset",
                "-                + \", numFails=\" + numFails",
                "-                + \", thread='\" + currThread.getName() + \"'\"",
                "-                + '}';",
                "-    }",
                "-",
                "     @Override",
                "@@ -107,6 +93,7 @@ public class KafkaSpoutMessageId implements Serializable {",
                "         return \"{\"",
                "-                + \"topic-partition=\" + topicPart",
                "-                + \", offset=\" + offset",
                "-                + \", numFails=\" + numFails",
                "-                + '}';",
                "+            + \"topic-partition=\" + topicPart",
                "+            + \", offset=\" + offset",
                "+            + \", numFails=\" + numFails",
                "+            + \", emitted=\" + emitted",
                "+            + '}';",
                "     }",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/CommitMetadata.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/CommitMetadata.java",
                "new file mode 100644",
                "index 000000000..b7fd1a6ed",
                "--- /dev/null",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/CommitMetadata.java",
                "@@ -0,0 +1,63 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ *   or more contributor license agreements.  See the NOTICE file",
                "+ *   distributed with this work for additional information",
                "+ *   regarding copyright ownership.  The ASF licenses this file",
                "+ *   to you under the Apache License, Version 2.0 (the",
                "+ *   \"License\"); you may not use this file except in compliance",
                "+ *   with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ *   http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ *   Unless required by applicable law or agreed to in writing, software",
                "+ *   distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ *   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ *   See the License for the specific language governing permissions and",
                "+ *   limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.kafka.spout.internal;",
                "+",
                "+import com.fasterxml.jackson.annotation.JsonCreator;",
                "+import com.fasterxml.jackson.annotation.JsonProperty;",
                "+",
                "+/**",
                "+ * Object representing metadata committed to Kafka.",
                "+ */",
                "+public class CommitMetadata {",
                "+    private final String topologyId;",
                "+    private final int taskId;",
                "+    private final String threadName;",
                "+",
                "+    /** Kafka metadata. */",
                "+    @JsonCreator",
                "+    public CommitMetadata(@JsonProperty(\"topologyId\") String topologyId,",
                "+                          @JsonProperty(\"taskId\") int taskId,",
                "+                          @JsonProperty(\"threadName\") String threadName) {",
                "+",
                "+        this.topologyId = topologyId;",
                "+        this.taskId = taskId;",
                "+        this.threadName = threadName;",
                "+    }",
                "+",
                "+    public String getTopologyId() {",
                "+        return topologyId;",
                "+    }",
                "+",
                "+    public int getTaskId() {",
                "+        return taskId;",
                "+    }",
                "+",
                "+    public String getThreadName() {",
                "+        return threadName;",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        return \"CommitMetadata{\"",
                "+            + \"topologyId='\" + topologyId + '\\''",
                "+            + \", taskId=\" + taskId",
                "+            + \", threadName='\" + threadName + '\\''",
                "+            + '}';",
                "+    }",
                "+}",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "index e7711b0bb..ec6f2a1b7 100755",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "@@ -19,2 +19,3 @@ package org.apache.storm.kafka.spout.internal;",
                " import com.google.common.annotations.VisibleForTesting;",
                "+",
                " import java.util.Comparator;",
                "@@ -24,3 +25,3 @@ import java.util.NoSuchElementException;",
                " import java.util.TreeSet;",
                "-import org.apache.kafka.clients.consumer.ConsumerRecord;",
                "+",
                " import org.apache.kafka.clients.consumer.OffsetAndMetadata;",
                "@@ -35,11 +36,6 @@ import org.slf4j.LoggerFactory;",
                " public class OffsetManager {",
                "-",
                "     private static final Comparator<KafkaSpoutMessageId> OFFSET_COMPARATOR = new OffsetComparator();",
                "     private static final Logger LOG = LoggerFactory.getLogger(OffsetManager.class);",
                "+",
                "     private final TopicPartition tp;",
                "-    /* First offset to be fetched. It is either set to the beginning, end, or to the first uncommitted offset.",
                "-    * Initial value depends on offset strategy. See KafkaSpoutConsumerRebalanceListener */",
                "-    private final long initialFetchOffset;",
                "-    // Committed offset, i.e. the offset where processing will resume if the spout restarts. Initially it is set to fetchOffset.",
                "-    private long committedOffset;",
                "     // Emitted Offsets List",
                "@@ -48,2 +44,6 @@ public class OffsetManager {",
                "     private final NavigableSet<KafkaSpoutMessageId> ackedMsgs = new TreeSet<>(OFFSET_COMPARATOR);",
                "+    // Committed offset, i.e. the offset where processing will resume upon spout restart. Initially it is set to fetchOffset.",
                "+    private long committedOffset;",
                "+    // True if this OffsetManager has made at least one commit to Kafka",
                "+    private boolean committed;",
                "@@ -51,3 +51,3 @@ public class OffsetManager {",
                "      * Creates a new OffsetManager.",
                "-     * @param tp The TopicPartition ",
                "+     * @param tp The TopicPartition",
                "      * @param initialFetchOffset The initial fetch offset for the given TopicPartition",
                "@@ -56,3 +56,2 @@ public class OffsetManager {",
                "         this.tp = tp;",
                "-        this.initialFetchOffset = initialFetchOffset;",
                "         this.committedOffset = initialFetchOffset;",
                "@@ -97,2 +96,3 @@ public class OffsetManager {",
                "      *",
                "+     * @param commitMetadata Metadata information to commit to Kafka. It is constant per KafkaSpout instance per topology",
                "      * @return the next OffsetAndMetadata to commit, or null if no offset is",
                "@@ -100,6 +100,6 @@ public class OffsetManager {",
                "      */",
                "-    public OffsetAndMetadata findNextCommitOffset() {",
                "+    public OffsetAndMetadata findNextCommitOffset(final String commitMetadata) {",
                "+        boolean found = false;",
                "         long currOffset;",
                "         long nextCommitOffset = committedOffset;",
                "-        KafkaSpoutMessageId nextCommitMsg = null;     // this is a convenience variable to make it faster to create OffsetAndMetadata",
                "@@ -107,4 +107,5 @@ public class OffsetManager {",
                "             currOffset = currAckedMsg.offset();",
                "-            if (currOffset == nextCommitOffset) {            // found the next offset to commit",
                "-                nextCommitMsg = currAckedMsg;",
                "+            if (currOffset == nextCommitOffset) {",
                "+                // found the next offset to commit",
                "+                found = true;",
                "                 nextCommitOffset = currOffset + 1;",
                "@@ -130,3 +131,2 @@ public class OffsetManager {",
                "                             currOffset, nextCommitOffset);",
                "-                        nextCommitMsg = currAckedMsg;",
                "                         nextCommitOffset = currOffset + 1;",
                "@@ -146,7 +146,7 @@ public class OffsetManager {",
                "         OffsetAndMetadata nextCommitOffsetAndMetadata = null;",
                "-        if (nextCommitMsg != null) {",
                "-            nextCommitOffsetAndMetadata = new OffsetAndMetadata(nextCommitOffset,",
                "-                nextCommitMsg.getMetadata(Thread.currentThread()));",
                "+        if (found) {",
                "+            nextCommitOffsetAndMetadata = new OffsetAndMetadata(nextCommitOffset, commitMetadata);",
                "+",
                "             LOG.debug(\"Topic-partition [{}] has offsets [{}-{}] ready to be committed.\"",
                "-                + \" Processing will resume at [{}] if the spout restarts\",",
                "+                + \" Processing will resume at offset [{}] upon spout restart\",",
                "                 tp, committedOffset, nextCommitOffsetAndMetadata.offset() - 1, nextCommitOffsetAndMetadata.offset());",
                "@@ -162,14 +162,15 @@ public class OffsetManager {",
                "      * internal state in such a way that future calls to",
                "-     * {@link #findNextCommitOffset()} will return offsets greater than or equal to the",
                "+     * {@link #findNextCommitOffset(String)} will return offsets greater than or equal to the",
                "      * offset specified, if any.",
                "      *",
                "-     * @param committedOffset The committed offset. All lower offsets are expected to have been committed.",
                "+     * @param committedOffsetAndMeta The committed offset. All lower offsets are expected to have been committed.",
                "      * @return Number of offsets committed in this commit",
                "      */",
                "-    public long commit(OffsetAndMetadata committedOffset) {",
                "+    public long commit(OffsetAndMetadata committedOffsetAndMeta) {",
                "+        committed = true;",
                "         final long preCommitCommittedOffset = this.committedOffset;",
                "         long numCommittedOffsets = 0;",
                "-        this.committedOffset = committedOffset.offset();",
                "+        this.committedOffset = committedOffsetAndMeta.offset();",
                "         for (Iterator<KafkaSpoutMessageId> iterator = ackedMsgs.iterator(); iterator.hasNext();) {",
                "-            if (iterator.next().offset() < committedOffset.offset()) {",
                "+            if (iterator.next().offset() < committedOffsetAndMeta.offset()) {",
                "                 iterator.remove();",
                "@@ -182,3 +183,3 @@ public class OffsetManager {",
                "         for (Iterator<Long> iterator = emittedOffsets.iterator(); iterator.hasNext();) {",
                "-            if (iterator.next() < committedOffset.offset()) {",
                "+            if (iterator.next() < committedOffsetAndMeta.offset()) {",
                "                 iterator.remove();",
                "@@ -192,3 +193,3 @@ public class OffsetManager {",
                "         LOG.debug(\"Committed [{}] offsets in the range [{}-{}] for topic-partition [{}].\"",
                "-            + \" Processing will resume at offset [{}] if the spout restarts.\",",
                "+            + \" Processing will resume at [{}] upon spout restart\",",
                "                 numCommittedOffsets, preCommitCommittedOffset, this.committedOffset - 1, tp, this.committedOffset);",
                "@@ -198,12 +199,9 @@ public class OffsetManager {",
                "-    public long getCommittedOffset() {",
                "-        return committedOffset;",
                "-    }",
                "-",
                "-    public boolean isEmpty() {",
                "-        return ackedMsgs.isEmpty();",
                "-    }",
                "-",
                "-    public boolean contains(ConsumerRecord record) {",
                "-        return contains(new KafkaSpoutMessageId(record));",
                "+    /**",
                "+     * Checks if this OffsetManager has committed to Kafka.",
                "+     *",
                "+     * @return true if this OffsetManager has made at least one commit to Kafka, false otherwise",
                "+     */",
                "+    public boolean hasCommitted() {",
                "+        return committed;",
                "     }",
                "@@ -213,3 +211,3 @@ public class OffsetManager {",
                "     }",
                "-    ",
                "+",
                "     @VisibleForTesting",
                "@@ -223,3 +221,2 @@ public class OffsetManager {",
                "             + \"topic-partition=\" + tp",
                "-            + \", fetchOffset=\" + initialFetchOffset",
                "             + \", committedOffset=\" + committedOffset"
            ],
            "changed_files": [
                "external/storm-kafka-client/pom.xml",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutMessageId.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/CommitMetadata.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2844": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "0d114aaa900f03e898bb35e32666de3727a0b8e2"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2844",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "a0308efd6c934416206183561194f3470e415edb",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510420980,
            "hunks": 48,
            "message": "STORM-2810: Fix resource leaks in storm-hdfs tests",
            "diff": [
                "diff --git a/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreImpl.java b/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreImpl.java",
                "index a4c88ce63..e90e2d3ea 100644",
                "--- a/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreImpl.java",
                "+++ b/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreImpl.java",
                "@@ -47,5 +47,6 @@ public class HdfsBlobStoreImpl {",
                "     private static final int BUCKETS = 1024;",
                "-    private static final Timer timer = new Timer(\"HdfsBlobStore cleanup thread\", true);",
                "     private static final String BLOBSTORE_DATA = \"data\";",
                "+    private Timer timer;",
                "+",
                "     public class KeyInHashDirIterator implements Iterator<String> {",
                "@@ -114,3 +115,2 @@ public class HdfsBlobStoreImpl {",
                "     private FileSystem _fs;",
                "-    private TimerTask _cleanup = null;",
                "     private Configuration _hadoopConf;",
                "@@ -143,3 +143,3 @@ public class HdfsBlobStoreImpl {",
                "             LOG.debug(\"Starting hdfs blobstore cleaner\");",
                "-            _cleanup = new TimerTask() {",
                "+            TimerTask cleanup = new TimerTask() {",
                "                 @Override",
                "@@ -153,3 +153,4 @@ public class HdfsBlobStoreImpl {",
                "             };",
                "-            timer.scheduleAtFixedRate(_cleanup, 0, FULL_CLEANUP_FREQ);",
                "+            timer = new Timer(\"HdfsBlobStore cleanup thread\", true);",
                "+            timer.scheduleAtFixedRate(cleanup, 0, FULL_CLEANUP_FREQ);",
                "         }",
                "@@ -306,5 +307,4 @@ public class HdfsBlobStoreImpl {",
                "     public void shutdown() {",
                "-        if (_cleanup != null) {",
                "-            _cleanup.cancel();",
                "-            _cleanup = null;",
                "+        if (timer != null) {",
                "+            timer.cancel();",
                "         }",
                "diff --git a/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java b/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java",
                "index 12f835c22..7a2402c30 100644",
                "--- a/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java",
                "+++ b/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java",
                "@@ -1 +1,2 @@",
                "+",
                " /**",
                "@@ -51,2 +52,3 @@ import java.util.TimerTask;",
                " public abstract class AbstractHdfsBolt extends BaseRichBolt {",
                "+",
                "     private static final Logger LOG = LoggerFactory.getLogger(AbstractHdfsBolt.class);",
                "@@ -97,2 +99,3 @@ public abstract class AbstractHdfsBolt extends BaseRichBolt {",
                "      * Marked as final to prevent override. Subclasses should implement the doPrepare() method.",
                "+     *",
                "      * @param conf",
                "@@ -103,4 +106,8 @@ public abstract class AbstractHdfsBolt extends BaseRichBolt {",
                "         this.writeLock = new Object();",
                "-        if (this.syncPolicy == null) throw new IllegalStateException(\"SyncPolicy must be specified.\");",
                "-        if (this.rotationPolicy == null) throw new IllegalStateException(\"RotationPolicy must be specified.\");",
                "+        if (this.syncPolicy == null) {",
                "+            throw new IllegalStateException(\"SyncPolicy must be specified.\");",
                "+        }",
                "+        if (this.rotationPolicy == null) {",
                "+            throw new IllegalStateException(\"RotationPolicy must be specified.\");",
                "+        }",
                "         if (this.fsUrl == null) {",
                "@@ -217,6 +224,4 @@ public abstract class AbstractHdfsBolt extends BaseRichBolt {",
                "     /**",
                "-     * A tuple must be mapped to a writer based on two factors:",
                "-     *  - bolt specific logic that must separate tuples into different files in the same directory (see the avro bolt",
                "-     *    for an example of this)",
                "-     *  - the directory the tuple will be partioned into",
                "+     * A tuple must be mapped to a writer based on two factors: - bolt specific logic that must separate tuples into different files in the",
                "+     * same directory (see the avro bolt for an example of this) - the directory the tuple will be partioned into",
                "      *",
                "@@ -254,2 +259,7 @@ public abstract class AbstractHdfsBolt extends BaseRichBolt {",
                "+    @Override",
                "+    public void cleanup() {",
                "+        this.rotationTimer.cancel();",
                "+    }",
                "+",
                "     private void syncAllWriters() throws IOException {",
                "@@ -283,4 +293,3 @@ public abstract class AbstractHdfsBolt extends BaseRichBolt {",
                "         final int rotation;",
                "-        if (rotationCounterMap.containsKey(partitionPath))",
                "-        {",
                "+        if (rotationCounterMap.containsKey(partitionPath)) {",
                "             rotation = rotationCounterMap.get(partitionPath) + 1;",
                "@@ -302,2 +311,3 @@ public abstract class AbstractHdfsBolt extends BaseRichBolt {",
                "     static class WritersMap extends LinkedHashMap<String, AbstractHDFSWriter> {",
                "+",
                "         final long maxWriters;",
                "@@ -314,2 +324,2 @@ public abstract class AbstractHdfsBolt extends BaseRichBolt {",
                "     }",
                "-}",
                "\\ No newline at end of file",
                "+}",
                "diff --git a/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java b/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java",
                "index b7627f241..38a791b39 100644",
                "--- a/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java",
                "+++ b/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java",
                "@@ -1 +1,2 @@",
                "+",
                " /**",
                "@@ -11,9 +12,6 @@",
                "  * <p/>",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions",
                "+ * and limitations under the License.",
                "  */",
                "-",
                " package org.apache.storm.hdfs.spout;",
                "@@ -49,768 +47,751 @@ public class HdfsSpout extends BaseRichSpout {",
                "-  // user configurable",
                "-  private String hdfsUri;            // required",
                "-  private String readerType;         // required",
                "-  private Fields outputFields;       // required",
                "+    // user configurable",
                "+    private String hdfsUri;            // required",
                "+    private String readerType;         // required",
                "+    private Fields outputFields;       // required",
                "-  private String sourceDir;        // required",
                "-  private Path sourceDirPath;        // required",
                "+    private String sourceDir;        // required",
                "+    private Path sourceDirPath;        // required",
                "-  private String archiveDir;       // required",
                "-  private Path archiveDirPath;       // required",
                "+    private String archiveDir;       // required",
                "+    private Path archiveDirPath;       // required",
                "-  private String badFilesDir;      // required",
                "-  private Path badFilesDirPath;      // required",
                "+    private String badFilesDir;      // required",
                "+    private Path badFilesDirPath;      // required",
                "-  private String lockDir;",
                "-  private Path lockDirPath;",
                "+    private String lockDir;",
                "+    private Path lockDirPath;",
                "-  private int commitFrequencyCount = Configs.DEFAULT_COMMIT_FREQ_COUNT;",
                "-  private int commitFrequencySec = Configs.DEFAULT_COMMIT_FREQ_SEC;",
                "-  private int maxOutstanding = Configs.DEFAULT_MAX_OUTSTANDING;",
                "-  private int lockTimeoutSec = Configs.DEFAULT_LOCK_TIMEOUT;",
                "-  private boolean clocksInSync = true;",
                "+    private int commitFrequencyCount = Configs.DEFAULT_COMMIT_FREQ_COUNT;",
                "+    private int commitFrequencySec = Configs.DEFAULT_COMMIT_FREQ_SEC;",
                "+    private int maxOutstanding = Configs.DEFAULT_MAX_OUTSTANDING;",
                "+    private int lockTimeoutSec = Configs.DEFAULT_LOCK_TIMEOUT;",
                "+    private boolean clocksInSync = true;",
                "-  private String inprogress_suffix = \".inprogress\"; // not configurable to prevent change between topology restarts",
                "-  private String ignoreSuffix = \".ignore\";",
                "+    private String inprogress_suffix = \".inprogress\"; // not configurable to prevent change between topology restarts",
                "+    private String ignoreSuffix = \".ignore\";",
                "-  private String outputStreamName= null;",
                "+    private String outputStreamName = null;",
                "-  // other members",
                "-  private static final Logger LOG = LoggerFactory.getLogger(HdfsSpout.class);",
                "+    // other members",
                "+    private static final Logger LOG = LoggerFactory.getLogger(HdfsSpout.class);",
                "-  private ProgressTracker tracker = null;",
                "+    private ProgressTracker tracker = null;",
                "-  private FileSystem hdfs;",
                "-  private FileReader reader;",
                "+    private FileSystem hdfs;",
                "+    private FileReader reader;",
                "-  private SpoutOutputCollector collector;",
                "-  HashMap<MessageId, List<Object> > inflight = new HashMap<>();",
                "-  LinkedBlockingQueue<HdfsUtils.Pair<MessageId, List<Object>>> retryList = new LinkedBlockingQueue<>();",
                "-",
                "-  private Configuration hdfsConfig;",
                "-",
                "-  private Map conf = null;",
                "-  private FileLock lock;",
                "-  private String spoutId = null;",
                "-",
                "-  HdfsUtils.Pair<Path,FileLock.LogEntry> lastExpiredLock = null;",
                "-  private long lastExpiredLockTime = 0;",
                "-",
                "-  private long tupleCounter = 0;",
                "-  private boolean ackEnabled = false;",
                "-  private int acksSinceLastCommit = 0 ;",
                "-  private final AtomicBoolean commitTimeElapsed = new AtomicBoolean(false);",
                "-  private Timer commitTimer;",
                "-  private boolean fileReadCompletely = true;",
                "-",
                "-  private String configKey = Configs.DEFAULT_HDFS_CONFIG_KEY; // key for hdfs Kerberos configs",
                "-",
                "-  public HdfsSpout() {",
                "-  }",
                "-",
                "-  public HdfsSpout setHdfsUri(String hdfsUri) {",
                "-    this.hdfsUri = hdfsUri;",
                "-    return this;",
                "-  }",
                "-",
                "-  public HdfsSpout setReaderType(String readerType) {",
                "-    this.readerType = readerType;",
                "-    return this;",
                "-  }",
                "-",
                "-  public HdfsSpout setSourceDir(String sourceDir) {",
                "-    this.sourceDir = sourceDir;",
                "-    return this;",
                "-  }",
                "-",
                "-  public HdfsSpout setArchiveDir(String archiveDir) {",
                "-    this.archiveDir = archiveDir;",
                "-    return this;",
                "-  }",
                "-",
                "-  public HdfsSpout setBadFilesDir(String badFilesDir) {",
                "-    this.badFilesDir = badFilesDir;",
                "-    return this;",
                "-  }",
                "-",
                "-  public HdfsSpout setLockDir(String lockDir) {",
                "-    this.lockDir = lockDir;",
                "-    return this;",
                "-  }",
                "-",
                "-  public HdfsSpout setCommitFrequencyCount(int commitFrequencyCount) {",
                "-    this.commitFrequencyCount = commitFrequencyCount;",
                "-    return this;",
                "-  }",
                "-",
                "-  public HdfsSpout setCommitFrequencySec(int commitFrequencySec) {",
                "-    this.commitFrequencySec = commitFrequencySec;",
                "-    return this;",
                "-  }",
                "-",
                "-  public HdfsSpout setMaxOutstanding(int maxOutstanding) {",
                "-    this.maxOutstanding = maxOutstanding;",
                "-    return this;",
                "-  }",
                "-",
                "-  public HdfsSpout setLockTimeoutSec(int lockTimeoutSec) {",
                "-    this.lockTimeoutSec = lockTimeoutSec;",
                "-    return this;",
                "-  }",
                "-",
                "-  public HdfsSpout setClocksInSync(boolean clocksInSync) {",
                "-    this.clocksInSync = clocksInSync;",
                "-    return this;",
                "-  }",
                "-",
                "-",
                "-  public HdfsSpout setIgnoreSuffix(String ignoreSuffix) {",
                "-    this.ignoreSuffix = ignoreSuffix;",
                "-    return this;",
                "-  }",
                "-",
                "-  /** Output field names. Number of fields depends upon the reader type */",
                "-  public HdfsSpout withOutputFields(String... fields) {",
                "-    outputFields = new Fields(fields);",
                "-    return this;",
                "-  }",
                "-",
                "-  /** set key name under which HDFS options are placed. (similar to HDFS bolt).",
                "-   * default key name is 'hdfs.config' */",
                "-  public HdfsSpout withConfigKey(String configKey) {",
                "-    this.configKey = configKey;",
                "-    return this;",
                "-  }",
                "-",
                "-  /**",
                "-   * Set output stream name",
                "-   */",
                "-  public HdfsSpout withOutputStream(String streamName) {",
                "-    this.outputStreamName = streamName;",
                "-    return this;",
                "-  }",
                "-",
                "-  public Path getLockDirPath() {",
                "-    return lockDirPath;",
                "-  }",
                "-",
                "-  public SpoutOutputCollector getCollector() {",
                "-    return collector;",
                "-  }",
                "-",
                "-  public void nextTuple() {",
                "-    LOG.trace(\"Next Tuple {}\", spoutId);",
                "-    // 1) First re-emit any previously failed tuples (from retryList)",
                "-    if (!retryList.isEmpty()) {",
                "-      LOG.debug(\"Sending tuple from retry list\");",
                "-      HdfsUtils.Pair<MessageId, List<Object>> pair = retryList.remove();",
                "-      emitData(pair.getValue(), pair.getKey());",
                "-      return;",
                "-    }",
                "-",
                "-    if ( ackEnabled  &&  tracker.size()>= maxOutstanding ) {",
                "-      LOG.warn(\"Waiting for more ACKs before generating new tuples. \" +",
                "-              \"Progress tracker size has reached limit {}, SpoutID {}\"",
                "-              , maxOutstanding, spoutId);",
                "-      // Don't emit anything .. allow configured spout wait strategy to kick in",
                "-      return;",
                "-    }",
                "-",
                "-    // 2) If no failed tuples to be retried, then send tuples from hdfs",
                "-    while (true) {",
                "-      try {",
                "-        // 3) Select a new file if one is not open already",
                "-        if (reader == null) {",
                "-          reader = pickNextFile();",
                "-          if (reader == null) {",
                "-            LOG.debug(\"Currently no new files to process under : \" + sourceDirPath);",
                "-            return;",
                "-          } else {",
                "-            fileReadCompletely=false;",
                "-          }",
                "-        }",
                "-        if ( fileReadCompletely ) { // wait for more ACKs before proceeding",
                "-          return;",
                "-        }",
                "-        // 4) Read record from file, emit to collector and record progress",
                "-        List<Object> tuple = reader.next();",
                "-        if (tuple != null) {",
                "-          fileReadCompletely= false;",
                "-          ++tupleCounter;",
                "-          MessageId msgId = new MessageId(tupleCounter, reader.getFilePath(), reader.getFileOffset());",
                "-          emitData(tuple, msgId);",
                "-",
                "-          if ( !ackEnabled ) {",
                "-            ++acksSinceLastCommit; // assume message is immediately ACKed in non-ack mode",
                "-            commitProgress(reader.getFileOffset());",
                "-          } else {",
                "-            commitProgress(tracker.getCommitPosition());",
                "-          }",
                "-          return;",
                "-        } else {",
                "-          fileReadCompletely = true;",
                "-          if ( !ackEnabled ) {",
                "-            markFileAsDone(reader.getFilePath());",
                "-          }",
                "-        }",
                "-      } catch (IOException e) {",
                "-        LOG.error(\"I/O Error processing at file location \" + getFileProgress(reader), e);",
                "-        // don't emit anything .. allow configured spout wait strategy to kick in",
                "-        return;",
                "-      } catch (ParseException e) {",
                "-        LOG.error(\"Parsing error when processing at file location \" + getFileProgress(reader) +",
                "-                \". Skipping remainder of file.\", e);",
                "-        markFileAsBad(reader.getFilePath());",
                "-        // Note: We don't return from this method on ParseException to avoid triggering the",
                "-        // spout wait strategy (due to no emits). Instead we go back into the loop and",
                "-        // generate a tuple from next file",
                "-      }",
                "-    } // while",
                "-  }",
                "-",
                "-  // will commit progress into lock file if commit threshold is reached",
                "-  private void commitProgress(FileOffset position) {",
                "-    if ( position==null ) {",
                "-      return;",
                "-    }",
                "-    if ( lock!=null && canCommitNow() ) {",
                "-      try {",
                "-        String pos = position.toString();",
                "-        lock.heartbeat(pos);",
                "-        LOG.debug(\"{} Committed progress. {}\", spoutId, pos);",
                "-        acksSinceLastCommit = 0;",
                "-        commitTimeElapsed.set(false);",
                "-        setupCommitElapseTimer();",
                "-      } catch (IOException e) {",
                "-        LOG.error(\"Unable to commit progress Will retry later. Spout ID = \" + spoutId, e);",
                "-      }",
                "-    }",
                "-  }",
                "-",
                "-  private void setupCommitElapseTimer() {",
                "-    if ( commitFrequencySec<=0 ) {",
                "-      return;",
                "-    }",
                "-    TimerTask timerTask = new TimerTask() {",
                "-      @Override",
                "-      public void run() {",
                "-        commitTimeElapsed.set(true);",
                "-      }",
                "-    };",
                "-    commitTimer.schedule(timerTask, commitFrequencySec * 1000);",
                "-  }",
                "-",
                "-  private static String getFileProgress(FileReader reader) {",
                "-    return reader.getFilePath() + \" \" + reader.getFileOffset();",
                "-  }",
                "-",
                "-  private void markFileAsDone(Path filePath) {",
                "-    try {",
                "-      Path newFile = renameCompletedFile(reader.getFilePath());",
                "-      LOG.info(\"Completed processing {}. Spout Id = {}\", newFile, spoutId);",
                "-    } catch (IOException e) {",
                "-      LOG.error(\"Unable to archive completed file\" + filePath + \" Spout ID \" + spoutId, e);",
                "-    }",
                "-    closeReaderAndResetTrackers();",
                "-  }",
                "-",
                "-  private void markFileAsBad(Path file) {",
                "-    String fileName = file.toString();",
                "-    String fileNameMinusSuffix = fileName.substring(0, fileName.indexOf(inprogress_suffix));",
                "-    String originalName = new Path(fileNameMinusSuffix).getName();",
                "-    Path  newFile = new Path( badFilesDirPath + Path.SEPARATOR + originalName);",
                "-",
                "-    LOG.info(\"Moving bad file {} to {}. Processed it till offset {}. SpoutID= {}\", originalName, newFile, tracker.getCommitPosition(), spoutId);",
                "-    try {",
                "-      if (!hdfs.rename(file, newFile) ) { // seems this can fail by returning false or throwing exception",
                "-        throw new IOException(\"Move failed for bad file: \" + file); // convert false ret value to exception",
                "-      }",
                "-    } catch (IOException e) {",
                "-      LOG.warn(\"Error moving bad file: \" + file + \" to destination \" + newFile + \" SpoutId =\" + spoutId, e);",
                "-    }",
                "-    closeReaderAndResetTrackers();",
                "-  }",
                "-",
                "-  private void closeReaderAndResetTrackers() {",
                "-    inflight.clear();",
                "-    tracker.offsets.clear();",
                "-    retryList.clear();",
                "-",
                "-    reader.close();",
                "-    reader = null;",
                "-    releaseLockAndLog(lock, spoutId);",
                "-    lock = null;",
                "-  }",
                "-",
                "-  private static void releaseLockAndLog(FileLock fLock, String spoutId) {",
                "-    try {",
                "-      if ( fLock!=null ) {",
                "-        fLock.release();",
                "-        LOG.debug(\"Spout {} released FileLock. SpoutId = {}\", fLock.getLockFile(), spoutId);",
                "-      }",
                "-    } catch (IOException e) {",
                "-      LOG.error(\"Unable to delete lock file : \" +fLock.getLockFile() + \" SpoutId =\" + spoutId, e);",
                "-    }",
                "-  }",
                "-",
                "-  protected void emitData(List<Object> tuple, MessageId id) {",
                "-    LOG.trace(\"Emitting - {}\", id);",
                "-",
                "-    if ( outputStreamName==null )",
                "-      collector.emit( tuple, id );",
                "-    else",
                "-      collector.emit( outputStreamName, tuple, id );",
                "-",
                "-    inflight.put(id, tuple);",
                "-  }",
                "-",
                "-  public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {",
                "-    LOG.info(\"Opening HDFS Spout\");",
                "-    this.conf = conf;",
                "-    this.commitTimer = new Timer();",
                "-    this.tracker = new ProgressTracker();",
                "-    this.hdfsConfig = new Configuration();",
                "-",
                "-    this.collector = collector;",
                "-    this.hdfsConfig = new Configuration();",
                "-    this.tupleCounter = 0;",
                "-",
                "-    // Hdfs related settings",
                "-    if ( this.hdfsUri==null && conf.containsKey(Configs.HDFS_URI) ) {",
                "-      this.hdfsUri = conf.get(Configs.HDFS_URI).toString();",
                "-    }",
                "-    if ( this.hdfsUri==null ) {",
                "-      throw new RuntimeException(\"HDFS Uri not set on spout\");",
                "-    }",
                "-",
                "-    try {",
                "-      this.hdfs = FileSystem.get(URI.create(hdfsUri), hdfsConfig);",
                "-    } catch (IOException e) {",
                "-      LOG.error(\"Unable to instantiate file system\", e);",
                "-      throw new RuntimeException(\"Unable to instantiate file system\", e);",
                "-    }",
                "-",
                "-",
                "-    if ( conf.containsKey(configKey) ) {",
                "-      Map<String, Object> map = (Map<String, Object>)conf.get(configKey);",
                "-        if ( map != null ) {",
                "-          for(String keyName : map.keySet()){",
                "-            LOG.info(\"HDFS Config override : {} = {} \", keyName, String.valueOf(map.get(keyName)));",
                "-            this.hdfsConfig.set(keyName, String.valueOf(map.get(keyName)));",
                "-          }",
                "-          try {",
                "-            HdfsSecurityUtil.login(conf, hdfsConfig);",
                "-          } catch (IOException e) {",
                "-            LOG.error(\"HDFS Login failed \", e);",
                "-            throw new RuntimeException(e);",
                "-          }",
                "-        } // if (map != null)",
                "-      }",
                "+    private SpoutOutputCollector collector;",
                "+    HashMap<MessageId, List<Object>> inflight = new HashMap<>();",
                "+    LinkedBlockingQueue<HdfsUtils.Pair<MessageId, List<Object>>> retryList = new LinkedBlockingQueue<>();",
                "+",
                "+    private Configuration hdfsConfig;",
                "+",
                "+    private Map conf = null;",
                "+    private FileLock lock;",
                "+    private String spoutId = null;",
                "+",
                "+    HdfsUtils.Pair<Path, FileLock.LogEntry> lastExpiredLock = null;",
                "+    private long lastExpiredLockTime = 0;",
                "+",
                "+    private long tupleCounter = 0;",
                "+    private boolean ackEnabled = false;",
                "+    private int acksSinceLastCommit = 0;",
                "+    private final AtomicBoolean commitTimeElapsed = new AtomicBoolean(false);",
                "+    private Timer commitTimer;",
                "+    private boolean fileReadCompletely = true;",
                "+",
                "+    private String configKey = Configs.DEFAULT_HDFS_CONFIG_KEY; // key for hdfs Kerberos configs",
                "-    // Reader type config",
                "-    if ( readerType==null && conf.containsKey(Configs.READER_TYPE) ) {",
                "-      readerType = conf.get(Configs.READER_TYPE).toString();",
                "+    public HdfsSpout() {",
                "     }",
                "-    checkValidReader(readerType);",
                "-    // -- source dir config",
                "-    if ( sourceDir==null && conf.containsKey(Configs.SOURCE_DIR) ) {",
                "-      sourceDir = conf.get(Configs.SOURCE_DIR).toString();",
                "+    public HdfsSpout setHdfsUri(String hdfsUri) {",
                "+        this.hdfsUri = hdfsUri;",
                "+        return this;",
                "     }",
                "-    if ( sourceDir==null ) {",
                "-      LOG.error(Configs.SOURCE_DIR + \" setting is required\");",
                "-      throw new RuntimeException(Configs.SOURCE_DIR + \" setting is required\");",
                "+",
                "+    public HdfsSpout setReaderType(String readerType) {",
                "+        this.readerType = readerType;",
                "+        return this;",
                "+    }",
                "+",
                "+    public HdfsSpout setSourceDir(String sourceDir) {",
                "+        this.sourceDir = sourceDir;",
                "+        return this;",
                "     }",
                "-    this.sourceDirPath = new Path( sourceDir );",
                "-    // -- archive dir config",
                "-    if ( archiveDir==null && conf.containsKey(Configs.ARCHIVE_DIR) ) {",
                "-      archiveDir = conf.get(Configs.ARCHIVE_DIR).toString();",
                "+    public HdfsSpout setArchiveDir(String archiveDir) {",
                "+        this.archiveDir = archiveDir;",
                "+        return this;",
                "     }",
                "-    if ( archiveDir==null ) {",
                "-      LOG.error(Configs.ARCHIVE_DIR + \" setting is required\");",
                "-      throw new RuntimeException(Configs.ARCHIVE_DIR + \" setting is required\");",
                "+",
                "+    public HdfsSpout setBadFilesDir(String badFilesDir) {",
                "+        this.badFilesDir = badFilesDir;",
                "+        return this;",
                "+    }",
                "+",
                "+    public HdfsSpout setLockDir(String lockDir) {",
                "+        this.lockDir = lockDir;",
                "+        return this;",
                "     }",
                "-    this.archiveDirPath = new Path( archiveDir );",
                "-    validateOrMakeDir(hdfs, archiveDirPath, \"Archive\");",
                "-    // -- bad files dir config",
                "-    if ( badFilesDir==null && conf.containsKey(Configs.BAD_DIR) ) {",
                "-      badFilesDir = conf.get(Configs.BAD_DIR).toString();",
                "+    public HdfsSpout setCommitFrequencyCount(int commitFrequencyCount) {",
                "+        this.commitFrequencyCount = commitFrequencyCount;",
                "+        return this;",
                "     }",
                "-    if ( badFilesDir==null ) {",
                "-      LOG.error(Configs.BAD_DIR + \" setting is required\");",
                "-      throw new RuntimeException(Configs.BAD_DIR + \" setting is required\");",
                "+",
                "+    public HdfsSpout setCommitFrequencySec(int commitFrequencySec) {",
                "+        this.commitFrequencySec = commitFrequencySec;",
                "+        return this;",
                "+    }",
                "+",
                "+    public HdfsSpout setMaxOutstanding(int maxOutstanding) {",
                "+        this.maxOutstanding = maxOutstanding;",
                "+        return this;",
                "+    }",
                "+",
                "+    public HdfsSpout setLockTimeoutSec(int lockTimeoutSec) {",
                "+        this.lockTimeoutSec = lockTimeoutSec;",
                "+        return this;",
                "     }",
                "-    this.badFilesDirPath = new Path(badFilesDir);",
                "-    validateOrMakeDir(hdfs, badFilesDirPath, \"bad files\");",
                "-    // -- ignore file names config",
                "-    if ( conf.containsKey(Configs.IGNORE_SUFFIX) ) {",
                "-      this.ignoreSuffix = conf.get(Configs.IGNORE_SUFFIX).toString();",
                "+    public HdfsSpout setClocksInSync(boolean clocksInSync) {",
                "+        this.clocksInSync = clocksInSync;",
                "+        return this;",
                "     }",
                "-    // -- lock dir config",
                "-    if ( lockDir==null && conf.containsKey(Configs.LOCK_DIR) ) {",
                "-      lockDir = conf.get(Configs.LOCK_DIR).toString();",
                "+    public HdfsSpout setIgnoreSuffix(String ignoreSuffix) {",
                "+        this.ignoreSuffix = ignoreSuffix;",
                "+        return this;",
                "     }",
                "-    if ( lockDir==null ) {",
                "-      lockDir = getDefaultLockDir(sourceDirPath);",
                "+",
                "+    /**",
                "+     * Output field names. Number of fields depends upon the reader type",
                "+     */",
                "+    public HdfsSpout withOutputFields(String... fields) {",
                "+        outputFields = new Fields(fields);",
                "+        return this;",
                "     }",
                "-    this.lockDirPath = new Path(lockDir);",
                "-    validateOrMakeDir(hdfs,lockDirPath, \"locks\");",
                "+    /**",
                "+     * set key name under which HDFS options are placed. (similar to HDFS bolt). default key name is 'hdfs.config'",
                "+     */",
                "+    public HdfsSpout withConfigKey(String configKey) {",
                "+        this.configKey = configKey;",
                "+        return this;",
                "+    }",
                "-    // -- lock timeout",
                "-    if ( conf.get(Configs.LOCK_TIMEOUT) !=null ) {",
                "-      this.lockTimeoutSec = Integer.parseInt(conf.get(Configs.LOCK_TIMEOUT).toString());",
                "+    /**",
                "+     * Set output stream name",
                "+     */",
                "+    public HdfsSpout withOutputStream(String streamName) {",
                "+        this.outputStreamName = streamName;",
                "+        return this;",
                "     }",
                "-    // -- enable/disable ACKing",
                "-    Object ackers = conf.get(Config.TOPOLOGY_ACKER_EXECUTORS);",
                "-    if ( ackers!=null ) {",
                "-      int ackerCount = Integer.parseInt(ackers.toString());",
                "-      this.ackEnabled = (ackerCount>0);",
                "-      LOG.debug(\"ACKer count = {}\", ackerCount);",
                "+    public Path getLockDirPath() {",
                "+        return lockDirPath;",
                "     }",
                "-    else { // ackers==null when ackerCount not explicitly set on the topology",
                "-      this.ackEnabled = true;",
                "-      LOG.debug(\"ACK count not explicitly set on topology.\");",
                "+",
                "+    public SpoutOutputCollector getCollector() {",
                "+        return collector;",
                "     }",
                "-    LOG.info(\"ACK mode is {}\", ackEnabled ? \"enabled\" : \"disabled\");",
                "+    public void nextTuple() {",
                "+        LOG.trace(\"Next Tuple {}\", spoutId);",
                "+        // 1) First re-emit any previously failed tuples (from retryList)",
                "+        if (!retryList.isEmpty()) {",
                "+            LOG.debug(\"Sending tuple from retry list\");",
                "+            HdfsUtils.Pair<MessageId, List<Object>> pair = retryList.remove();",
                "+            emitData(pair.getValue(), pair.getKey());",
                "+            return;",
                "+        }",
                "-    // -- commit frequency - count",
                "-    if ( conf.get(Configs.COMMIT_FREQ_COUNT) != null ) {",
                "-      commitFrequencyCount = Integer.parseInt(conf.get(Configs.COMMIT_FREQ_COUNT).toString());",
                "+        if (ackEnabled && tracker.size() >= maxOutstanding) {",
                "+            LOG.warn(\"Waiting for more ACKs before generating new tuples. \"",
                "+                + \"Progress tracker size has reached limit {}, SpoutID {}\",",
                "+                 maxOutstanding, spoutId);",
                "+            // Don't emit anything .. allow configured spout wait strategy to kick in",
                "+            return;",
                "+        }",
                "+",
                "+        // 2) If no failed tuples to be retried, then send tuples from hdfs",
                "+        while (true) {",
                "+            try {",
                "+                // 3) Select a new file if one is not open already",
                "+                if (reader == null) {",
                "+                    reader = pickNextFile();",
                "+                    if (reader == null) {",
                "+                        LOG.debug(\"Currently no new files to process under : \" + sourceDirPath);",
                "+                        return;",
                "+                    } else {",
                "+                        fileReadCompletely = false;",
                "+                    }",
                "+                }",
                "+                if (fileReadCompletely) { // wait for more ACKs before proceeding",
                "+                    return;",
                "+                }",
                "+                // 4) Read record from file, emit to collector and record progress",
                "+                List<Object> tuple = reader.next();",
                "+                if (tuple != null) {",
                "+                    fileReadCompletely = false;",
                "+                    ++tupleCounter;",
                "+                    MessageId msgId = new MessageId(tupleCounter, reader.getFilePath(), reader.getFileOffset());",
                "+                    emitData(tuple, msgId);",
                "+",
                "+                    if (!ackEnabled) {",
                "+                        ++acksSinceLastCommit; // assume message is immediately ACKed in non-ack mode",
                "+                        commitProgress(reader.getFileOffset());",
                "+                    } else {",
                "+                        commitProgress(tracker.getCommitPosition());",
                "+                    }",
                "+                    return;",
                "+                } else {",
                "+                    fileReadCompletely = true;",
                "+                    if (!ackEnabled) {",
                "+                        markFileAsDone(reader.getFilePath());",
                "+                    }",
                "+                }",
                "+            } catch (IOException e) {",
                "+                LOG.error(\"I/O Error processing at file location \" + getFileProgress(reader), e);",
                "+                // don't emit anything .. allow configured spout wait strategy to kick in",
                "+                return;",
                "+            } catch (ParseException e) {",
                "+                LOG.error(\"Parsing error when processing at file location \" + getFileProgress(reader)",
                "+                    + \". Skipping remainder of file.\", e);",
                "+                markFileAsBad(reader.getFilePath());",
                "+                // Note: We don't return from this method on ParseException to avoid triggering the",
                "+                // spout wait strategy (due to no emits). Instead we go back into the loop and",
                "+                // generate a tuple from next file",
                "+            }",
                "+        } // while",
                "+    }",
                "+",
                "+    // will commit progress into lock file if commit threshold is reached",
                "+    private void commitProgress(FileOffset position) {",
                "+        if (position == null) {",
                "+            return;",
                "+        }",
                "+        if (lock != null && canCommitNow()) {",
                "+            try {",
                "+                String pos = position.toString();",
                "+                lock.heartbeat(pos);",
                "+                LOG.debug(\"{} Committed progress. {}\", spoutId, pos);",
                "+                acksSinceLastCommit = 0;",
                "+                commitTimeElapsed.set(false);",
                "+                setupCommitElapseTimer();",
                "+            } catch (IOException e) {",
                "+                LOG.error(\"Unable to commit progress Will retry later. Spout ID = \" + spoutId, e);",
                "+            }",
                "+        }",
                "     }",
                "-    // -- commit frequency - seconds",
                "-    if ( conf.get(Configs.COMMIT_FREQ_SEC) != null ) {",
                "-      commitFrequencySec = Integer.parseInt(conf.get(Configs.COMMIT_FREQ_SEC).toString());",
                "-      if ( commitFrequencySec<=0 ) {",
                "-        throw new RuntimeException(Configs.COMMIT_FREQ_SEC + \" setting must be greater than 0\");",
                "-      }",
                "-    }",
                "-",
                "-    // -- max outstanding tuples",
                "-    if ( conf.get(Configs.MAX_OUTSTANDING) !=null ) {",
                "-      maxOutstanding = Integer.parseInt(conf.get(Configs.MAX_OUTSTANDING).toString());",
                "-    }",
                "+    private void setupCommitElapseTimer() {",
                "+        if (commitFrequencySec <= 0) {",
                "+            return;",
                "+        }",
                "+        TimerTask timerTask = new TimerTask() {",
                "+            @Override",
                "+            public void run() {",
                "+                commitTimeElapsed.set(true);",
                "+            }",
                "+        };",
                "+        commitTimer.schedule(timerTask, commitFrequencySec * 1000);",
                "+    }",
                "-    // -- clocks in sync",
                "-    if ( conf.get(Configs.CLOCKS_INSYNC) !=null ) {",
                "-      clocksInSync = Boolean.parseBoolean(conf.get(Configs.CLOCKS_INSYNC).toString());",
                "+    private static String getFileProgress(FileReader reader) {",
                "+        return reader.getFilePath() + \" \" + reader.getFileOffset();",
                "     }",
                "-    // -- spout id",
                "-    spoutId = context.getThisComponentId();",
                "+    private void markFileAsDone(Path filePath) {",
                "+        try {",
                "+            Path newFile = renameCompletedFile(reader.getFilePath());",
                "+            LOG.info(\"Completed processing {}. Spout Id = {}\", newFile, spoutId);",
                "+        } catch (IOException e) {",
                "+            LOG.error(\"Unable to archive completed file\" + filePath + \" Spout ID \" + spoutId, e);",
                "+        }",
                "+        closeReaderAndResetTrackers();",
                "+    }",
                "-    // setup timer for commit elapse time tracking",
                "-    setupCommitElapseTimer();",
                "-  }",
                "+    private void markFileAsBad(Path file) {",
                "+        String fileName = file.toString();",
                "+        String fileNameMinusSuffix = fileName.substring(0, fileName.indexOf(inprogress_suffix));",
                "+        String originalName = new Path(fileNameMinusSuffix).getName();",
                "+        Path newFile = new Path(badFilesDirPath + Path.SEPARATOR + originalName);",
                "-  private static void validateOrMakeDir(FileSystem fs, Path dir, String dirDescription) {",
                "-    try {",
                "-      if ( fs.exists(dir) ) {",
                "-        if ( !fs.isDirectory(dir) ) {",
                "-          LOG.error(dirDescription + \" directory is a file, not a dir. \" + dir);",
                "-          throw new RuntimeException(dirDescription + \" directory is a file, not a dir. \" + dir);",
                "+        LOG.info(\"Moving bad file {} to {}. Processed it till offset {}. SpoutID= {}\", originalName, newFile, tracker.getCommitPosition(), spoutId);",
                "+        try {",
                "+            if (!hdfs.rename(file, newFile)) { // seems this can fail by returning false or throwing exception",
                "+                throw new IOException(\"Move failed for bad file: \" + file); // convert false ret value to exception",
                "+            }",
                "+        } catch (IOException e) {",
                "+            LOG.warn(\"Error moving bad file: \" + file + \" to destination \" + newFile + \" SpoutId =\" + spoutId, e);",
                "         }",
                "-      } else if ( ! fs.mkdirs(dir) ) {",
                "-        LOG.error(\"Unable to create \" + dirDescription + \" directory \" + dir);",
                "-        throw new RuntimeException(\"Unable to create \" + dirDescription + \" directory \" + dir);",
                "-      }",
                "-    } catch (IOException e) {",
                "-      LOG.error(\"Unable to create \" + dirDescription + \" directory \" + dir, e);",
                "-      throw new RuntimeException(\"Unable to create \" + dirDescription + \" directory \" + dir, e);",
                "+        closeReaderAndResetTrackers();",
                "     }",
                "-  }",
                "-  private String getDefaultLockDir(Path sourceDirPath) {",
                "-    return sourceDirPath.toString() + Path.SEPARATOR + Configs.DEFAULT_LOCK_DIR;",
                "-  }",
                "+    private void closeReaderAndResetTrackers() {",
                "+        inflight.clear();",
                "+        tracker.offsets.clear();",
                "+        retryList.clear();",
                "+",
                "+        reader.close();",
                "+        reader = null;",
                "+        releaseLockAndLog(lock, spoutId);",
                "+        lock = null;",
                "+    }",
                "-  private static void checkValidReader(String readerType) {",
                "-    if ( readerType.equalsIgnoreCase(Configs.TEXT)  || readerType.equalsIgnoreCase(Configs.SEQ) )",
                "-      return;",
                "-    try {",
                "-      Class<?> classType = Class.forName(readerType);",
                "-      classType.getConstructor(FileSystem.class, Path.class, Map.class);",
                "-      return;",
                "-    } catch (ClassNotFoundException e) {",
                "-      LOG.error(readerType + \" not found in classpath.\", e);",
                "-      throw new IllegalArgumentException(readerType + \" not found in classpath.\", e);",
                "-    } catch (NoSuchMethodException e) {",
                "-      LOG.error(readerType + \" is missing the expected constructor for Readers.\", e);",
                "-      throw new IllegalArgumentException(readerType + \" is missing the expected constuctor for Readers.\");",
                "+    private static void releaseLockAndLog(FileLock fLock, String spoutId) {",
                "+        try {",
                "+            if (fLock != null) {",
                "+                fLock.release();",
                "+                LOG.debug(\"Spout {} released FileLock. SpoutId = {}\", fLock.getLockFile(), spoutId);",
                "+            }",
                "+        } catch (IOException e) {",
                "+            LOG.error(\"Unable to delete lock file : \" + fLock.getLockFile() + \" SpoutId =\" + spoutId, e);",
                "+        }",
                "     }",
                "-  }",
                "-  @Override",
                "-  public void ack(Object msgId) {",
                "-    LOG.trace(\"Ack received for msg {} on spout {}\", msgId, spoutId);",
                "-    if ( !ackEnabled ) {",
                "-      return;",
                "+    protected void emitData(List<Object> tuple, MessageId id) {",
                "+        LOG.trace(\"Emitting - {}\", id);",
                "+",
                "+        if (outputStreamName == null) {",
                "+            collector.emit(tuple, id);",
                "+        } else {",
                "+            collector.emit(outputStreamName, tuple, id);",
                "+        }",
                "+",
                "+        inflight.put(id, tuple);",
                "     }",
                "-    MessageId id = (MessageId) msgId;",
                "-    inflight.remove(id);",
                "-    ++acksSinceLastCommit;",
                "-    tracker.recordAckedOffset(id.offset);",
                "-    commitProgress(tracker.getCommitPosition());",
                "-    if ( fileReadCompletely && inflight.isEmpty() ) {",
                "-      markFileAsDone(reader.getFilePath());",
                "-      reader = null;",
                "-    }",
                "-    super.ack(msgId);",
                "-  }",
                "-",
                "-  private boolean canCommitNow() {",
                "-",
                "-    if ( commitFrequencyCount>0 &&  acksSinceLastCommit >= commitFrequencyCount ) {",
                "-      return true;",
                "-    }",
                "-    return commitTimeElapsed.get();",
                "-  }",
                "-",
                "-  @Override",
                "-  public void fail(Object msgId) {",
                "-    LOG.trace(\"Fail received for msg id {} on spout {}\", msgId, spoutId);",
                "-    super.fail(msgId);",
                "-    if ( ackEnabled ) {",
                "-      HdfsUtils.Pair<MessageId, List<Object>> item = HdfsUtils.Pair.of(msgId, inflight.remove(msgId));",
                "-      retryList.add(item);",
                "-    }",
                "-  }",
                "-",
                "-  private FileReader pickNextFile() {",
                "-    try {",
                "-      // 1) If there are any abandoned files, pick oldest one",
                "-      lock = getOldestExpiredLock();",
                "-      if ( lock!=null ) {",
                "-        LOG.debug(\"Spout {} now took over ownership of abandoned FileLock {}\", spoutId, lock.getLockFile());",
                "-        Path file = getFileForLockFile(lock.getLockFile(), sourceDirPath);",
                "-        String resumeFromOffset = lock.getLastLogEntry().fileOffset;",
                "-        LOG.info(\"Resuming processing of abandoned file : {}\", file);",
                "-        return createFileReader(file, resumeFromOffset);",
                "-      }",
                "-",
                "-      // 2) If no abandoned files, then pick oldest file in sourceDirPath, lock it and rename it",
                "-      Collection<Path> listing = HdfsUtils.listFilesByModificationTime(hdfs, sourceDirPath, 0);",
                "-",
                "-      for (Path file : listing) {",
                "-        if (file.getName().endsWith(inprogress_suffix)) {",
                "-          continue;",
                "-        }",
                "-        if (file.getName().endsWith(ignoreSuffix)) {",
                "-          continue;",
                "-        }",
                "-        lock = FileLock.tryLock(hdfs, file, lockDirPath, spoutId);",
                "-        if (lock == null) {",
                "-          LOG.debug(\"Unable to get FileLock for {}, so skipping it.\", file);",
                "-          continue; // could not lock, so try another file.",
                "+",
                "+    public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {",
                "+        LOG.info(\"Opening HDFS Spout\");",
                "+        this.conf = conf;",
                "+        this.commitTimer = new Timer();",
                "+        this.tracker = new ProgressTracker();",
                "+        this.hdfsConfig = new Configuration();",
                "+",
                "+        this.collector = collector;",
                "+        this.hdfsConfig = new Configuration();",
                "+        this.tupleCounter = 0;",
                "+",
                "+        // Hdfs related settings",
                "+        if (this.hdfsUri == null && conf.containsKey(Configs.HDFS_URI)) {",
                "+            this.hdfsUri = conf.get(Configs.HDFS_URI).toString();",
                "+        }",
                "+        if (this.hdfsUri == null) {",
                "+            throw new RuntimeException(\"HDFS Uri not set on spout\");",
                "         }",
                "+",
                "         try {",
                "-          Path newFile = renameToInProgressFile(file);",
                "-          FileReader result = createFileReader(newFile);",
                "-          LOG.info(\"Processing : {} \", file);",
                "-          return result;",
                "+            this.hdfs = FileSystem.get(URI.create(hdfsUri), hdfsConfig);",
                "+        } catch (IOException e) {",
                "+            LOG.error(\"Unable to instantiate file system\", e);",
                "+            throw new RuntimeException(\"Unable to instantiate file system\", e);",
                "+        }",
                "+",
                "+        if (conf.containsKey(configKey)) {",
                "+            Map<String, Object> map = (Map<String, Object>) conf.get(configKey);",
                "+            if (map != null) {",
                "+                for (String keyName : map.keySet()) {",
                "+                    LOG.info(\"HDFS Config override : {} = {} \", keyName, String.valueOf(map.get(keyName)));",
                "+                    this.hdfsConfig.set(keyName, String.valueOf(map.get(keyName)));",
                "+                }",
                "+                try {",
                "+                    HdfsSecurityUtil.login(conf, hdfsConfig);",
                "+                } catch (IOException e) {",
                "+                    LOG.error(\"HDFS Login failed \", e);",
                "+                    throw new RuntimeException(e);",
                "+                }",
                "+            } // if (map != null)",
                "+        }",
                "+",
                "+        // Reader type config",
                "+        if (readerType == null && conf.containsKey(Configs.READER_TYPE)) {",
                "+            readerType = conf.get(Configs.READER_TYPE).toString();",
                "+        }",
                "+        checkValidReader(readerType);",
                "+",
                "+        // -- source dir config",
                "+        if (sourceDir == null && conf.containsKey(Configs.SOURCE_DIR)) {",
                "+            sourceDir = conf.get(Configs.SOURCE_DIR).toString();",
                "+        }",
                "+        if (sourceDir == null) {",
                "+            LOG.error(Configs.SOURCE_DIR + \" setting is required\");",
                "+            throw new RuntimeException(Configs.SOURCE_DIR + \" setting is required\");",
                "+        }",
                "+        this.sourceDirPath = new Path(sourceDir);",
                "+",
                "+        // -- archive dir config",
                "+        if (archiveDir == null && conf.containsKey(Configs.ARCHIVE_DIR)) {",
                "+            archiveDir = conf.get(Configs.ARCHIVE_DIR).toString();",
                "+        }",
                "+        if (archiveDir == null) {",
                "+            LOG.error(Configs.ARCHIVE_DIR + \" setting is required\");",
                "+            throw new RuntimeException(Configs.ARCHIVE_DIR + \" setting is required\");",
                "+        }",
                "+        this.archiveDirPath = new Path(archiveDir);",
                "+        validateOrMakeDir(hdfs, archiveDirPath, \"Archive\");",
                "+",
                "+        // -- bad files dir config",
                "+        if (badFilesDir == null && conf.containsKey(Configs.BAD_DIR)) {",
                "+            badFilesDir = conf.get(Configs.BAD_DIR).toString();",
                "+        }",
                "+        if (badFilesDir == null) {",
                "+            LOG.error(Configs.BAD_DIR + \" setting is required\");",
                "+            throw new RuntimeException(Configs.BAD_DIR + \" setting is required\");",
                "+        }",
                "+        this.badFilesDirPath = new Path(badFilesDir);",
                "+        validateOrMakeDir(hdfs, badFilesDirPath, \"bad files\");",
                "+",
                "+        // -- ignore file names config",
                "+        if (conf.containsKey(Configs.IGNORE_SUFFIX)) {",
                "+            this.ignoreSuffix = conf.get(Configs.IGNORE_SUFFIX).toString();",
                "+        }",
                "+",
                "+        // -- lock dir config",
                "+        if (lockDir == null && conf.containsKey(Configs.LOCK_DIR)) {",
                "+            lockDir = conf.get(Configs.LOCK_DIR).toString();",
                "+        }",
                "+        if (lockDir == null) {",
                "+            lockDir = getDefaultLockDir(sourceDirPath);",
                "+        }",
                "+        this.lockDirPath = new Path(lockDir);",
                "+        validateOrMakeDir(hdfs, lockDirPath, \"locks\");",
                "+",
                "+        // -- lock timeout",
                "+        if (conf.get(Configs.LOCK_TIMEOUT) != null) {",
                "+            this.lockTimeoutSec = Integer.parseInt(conf.get(Configs.LOCK_TIMEOUT).toString());",
                "+        }",
                "+",
                "+        // -- enable/disable ACKing",
                "+        Object ackers = conf.get(Config.TOPOLOGY_ACKER_EXECUTORS);",
                "+        if (ackers != null) {",
                "+            int ackerCount = Integer.parseInt(ackers.toString());",
                "+            this.ackEnabled = (ackerCount > 0);",
                "+            LOG.debug(\"ACKer count = {}\", ackerCount);",
                "+        } else { // ackers==null when ackerCount not explicitly set on the topology",
                "+            this.ackEnabled = true;",
                "+            LOG.debug(\"ACK count not explicitly set on topology.\");",
                "+        }",
                "+",
                "+        LOG.info(\"ACK mode is {}\", ackEnabled ? \"enabled\" : \"disabled\");",
                "+",
                "+        // -- commit frequency - count",
                "+        if (conf.get(Configs.COMMIT_FREQ_COUNT) != null) {",
                "+            commitFrequencyCount = Integer.parseInt(conf.get(Configs.COMMIT_FREQ_COUNT).toString());",
                "+        }",
                "+",
                "+        // -- commit frequency - seconds",
                "+        if (conf.get(Configs.COMMIT_FREQ_SEC) != null) {",
                "+            commitFrequencySec = Integer.parseInt(conf.get(Configs.COMMIT_FREQ_SEC).toString());",
                "+            if (commitFrequencySec <= 0) {",
                "+                throw new RuntimeException(Configs.COMMIT_FREQ_SEC + \" setting must be greater than 0\");",
                "+            }",
                "+        }",
                "+",
                "+        // -- max outstanding tuples",
                "+        if (conf.get(Configs.MAX_OUTSTANDING) != null) {",
                "+            maxOutstanding = Integer.parseInt(conf.get(Configs.MAX_OUTSTANDING).toString());",
                "+        }",
                "+",
                "+        // -- clocks in sync",
                "+        if (conf.get(Configs.CLOCKS_INSYNC) != null) {",
                "+            clocksInSync = Boolean.parseBoolean(conf.get(Configs.CLOCKS_INSYNC).toString());",
                "+        }",
                "+",
                "+        // -- spout id",
                "+        spoutId = context.getThisComponentId();",
                "+",
                "+        // setup timer for commit elapse time tracking",
                "+        setupCommitElapseTimer();",
                "+    }",
                "+",
                "+    @Override",
                "+    public void close() {",
                "+        this.commitTimer.cancel();",
                "+    }",
                "+",
                "+    private static void validateOrMakeDir(FileSystem fs, Path dir, String dirDescription) {",
                "+        try {",
                "+            if (fs.exists(dir)) {",
                "+                if (!fs.isDirectory(dir)) {",
                "+                    LOG.error(dirDescription + \" directory is a file, not a dir. \" + dir);",
                "+                    throw new RuntimeException(dirDescription + \" directory is a file, not a dir. \" + dir);",
                "+                }",
                "+            } else if (!fs.mkdirs(dir)) {",
                "+                LOG.error(\"Unable to create \" + dirDescription + \" directory \" + dir);",
                "+                throw new RuntimeException(\"Unable to create \" + dirDescription + \" directory \" + dir);",
                "+            }",
                "+        } catch (IOException e) {",
                "+            LOG.error(\"Unable to create \" + dirDescription + \" directory \" + dir, e);",
                "+            throw new RuntimeException(\"Unable to create \" + dirDescription + \" directory \" + dir, e);",
                "+        }",
                "+    }",
                "+",
                "+    private String getDefaultLockDir(Path sourceDirPath) {",
                "+        return sourceDirPath.toString() + Path.SEPARATOR + Configs.DEFAULT_LOCK_DIR;",
                "+    }",
                "+",
                "+    private static void checkValidReader(String readerType) {",
                "+        if (readerType.equalsIgnoreCase(Configs.TEXT) || readerType.equalsIgnoreCase(Configs.SEQ)) {",
                "+            return;",
                "+        }",
                "+        try {",
                "+            Class<?> classType = Class.forName(readerType);",
                "+            classType.getConstructor(FileSystem.class, Path.class, Map.class);",
                "+            return;",
                "+        } catch (ClassNotFoundException e) {",
                "+            LOG.error(readerType + \" not found in classpath.\", e);",
                "+            throw new IllegalArgumentException(readerType + \" not found in classpath.\", e);",
                "+        } catch (NoSuchMethodException e) {",
                "+            LOG.error(readerType + \" is missing the expected constructor for Readers.\", e);",
                "+            throw new IllegalArgumentException(readerType + \" is missing the expected constuctor for Readers.\");",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public void ack(Object msgId) {",
                "+        LOG.trace(\"Ack received for msg {} on spout {}\", msgId, spoutId);",
                "+        if (!ackEnabled) {",
                "+            return;",
                "+        }",
                "+        MessageId id = (MessageId) msgId;",
                "+        inflight.remove(id);",
                "+        ++acksSinceLastCommit;",
                "+        tracker.recordAckedOffset(id.offset);",
                "+        commitProgress(tracker.getCommitPosition());",
                "+        if (fileReadCompletely && inflight.isEmpty()) {",
                "+            markFileAsDone(reader.getFilePath());",
                "+            reader = null;",
                "+        }",
                "+        super.ack(msgId);",
                "+    }",
                "+",
                "+    private boolean canCommitNow() {",
                "+",
                "+        if (commitFrequencyCount > 0 && acksSinceLastCommit >= commitFrequencyCount) {",
                "+            return true;",
                "+        }",
                "+        return commitTimeElapsed.get();",
                "+    }",
                "+",
                "+    @Override",
                "+    public void fail(Object msgId) {",
                "+        LOG.trace(\"Fail received for msg id {} on spout {}\", msgId, spoutId);",
                "+        super.fail(msgId);",
                "+        if (ackEnabled) {",
                "+            HdfsUtils.Pair<MessageId, List<Object>> item = HdfsUtils.Pair.of(msgId, inflight.remove(msgId));",
                "+            retryList.add(item);",
                "+        }",
                "+    }",
                "+",
                "+    private FileReader pickNextFile() {",
                "+        try {",
                "+            // 1) If there are any abandoned files, pick oldest one",
                "+            lock = getOldestExpiredLock();",
                "+            if (lock != null) {",
                "+                LOG.debug(\"Spout {} now took over ownership of abandoned FileLock {}\", spoutId, lock.getLockFile());",
                "+                Path file = getFileForLockFile(lock.getLockFile(), sourceDirPath);",
                "+                String resumeFromOffset = lock.getLastLogEntry().fileOffset;",
                "+                LOG.info(\"Resuming processing of abandoned file : {}\", file);",
                "+                return createFileReader(file, resumeFromOffset);",
                "+            }",
                "+",
                "+            // 2) If no abandoned files, then pick oldest file in sourceDirPath, lock it and rename it",
                "+            Collection<Path> listing = HdfsUtils.listFilesByModificationTime(hdfs, sourceDirPath, 0);",
                "+",
                "+            for (Path file : listing) {",
                "+                if (file.getName().endsWith(inprogress_suffix)) {",
                "+                    continue;",
                "+                }",
                "+                if (file.getName().endsWith(ignoreSuffix)) {",
                "+                    continue;",
                "+                }",
                "+                lock = FileLock.tryLock(hdfs, file, lockDirPath, spoutId);",
                "+                if (lock == null) {",
                "+                    LOG.debug(\"Unable to get FileLock for {}, so skipping it.\", file);",
                "+                    continue; // could not lock, so try another file.",
                "+                }",
                "+                try {",
                "+                    Path newFile = renameToInProgressFile(file);",
                "+                    FileReader result = createFileReader(newFile);",
                "+                    LOG.info(\"Processing : {} \", file);",
                "+                    return result;",
                "+                } catch (Exception e) {",
                "+                    LOG.error(\"Skipping file \" + file, e);",
                "+                    releaseLockAndLog(lock, spoutId);",
                "+                    continue;",
                "+                }",
                "+            }",
                "+",
                "+            return null;",
                "+        } catch (IOException e) {",
                "+            LOG.error(\"Unable to select next file for consumption \" + sourceDirPath, e);",
                "+            return null;",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * If clocks in sync, then acquires the oldest expired lock Else, on first call, just remembers the oldest expired lock, on next call",
                "+     * check if the lock is updated. if not updated then acquires the lock",
                "+     *",
                "+     * @return a lock object",
                "+     * @throws IOException",
                "+     */",
                "+    private FileLock getOldestExpiredLock() throws IOException {",
                "+        // 1 - acquire lock on dir",
                "+        DirLock dirlock = DirLock.tryLock(hdfs, lockDirPath);",
                "+        if (dirlock == null) {",
                "+            dirlock = DirLock.takeOwnershipIfStale(hdfs, lockDirPath, lockTimeoutSec);",
                "+            if (dirlock == null) {",
                "+                LOG.debug(\"Spout {} could not take over ownership of DirLock for {}\", spoutId, lockDirPath);",
                "+                return null;",
                "+            }",
                "+            LOG.debug(\"Spout {} now took over ownership of abandoned DirLock for {}\", spoutId, lockDirPath);",
                "+        } else {",
                "+            LOG.debug(\"Spout {} now owns DirLock for {}\", spoutId, lockDirPath);",
                "+        }",
                "+",
                "+        try {",
                "+            // 2 - if clocks are in sync then simply take ownership of the oldest expired lock",
                "+            if (clocksInSync) {",
                "+                return FileLock.acquireOldestExpiredLock(hdfs, lockDirPath, lockTimeoutSec, spoutId);",
                "+            }",
                "+",
                "+            // 3 - if clocks are not in sync ..",
                "+            if (lastExpiredLock == null) {",
                "+                // just make a note of the oldest expired lock now and check if its still unmodified after lockTimeoutSec",
                "+                lastExpiredLock = FileLock.locateOldestExpiredLock(hdfs, lockDirPath, lockTimeoutSec);",
                "+                lastExpiredLockTime = System.currentTimeMillis();",
                "+                return null;",
                "+            }",
                "+            // see if lockTimeoutSec time has elapsed since we last selected the lock file",
                "+            if (hasExpired(lastExpiredLockTime)) {",
                "+                return null;",
                "+            }",
                "+",
                "+            // If lock file has expired, then own it",
                "+            FileLock.LogEntry lastEntry = FileLock.getLastEntry(hdfs, lastExpiredLock.getKey());",
                "+            if (lastEntry.equals(lastExpiredLock.getValue())) {",
                "+                FileLock result = FileLock.takeOwnership(hdfs, lastExpiredLock.getKey(), lastEntry, spoutId);",
                "+                lastExpiredLock = null;",
                "+                return result;",
                "+            } else {",
                "+                // if lock file has been updated since last time, then leave this lock file alone",
                "+                lastExpiredLock = null;",
                "+                return null;",
                "+            }",
                "+        } finally {",
                "+            dirlock.release();",
                "+            LOG.debug(\"Released DirLock {}, SpoutID {} \", dirlock.getLockFile(), spoutId);",
                "+        }",
                "+    }",
                "+",
                "+    private boolean hasExpired(long lastModifyTime) {",
                "+        return (System.currentTimeMillis() - lastModifyTime) < lockTimeoutSec * 1000;",
                "+    }",
                "+",
                "+    /**",
                "+     * Creates a reader that reads from beginning of file",
                "+     *",
                "+     * @param file file to read",
                "+     * @return",
                "+     * @throws IOException",
                "+     */",
                "+    private FileReader createFileReader(Path file)",
                "+        throws IOException {",
                "+        if (readerType.equalsIgnoreCase(Configs.SEQ)) {",
                "+            return new SequenceFileReader(this.hdfs, file, conf);",
                "+        }",
                "+        if (readerType.equalsIgnoreCase(Configs.TEXT)) {",
                "+            return new TextFileReader(this.hdfs, file, conf);",
                "+        }",
                "+        try {",
                "+            Class<?> clsType = Class.forName(readerType);",
                "+            Constructor<?> constructor = clsType.getConstructor(FileSystem.class, Path.class, Map.class);",
                "+            return (FileReader) constructor.newInstance(this.hdfs, file, conf);",
                "         } catch (Exception e) {",
                "-          LOG.error(\"Skipping file \" + file, e);",
                "-          releaseLockAndLog(lock, spoutId);",
                "-          continue;",
                "-        }",
                "-      }",
                "-",
                "-      return null;",
                "-    } catch (IOException e) {",
                "-      LOG.error(\"Unable to select next file for consumption \" + sourceDirPath, e);",
                "-      return null;",
                "-    }",
                "-  }",
                "-",
                "-  /**",
                "-   * If clocks in sync, then acquires the oldest expired lock",
                "-   * Else, on first call, just remembers the oldest expired lock, on next call check if the lock is updated. if not updated then acquires the lock",
                "-   * @return a lock object",
                "-   * @throws IOException",
                "-   */",
                "-  private FileLock getOldestExpiredLock() throws IOException {",
                "-    // 1 - acquire lock on dir",
                "-    DirLock dirlock = DirLock.tryLock(hdfs, lockDirPath);",
                "-    if (dirlock == null) {",
                "-      dirlock = DirLock.takeOwnershipIfStale(hdfs, lockDirPath, lockTimeoutSec);",
                "-      if (dirlock == null) {",
                "-        LOG.debug(\"Spout {} could not take over ownership of DirLock for {}\", spoutId, lockDirPath);",
                "-        return null;",
                "-      }",
                "-      LOG.debug(\"Spout {} now took over ownership of abandoned DirLock for {}\", spoutId, lockDirPath);",
                "-    } else {",
                "-      LOG.debug(\"Spout {} now owns DirLock for {}\", spoutId, lockDirPath);",
                "-    }",
                "-",
                "-    try {",
                "-      // 2 - if clocks are in sync then simply take ownership of the oldest expired lock",
                "-      if (clocksInSync) {",
                "-        return FileLock.acquireOldestExpiredLock(hdfs, lockDirPath, lockTimeoutSec, spoutId);",
                "-      }",
                "-",
                "-      // 3 - if clocks are not in sync ..",
                "-      if ( lastExpiredLock == null ) {",
                "-        // just make a note of the oldest expired lock now and check if its still unmodified after lockTimeoutSec",
                "-        lastExpiredLock = FileLock.locateOldestExpiredLock(hdfs, lockDirPath, lockTimeoutSec);",
                "-        lastExpiredLockTime = System.currentTimeMillis();",
                "-        return null;",
                "-      }",
                "-      // see if lockTimeoutSec time has elapsed since we last selected the lock file",
                "-      if ( hasExpired(lastExpiredLockTime) ) {",
                "-        return null;",
                "-      }",
                "-",
                "-      // If lock file has expired, then own it",
                "-      FileLock.LogEntry lastEntry = FileLock.getLastEntry(hdfs, lastExpiredLock.getKey());",
                "-      if ( lastEntry.equals(lastExpiredLock.getValue()) ) {",
                "-        FileLock result = FileLock.takeOwnership(hdfs, lastExpiredLock.getKey(), lastEntry, spoutId);",
                "-        lastExpiredLock = null;",
                "-        return  result;",
                "-      } else {",
                "-        // if lock file has been updated since last time, then leave this lock file alone",
                "-        lastExpiredLock = null;",
                "+            LOG.error(e.getMessage(), e);",
                "+            throw new RuntimeException(\"Unable to instantiate \" + readerType + \" reader\", e);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Creates a reader that starts reading from 'offset'",
                "+     *",
                "+     * @param file the file to read",
                "+     * @param offset the offset string should be understandable by the reader type being used",
                "+     * @return",
                "+     * @throws IOException",
                "+     */",
                "+    private FileReader createFileReader(Path file, String offset)",
                "+        throws IOException {",
                "+        if (readerType.equalsIgnoreCase(Configs.SEQ)) {",
                "+            return new SequenceFileReader(this.hdfs, file, conf, offset);",
                "+        }",
                "+        if (readerType.equalsIgnoreCase(Configs.TEXT)) {",
                "+            return new TextFileReader(this.hdfs, file, conf, offset);",
                "+        }",
                "+",
                "+        try {",
                "+            Class<?> clsType = Class.forName(readerType);",
                "+            Constructor<?> constructor = clsType.getConstructor(FileSystem.class, Path.class, Map.class, String.class);",
                "+            return (FileReader) constructor.newInstance(this.hdfs, file, conf, offset);",
                "+        } catch (Exception e) {",
                "+            LOG.error(e.getMessage(), e);",
                "+            throw new RuntimeException(\"Unable to instantiate \" + readerType, e);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Renames files with .inprogress suffix",
                "+     *",
                "+     * @return path of renamed file",
                "+     * @throws if operation fails",
                "+     */",
                "+    private Path renameToInProgressFile(Path file)",
                "+        throws IOException {",
                "+        Path newFile = new Path(file.toString() + inprogress_suffix);",
                "+        try {",
                "+            if (hdfs.rename(file, newFile)) {",
                "+                return newFile;",
                "+            }",
                "+            throw new RenameException(file, newFile);",
                "+        } catch (IOException e) {",
                "+            throw new RenameException(file, newFile, e);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Returns the corresponding input file in the 'sourceDirPath' for the specified lock file. If no such file is found then returns null",
                "+     */",
                "+    private Path getFileForLockFile(Path lockFile, Path sourceDirPath)",
                "+        throws IOException {",
                "+        String lockFileName = lockFile.getName();",
                "+        Path dataFile = new Path(sourceDirPath + Path.SEPARATOR + lockFileName + inprogress_suffix);",
                "+        if (hdfs.exists(dataFile)) {",
                "+            return dataFile;",
                "+        }",
                "+        dataFile = new Path(sourceDirPath + Path.SEPARATOR + lockFileName);",
                "+        if (hdfs.exists(dataFile)) {",
                "+            return dataFile;",
                "+        }",
                "         return null;",
                "-      }",
                "-    } finally {",
                "-      dirlock.release();",
                "-      LOG.debug(\"Released DirLock {}, SpoutID {} \", dirlock.getLockFile(), spoutId);",
                "-    }",
                "-  }",
                "-",
                "-  private boolean hasExpired(long lastModifyTime) {",
                "-    return (System.currentTimeMillis() - lastModifyTime ) < lockTimeoutSec*1000;",
                "-  }",
                "-",
                "-  /**",
                "-   * Creates a reader that reads from beginning of file",
                "-   * @param file file to read",
                "-   * @return",
                "-   * @throws IOException",
                "-   */",
                "-  private FileReader createFileReader(Path file)",
                "-          throws IOException {",
                "-    if ( readerType.equalsIgnoreCase(Configs.SEQ) ) {",
                "-      return new SequenceFileReader(this.hdfs, file, conf);",
                "-    }",
                "-    if ( readerType.equalsIgnoreCase(Configs.TEXT) ) {",
                "-      return new TextFileReader(this.hdfs, file, conf);",
                "-    }",
                "-    try {",
                "-      Class<?> clsType = Class.forName(readerType);",
                "-      Constructor<?> constructor = clsType.getConstructor(FileSystem.class, Path.class, Map.class);",
                "-      return (FileReader) constructor.newInstance(this.hdfs, file, conf);",
                "-    } catch (Exception e) {",
                "-      LOG.error(e.getMessage(), e);",
                "-      throw new RuntimeException(\"Unable to instantiate \" + readerType + \" reader\", e);",
                "-    }",
                "-  }",
                "-",
                "-",
                "-  /**",
                "-   * Creates a reader that starts reading from 'offset'",
                "-   * @param file the file to read",
                "-   * @param offset the offset string should be understandable by the reader type being used",
                "-   * @return",
                "-   * @throws IOException",
                "-   */",
                "-  private FileReader createFileReader(Path file, String offset)",
                "-          throws IOException {",
                "-    if ( readerType.equalsIgnoreCase(Configs.SEQ) ) {",
                "-      return new SequenceFileReader(this.hdfs, file, conf, offset);",
                "-    }",
                "-    if ( readerType.equalsIgnoreCase(Configs.TEXT) ) {",
                "-      return new TextFileReader(this.hdfs, file, conf, offset);",
                "-    }",
                "-",
                "-    try {",
                "-      Class<?> clsType = Class.forName(readerType);",
                "-      Constructor<?> constructor = clsType.getConstructor(FileSystem.class, Path.class, Map.class, String.class);",
                "-      return (FileReader) constructor.newInstance(this.hdfs, file, conf, offset);",
                "-    } catch (Exception e) {",
                "-      LOG.error(e.getMessage(), e);",
                "-      throw new RuntimeException(\"Unable to instantiate \" + readerType, e);",
                "-    }",
                "-  }",
                "-",
                "-  /**",
                "-   * Renames files with .inprogress suffix",
                "-   * @return path of renamed file",
                "-   * @throws if operation fails",
                "-   */",
                "-  private Path renameToInProgressFile(Path file)",
                "-          throws IOException {",
                "-    Path newFile =  new Path( file.toString() + inprogress_suffix );",
                "-    try {",
                "-      if (hdfs.rename(file, newFile)) {",
                "-        return newFile;",
                "-      }",
                "-      throw new RenameException(file, newFile);",
                "-    } catch (IOException e){",
                "-      throw new RenameException(file, newFile, e);",
                "-    }",
                "-  }",
                "-",
                "-  /** Returns the corresponding input file in the 'sourceDirPath' for the specified lock file.",
                "-   *  If no such file is found then returns null",
                "-   */",
                "-  private Path getFileForLockFile(Path lockFile, Path sourceDirPath)",
                "-          throws IOException {",
                "-    String lockFileName = lockFile.getName();",
                "-    Path dataFile = new Path(sourceDirPath + Path.SEPARATOR + lockFileName + inprogress_suffix);",
                "-    if ( hdfs.exists(dataFile) ) {",
                "-      return dataFile;",
                "-    }",
                "-    dataFile = new Path(sourceDirPath + Path.SEPARATOR +  lockFileName);",
                "-    if ( hdfs.exists(dataFile) ) {",
                "-      return dataFile;",
                "-    }",
                "-    return null;",
                "-  }",
                "-",
                "-",
                "-  // renames files and returns the new file path",
                "-  private Path renameCompletedFile(Path file) throws IOException {",
                "-    String fileName = file.toString();",
                "-    String fileNameMinusSuffix = fileName.substring(0, fileName.indexOf(inprogress_suffix));",
                "-    String newName = new Path(fileNameMinusSuffix).getName();",
                "-",
                "-    Path  newFile = new Path( archiveDirPath + Path.SEPARATOR + newName );",
                "-    LOG.info(\"Completed consuming file {}\", fileNameMinusSuffix);",
                "-    if ( !hdfs.rename(file, newFile) ) {",
                "-      throw new IOException(\"Rename failed for file: \" + file);",
                "-    }",
                "-    LOG.debug(\"Renamed file {} to {} \", file, newFile);",
                "-    return newFile;",
                "-  }",
                "-",
                "-  @Override",
                "-  public void declareOutputFields(OutputFieldsDeclarer declarer) {",
                "-    if (outputStreamName!=null) {",
                "-      declarer.declareStream(outputStreamName, outputFields);",
                "-    } else {",
                "-      declarer.declare(outputFields);",
                "-    }",
                "-  }",
                "-",
                "-  static class MessageId implements  Comparable<MessageId> {",
                "-    public long msgNumber; // tracks order in which msg came in",
                "-    public String fullPath;",
                "-    public FileOffset offset;",
                "-",
                "-    public MessageId(long msgNumber, Path fullPath, FileOffset offset) {",
                "-      this.msgNumber = msgNumber;",
                "-      this.fullPath = fullPath.toString();",
                "-      this.offset = offset;",
                "     }",
                "-    @Override",
                "-    public String toString() {",
                "-      return \"{'\" +  fullPath + \"':\" + offset + \"}\";",
                "+    // renames files and returns the new file path",
                "+    private Path renameCompletedFile(Path file) throws IOException {",
                "+        String fileName = file.toString();",
                "+        String fileNameMinusSuffix = fileName.substring(0, fileName.indexOf(inprogress_suffix));",
                "+        String newName = new Path(fileNameMinusSuffix).getName();",
                "+",
                "+        Path newFile = new Path(archiveDirPath + Path.SEPARATOR + newName);",
                "+        LOG.info(\"Completed consuming file {}\", fileNameMinusSuffix);",
                "+        if (!hdfs.rename(file, newFile)) {",
                "+            throw new IOException(\"Rename failed for file: \" + file);",
                "+        }",
                "+        LOG.debug(\"Renamed file {} to {} \", file, newFile);",
                "+        return newFile;",
                "     }",
                "@@ -818,29 +799,56 @@ public class HdfsSpout extends BaseRichSpout {",
                "     @Override",
                "-    public int compareTo(MessageId rhs) {",
                "-      if ( msgNumber<rhs.msgNumber ) {",
                "-        return -1;",
                "-      }",
                "-      if ( msgNumber>rhs.msgNumber ) {",
                "-        return 1;",
                "-      }",
                "-      return 0;",
                "+    public void declareOutputFields(OutputFieldsDeclarer declarer) {",
                "+        if (outputStreamName != null) {",
                "+            declarer.declareStream(outputStreamName, outputFields);",
                "+        } else {",
                "+            declarer.declare(outputFields);",
                "+        }",
                "     }",
                "-  }",
                "-  private static class RenameException extends IOException {",
                "-    public final Path oldFile;",
                "-    public final Path newFile;",
                "+    static class MessageId implements Comparable<MessageId> {",
                "+",
                "+        public long msgNumber; // tracks order in which msg came in",
                "+        public String fullPath;",
                "+        public FileOffset offset;",
                "-    public RenameException(Path oldFile, Path newFile) {",
                "-      super(\"Rename of \" + oldFile + \" to \" + newFile + \" failed\");",
                "-      this.oldFile = oldFile;",
                "-      this.newFile = newFile;",
                "+        public MessageId(long msgNumber, Path fullPath, FileOffset offset) {",
                "+            this.msgNumber = msgNumber;",
                "+            this.fullPath = fullPath.toString();",
                "+            this.offset = offset;",
                "+        }",
                "+",
                "+        @Override",
                "+        public String toString() {",
                "+            return \"{'\" + fullPath + \"':\" + offset + \"}\";",
                "+        }",
                "+",
                "+        @Override",
                "+        public int compareTo(MessageId rhs) {",
                "+            if (msgNumber < rhs.msgNumber) {",
                "+                return -1;",
                "+            }",
                "+            if (msgNumber > rhs.msgNumber) {",
                "+                return 1;",
                "+            }",
                "+            return 0;",
                "+        }",
                "     }",
                "-    public RenameException(Path oldFile, Path newFile, IOException cause) {",
                "-      super(\"Rename of \" + oldFile + \" to \" + newFile + \" failed\", cause);",
                "-      this.oldFile = oldFile;",
                "-      this.newFile = newFile;",
                "+    private static class RenameException extends IOException {",
                "+",
                "+        public final Path oldFile;",
                "+        public final Path newFile;",
                "+",
                "+        public RenameException(Path oldFile, Path newFile) {",
                "+            super(\"Rename of \" + oldFile + \" to \" + newFile + \" failed\");",
                "+            this.oldFile = oldFile;",
                "+            this.newFile = newFile;",
                "+        }",
                "+",
                "+        public RenameException(Path oldFile, Path newFile, IOException cause) {",
                "+            super(\"Rename of \" + oldFile + \" to \" + newFile + \" failed\", cause);",
                "+            this.oldFile = oldFile;",
                "+            this.newFile = newFile;",
                "+        }",
                "     }",
                "-  }",
                " }"
            ],
            "changed_files": [
                "external/storm-hdfs/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreImpl.java",
                "external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java",
                "external/storm-hdfs/src/main/java/org/apache/storm/hdfs/spout/HdfsSpout.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2810": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "83adfd9d2c5c352efeb076303cacab38f9f5c01c"
                ],
                [
                    "no-tag",
                    "caca8292772650c8ce041d569b624b4813a13b46"
                ]
            ],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2810",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7426c9874e9afb971a32da77f4e05c65c9078bd3",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513781489,
            "hunks": 19,
            "message": "STORM-2864: Minor optimisation about trident kafka state Make TridentKafkaState a template class to eliminate warning messages in eclipse, and a minor optimization that use StringBuilder.append instead of string concat operation.",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaState.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaState.java",
                "index 44b6d5f52..2e2c13b7c 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaState.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaState.java",
                "@@ -39,12 +39,11 @@ import java.util.concurrent.Future;",
                "-public class TridentKafkaState implements State {",
                "+public class TridentKafkaState<K, V> implements State {",
                "     private static final Logger LOG = LoggerFactory.getLogger(TridentKafkaState.class);",
                "-    private KafkaProducer producer;",
                "-    private OutputCollector collector;",
                "+    private KafkaProducer<K, V> producer;",
                "-    private TridentTupleToKafkaMapper mapper;",
                "+    private TridentTupleToKafkaMapper<K, V> mapper;",
                "     private KafkaTopicSelector topicSelector;",
                "-    public TridentKafkaState withTridentTupleToKafkaMapper(TridentTupleToKafkaMapper mapper) {",
                "+    public TridentKafkaState<K, V> withTridentTupleToKafkaMapper(TridentTupleToKafkaMapper<K, V> mapper) {",
                "         this.mapper = mapper;",
                "@@ -53,3 +52,3 @@ public class TridentKafkaState implements State {",
                "-    public TridentKafkaState withKafkaTopicSelector(KafkaTopicSelector selector) {",
                "+    public TridentKafkaState<K, V> withKafkaTopicSelector(KafkaTopicSelector selector) {",
                "         this.topicSelector = selector;",
                "@@ -75,3 +74,3 @@ public class TridentKafkaState implements State {",
                "         Objects.requireNonNull(topicSelector, \"topicSelector can not be null\");",
                "-        producer = new KafkaProducer(options);",
                "+        producer = new KafkaProducer<>(options);",
                "     }",
                "@@ -87,22 +86,22 @@ public class TridentKafkaState implements State {",
                "             long startTime = System.currentTimeMillis();",
                "-\t     int numberOfRecords = tuples.size();",
                "-\t     List<Future<RecordMetadata>> futures = new ArrayList<>(numberOfRecords);",
                "+            int numberOfRecords = tuples.size();",
                "+            List<Future<RecordMetadata>> futures = new ArrayList<>(numberOfRecords);",
                "             for (TridentTuple tuple : tuples) {",
                "                 topic = topicSelector.getTopic(tuple);",
                "-                Object messageFromTuple = mapper.getMessageFromTuple(tuple);",
                "-\t\t Object keyFromTuple = mapper.getKeyFromTuple(tuple);",
                "-\t\t\t\t",
                "+                V messageFromTuple = mapper.getMessageFromTuple(tuple);",
                "+                K keyFromTuple = mapper.getKeyFromTuple(tuple);",
                "+",
                "                 if (topic != null) {",
                "-                   if (messageFromTuple != null) {",
                "-\t\t      Future<RecordMetadata> result = producer.send(new ProducerRecord(topic,keyFromTuple, messageFromTuple));",
                "-\t\t      futures.add(result);",
                "-\t\t   } else {",
                "-\t\t      LOG.warn(\"skipping Message with Key \"+ keyFromTuple +\" as message was null\");",
                "-\t\t   }",
                "-\t\t\t",
                "+                    if (messageFromTuple != null) {",
                "+                        Future<RecordMetadata> result = producer.send(new ProducerRecord<>(topic, keyFromTuple, messageFromTuple));",
                "+                        futures.add(result);",
                "+                    } else {",
                "+                        LOG.warn(\"skipping Message with Key {} as message was null\", keyFromTuple);",
                "+                    }",
                "+",
                "                 } else {",
                "-                      LOG.warn(\"skipping key = \" + keyFromTuple + \", topic selector returned null.\");",
                "+                    LOG.warn(\"skipping key = {}, topic selector returned null.\", keyFromTuple);",
                "                 }",
                "             }",
                "-            ",
                "+",
                "             int emittedRecords = futures.size();",
                "@@ -117,16 +116,17 @@ public class TridentKafkaState implements State {",
                "-            if (exceptions.size() > 0){",
                "-\t\tStringBuilder errorMsg = new StringBuilder(\"Could not retrieve result for messages \" + tuples + \" from topic = \" + topic ",
                "-\t\t\t\t+ \" because of the following exceptions:\" + System.lineSeparator());",
                "-\t\t\t\t",
                "-\t\tfor (ExecutionException exception : exceptions) {",
                "-\t\t\terrorMsg = errorMsg.append(exception.getMessage()).append(System.lineSeparator()); ;",
                "-\t\t}",
                "-\t\tString message = errorMsg.toString();",
                "-\t\tLOG.error(message);",
                "-\t\tthrow new FailedException(message);",
                "-\t    }",
                "-\t    long latestTime = System.currentTimeMillis();",
                "-\t    LOG.info(\"Emitted record {} sucessfully in {} ms to topic {} \", emittedRecords, latestTime-startTime, topic);",
                "-\t\t\t",
                "+            if (exceptions.size() > 0) {",
                "+                StringBuilder errorMsg = new StringBuilder(\"Could not retrieve result for messages \");",
                "+                errorMsg.append(tuples).append(\" from topic = \").append(topic)",
                "+                        .append(\" because of the following exceptions:\").append(System.lineSeparator());",
                "+",
                "+                for (ExecutionException exception : exceptions) {",
                "+                    errorMsg = errorMsg.append(exception.getMessage()).append(System.lineSeparator());",
                "+                }",
                "+                String message = errorMsg.toString();",
                "+                LOG.error(message);",
                "+                throw new FailedException(message);",
                "+            }",
                "+            long latestTime = System.currentTimeMillis();",
                "+            LOG.info(\"Emitted record {} sucessfully in {} ms to topic {} \", emittedRecords, latestTime - startTime, topic);",
                "+",
                "         } catch (Exception ex) {",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaStateFactory.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaStateFactory.java",
                "index f56451077..35620defb 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaStateFactory.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaStateFactory.java",
                "@@ -30,7 +30,8 @@ import java.util.Properties;",
                "-public class TridentKafkaStateFactory implements StateFactory {",
                "+public class TridentKafkaStateFactory<K, V> implements StateFactory {",
                "+    private static final long serialVersionUID = -3613240970062343385L;",
                "     private static final Logger LOG = LoggerFactory.getLogger(TridentKafkaStateFactory.class);",
                "-    private TridentTupleToKafkaMapper mapper;",
                "+    private TridentTupleToKafkaMapper<K, V> mapper;",
                "     private KafkaTopicSelector topicSelector;",
                "@@ -38,3 +39,3 @@ public class TridentKafkaStateFactory implements StateFactory {",
                "-    public TridentKafkaStateFactory withTridentTupleToKafkaMapper(TridentTupleToKafkaMapper mapper) {",
                "+    public TridentKafkaStateFactory<K, V> withTridentTupleToKafkaMapper(TridentTupleToKafkaMapper<K, V> mapper) {",
                "         this.mapper = mapper;",
                "@@ -43,3 +44,3 @@ public class TridentKafkaStateFactory implements StateFactory {",
                "-    public TridentKafkaStateFactory withKafkaTopicSelector(KafkaTopicSelector selector) {",
                "+    public TridentKafkaStateFactory<K, V> withKafkaTopicSelector(KafkaTopicSelector selector) {",
                "         this.topicSelector = selector;",
                "@@ -48,3 +49,3 @@ public class TridentKafkaStateFactory implements StateFactory {",
                "-    public TridentKafkaStateFactory withProducerProperties(Properties props) {",
                "+    public TridentKafkaStateFactory<K, V> withProducerProperties(Properties props) {",
                "         this.producerProperties = props;",
                "@@ -56,5 +57,5 @@ public class TridentKafkaStateFactory implements StateFactory {",
                "         LOG.info(\"makeState(partitonIndex={}, numpartitions={}\", partitionIndex, numPartitions);",
                "-        TridentKafkaState state = new TridentKafkaState()",
                "-                .withKafkaTopicSelector(this.topicSelector)",
                "-                .withTridentTupleToKafkaMapper(this.mapper);",
                "+        TridentKafkaState<K, V> state = new TridentKafkaState<>();",
                "+        state.withKafkaTopicSelector(this.topicSelector)",
                "+            .withTridentTupleToKafkaMapper(this.mapper);",
                "         state.prepare(producerProperties);",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaStateUpdater.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaStateUpdater.java",
                "index 89535c64e..19e3d332e 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaStateUpdater.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaStateUpdater.java",
                "@@ -25,6 +25,8 @@ import org.apache.storm.trident.tuple.TridentTuple;",
                "-public class TridentKafkaStateUpdater extends BaseStateUpdater<TridentKafkaState> {",
                "+public class TridentKafkaStateUpdater<K, V> extends BaseStateUpdater<TridentKafkaState<K, V>> {",
                "+",
                "+    private static final long serialVersionUID = 3352659585225274402L;",
                "     @Override",
                "-    public void updateState(TridentKafkaState state, List<TridentTuple> tuples, TridentCollector collector) {",
                "+    public void updateState(TridentKafkaState<K, V> state, List<TridentTuple> tuples, TridentCollector collector) {",
                "         state.updateState(tuples, collector);"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaState.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaStateFactory.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/trident/TridentKafkaStateUpdater.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2864": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "74a718cdf9ba5d8f159ea80ea706012c78aab070"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2864",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "44c1e0031a270a367150bc76d0a1077fa5d3e1d1",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513269140,
            "hunks": 1,
            "message": "STORM-2855: Revert to 2017Q4 Ubuntu image in Travis to fix build",
            "diff": [
                "diff --git a/.travis.yml b/.travis.yml",
                "index 128b5e045..89df2f89e 100644",
                "--- a/.travis.yml",
                "+++ b/.travis.yml",
                "@@ -24,2 +24,3 @@ dist: trusty",
                " sudo: required",
                "+group: deprecated-2017Q4"
            ],
            "changed_files": [
                ".travis.yml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2855": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "090c26088d146a646ac33a476388f9171f0ffae1"
                ],
                [
                    "no-tag",
                    "9a935b023eee3de660f043bae7447b2f77ec8d52"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2855",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8a3619106c7f03d574740688ce93b8bd66e3fabb",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511802883,
            "hunks": 3,
            "message": "STORM-2833: use the same host name for removal too",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/messaging/netty/Client.java b/storm-core/src/jvm/org/apache/storm/messaging/netty/Client.java",
                "index 23aa203d3..bf77f61a1 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/messaging/netty/Client.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/messaging/netty/Client.java",
                "@@ -83,2 +83,6 @@ public class Client extends ConnectionWithStatus implements IStatefulObject, ISa",
                "     protected final String dstAddressPrefixedName;",
                "+    //The actual name of the host we are trying to connect to so that",
                "+    // when we remove ourselves from the connection cache there is no concern that",
                "+    // the resolved host name is different.",
                "+    private final String host;",
                "     private volatile Map<Integer, Double> serverLoad = null;",
                "@@ -158,2 +162,3 @@ public class Client extends ConnectionWithStatus implements IStatefulObject, ISa",
                "         bootstrap = createClientBootstrap(factory, bufferSize, stormConf);",
                "+        this.host = host;",
                "         dstAddress = new InetSocketAddress(host, port);",
                "@@ -422,3 +427,3 @@ public class Client extends ConnectionWithStatus implements IStatefulObject, ISa",
                "             LOG.info(\"closing Netty Client {}\", dstAddressPrefixedName);",
                "-            context.removeClient(dstAddress.getHostName(),dstAddress.getPort());",
                "+            context.removeClient(host, dstAddress.getPort());",
                "             // Set closing to true to prevent any further reconnection attempts."
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/messaging/netty/Client.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2833": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "63e74b8895889fadf0822776a802f012bbf5851f"
                ]
            ],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2833",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "969c232618725ad254c4d28bcf91d6b0805f189f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512806433,
            "hunks": 28,
            "message": "STORM-2847: Ensure spout can handle being activated and deactivated",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 030735bb0..373630eee 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -83,4 +83,2 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private transient Timer commitTimer;",
                "-    // Flag indicating that the spout is still undergoing initialization process.",
                "-    private transient boolean initialized;",
                "     // Initialization is only complete after the first call to  KafkaSpoutConsumerRebalanceListener.onPartitionsAssigned()",
                "@@ -111,3 +109,2 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {",
                "-        initialized = false;",
                "         this.context = context;",
                "@@ -119,3 +116,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         firstPollOffsetStrategy = kafkaSpoutConfig.getFirstPollOffsetStrategy();",
                "-        ",
                "+",
                "         // Retries management",
                "@@ -148,6 +145,5 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         private Collection<TopicPartition> previousAssignment = new HashSet<>();",
                "-        ",
                "+",
                "         @Override",
                "         public void onPartitionsRevoked(Collection<TopicPartition> partitions) {",
                "-            initialized = false;",
                "             previousAssignment = partitions;",
                "@@ -158,3 +154,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             if (isAtLeastOnceProcessing()) {",
                "-                commitOffsetsForAckedTuples();",
                "+                commitOffsetsForAckedTuples(new HashSet<>(partitions));",
                "             }",
                "@@ -189,2 +185,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 final long fetchOffset = doSeek(tp, committedOffset);",
                "+                LOG.debug(\"Set consumer position to [{}] for topic-partition [{}], based on strategy [{}] and committed offset [{}]\",",
                "+                    fetchOffset, tp, firstPollOffsetStrategy, committedOffset);",
                "                 // If this partition was previously assigned to this spout, leave the acked offsets as they were to resume where it left off",
                "@@ -194,3 +192,2 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             }",
                "-            initialized = true;",
                "             LOG.info(\"Initialization complete\");",
                "@@ -226,25 +223,20 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         try {",
                "-            if (initialized) {             ",
                "-             ",
                "-                if (refreshSubscriptionTimer.isExpiredResetOnTrue()) {",
                "-                    kafkaSpoutConfig.getSubscription().refreshAssignment();",
                "-                }",
                "+            if (refreshSubscriptionTimer.isExpiredResetOnTrue()) {",
                "+                kafkaSpoutConfig.getSubscription().refreshAssignment();",
                "+            }",
                "-                if (shouldCommit()) {",
                "-                    commitOffsetsForAckedTuples();",
                "-                }",
                "+            if (shouldCommit()) {",
                "+                commitOffsetsForAckedTuples(kafkaConsumer.assignment());",
                "+            }",
                "-                PollablePartitionsInfo pollablePartitionsInfo = getPollablePartitionsInfo();",
                "-                if (pollablePartitionsInfo.shouldPoll()) {",
                "-                    try {",
                "-                        setWaitingToEmit(pollKafkaBroker(pollablePartitionsInfo));",
                "-                    } catch (RetriableException e) {",
                "-                        LOG.error(\"Failed to poll from kafka.\", e);",
                "-                    }",
                "+            PollablePartitionsInfo pollablePartitionsInfo = getPollablePartitionsInfo();",
                "+            if (pollablePartitionsInfo.shouldPoll()) {",
                "+                try {",
                "+                    setWaitingToEmit(pollKafkaBroker(pollablePartitionsInfo));",
                "+                } catch (RetriableException e) {",
                "+                    LOG.error(\"Failed to poll from kafka.\", e);",
                "                 }",
                "-",
                "-                emitIfWaitingNotEmitted();",
                "-            } else {",
                "-                LOG.debug(\"Spout not initialized. Not sending tuples until initialization completes\");",
                "             }",
                "+",
                "+            emitIfWaitingNotEmitted();",
                "         } catch (InterruptException e) {",
                "@@ -263,3 +255,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     }",
                "-    ",
                "+",
                "     private PollablePartitionsInfo getPollablePartitionsInfo() {",
                "@@ -269,3 +261,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         }",
                "-        ",
                "+",
                "         Set<TopicPartition> assignment = kafkaConsumer.assignment();",
                "@@ -274,3 +266,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         }",
                "-        ",
                "+",
                "         Map<TopicPartition, Long> earliestRetriableOffsets = retryService.earliestRetriableOffsets();",
                "@@ -369,5 +361,5 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         while (isWaitingToEmit()) {",
                "-            final boolean emitted = emitOrRetryTuple(waitingToEmit.next());",
                "+            final boolean emittedTuple = emitOrRetryTuple(waitingToEmit.next());",
                "             waitingToEmit.remove();",
                "-            if (emitted) {",
                "+            if (emittedTuple) {",
                "                 break;",
                "@@ -392,3 +384,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         } else {",
                "-            if (kafkaConsumer.committed(tp) != null && (kafkaConsumer.committed(tp).offset() >= kafkaConsumer.position(tp))) {",
                "+            if (kafkaConsumer.committed(tp) != null && (kafkaConsumer.committed(tp).offset() > kafkaConsumer.position(tp))) {",
                "                 throw new IllegalStateException(\"Attempting to emit a message that has already been committed.\");",
                "@@ -439,6 +431,10 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    private void commitOffsetsForAckedTuples() {",
                "-        // Find offsets that are ready to be committed for every topic partition",
                "+    private void commitOffsetsForAckedTuples(Set<TopicPartition> assignedPartitions) {",
                "+        // Find offsets that are ready to be committed for every assigned topic partition",
                "+        final Map<TopicPartition, OffsetManager> assignedOffsetManagers = offsetManagers.entrySet().stream()",
                "+            .filter(entry -> assignedPartitions.contains(entry.getKey()))",
                "+            .collect(Collectors.toMap(Entry::getKey, Entry::getValue));",
                "+",
                "         final Map<TopicPartition, OffsetAndMetadata> nextCommitOffsets = new HashMap<>();",
                "-        for (Map.Entry<TopicPartition, OffsetManager> tpOffset : offsetManagers.entrySet()) {",
                "+        for (Map.Entry<TopicPartition, OffsetManager> tpOffset : assignedOffsetManagers.entrySet()) {",
                "             final OffsetAndMetadata nextCommitOffset = tpOffset.getValue().findNextCommitOffset();",
                "@@ -473,5 +469,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 }",
                "-                ",
                "-                ",
                "-                final OffsetManager offsetManager = offsetManagers.get(tp);",
                "+",
                "+                final OffsetManager offsetManager = assignedOffsetManagers.get(tp);",
                "                 offsetManager.commit(tpOffset.getValue());",
                "@@ -579,3 +574,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             if (isAtLeastOnceProcessing()) {",
                "-                commitOffsetsForAckedTuples();",
                "+                commitOffsetsForAckedTuples(kafkaConsumer.assignment());",
                "             }",
                "@@ -622,4 +617,5 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     }",
                "-    ",
                "+",
                "     private static class PollablePartitionsInfo {",
                "+",
                "         private final Set<TopicPartition> pollablePartitions;",
                "@@ -627,3 +623,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         private final Map<TopicPartition, Long> pollableEarliestRetriableOffsets;",
                "-        ",
                "+",
                "         public PollablePartitionsInfo(Set<TopicPartition> pollablePartitions, Map<TopicPartition, Long> earliestRetriableOffsets) {",
                "@@ -634,3 +630,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         }",
                "-        ",
                "+",
                "         public boolean shouldPoll() {",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/ManualPartitionSubscription.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/ManualPartitionSubscription.java",
                "index ebfd30ca5..8e74abbd3 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/ManualPartitionSubscription.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/ManualPartitionSubscription.java",
                "@@ -34,3 +34,2 @@ public class ManualPartitionSubscription extends Subscription {",
                "     private final TopicFilter partitionFilter;",
                "-    private transient Set<TopicPartition> currentAssignment = null;",
                "     private transient KafkaConsumer<?, ?> consumer = null;",
                "@@ -57,7 +56,5 @@ public class ManualPartitionSubscription extends Subscription {",
                "         Set<TopicPartition> newAssignment = new HashSet<>(partitioner.partition(allPartitions, context));",
                "+        Set<TopicPartition> currentAssignment = consumer.assignment();",
                "         if (!newAssignment.equals(currentAssignment)) {",
                "-            if (currentAssignment != null) {",
                "-                listener.onPartitionsRevoked(currentAssignment);",
                "-            }",
                "-            currentAssignment = newAssignment;",
                "+            listener.onPartitionsRevoked(currentAssignment);",
                "             consumer.assign(newAssignment);"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/subscription/ManualPartitionSubscription.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2847": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "45ec11bbb5f4b1e38db9beb03219e7a67c700bd8"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2847",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f57a498486e852520e2cda80dd694542a37c2479",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516861501,
            "hunks": 1,
            "message": "STORM-2912 Revert optimization of sharing tick tuple * since it incurs side effect and messes metrics",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index a630dab90..e0c048f52 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -358,9 +358,11 @@",
                "         (log-message \"Timeouts disabled for executor \" comp-id \":\" (:executor-id executor-data))",
                "-        (let [val [(AddressedTuple. AddressedTuple/BROADCAST_DEST (TupleImpl. context [tick-time-secs] Constants/SYSTEM_TASK_ID Constants/SYSTEM_TICK_STREAM_ID))]]",
                "-          (schedule-recurring",
                "-            (:user-timer worker)",
                "-            tick-time-secs",
                "-            tick-time-secs",
                "-            (fn []",
                "-                (disruptor/publish receive-queue val))))))))",
                "+        (schedule-recurring",
                "+          (:user-timer worker)",
                "+          tick-time-secs",
                "+          tick-time-secs",
                "+          (fn []",
                "+            ;; We should create a new tick tuple for each recurrence instead of sharing object",
                "+            ;; More detail on https://issues.apache.org/jira/browse/STORM-2912",
                "+            (let [val [(AddressedTuple. AddressedTuple/BROADCAST_DEST (TupleImpl. context [tick-time-secs] Constants/SYSTEM_TASK_ID Constants/SYSTEM_TICK_STREAM_ID))]]",
                "+              (disruptor/publish receive-queue val))))))))"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2912": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "923dcc56097d050d8a9ace86b7842a6e0edd4da0"
                ],
                [
                    "no-tag",
                    "1b918e5764c67dfc6587c2fc64aae5d792f79a73"
                ],
                [
                    "no-tag",
                    "e07e9894520989ef185a81cc21eec8c59d5f7626"
                ]
            ],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2912",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8fe49599c3bd3b21cbc1a83564dd9d91fd7aeb3b",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517447328,
            "hunks": 6,
            "message": "STORM-2853 Initialize tick tuple after initializing spouts/bolts * this prevents newly-initializing executor in deactivated topology to show high CPU usage * this is based on the fact that all the tasks in executor are for same component",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index 2473e1ab8..5f768c658 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -347,3 +347,3 @@",
                "-(defn setup-ticks! [worker executor-data]",
                "+(defn setup-ticks! [executor-data]",
                "   (let [storm-conf (:storm-conf executor-data)",
                "@@ -359,3 +359,3 @@",
                "         (schedule-recurring",
                "-          (:user-timer worker)",
                "+          (:user-timer (:worker executor-data))",
                "           tick-time-secs",
                "@@ -387,3 +387,3 @@",
                "-        ;; starting the batch-transfer->worker ensures that anything publishing to that queue ",
                "+        ;; starting the batch-transfer->worker ensures that anything publishing to that queue",
                "         ;; doesn't block (because it's a single threaded queue and the caching/consumer started",
                "@@ -393,4 +393,3 @@",
                "                    (mk-threads executor-data task-datas initial-credentials))",
                "-        threads (concat handlers system-threads)]    ",
                "-    (setup-ticks! worker executor-data)",
                "+        threads (concat handlers system-threads)]",
                "@@ -624,2 +623,3 @@",
                "         (log-message \"Opened spout \" component-id \":\" (keys task-datas))",
                "+        (setup-ticks! executor-data)",
                "         (setup-metrics! executor-data)",
                "@@ -847,2 +847,3 @@",
                "         (log-message \"Prepared bolt \" component-id \":\" (keys task-datas))",
                "+        (setup-ticks! executor-data)",
                "         (setup-metrics! executor-data)"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2853": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "a5540d79fa8b3b6f625308972fe922786680b833"
                ],
                [
                    "no-tag",
                    "4841475a7e642e8148d67b320941a6bdd8e4a230"
                ],
                [
                    "no-tag",
                    "91f1522820f3ac1c53594e943dfa8f3a2cde1a8f"
                ]
            ],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2853",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d9fe768a9f7cb47bb2b75a26e81676fc69b1fde0",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1504389033,
            "hunks": 9,
            "message": "STORM-2546: Fix storm-kafka-client spout getting stuck when retriable offsets were deleted from the Kafka log due to topic compaction",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 51d81c7c6..4cda53cb7 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -93,3 +93,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     // Records that have been polled and are queued to be emitted in the nextTuple() call. One record is emitted per nextTuple()",
                "-    private transient Iterator<ConsumerRecord<K, V>> waitingToEmit;                         ",
                "+    private transient Iterator<ConsumerRecord<K, V>> waitingToEmit;",
                "     // Triggers when a subscription should be refreshed",
                "@@ -154,3 +154,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             LOG.info(\"Partitions revoked. [consumer-group={}, consumer={}, topic-partitions={}]\",",
                "-                    kafkaSpoutConfig.getConsumerGroupId(), kafkaConsumer, partitions);",
                "+                kafkaSpoutConfig.getConsumerGroupId(), kafkaConsumer, partitions);",
                "@@ -273,3 +273,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         final int maxUncommittedOffsets = kafkaSpoutConfig.getMaxUncommittedOffsets();",
                "-        ",
                "+",
                "         if (waitingToEmit()) {",
                "@@ -319,3 +319,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private ConsumerRecords<K, V> pollKafkaBroker(Set<TopicPartition> pollablePartitions) {",
                "-        doSeekRetriableTopicPartitions(pollablePartitions);",
                "+        final Map<TopicPartition, Long> retriableOffsets = doSeekRetriableTopicPartitions(pollablePartitions);",
                "         Set<TopicPartition> pausedPartitions = new HashSet<>(kafkaConsumer.assignment());",
                "@@ -330,2 +330,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             final ConsumerRecords<K, V> consumerRecords = kafkaConsumer.poll(kafkaSpoutConfig.getPollTimeoutMs());",
                "+            ackRetriableOffsetsIfCompactedAway(retriableOffsets, consumerRecords);",
                "             final int numPolledRecords = consumerRecords.count();",
                "@@ -343,9 +344,38 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    private void doSeekRetriableTopicPartitions(Set<TopicPartition> pollablePartitions) {",
                "+    private Map<TopicPartition, Long> doSeekRetriableTopicPartitions(Set<TopicPartition> pollablePartitions) {",
                "         final Map<TopicPartition, Long> retriableTopicPartitions = retryService.earliestRetriableOffsets();",
                "-",
                "+        for (TopicPartition tp : retriableTopicPartitions.keySet()) {",
                "+            if (!pollablePartitions.contains(tp)) {",
                "+                retriableTopicPartitions.remove(tp);",
                "+            }",
                "+        }",
                "         for (Entry<TopicPartition, Long> retriableTopicPartitionAndOffset : retriableTopicPartitions.entrySet()) {",
                "-            if (pollablePartitions.contains(retriableTopicPartitionAndOffset.getKey())) {",
                "-                //Seek directly to the earliest retriable message for each retriable topic partition",
                "-                kafkaConsumer.seek(retriableTopicPartitionAndOffset.getKey(), retriableTopicPartitionAndOffset.getValue());",
                "+            //Seek directly to the earliest retriable message for each retriable topic partition",
                "+            kafkaConsumer.seek(retriableTopicPartitionAndOffset.getKey(), retriableTopicPartitionAndOffset.getValue());",
                "+        }",
                "+        return retriableTopicPartitions;",
                "+    }",
                "+",
                "+    private void ackRetriableOffsetsIfCompactedAway(Map<TopicPartition, Long> earliestRetriableOffsets,",
                "+        ConsumerRecords<K, V> consumerRecords) {",
                "+        for (Entry<TopicPartition, Long> entry : earliestRetriableOffsets.entrySet()) {",
                "+            TopicPartition tp = entry.getKey();",
                "+            List<ConsumerRecord<K, V>> records = consumerRecords.records(tp);",
                "+            if (!records.isEmpty()) {",
                "+                ConsumerRecord<K, V> record = records.get(0);",
                "+                long seekOffset = entry.getValue();",
                "+                long earliestReceivedOffset = record.offset();",
                "+                if (seekOffset < earliestReceivedOffset) {",
                "+                    //Since we asked for tuples starting at seekOffset, some retriable records must have been compacted away.",
                "+                    //Ack up to the first offset received if the record is not already acked or currently in the topology",
                "+                    for (long i = seekOffset; i < earliestReceivedOffset; i++) {",
                "+                        KafkaSpoutMessageId msgId = retryService.getMessageId(new ConsumerRecord<>(tp.topic(), tp.partition(), i, null, null));",
                "+                        if (!offsetManagers.get(tp).contains(msgId) && !emitted.contains(msgId)) {",
                "+                            LOG.debug(\"Record at offset [{}] appears to have been compacted away from topic [{}], marking as acked\", i, tp);",
                "+                            retryService.remove(msgId);",
                "+                            emitted.add(msgId);",
                "+                            ack(msgId);",
                "+                        }",
                "+                    }",
                "+                }",
                "             }",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "index 8ff21b67c..58d4753b3 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "@@ -730,2 +730,23 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         } else {",
                "+            String autoOffsetResetPolicy = (String)builder.kafkaProps.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);",
                "+            if (builder.processingGuarantee == ProcessingGuarantee.AT_LEAST_ONCE) {",
                "+                if (autoOffsetResetPolicy == null) {",
                "+                    /*",
                "+                    If the user wants to explicitly set an auto offset reset policy, we should respect it, but when the spout is configured",
                "+                    for at-least-once processing we should default to seeking to the earliest offset in case there's an offset out of range",
                "+                    error, rather than seeking to the latest (Kafka's default). This type of error will typically happen when the consumer ",
                "+                    requests an offset that was deleted.",
                "+                     */",
                "+                    builder.kafkaProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");",
                "+                } else if (!autoOffsetResetPolicy.equals(\"earliest\") && !autoOffsetResetPolicy.equals(\"none\")) {",
                "+                    LOG.warn(\"Cannot guarantee at-least-once processing with auto.offset.reset.policy other than 'earliest' or 'none'.\"",
                "+                        + \" Some messages may be skipped.\");",
                "+                }",
                "+            } else if (builder.processingGuarantee == ProcessingGuarantee.AT_MOST_ONCE) {",
                "+                if (autoOffsetResetPolicy != null",
                "+                    && (!autoOffsetResetPolicy.equals(\"latest\") && !autoOffsetResetPolicy.equals(\"none\"))) {",
                "+                    LOG.warn(\"Cannot guarantee at-most-once processing with auto.offset.reset.policy other than 'latest' or 'none'.\"",
                "+                        + \" Some messages may be processed more than once.\");",
                "+                }",
                "+            }",
                "             builder.kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false\");"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2546": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "88583feabf6e39c6bc7c4e657c989ca5ba2671d2"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2546",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f1d3a875a9dd3e4c5b7d837bc1df10f0da022833",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514797995,
            "hunks": 19,
            "message": "STORM-2860: Add Kerberos support to Solr bolt",
            "diff": [
                "diff --git a/examples/storm-solr-examples/pom.xml b/examples/storm-solr-examples/pom.xml",
                "index b4864c77c..4f44fffe1 100644",
                "--- a/examples/storm-solr-examples/pom.xml",
                "+++ b/examples/storm-solr-examples/pom.xml",
                "@@ -40,2 +40,23 @@",
                "             <version>${project.version}</version>",
                "+            <exclusions>",
                "+                <exclusion>",
                "+                    <groupId>org.apache.httpcomponents</groupId>",
                "+                    <artifactId>httpclient</artifactId>",
                "+                </exclusion>",
                "+            </exclusions>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.apache.httpcomponents</groupId>",
                "+            <artifactId>httpclient</artifactId>",
                "+            <version>4.5</version>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>commons-lang</groupId>",
                "+            <artifactId>commons-lang</artifactId>",
                "+            <version>2.6</version>",
                "+         </dependency>",
                "+        <dependency>",
                "+            <groupId>commons-logging</groupId>",
                "+            <artifactId>commons-logging</artifactId>",
                "+            <version>1.1.3</version>",
                "         </dependency>",
                "diff --git a/external/storm-solr/pom.xml b/external/storm-solr/pom.xml",
                "index 684cbc580..6d6cf249a 100644",
                "--- a/external/storm-solr/pom.xml",
                "+++ b/external/storm-solr/pom.xml",
                "@@ -37,2 +37,6 @@",
                "+    <properties>",
                "+        <solr.version>5.5.5</solr.version>",
                "+    </properties>",
                "+",
                "     <dependencies>",
                "@@ -48,4 +52,14 @@",
                "             <artifactId>solr-solrj</artifactId>",
                "-            <version>5.2.1</version>",
                "-            <scope>compile</scope>",
                "+            <version>${solr.version}</version>",
                "+            <exclusions>",
                "+                <exclusion>",
                "+                    <groupId>org.apache.httpcomponents</groupId>",
                "+                    <artifactId>httpclient</artifactId>",
                "+                </exclusion>",
                "+            </exclusions>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.apache.httpcomponents</groupId>",
                "+            <artifactId>httpclient</artifactId>",
                "+            <version>4.5</version>",
                "         </dependency>",
                "@@ -56,7 +70,2 @@",
                "         </dependency>",
                "-        <dependency>",
                "-            <groupId>commons-httpclient</groupId>",
                "-            <artifactId>commons-httpclient</artifactId>",
                "-            <version>3.1</version>",
                "-        </dependency>",
                "         <dependency>",
                "@@ -69,3 +78,3 @@",
                "             <artifactId>solr-core</artifactId>",
                "-            <version>5.2.1</version>",
                "+            <version>${solr.version}</version>",
                "             <scope>test</scope>",
                "@@ -75,3 +84,3 @@",
                "             <artifactId>solr-test-framework</artifactId>",
                "-            <version>5.2.1</version>",
                "+            <version>${solr.version}</version>",
                "             <scope>test</scope>",
                "diff --git a/external/storm-solr/src/main/java/org/apache/storm/solr/bolt/SolrUpdateBolt.java b/external/storm-solr/src/main/java/org/apache/storm/solr/bolt/SolrUpdateBolt.java",
                "index 4feaaf5e1..9eb4ade60 100644",
                "--- a/external/storm-solr/src/main/java/org/apache/storm/solr/bolt/SolrUpdateBolt.java",
                "+++ b/external/storm-solr/src/main/java/org/apache/storm/solr/bolt/SolrUpdateBolt.java",
                "@@ -20,9 +20,2 @@ package org.apache.storm.solr.bolt;",
                "-import org.apache.storm.task.OutputCollector;",
                "-import org.apache.storm.task.TopologyContext;",
                "-import org.apache.storm.topology.OutputFieldsDeclarer;",
                "-import org.apache.storm.topology.base.BaseRichBolt;",
                "-import org.apache.storm.topology.base.BaseTickTupleAwareRichBolt;",
                "-import org.apache.storm.tuple.Tuple;",
                "-import org.apache.storm.utils.TupleUtils;",
                " import org.apache.solr.client.solrj.SolrClient;",
                "@@ -31,2 +24,4 @@ import org.apache.solr.client.solrj.SolrServerException;",
                " import org.apache.solr.client.solrj.impl.CloudSolrClient;",
                "+import org.apache.solr.client.solrj.impl.HttpClientUtil;",
                "+import org.apache.solr.client.solrj.impl.Krb5HttpClientConfigurer;",
                " import org.apache.storm.solr.config.CountBasedCommit;",
                "@@ -35,2 +30,8 @@ import org.apache.storm.solr.config.SolrConfig;",
                " import org.apache.storm.solr.mapper.SolrMapper;",
                "+import org.apache.storm.task.OutputCollector;",
                "+import org.apache.storm.task.TopologyContext;",
                "+import org.apache.storm.topology.OutputFieldsDeclarer;",
                "+import org.apache.storm.topology.base.BaseTickTupleAwareRichBolt;",
                "+import org.apache.storm.tuple.Tuple;",
                "+import org.apache.storm.utils.TupleUtils;",
                " import org.slf4j.Logger;",
                "@@ -76,2 +77,4 @@ public class SolrUpdateBolt extends BaseTickTupleAwareRichBolt {",
                "         this.collector = collector;",
                "+        if (solrConfig.isKerberosEnabled())",
                "+            HttpClientUtil.setConfigurer(new Krb5HttpClientConfigurer());",
                "         this.solrClient = new CloudSolrClient(solrConfig.getZkHostString());",
                "@@ -155,2 +158,13 @@ public class SolrUpdateBolt extends BaseTickTupleAwareRichBolt {",
                "+    @Override",
                "+    public void cleanup() {",
                "+        if (solrClient != null) {",
                "+            try {",
                "+                solrClient.close();",
                "+            } catch (IOException e) {",
                "+                LOG.error(\"Error while closing solrClient\", e);",
                "+            }",
                "+        }",
                "+    }",
                "+",
                " }",
                "diff --git a/external/storm-solr/src/main/java/org/apache/storm/solr/config/SolrConfig.java b/external/storm-solr/src/main/java/org/apache/storm/solr/config/SolrConfig.java",
                "index 1803a968f..ecb3630d1 100644",
                "--- a/external/storm-solr/src/main/java/org/apache/storm/solr/config/SolrConfig.java",
                "+++ b/external/storm-solr/src/main/java/org/apache/storm/solr/config/SolrConfig.java",
                "@@ -31,2 +31,3 @@ public class SolrConfig implements Serializable {",
                "     private final int tickTupleInterval;",
                "+    private final boolean enableKerberos;",
                "@@ -44,4 +45,14 @@ public class SolrConfig implements Serializable {",
                "     public SolrConfig(String zkHostString, int tickTupleInterval) {",
                "+        this(zkHostString, tickTupleInterval, false);",
                "+    }",
                "+",
                "+    /**",
                "+     * @param zkHostString Zookeeper host string as defined in the {@link CloudSolrClient} constructor",
                "+     * @param tickTupleInterval interval for tick tuples",
                "+     * @param enableKerberos true to enable kerberos else false",
                "+     * */",
                "+    public SolrConfig(String zkHostString, int tickTupleInterval, boolean enableKerberos) {",
                "         this.zkHostString = zkHostString;",
                "         this.tickTupleInterval = tickTupleInterval;",
                "+        this.enableKerberos =  enableKerberos;",
                "     }",
                "@@ -56,2 +67,5 @@ public class SolrConfig implements Serializable {",
                "+    public boolean isKerberosEnabled() {",
                "+        return enableKerberos;",
                "+    }",
                " }",
                "diff --git a/external/storm-solr/src/main/java/org/apache/storm/solr/mapper/SolrJsonMapper.java b/external/storm-solr/src/main/java/org/apache/storm/solr/mapper/SolrJsonMapper.java",
                "index 704ec2d98..0c39cb5e7 100644",
                "--- a/external/storm-solr/src/main/java/org/apache/storm/solr/mapper/SolrJsonMapper.java",
                "+++ b/external/storm-solr/src/main/java/org/apache/storm/solr/mapper/SolrJsonMapper.java",
                "@@ -60,2 +60,3 @@ public class SolrJsonMapper implements SolrMapper {",
                "          * @param jsonTupleField Name of the tuple field that contains the JSON object used to update the Solr index",
                "+         * This doesn't work in secure mode, in secure mode we need to pass the collection",
                "          */",
                "diff --git a/external/storm-solr/src/main/java/org/apache/storm/solr/schema/builder/RestJsonSchemaBuilder.java b/external/storm-solr/src/main/java/org/apache/storm/solr/schema/builder/RestJsonSchemaBuilder.java",
                "index 662c768df..a9e0b49dc 100644",
                "--- a/external/storm-solr/src/main/java/org/apache/storm/solr/schema/builder/RestJsonSchemaBuilder.java",
                "+++ b/external/storm-solr/src/main/java/org/apache/storm/solr/schema/builder/RestJsonSchemaBuilder.java",
                "@@ -33,2 +33,3 @@ import java.util.Scanner;",
                "  * in JSON format for the gettingstarted example running locally.",
                "+ * This doesn't work in kerberos mode. Please check new versions for kerberos support",
                "  */",
                "diff --git a/external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrState.java b/external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrState.java",
                "index d84d140ae..aeb06b63e 100644",
                "--- a/external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrState.java",
                "+++ b/external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrState.java",
                "@@ -20,2 +20,4 @@ package org.apache.storm.solr.trident;",
                "+import org.apache.solr.client.solrj.impl.HttpClientUtil;",
                "+import org.apache.solr.client.solrj.impl.Krb5HttpClientConfigurer;",
                " import org.apache.storm.topology.FailedException;",
                "@@ -46,2 +48,4 @@ public class SolrState implements State {",
                "     protected void prepare() {",
                "+        if (solrConfig.isKerberosEnabled())",
                "+            HttpClientUtil.setConfigurer(new Krb5HttpClientConfigurer());",
                "         solrClient = new CloudSolrClient(solrConfig.getZkHostString());"
            ],
            "changed_files": [
                "examples/storm-solr-examples/pom.xml",
                "external/storm-solr/pom.xml",
                "external/storm-solr/src/main/java/org/apache/storm/solr/bolt/SolrUpdateBolt.java",
                "external/storm-solr/src/main/java/org/apache/storm/solr/config/SolrConfig.java",
                "external/storm-solr/src/main/java/org/apache/storm/solr/mapper/SolrJsonMapper.java",
                "external/storm-solr/src/main/java/org/apache/storm/solr/schema/builder/RestJsonSchemaBuilder.java",
                "external/storm-solr/src/main/java/org/apache/storm/solr/trident/SolrState.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2860": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "8e256a532bc76435c3a292db814df57579200cd0"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2860",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "4fc47b2a841bf05ec89c4ccbf089d95c28c965a1",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1498748780,
            "hunks": 7,
            "message": "[STORM-2607] Offset consumer + 1",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "index b6d36d83b..1c474e34d 100755",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "@@ -73,2 +73,3 @@ public class OffsetManager {",
                "         long nextCommitOffset = committedOffset;",
                "+        long lastOffMessageOffset = committedOffset;",
                "         KafkaSpoutMessageId nextCommitMsg = null;     // this is a convenience variable to make it faster to create OffsetAndMetadata",
                "@@ -77,8 +78,9 @@ public class OffsetManager {",
                "             currOffset = currAckedMsg.offset();",
                "-            if (currOffset == nextCommitOffset + 1) {            // found the next offset to commit",
                "+            if (currOffset == lastOffMessageOffset + 1) {            // found the next offset to commit",
                "                 found = true;",
                "                 nextCommitMsg = currAckedMsg;",
                "-                nextCommitOffset = currOffset;",
                "-            } else if (currOffset > nextCommitOffset + 1) {",
                "-                if (emittedOffsets.contains(nextCommitOffset + 1)) {",
                "+                lastOffMessageOffset = currOffset;",
                "+                nextCommitOffset = lastOffMessageOffset + 1;",
                "+            } else if (currOffset > lastOffMessageOffset + 1) {",
                "+                if (emittedOffsets.contains(lastOffMessageOffset + 1)) {",
                "                     LOG.debug(\"topic-partition [{}] has non-continuous offset [{}]. It will be processed in a subsequent batch.\", tp, currOffset);",
                "@@ -94,3 +96,3 @@ public class OffsetManager {",
                "                     LOG.debug(\"Processed non contiguous offset. (committedOffset+1) is no longer part of the topic. Committed: [{}], Processed: [{}]\", committedOffset, currOffset);",
                "-                    final Long nextEmittedOffset = emittedOffsets.ceiling(nextCommitOffset + 1);",
                "+                    final Long nextEmittedOffset = emittedOffsets.ceiling(lastOffMessageOffset);",
                "                     if (nextEmittedOffset != null && currOffset == nextEmittedOffset) {",
                "@@ -98,3 +100,4 @@ public class OffsetManager {",
                "                         nextCommitMsg = currAckedMsg;",
                "-                        nextCommitOffset = currOffset;",
                "+                        lastOffMessageOffset = currOffset;",
                "+                        nextCommitOffset = lastOffMessageOffset + 1;",
                "                     } else {",
                "@@ -114,3 +117,4 @@ public class OffsetManager {",
                "             nextCommitOffsetAndMetadata = new OffsetAndMetadata(nextCommitOffset, nextCommitMsg.getMetadata(Thread.currentThread()));",
                "-            LOG.debug(\"topic-partition [{}] has offsets [{}-{}] ready to be committed\", tp, committedOffset + 1, nextCommitOffsetAndMetadata.offset());",
                "+            LOG.debug(\"topic-partition [{}] has offsets [{}-{}] ready to be committed\",",
                "+                tp, earliestUncommittedOffset, nextCommitOffsetAndMetadata.offset() - 1);",
                "         } else {",
                "@@ -133,3 +137,3 @@ public class OffsetManager {",
                "         final long preCommitCommittedOffsets = this.committedOffset;",
                "-        long numCommittedOffsets = 0;",
                "+        final long numCommittedOffsets = committedOffset.offset() - this.committedOffset - 1;",
                "         this.committedOffset = committedOffset.offset();"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2607": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "14e98e73f42700b0c35519f89d8ef3bc41e9d9db"
                ],
                [
                    "no-tag",
                    "a1ffcb936336d71bdf3693108e33e92c8d2a88ef"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2607",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "29d37124d4f55fecc3d29fb5304553e0b539f4b2",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510074598,
            "hunks": 0,
            "message": "Merge branch 'STORM-2799' of https://github.com/srdo/storm into STORM-2799 STORM-2799: Exclude jdk.tools for JDK 9 compatibility This closes #2401",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2799": ""
            },
            "ghissue_refs": {
                "2401": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2799",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2401",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5a3cefbace63a593b31e0193656b46143e113f78",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514343139,
            "hunks": 7,
            "message": "STORM-2870 Properly shutdown ExecutorService in FileBasedEventLogger * extract local variable 'scheduler' to one of fields * gracefully shutdown the scheduler * address review comments   * wait once, and call shutdownNow, don't wait afterwards   * set daemon to true",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/metric/FileBasedEventLogger.java b/storm-core/src/jvm/org/apache/storm/metric/FileBasedEventLogger.java",
                "index 6a07c0ece..d8ed5f63b 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metric/FileBasedEventLogger.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metric/FileBasedEventLogger.java",
                "@@ -36,4 +36,7 @@ import java.util.concurrent.Executors;",
                " import java.util.concurrent.ScheduledExecutorService;",
                "+import java.util.concurrent.ThreadFactory;",
                " import java.util.concurrent.TimeUnit;",
                "+import com.google.common.util.concurrent.ThreadFactoryBuilder;",
                "+",
                " public class FileBasedEventLogger implements IEventLogger {",
                "@@ -45,2 +48,3 @@ public class FileBasedEventLogger implements IEventLogger {",
                "     private BufferedWriter eventLogWriter;",
                "+    private ScheduledExecutorService flushScheduler;",
                "     private volatile boolean dirty = false;",
                "@@ -61,4 +65,9 @@ public class FileBasedEventLogger implements IEventLogger {",
                "     private void setUpFlushTask() {",
                "-        ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor();",
                "-        Runnable task = new Runnable() {",
                "+        ThreadFactory threadFactory = new ThreadFactoryBuilder()",
                "+                .setNameFormat(\"event-logger-flush-%d\")",
                "+                .setDaemon(true)",
                "+                .build();",
                "+",
                "+        flushScheduler = Executors.newSingleThreadScheduledExecutor(threadFactory);",
                "+        Runnable runnable = new Runnable() {",
                "             @Override",
                "@@ -77,3 +86,3 @@ public class FileBasedEventLogger implements IEventLogger {",
                "-        scheduler.scheduleAtFixedRate(task, FLUSH_INTERVAL_MILLIS, FLUSH_INTERVAL_MILLIS, TimeUnit.MILLISECONDS);",
                "+        flushScheduler.scheduleAtFixedRate(runnable, FLUSH_INTERVAL_MILLIS, FLUSH_INTERVAL_MILLIS, TimeUnit.MILLISECONDS);",
                "     }",
                "@@ -144,2 +153,3 @@ public class FileBasedEventLogger implements IEventLogger {",
                "             eventLogWriter.close();",
                "+",
                "         } catch (IOException ex) {",
                "@@ -147,2 +157,20 @@ public class FileBasedEventLogger implements IEventLogger {",
                "         }",
                "+",
                "+        closeFlushScheduler();",
                "+    }",
                "+",
                "+    private void closeFlushScheduler() {",
                "+        if (flushScheduler != null) {",
                "+            flushScheduler.shutdown();",
                "+            try {",
                "+                if (!flushScheduler.awaitTermination(2, TimeUnit.SECONDS)) {",
                "+                    flushScheduler.shutdownNow();",
                "+                }",
                "+            } catch (InterruptedException ie) {",
                "+                // (Re-)Cancel if current thread also interrupted",
                "+                flushScheduler.shutdownNow();",
                "+                // Preserve interrupt status",
                "+                Thread.currentThread().interrupt();",
                "+            }",
                "+        }",
                "     }"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/metric/FileBasedEventLogger.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2870": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "d4cfb729ed72c121978476a8966e6413e31b38f1"
                ],
                [
                    "no-tag",
                    "628a765d5e1a04a16d19720c229617a0ba2e4dd5"
                ],
                [
                    "no-tag",
                    "a894fe63a0814719634fab78783079bbcd121735"
                ]
            ],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2870",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "08a0d41d80ac09f9df651a2f57231f2f46703281",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517305358,
            "hunks": 4,
            "message": "STORM-2917: Check the derecated config nimbus.host There is a situation: the deployer wants to use the new nimbus config(nimbus.seeds), but still leave the blank deprecated config(nimbus.host) in storm.yaml. It will not work. Fixed merge conflict by Jungtaek Lim <kabhwan@gmail.com> To avoid this, the program should at least check whether the deprecated config is blank.",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/NimbusClient.java b/storm-core/src/jvm/org/apache/storm/utils/NimbusClient.java",
                "index e8cef0918..3f49f1d7d 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/utils/NimbusClient.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/NimbusClient.java",
                "@@ -19,2 +19,8 @@ package org.apache.storm.utils;",
                "+import com.google.common.collect.Lists;",
                "+import java.security.Principal;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+",
                "+import org.apache.commons.lang.StringUtils;",
                " import org.apache.storm.Config;",
                "@@ -26,3 +32,2 @@ import org.apache.storm.security.auth.ThriftClient;",
                " import org.apache.storm.security.auth.ThriftConnectionType;",
                "-import com.google.common.collect.Lists;",
                " import org.apache.thrift.transport.TTransportException;",
                "@@ -31,6 +36,2 @@ import org.slf4j.LoggerFactory;",
                "-import java.security.Principal;",
                "-import java.util.List;",
                "-import java.util.Map;",
                "-",
                " public class NimbusClient extends ThriftClient {",
                "@@ -70,3 +71,3 @@ public class NimbusClient extends ThriftClient {",
                "         List<String> seeds;",
                "-        if(conf.containsKey(Config.NIMBUS_HOST)) {",
                "+        if (conf.containsKey(Config.NIMBUS_HOST) && StringUtils.isNotBlank(conf.get(Config.NIMBUS_HOST).toString())) {",
                "             LOG.warn(\"Using deprecated config {} for backward compatibility. Please update your storm.yaml so it only has config {}\","
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/utils/NimbusClient.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2917": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "49476be60cf525bf04a2bf6bc3ce3107ff390e25"
                ],
                [
                    "no-tag",
                    "2f869cd4df2d4a1683e9802b87befed2e02e451d"
                ],
                [
                    "no-tag",
                    "18045a3fca0c2cc2f5f7cc0241233c25b17432fe"
                ]
            ],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2917",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6482b2a140e8cc19ea2357822543324a1c570d38",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514127975,
            "hunks": 24,
            "message": "STORM-2843: [Flux] properties file not found when loading resources from classpath",
            "diff": [
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/Flux.java b/flux/flux-core/src/main/java/org/apache/storm/flux/Flux.java",
                "index e88f4db96..b801a52cf 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/Flux.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/Flux.java",
                "@@ -25,2 +25,3 @@ import java.io.InputStream;",
                " import java.io.InputStreamReader;",
                "+import java.util.Properties;",
                "@@ -147,3 +148,3 @@ public class Flux {",
                "-",
                "+        Properties properties = null;",
                "         boolean envFilter = cmd.hasOption(OPTION_ENV_FILTER);",
                "@@ -151,10 +152,10 @@ public class Flux {",
                "             printf(\"Parsing classpath resource: %s\", filePath);",
                "-            topologyDef = FluxParser.parseResource(filePath, dumpYaml, true, filterProps, envFilter);",
                "+            properties = FluxParser.parseProperties(filterProps, true);",
                "+            topologyDef = FluxParser.parseResource(filePath, dumpYaml, true, properties, envFilter);",
                "         } else {",
                "-            printf(\"Parsing file: %s\",",
                "-                    new File(filePath).getAbsolutePath());",
                "-            topologyDef = FluxParser.parseFile(filePath, dumpYaml, true, filterProps, envFilter);",
                "+            printf(\"Parsing file: %s\", new File(filePath).getAbsolutePath());",
                "+            properties = FluxParser.parseProperties(filterProps, false);",
                "+            topologyDef = FluxParser.parseFile(filePath, dumpYaml, true, properties, envFilter);",
                "         }",
                "-",
                "         String topologyName = topologyDef.getName();",
                "diff --git a/flux/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java b/flux/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java",
                "index cc23fb88f..8299c14ff 100644",
                "--- a/flux/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java",
                "+++ b/flux/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java",
                "@@ -46,4 +46,2 @@ public class FluxParser {",
                "-    // TODO refactor input stream processing (see parseResource() method).",
                "-",
                "     /**",
                "@@ -53,3 +51,3 @@ public class FluxParser {",
                "      * @param processIncludes whether or not to process includes",
                "-     * @param propertiesFile properties file for variable substitution",
                "+     * @param properties properties file for variable substitution",
                "      * @param envSub whether or not to perform environment variable substitution",
                "@@ -59,6 +57,5 @@ public class FluxParser {",
                "     public static TopologyDef parseFile(String inputFile, boolean dumpYaml, boolean processIncludes,",
                "-                                        String propertiesFile, boolean envSub) throws IOException {",
                "-",
                "+                                        Properties properties, boolean envSub) throws IOException {",
                "         FileInputStream in = new FileInputStream(inputFile);",
                "-        TopologyDef topology = parseInputStream(in, dumpYaml, processIncludes, propertiesFile, envSub);",
                "+        TopologyDef topology = parseInputStream(in, dumpYaml, processIncludes, properties, envSub);",
                "         in.close();",
                "@@ -73,3 +70,3 @@ public class FluxParser {",
                "      * @param processIncludes whether or not to process includes",
                "-     * @param propertiesFile properties file for variable substitution",
                "+     * @param properties properties file for variable substitution",
                "      * @param envSub whether or not to perform environment variable substitution",
                "@@ -79,6 +76,5 @@ public class FluxParser {",
                "     public static TopologyDef parseResource(String resource, boolean dumpYaml, boolean processIncludes,",
                "-                                            String propertiesFile, boolean envSub) throws IOException {",
                "-",
                "+                                            Properties properties, boolean envSub) throws IOException {",
                "         InputStream in = FluxParser.class.getResourceAsStream(resource);",
                "-        TopologyDef topology = parseInputStream(in, dumpYaml, processIncludes, propertiesFile, envSub);",
                "+        TopologyDef topology = parseInputStream(in, dumpYaml, processIncludes, properties, envSub);",
                "         in.close();",
                "@@ -93,5 +89,5 @@ public class FluxParser {",
                "      * @param processIncludes whether or not to process includes",
                "-     * @param propertiesFile properties file for variable substitution",
                "+     * @param properties properties file for variable substitution",
                "      * @param envSub whether or not to perform environment variable substitution",
                "-     * @return resulting topologuy definition",
                "+     * @return resulting topology definition",
                "      * @throws IOException if there is a problem reading file(s)",
                "@@ -99,4 +95,3 @@ public class FluxParser {",
                "     public static TopologyDef parseInputStream(InputStream inputStream, boolean dumpYaml, boolean processIncludes,",
                "-                                               String propertiesFile, boolean envSub) throws IOException {",
                "-",
                "+                                               Properties properties, boolean envSub) throws IOException {",
                "         Yaml yaml = yaml();",
                "@@ -108,3 +103,3 @@ public class FluxParser {",
                "-        TopologyDef topology = loadYaml(yaml, inputStream, propertiesFile, envSub);",
                "+        TopologyDef topology = loadYaml(yaml, inputStream, properties, envSub);",
                "@@ -115,3 +110,3 @@ public class FluxParser {",
                "         if (processIncludes) {",
                "-            return processIncludes(yaml, topology, propertiesFile, envSub);",
                "+            return processIncludes(yaml, topology, properties, envSub);",
                "         } else {",
                "@@ -121,3 +116,28 @@ public class FluxParser {",
                "-    private static TopologyDef loadYaml(Yaml yaml, InputStream in, String propsFile, boolean envSubstitution) throws IOException {",
                "+    /**",
                "+     * Parse filter properties file.",
                "+     * @param propertiesFile properties file for variable substitution",
                "+     * @param resource whether or not to load properties file from classpath",
                "+     * @return resulting filter properties",
                "+     * @throws IOException  if there is a problem reading file",
                "+     */",
                "+    public static Properties parseProperties(String propertiesFile, boolean resource) throws IOException {",
                "+        Properties properties = null;",
                "+",
                "+        if (propertiesFile != null) {",
                "+            properties = new Properties();",
                "+            InputStream in = null;",
                "+            if (resource) {",
                "+                in = FluxParser.class.getResourceAsStream(propertiesFile);",
                "+            } else {",
                "+                in = new FileInputStream(propertiesFile);",
                "+            }",
                "+            properties.load(in);",
                "+            in.close();",
                "+        }",
                "+",
                "+        return properties;",
                "+    }",
                "+",
                "+    private static TopologyDef loadYaml(Yaml yaml, InputStream in, Properties properties, boolean envSubstitution) throws IOException {",
                "         ByteArrayOutputStream bos = new ByteArrayOutputStream();",
                "@@ -132,10 +152,6 @@ public class FluxParser {",
                "         // properties file substitution",
                "-        if (propsFile != null) {",
                "+        if (properties != null) {",
                "             LOG.info(\"Performing property substitution.\");",
                "-            try (InputStream propsIn = new FileInputStream(propsFile)) {",
                "-                Properties props = new Properties();",
                "-                props.load(propsIn);",
                "-                for (Object key : props.keySet()) {",
                "-                    str = str.replace(\"${\" + key + \"}\", props.getProperty((String) key));",
                "-                }",
                "+            for (Object key : properties.keySet()) {",
                "+                str = str.replace(\"${\" + key + \"}\", properties.getProperty((String)key));",
                "             }",
                "@@ -179,6 +195,8 @@ public class FluxParser {",
                "      * @param topologyDef the topology definition containing (possibly zero) includes",
                "+     * @param properties properties file for variable substitution",
                "+     * @param envSub whether or not to perform environment variable substitution",
                "      * @return The TopologyDef with includes resolved.",
                "      */",
                "-    private static TopologyDef processIncludes(Yaml yaml, TopologyDef topologyDef, String propsFile, boolean envSub)",
                "-            throws IOException {",
                "+    private static TopologyDef processIncludes(Yaml yaml, TopologyDef topologyDef, Properties properties, boolean envSub)",
                "+        throws IOException {",
                "         //TODO support multiple levels of includes",
                "@@ -189,6 +207,6 @@ public class FluxParser {",
                "                     LOG.info(\"Loading includes from resource: {}\", include.getFile());",
                "-                    includeTopologyDef = parseResource(include.getFile(), true, false, propsFile, envSub);",
                "+                    includeTopologyDef = parseResource(include.getFile(), true, false, properties, envSub);",
                "                 } else {",
                "                     LOG.info(\"Loading includes from file: {}\", include.getFile());",
                "-                    includeTopologyDef = parseFile(include.getFile(), true, false, propsFile, envSub);",
                "+                    includeTopologyDef = parseFile(include.getFile(), true, false, properties, envSub);",
                "                 }"
            ],
            "changed_files": [
                "flux/flux-core/src/main/java/org/apache/storm/flux/Flux.java",
                "flux/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2843": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "febb31452163c361e3cc8e8bf63403662d74012a"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2843",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "82e00aa12631da4137ba9503cc8660c769e3d4f0",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509556644,
            "hunks": 13,
            "message": "STORM-2793 Track network data metrics * This closes #2399",
            "diff": [
                "diff --git a/conf/defaults.yaml b/conf/defaults.yaml",
                "index ad3405408..666158e50 100644",
                "--- a/conf/defaults.yaml",
                "+++ b/conf/defaults.yaml",
                "@@ -265,2 +265,3 @@ topology.localityaware.higher.bound.percent: 0.8",
                " topology.localityaware.lower.bound.percent: 0.2",
                "+topology.serialized.message.size.metrics: false",
                "diff --git a/storm-client/src/jvm/org/apache/storm/Config.java b/storm-client/src/jvm/org/apache/storm/Config.java",
                "index 29365d962..42a4b4498 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/Config.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/Config.java",
                "@@ -452,2 +452,9 @@ public class Config extends HashMap<String, Object> {",
                "+    /**",
                "+     * Enable tracking of network message byte counts per source-destination task. This is off by default as it",
                "+     * creates tasks^2 metric values, but is useful for debugging as it exposes data skew when tuple sizes are uneven.",
                "+     */",
                "+    @isBoolean",
                "+    public static final String TOPOLOGY_SERIALIZED_MESSAGE_SIZE_METRICS = \"topology.serialized.message.size.metrics\";",
                "+",
                "     /**",
                "diff --git a/storm-client/src/jvm/org/apache/storm/messaging/DeserializingConnectionCallback.java b/storm-client/src/jvm/org/apache/storm/messaging/DeserializingConnectionCallback.java",
                "index 75e58d960..b80f7c0db 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/messaging/DeserializingConnectionCallback.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/messaging/DeserializingConnectionCallback.java",
                "@@ -19,10 +19,18 @@ package org.apache.storm.messaging;",
                "+import org.apache.storm.Config;",
                " import org.apache.storm.daemon.worker.WorkerState;",
                "+import org.apache.storm.metric.api.IMetric;",
                "+import org.apache.storm.serialization.KryoTupleDeserializer;",
                " import org.apache.storm.task.GeneralTopologyContext;",
                " import org.apache.storm.tuple.AddressedTuple;",
                "-import org.apache.storm.serialization.KryoTupleDeserializer;",
                "+import org.apache.storm.tuple.Tuple;",
                "+import org.apache.storm.utils.ObjectReader;",
                " import java.util.ArrayList;",
                "+import java.util.HashMap;",
                " import java.util.List;",
                " import java.util.Map;",
                "+import java.util.concurrent.ConcurrentHashMap;",
                "+import java.util.concurrent.atomic.AtomicLong;",
                "+",
                "@@ -31,18 +39,26 @@ import java.util.Map;",
                "  */",
                "-public class DeserializingConnectionCallback implements IConnectionCallback {",
                "-    private final WorkerState.ILocalTransferCallback _cb;",
                "-    private final Map _conf;",
                "-    private final GeneralTopologyContext _context;",
                "+public class DeserializingConnectionCallback implements IConnectionCallback, IMetric {",
                "+    private final WorkerState.ILocalTransferCallback cb;",
                "+    private final Map conf;",
                "+    private final GeneralTopologyContext context;",
                "+",
                "     private final ThreadLocal<KryoTupleDeserializer> _des =",
                "-         new ThreadLocal<KryoTupleDeserializer>() {",
                "-             @Override",
                "-             protected KryoTupleDeserializer initialValue() {",
                "-                 return new KryoTupleDeserializer(_conf, _context);",
                "-             }",
                "-         };",
                "-",
                "-    public DeserializingConnectionCallback(final Map<String, Object> conf, final GeneralTopologyContext context, WorkerState.ILocalTransferCallback callback) {",
                "-        _conf = conf;",
                "-        _context = context;",
                "-        _cb = callback;",
                "+        new ThreadLocal<KryoTupleDeserializer>() {",
                "+            @Override",
                "+            protected KryoTupleDeserializer initialValue() {",
                "+                return new KryoTupleDeserializer(conf, context);",
                "+            }",
                "+        };",
                "+",
                "+    // Track serialized size of messages.",
                "+    private final boolean sizeMetricsEnabled;",
                "+    private final ConcurrentHashMap<String, AtomicLong> byteCounts = new ConcurrentHashMap<>();",
                "+",
                "+",
                "+    public DeserializingConnectionCallback(final Map conf, final GeneralTopologyContext context, WorkerState.ILocalTransferCallback callback) {",
                "+        this.conf = conf;",
                "+        this.context = context;",
                "+        cb = callback;",
                "+        sizeMetricsEnabled = ObjectReader.getBoolean(conf.get(Config.TOPOLOGY_SERIALIZED_MESSAGE_SIZE_METRICS), false);",
                "+",
                "     }",
                "@@ -54,5 +70,41 @@ public class DeserializingConnectionCallback implements IConnectionCallback {",
                "         for (TaskMessage message: batch) {",
                "-            ret.add(new AddressedTuple(message.task(), des.deserialize(message.message())));",
                "+            Tuple tuple = des.deserialize(message.message());",
                "+            AddressedTuple addrTuple = new AddressedTuple(message.task(), tuple);",
                "+            updateMetrics(tuple.getSourceTask(), message);",
                "+            ret.add(addrTuple);",
                "+        }",
                "+        cb.transfer(ret);",
                "+    }",
                "+",
                "+    /**",
                "+     * Returns serialized byte count traffic metrics.",
                "+     * @return Map of metric counts, or null if disabled",
                "+     */",
                "+    @Override",
                "+    public Object getValueAndReset() {",
                "+        if (!sizeMetricsEnabled) {",
                "+            return null;",
                "+        }",
                "+        HashMap<String, Long> outMap = new HashMap<>();",
                "+        for (Map.Entry<String, AtomicLong> ent : byteCounts.entrySet()) {",
                "+            AtomicLong count = ent.getValue();",
                "+            if (count.get() > 0) {",
                "+                outMap.put(ent.getKey(), count.getAndSet(0L));",
                "+            }",
                "+        }",
                "+        return outMap;",
                "+    }",
                "+",
                "+    /**",
                "+     * Update serialized byte counts for each message.",
                "+     * @param sourceTaskId source task",
                "+     * @param message serialized message",
                "+     */",
                "+    protected void updateMetrics(int sourceTaskId, TaskMessage message) {",
                "+        if (sizeMetricsEnabled) {",
                "+            int dest = message.task();",
                "+            int len = message.message().length;",
                "+            String key = Integer.toString(sourceTaskId) + \"-\" + Integer.toString(dest);",
                "+            byteCounts.computeIfAbsent(key, k -> new AtomicLong(0L)).addAndGet(len);",
                "         }",
                "-        _cb.transfer(ret);",
                "     }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/messaging/netty/Server.java b/storm-client/src/jvm/org/apache/storm/messaging/netty/Server.java",
                "index 91ef702fd..1d07497a3 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/messaging/netty/Server.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/messaging/netty/Server.java",
                "@@ -24,2 +24,3 @@ import org.apache.storm.messaging.IConnectionCallback;",
                " import org.apache.storm.messaging.TaskMessage;",
                "+import org.apache.storm.metric.api.IMetric;",
                " import org.apache.storm.metric.api.IStatefulObject;",
                "@@ -251,2 +252,11 @@ class Server extends ConnectionWithStatus implements IStatefulObject, ISaslServe",
                "         ret.put(\"enqueued\", enqueued);",
                "+        ",
                "+        // Report messageSizes metric, if enabled (non-null).",
                "+        if (_cb instanceof IMetric) {",
                "+            Object metrics = ((IMetric) _cb).getValueAndReset();",
                "+            if (metrics instanceof Map) {",
                "+                ret.put(\"messageBytes\", metrics);",
                "+            }",
                "+        }",
                "+        ",
                "         return ret;"
            ],
            "changed_files": [
                "conf/defaults.yaml",
                "storm-client/src/jvm/org/apache/storm/Config.java",
                "storm-client/src/jvm/org/apache/storm/messaging/DeserializingConnectionCallback.java",
                "storm-client/src/jvm/org/apache/storm/messaging/netty/Server.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2793": ""
            },
            "ghissue_refs": {
                "2399": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2793",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2399",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "148ee6609557d7436ce826da29ff3ec301458586",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1498922996,
            "hunks": 27,
            "message": "STORM-2607: Switch OffsetManager to track earliest uncommitted offset instead of last committed offset for compatibility with commitSync consumer API",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 3582bdbb8..68bce11a3 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -204,3 +204,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         /**",
                "-         * sets the cursor to the location dictated by the first poll strategy and returns the fetch offset",
                "+         * Sets the cursor to the location dictated by the first poll strategy and returns the fetch offset.",
                "          */",
                "@@ -213,4 +213,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 } else {",
                "-                    // By default polling starts at the last committed offset. +1 to point fetch to the first uncommitted offset.",
                "-                    kafkaConsumer.seek(tp, committedOffset.offset() + 1);",
                "+                    // By default polling starts at the last committed offset, i.e. the first offset that was not marked as processed.",
                "+                    kafkaConsumer.seek(tp, committedOffset.offset());",
                "                 }",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "index 1c474e34d..7dfe7f65c 100755",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "@@ -39,3 +39,3 @@ public class OffsetManager {",
                "     private final long initialFetchOffset;",
                "-    // Last offset committed to Kafka. Initially it is set to fetchOffset - 1",
                "+    // Committed offset, i.e. the offset where processing will resume if the spout restarts. Initially it is set to fetchOffset.",
                "     private long committedOffset;",
                "@@ -49,4 +49,4 @@ public class OffsetManager {",
                "         this.initialFetchOffset = initialFetchOffset;",
                "-        this.committedOffset = initialFetchOffset - 1;",
                "-        LOG.debug(\"Instantiated {}\", this);",
                "+        this.committedOffset = initialFetchOffset;",
                "+        LOG.debug(\"Instantiated {}\", this.toString());",
                "     }",
                "@@ -62,5 +62,7 @@ public class OffsetManager {",
                "     /**",
                "-     * An offset is only committed when all records with lower offset have been",
                "+     * An offset can only be committed when all emitted records with lower offset have been",
                "      * acked. This guarantees that all offsets smaller than the committedOffset",
                "-     * have been delivered.",
                "+     * have been delivered, or that those offsets no longer exist in Kafka. ",
                "+     * <p/>",
                "+     * The returned offset points to the earliest uncommitted offset, and matches the semantics of the KafkaConsumer.commitSync API.",
                "      *",
                "@@ -73,3 +75,2 @@ public class OffsetManager {",
                "         long nextCommitOffset = committedOffset;",
                "-        long lastOffMessageOffset = committedOffset;",
                "         KafkaSpoutMessageId nextCommitMsg = null;     // this is a convenience variable to make it faster to create OffsetAndMetadata",
                "@@ -78,10 +79,10 @@ public class OffsetManager {",
                "             currOffset = currAckedMsg.offset();",
                "-            if (currOffset == lastOffMessageOffset + 1) {            // found the next offset to commit",
                "+            if (currOffset == nextCommitOffset) {            // found the next offset to commit",
                "                 found = true;",
                "                 nextCommitMsg = currAckedMsg;",
                "-                lastOffMessageOffset = currOffset;",
                "-                nextCommitOffset = lastOffMessageOffset + 1;",
                "-            } else if (currOffset > lastOffMessageOffset + 1) {",
                "-                if (emittedOffsets.contains(lastOffMessageOffset + 1)) {",
                "-                    LOG.debug(\"topic-partition [{}] has non-continuous offset [{}]. It will be processed in a subsequent batch.\", tp, currOffset);",
                "+                nextCommitOffset = currOffset + 1;",
                "+            } else if (currOffset > nextCommitOffset) {",
                "+                if (emittedOffsets.contains(nextCommitOffset)) {",
                "+                    LOG.debug(\"topic-partition [{}] has non-sequential offset [{}].\"",
                "+                        + \" It will be processed in a subsequent batch.\", tp, currOffset);",
                "                     break;",
                "@@ -89,17 +90,20 @@ public class OffsetManager {",
                "                     /*",
                "-                        This case will arise in case of non contiguous offset being processed.",
                "-                        So, if the topic doesn't contain offset = committedOffset + 1 (possible",
                "+                        This case will arise in case of non-sequential offset being processed.",
                "+                        So, if the topic doesn't contain offset = nextCommitOffset (possible",
                "                         if the topic is compacted or deleted), the consumer should jump to",
                "                         the next logical point in the topic. Next logical offset should be the",
                "-                        first element after committedOffset in the ascending ordered emitted set.",
                "+                        first element after nextCommitOffset in the ascending ordered emitted set.",
                "                      */",
                "-                    LOG.debug(\"Processed non contiguous offset. (committedOffset+1) is no longer part of the topic. Committed: [{}], Processed: [{}]\", committedOffset, currOffset);",
                "-                    final Long nextEmittedOffset = emittedOffsets.ceiling(lastOffMessageOffset);",
                "+                    LOG.debug(\"Processed non-sequential offset.\"",
                "+                        + \" The earliest uncommitted offset is no longer part of the topic.\"",
                "+                        + \" Missing offset: [{}], Processed: [{}]\", nextCommitOffset, currOffset);",
                "+                    final Long nextEmittedOffset = emittedOffsets.ceiling(nextCommitOffset);",
                "                     if (nextEmittedOffset != null && currOffset == nextEmittedOffset) {",
                "-                        found = true;",
                "+                        LOG.debug(\"Found committable offset: [{}] after missing offset: [{}], skipping to the committable offset\",",
                "+                            currOffset, nextCommitOffset);",
                "                         nextCommitMsg = currAckedMsg;",
                "-                        lastOffMessageOffset = currOffset;",
                "-                        nextCommitOffset = lastOffMessageOffset + 1;",
                "+                        nextCommitOffset = currOffset + 1;",
                "                     } else {",
                "-                        LOG.debug(\"topic-partition [{}] has non-continuous offset [{}]. Next Offset to commit should be [{}]\", tp, currOffset, nextEmittedOffset);",
                "+                        LOG.debug(\"Topic-partition [{}] has non-sequential offset [{}].\"",
                "+                            + \" Next offset to commit should be [{}]\", tp, currOffset, nextCommitOffset);",
                "                         break;",
                "@@ -108,4 +112,4 @@ public class OffsetManager {",
                "             } else {",
                "-                throw new IllegalStateException(\"The offset [\" + currOffset + \"] is below the current committed \"",
                "-                    + \"offset [\" + committedOffset + \"] for [\" + tp + \"].\"",
                "+                throw new IllegalStateException(\"The offset [\" + currOffset + \"] is below the current nextCommitOffset \"",
                "+                    + \"[\" + nextCommitOffset + \"] for [\" + tp + \"].\"",
                "                     + \" This should not be possible, and likely indicates a bug in the spout's acking or emit logic.\");",
                "@@ -116,7 +120,9 @@ public class OffsetManager {",
                "         if (found) {",
                "-            nextCommitOffsetAndMetadata = new OffsetAndMetadata(nextCommitOffset, nextCommitMsg.getMetadata(Thread.currentThread()));",
                "-            LOG.debug(\"topic-partition [{}] has offsets [{}-{}] ready to be committed\",",
                "-                tp, earliestUncommittedOffset, nextCommitOffsetAndMetadata.offset() - 1);",
                "+            nextCommitOffsetAndMetadata = new OffsetAndMetadata(nextCommitOffset,",
                "+                nextCommitMsg.getMetadata(Thread.currentThread()));",
                "+            LOG.debug(\"Topic-partition [{}] has offsets [{}-{}] ready to be committed.\"",
                "+                + \" Processing will resume at offset [{}] if the spout restarts\",",
                "+                tp, committedOffset, nextCommitOffsetAndMetadata.offset() - 1, nextCommitOffsetAndMetadata.offset());",
                "         } else {",
                "-            LOG.debug(\"topic-partition [{}] has NO offsets ready to be committed\", tp);",
                "+            LOG.debug(\"Topic-partition [{}] has no offsets ready to be committed\", tp);",
                "         }",
                "@@ -127,8 +133,8 @@ public class OffsetManager {",
                "     /**",
                "-     * Marks an offset has committed. This method has side effects - it sets the",
                "+     * Marks an offset as committed. This method has side effects - it sets the",
                "      * internal state in such a way that future calls to",
                "-     * {@link #findNextCommitOffset()} will return offsets greater than the",
                "+     * {@link #findNextCommitOffset()} will return offsets greater than or equal to the",
                "      * offset specified, if any.",
                "      *",
                "-     * @param committedOffset offset to be marked as committed",
                "+     * @param committedOffset The committed offset. All lower offsets are expected to have been committed.",
                "      * @return Number of offsets committed in this commit",
                "@@ -136,7 +142,7 @@ public class OffsetManager {",
                "     public long commit(OffsetAndMetadata committedOffset) {",
                "-        final long preCommitCommittedOffsets = this.committedOffset;",
                "-        final long numCommittedOffsets = committedOffset.offset() - this.committedOffset - 1;",
                "+        final long preCommitCommittedOffset = this.committedOffset;",
                "+        long numCommittedOffsets = 0;",
                "         this.committedOffset = committedOffset.offset();",
                "         for (Iterator<KafkaSpoutMessageId> iterator = ackedMsgs.iterator(); iterator.hasNext();) {",
                "-            if (iterator.next().offset() <= committedOffset.offset()) {",
                "+            if (iterator.next().offset() < committedOffset.offset()) {",
                "                 iterator.remove();",
                "@@ -149,3 +155,3 @@ public class OffsetManager {",
                "         for (Iterator<Long> iterator = emittedOffsets.iterator(); iterator.hasNext();) {",
                "-            if (iterator.next() <= committedOffset.offset()) {",
                "+            if (iterator.next() < committedOffset.offset()) {",
                "                 iterator.remove();",
                "@@ -158,4 +164,5 @@ public class OffsetManager {",
                "-        LOG.debug(\"Committed [{}] offsets in the range [{}-{}] for topic-partition [{}].\",",
                "-                numCommittedOffsets, preCommitCommittedOffsets + 1, this.committedOffset, tp);",
                "+        LOG.debug(\"Committed [{}] offsets in the range [{}-{}] for topic-partition [{}].\"",
                "+            + \" Processing will resume at [{}] if the spout restarts.\",",
                "+                numCommittedOffsets, preCommitCommittedOffset, this.committedOffset - 1, tp, this.committedOffset);",
                "@@ -179,5 +186,10 @@ public class OffsetManager {",
                "     }",
                "+    ",
                "+    //VisibleForTesting",
                "+    boolean containsEmitted(long offset) {",
                "+        return emittedOffsets.contains(offset);",
                "+    }",
                "     @Override",
                "-    public String toString() {",
                "+    public final String toString() {",
                "         return \"OffsetManager{\""
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2607": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "c9bbd544449eee98d0427c115ff13a6878706179"
                ],
                [
                    "no-tag",
                    "d7bdc2d34b1a09a14c76381284edaf6236096457"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2607",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d644e29a8808dd6cc619230f884d9be43a1e7509",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514929415,
            "hunks": 0,
            "message": "Merge branch 'STORM-2856' of https://github.com/srdo/storm into STORM-2856 STORM-2856: Make Storm build work on post-2017Q4 Travis Trusty image This closes #2486",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2856": ""
            },
            "ghissue_refs": {
                "2486": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2856",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2486",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "41db23fe86299f71b6ecc7deaa70ab731899c83f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515185814,
            "hunks": 136,
            "message": "Split up NormalizedResources into a few classes with more narrow responsibilities. Make NormalizedResources a regular class instead of an abstract class. Add some tests. Make calculateAvg/Min throw exceptions if the resource total is not a superset of the used resources.",
            "diff": [
                "diff --git a/storm-server/pom.xml b/storm-server/pom.xml",
                "index d6554de45..ff917caaf 100644",
                "--- a/storm-server/pom.xml",
                "+++ b/storm-server/pom.xml",
                "@@ -132,3 +132,3 @@",
                "                 <configuration>",
                "-                    <maxAllowedViolations>2630</maxAllowedViolations>",
                "+                    <maxAllowedViolations>2620</maxAllowedViolations>",
                "                 </configuration>",
                "diff --git a/storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java b/storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java",
                "index d660ab010..fd1cf40bd 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java",
                "@@ -26,5 +26,3 @@ import java.util.Map;",
                " import java.util.Set;",
                "-",
                " import javax.security.auth.Subject;",
                "-",
                " import org.apache.commons.collections.CollectionUtils;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index b495cfaef..2c785a5f9 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -54,3 +54,2 @@ import java.util.regex.Matcher;",
                " import java.util.regex.Pattern;",
                "-import java.util.stream.Collectors;",
                " import javax.security.auth.Subject;",
                "@@ -138,3 +137,2 @@ import org.apache.storm.nimbus.NimbusInfo;",
                " import org.apache.storm.scheduler.Cluster;",
                "-import org.apache.storm.scheduler.SupervisorResources;",
                " import org.apache.storm.scheduler.DefaultScheduler;",
                "@@ -146,2 +144,3 @@ import org.apache.storm.scheduler.SchedulerAssignmentImpl;",
                " import org.apache.storm.scheduler.SupervisorDetails;",
                "+import org.apache.storm.scheduler.SupervisorResources;",
                " import org.apache.storm.scheduler.Topologies;",
                "@@ -151,5 +150,5 @@ import org.apache.storm.scheduler.blacklist.BlacklistScheduler;",
                " import org.apache.storm.scheduler.multitenant.MultitenantScheduler;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                " import org.apache.storm.scheduler.resource.ResourceAwareScheduler;",
                " import org.apache.storm.scheduler.resource.ResourceUtils;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                " import org.apache.storm.security.INimbusCredentialPlugin;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "index 87e310ea4..3faf1e4b4 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "@@ -19,2 +19,7 @@ package org.apache.storm.daemon.supervisor.timer;",
                "+import java.util.ArrayList;",
                "+import java.util.Collections;",
                "+import java.util.HashMap;",
                "+import java.util.List;",
                "+import java.util.Map;",
                " import org.apache.storm.Config;",
                "@@ -24,2 +29,3 @@ import org.apache.storm.daemon.supervisor.Supervisor;",
                " import org.apache.storm.generated.SupervisorInfo;",
                "+import org.apache.storm.scheduler.resource.NormalizedResources;",
                " import org.apache.storm.utils.ObjectReader;",
                "@@ -27,10 +33,2 @@ import org.apache.storm.utils.Time;",
                "-import java.util.ArrayList;",
                "-import java.util.Collections;",
                "-import java.util.HashMap;",
                "-import java.util.List;",
                "-import java.util.Map;",
                "-",
                "-import static org.apache.storm.scheduler.resource.NormalizedResources.normalizedResourceMap;",
                "-",
                " public class SupervisorHeartbeat implements Runnable {",
                "@@ -94,3 +92,3 @@ public class SupervisorHeartbeat implements Runnable {",
                "-        return normalizedResourceMap(ret);",
                "+        return NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(ret);",
                "     }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "index 9ab0c9365..1ed88c738 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "@@ -39,2 +39,3 @@ import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                " import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                "+import org.apache.storm.scheduler.resource.NormalizedResources;",
                " import org.apache.storm.utils.ConfigUtils;",
                "@@ -46,4 +47,2 @@ import org.slf4j.LoggerFactory;",
                "-import static org.apache.storm.scheduler.resource.NormalizedResources.normalizedResourceMap;",
                "-",
                " public class Cluster implements ISchedulingState {",
                "@@ -442,3 +441,3 @@ public class Cluster implements ISchedulingState {",
                "         }",
                "-        sharedTotalResources = normalizedResourceMap(sharedTotalResources);",
                "+        sharedTotalResources = NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(sharedTotalResources);",
                "         WorkerResources ret = new WorkerResources();",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java",
                "index 58f12d047..787bbce75 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java",
                "@@ -20,7 +20,4 @@ package org.apache.storm.scheduler.resource;",
                "-import java.util.HashMap;",
                "-import java.util.List;",
                " import java.util.Map;",
                " import org.apache.storm.Constants;",
                "-import org.apache.storm.generated.WorkerResources;",
                " import org.slf4j.Logger;",
                "@@ -29,11 +26,12 @@ import org.slf4j.LoggerFactory;",
                " /**",
                "- * An offer of resources that has been normalized.",
                "+ * An offer of resources with normalized resource names.",
                "  */",
                "-public class NormalizedResourceOffer extends NormalizedResources {",
                "+public class NormalizedResourceOffer implements NormalizedResourcesWithMemory {",
                "+",
                "     private static final Logger LOG = LoggerFactory.getLogger(NormalizedResourceOffer.class);",
                "-    private double totalMemory;",
                "+    private final NormalizedResources normalizedResources;",
                "+    private double totalMemoryMb;",
                "     /**",
                "-     * Create a new normalized set of resources.  Note that memory is not covered here because it is not consistent in requests vs offers",
                "-     * because of how on heap vs off heap is used.",
                "+     * Create a new normalized resource offer.",
                "      *",
                "@@ -42,4 +40,5 @@ public class NormalizedResourceOffer extends NormalizedResources {",
                "     public NormalizedResourceOffer(Map<String, ? extends Number> resources) {",
                "-        super(resources, null);",
                "-        totalMemory = getNormalizedResources().getOrDefault(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, 0.0);",
                "+        Map<String, Double> normalizedResourceMap = NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(resources);",
                "+        totalMemoryMb = normalizedResourceMap.getOrDefault(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, 0.0);",
                "+        this.normalizedResources = new NormalizedResources(normalizedResourceMap);",
                "     }",
                "@@ -47,3 +46,3 @@ public class NormalizedResourceOffer extends NormalizedResources {",
                "     public NormalizedResourceOffer() {",
                "-        this((Map<String, ? extends Number>)null);",
                "+        this((Map<String, ? extends Number>) null);",
                "     }",
                "@@ -51,4 +50,4 @@ public class NormalizedResourceOffer extends NormalizedResources {",
                "     public NormalizedResourceOffer(NormalizedResourceOffer other) {",
                "-        super(other);",
                "-        this.totalMemory = other.totalMemory;",
                "+        this.totalMemoryMb = other.totalMemoryMb;",
                "+        this.normalizedResources = new NormalizedResources(other.normalizedResources);",
                "     }",
                "@@ -57,21 +56,53 @@ public class NormalizedResourceOffer extends NormalizedResources {",
                "     public double getTotalMemoryMb() {",
                "-        return totalMemory;",
                "+        return totalMemoryMb;",
                "     }",
                "-    @Override",
                "-    public String toString() {",
                "-        return super.toString() + \" MEM: \" + totalMemory;",
                "+    public Map<String, Double> toNormalizedMap() {",
                "+        Map<String, Double> ret = normalizedResources.toNormalizedMap();",
                "+        ret.put(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, totalMemoryMb);",
                "+        return ret;",
                "     }",
                "-    @Override",
                "-    public Map<String,Double> toNormalizedMap() {",
                "-        Map<String, Double> ret = super.toNormalizedMap();",
                "-        ret.put(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, totalMemory);",
                "-        return ret;",
                "+    public void add(NormalizedResourcesWithMemory other) {",
                "+        normalizedResources.add(other.getNormalizedResources());",
                "+        totalMemoryMb += other.getTotalMemoryMb();",
                "     }",
                "-    @Override",
                "-    public void add(NormalizedResources other) {",
                "-        super.add(other);",
                "-        totalMemory += other.getTotalMemoryMb();",
                "+    public void remove(NormalizedResourcesWithMemory other) {",
                "+        normalizedResources.remove(other.getNormalizedResources());",
                "+        totalMemoryMb -= other.getTotalMemoryMb();",
                "+        if (totalMemoryMb < 0.0) {",
                "+            normalizedResources.throwBecauseResourceBecameNegative(",
                "+                Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, totalMemoryMb, other.getTotalMemoryMb());",
                "+        };",
                "+    }",
                "+",
                "+    /**",
                "+     * @see NormalizedResources#calculateAveragePercentageUsedBy(org.apache.storm.scheduler.resource.normalization.NormalizedResources,",
                "+     * double, double).",
                "+     */",
                "+    public double calculateAveragePercentageUsedBy(NormalizedResourceOffer used) {",
                "+        return normalizedResources.calculateAveragePercentageUsedBy(",
                "+            used.getNormalizedResources(), getTotalMemoryMb(), used.getTotalMemoryMb());",
                "+    }",
                "+",
                "+    /**",
                "+     * @see NormalizedResources#calculateMinPercentageUsedBy(org.apache.storm.scheduler.resource.normalization.NormalizedResources, double,",
                "+     * double)",
                "+     */",
                "+    public double calculateMinPercentageUsedBy(NormalizedResourceOffer used) {",
                "+        return normalizedResources.calculateMinPercentageUsedBy(used.getNormalizedResources(), getTotalMemoryMb(), used.getTotalMemoryMb());",
                "+    }",
                "+",
                "+    /**",
                "+     * @see NormalizedResources#couldHoldIgnoringSharedMemory(org.apache.storm.scheduler.resource.normalization.NormalizedResources, double,",
                "+     * double).",
                "+     */",
                "+    public boolean couldHoldIgnoringSharedMemory(NormalizedResourcesWithMemory other) {",
                "+        return normalizedResources.couldHoldIgnoringSharedMemory(",
                "+            other.getNormalizedResources(), getTotalMemoryMb(), other.getTotalMemoryMb());",
                "+    }",
                "+",
                "+    public double getTotalCpu() {",
                "+        return normalizedResources.getTotalCpu();",
                "     }",
                "@@ -79,6 +110,4 @@ public class NormalizedResourceOffer extends NormalizedResources {",
                "     @Override",
                "-    public void remove(NormalizedResources other) {",
                "-        super.remove(other);",
                "-        totalMemory -= other.getTotalMemoryMb();",
                "-        assert totalMemory >= 0.0;",
                "+    public NormalizedResources getNormalizedResources() {",
                "+        return normalizedResources;",
                "     }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java",
                "index 596375003..2af90ab2d 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java",
                "@@ -20,8 +20,4 @@ package org.apache.storm.scheduler.resource;",
                "-import java.util.Collections;",
                " import java.util.HashMap;",
                "-import java.util.HashSet;",
                " import java.util.Map;",
                "-import java.util.Set;",
                "-import java.util.stream.Collectors;",
                " import org.apache.storm.Config;",
                "@@ -38,5 +34,6 @@ import org.slf4j.LoggerFactory;",
                " /**",
                "- * A request that has been normalized.",
                "+ * A resource request with normalized resource names.",
                "  */",
                "-public class NormalizedResourceRequest extends NormalizedResources {",
                "+public class NormalizedResourceRequest implements NormalizedResourcesWithMemory {",
                "+",
                "     private static final Logger LOG = LoggerFactory.getLogger(NormalizedResourceRequest.class);",
                "@@ -45,3 +42,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "         if (!dest.containsKey(destKey)) {",
                "-            Number value = (Number)src.get(srcKey);",
                "+            Number value = (Number) src.get(srcKey);",
                "             if (value != null) {",
                "@@ -53,3 +50,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "     private static Map<String, Double> getDefaultResources(Map<String, Object> topoConf) {",
                "-        Map<String, Double> ret = normalizedResourceMap((Map<String, Number>) topoConf.getOrDefault(",
                "+        Map<String, Double> ret = NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap((Map<String, Number>) topoConf.getOrDefault(",
                "             Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<>()));",
                "@@ -98,3 +95,2 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "-",
                "                 }",
                "@@ -108,2 +104,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "+    private final NormalizedResources normalizedResources;",
                "     private double onHeap;",
                "@@ -111,13 +108,9 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "-    /**",
                "-     * Create a new normalized set of resources.  Note that memory is not covered here because it is not consistent in requests vs offers",
                "-     * because of how on heap vs off heap is used.",
                "-     *",
                "-     * @param resources the resources to be normalized.",
                "-     * @param topologyConf the config for the topology",
                "-     */",
                "     private NormalizedResourceRequest(Map<String, ? extends Number> resources,",
                "-                                     Map<String, Object> topologyConf) {",
                "-        super(resources, getDefaultResources(topologyConf));",
                "-        initializeMemory(getNormalizedResources());",
                "+        Map<String, Double> defaultResources) {",
                "+        Map<String, Double> normalizedResourceMap = NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(defaultResources);",
                "+        normalizedResourceMap.putAll(NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(resources));",
                "+        onHeap = normalizedResourceMap.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+        offHeap = normalizedResourceMap.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+        normalizedResources = new NormalizedResources(normalizedResourceMap);",
                "     }",
                "@@ -125,3 +118,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "     public NormalizedResourceRequest(ComponentCommon component, Map<String, Object> topoConf) {",
                "-        this(parseResources(component.get_json_conf()), topoConf);",
                "+        this(parseResources(component.get_json_conf()), getDefaultResources(topoConf));",
                "     }",
                "@@ -129,3 +122,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "     public NormalizedResourceRequest(Map<String, Object> topoConf) {",
                "-        this((Map<String, ? extends Number>) null, topoConf);",
                "+        this((Map<String, ? extends Number>) null, getDefaultResources(topoConf));",
                "     }",
                "@@ -133,9 +126,7 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "     public NormalizedResourceRequest() {",
                "-        super(null, null);",
                "-        initializeMemory(getNormalizedResources());",
                "+        this((Map<String, ? extends Number>) null, null);",
                "     }",
                "-    @Override",
                "-    public Map<String,Double> toNormalizedMap() {",
                "-        Map<String, Double> ret = super.toNormalizedMap();",
                "+    public Map<String, Double> toNormalizedMap() {",
                "+        Map<String, Double> ret = this.normalizedResources.toNormalizedMap();",
                "         ret.put(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, offHeap);",
                "@@ -145,7 +136,2 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "-    private void initializeMemory(Map<String, Double> normalizedResources) {",
                "-        onHeap = normalizedResources.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "-        offHeap = normalizedResources.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "-    }",
                "-",
                "     public double getOnHeapMemoryMb() {",
                "@@ -168,2 +154,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "      * Add the resources in other to this.",
                "+     *",
                "      * @param other the other Request to add to this.",
                "@@ -171,3 +158,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "     public void add(NormalizedResourceRequest other) {",
                "-        super.add(other);",
                "+        this.normalizedResources.add(other.normalizedResources);",
                "         onHeap += other.onHeap;",
                "@@ -176,5 +163,4 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "-    @Override",
                "     public void add(WorkerResources value) {",
                "-        super.add(value);",
                "+        this.normalizedResources.add(value);",
                "         //The resources are already normalized",
                "@@ -187,3 +173,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "     public double getTotalMemoryMb() {",
                "-        return getOnHeapMemoryMb() + getOffHeapMemoryMb();",
                "+        return onHeap + offHeap;",
                "     }",
                "@@ -194,2 +180,11 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "     }",
                "+",
                "+    public double getTotalCpu() {",
                "+        return this.normalizedResources.getTotalCpu();",
                "+    }",
                "+",
                "+    @Override",
                "+    public NormalizedResources getNormalizedResources() {",
                "+        return this.normalizedResources;",
                "+    }",
                " }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "index 846ab6945..f8e911a37 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "@@ -22,10 +22,4 @@ import com.google.common.annotations.VisibleForTesting;",
                " import java.util.Arrays;",
                "-import java.util.Collections;",
                "-import java.util.HashMap;",
                " import java.util.Map;",
                "-import java.util.concurrent.ConcurrentHashMap;",
                "-import java.util.concurrent.ConcurrentMap;",
                "-import java.util.concurrent.atomic.AtomicInteger;",
                "-import java.util.stream.Collectors;",
                "-import org.apache.storm.Config;",
                "+import org.apache.commons.lang.Validate;",
                " import org.apache.storm.Constants;",
                "@@ -36,50 +30,21 @@ import org.slf4j.LoggerFactory;",
                " /**",
                "- * Resources that have been normalized.",
                "+ * Resources that have been normalized. This class is intended as a delegate for more specific types of normalized resource set, since it",
                "+ * does not keep track of memory as a resource.",
                "  */",
                "-public abstract class NormalizedResources {",
                "+public class NormalizedResources {",
                "     private static final Logger LOG = LoggerFactory.getLogger(NormalizedResources.class);",
                "-    public static final Map<String, String> RESOURCE_NAME_MAPPING;",
                "-    static {",
                "-        Map<String, String> tmp = new HashMap<>();",
                "-        tmp.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, Constants.COMMON_CPU_RESOURCE_NAME);",
                "-        tmp.put(Config.SUPERVISOR_CPU_CAPACITY, Constants.COMMON_CPU_RESOURCE_NAME);",
                "-        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "-        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "-        tmp.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME);",
                "-        RESOURCE_NAME_MAPPING = Collections.unmodifiableMap(tmp);",
                "-    }",
                "+    public static ResourceNameNormalizer RESOURCE_NAME_NORMALIZER;",
                "+    private static ResourceMapArrayBridge RESOURCE_MAP_ARRAY_BRIDGE;",
                "-    private static double[] makeArray(Map<String, Double> normalizedResources) {",
                "-        //To avoid locking we will go through the map twice.  It should be small so it is probably not a big deal",
                "-        for (String key : normalizedResources.keySet()) {",
                "-            //We are going to skip over CPU and Memory, because they are captured elsewhere",
                "-            if (!Constants.COMMON_CPU_RESOURCE_NAME.equals(key)",
                "-                && !Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME.equals(key)",
                "-                && !Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME.equals(key)",
                "-                && !Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME.equals(key)) {",
                "-                resourceNames.computeIfAbsent(key, (k) -> counter.getAndIncrement());",
                "-            }",
                "-        }",
                "-        //By default all of the values are 0",
                "-        double[] ret = new double[counter.get()];",
                "-        for (Map.Entry<String, Double> entry : normalizedResources.entrySet()) {",
                "-            Integer index = resourceNames.get(entry.getKey());",
                "-            if (index != null) {",
                "-                //index == null if it is memory or CPU",
                "-                ret[index] = entry.getValue();",
                "-            }",
                "-        }",
                "-        return ret;",
                "+    static {",
                "+        resetResourceNames();",
                "     }",
                "-    private static final ConcurrentMap<String, Integer> resourceNames = new ConcurrentHashMap<>();",
                "-    private static final AtomicInteger counter = new AtomicInteger(0);",
                "     private double cpu;",
                "     private double[] otherResources;",
                "-    private final Map<String, Double> normalizedResources;",
                "     /**",
                "-     * This is for testing only. It allows a test to reset the mapping of resource names in the array. We reset the mapping because some",
                "+     * This is for testing only. It allows a test to reset the static state relating to resource names. We reset the mapping because some",
                "      * algorithms sadly have different behavior if a resource exists or not.",
                "@@ -88,4 +53,4 @@ public abstract class NormalizedResources {",
                "     public static void resetResourceNames() {",
                "-        resourceNames.clear();",
                "-        counter.set(0);",
                "+        RESOURCE_NAME_NORMALIZER = new ResourceNameNormalizer();",
                "+        RESOURCE_MAP_ARRAY_BRIDGE = new ResourceMapArrayBridge();",
                "     }",
                "@@ -98,3 +63,2 @@ public abstract class NormalizedResources {",
                "         otherResources = Arrays.copyOf(other.otherResources, other.otherResources.length);",
                "-        normalizedResources = other.normalizedResources;",
                "     }",
                "@@ -102,44 +66,13 @@ public abstract class NormalizedResources {",
                "     /**",
                "-     * Create a new normalized set of resources. Note that memory is not covered here because it is not consistent in requests vs offers",
                "-     * because of how on heap vs off heap is used.",
                "+     * Create a new normalized set of resources. Note that memory is not managed by this class, as it is not consistent in requests vs",
                "+     * offers because of how on heap vs off heap is used.",
                "      *",
                "-     * @param resources the resources to be normalized.",
                "-     * @param defaults the default resources that will also be normalized and combined with the real resources.",
                "+     * @param normalizedResources the normalized resource map",
                "+     * @param getTotalMemoryMb Supplier of total memory in MB.",
                "      */",
                "-    public NormalizedResources(Map<String, ? extends Number> resources, Map<String, ? extends Number> defaults) {",
                "-        normalizedResources = normalizedResourceMap(defaults);",
                "-        normalizedResources.putAll(normalizedResourceMap(resources));",
                "+    public NormalizedResources(Map<String, Double> normalizedResources) {",
                "         cpu = normalizedResources.getOrDefault(Constants.COMMON_CPU_RESOURCE_NAME, 0.0);",
                "-        otherResources = makeArray(normalizedResources);",
                "-    }",
                "-",
                "-    protected final Map<String, Double> getNormalizedResources() {",
                "-        return this.normalizedResources;",
                "+        otherResources = RESOURCE_MAP_ARRAY_BRIDGE.translateToResourceArray(normalizedResources);",
                "     }",
                "-    /**",
                "-     * Normalizes a supervisor resource map or topology details map's keys to universal resource names.",
                "-     *",
                "-     * @param resourceMap resource map of either Supervisor or Topology",
                "-     * @return the resource map with common resource names",
                "-     */",
                "-    public static Map<String, Double> normalizedResourceMap(Map<String, ? extends Number> resourceMap) {",
                "-        if (resourceMap == null) {",
                "-            return new HashMap<>();",
                "-        }",
                "-        return new HashMap<>(resourceMap.entrySet().stream()",
                "-            .collect(Collectors.toMap(",
                "-                //Map the key if needed",
                "-                (e) -> RESOURCE_NAME_MAPPING.getOrDefault(e.getKey(), e.getKey()),",
                "-                //Map the value",
                "-                (e) -> e.getValue().doubleValue())));",
                "-    }",
                "-",
                "-    /**",
                "-     * Get the total amount of memory.",
                "-     *",
                "-     * @return the total amount of memory requested or provided.",
                "-     */",
                "-    public abstract double getTotalMemoryMb();",
                "-",
                "     /**",
                "@@ -152,2 +85,10 @@ public abstract class NormalizedResources {",
                "     }",
                "+    ",
                "+    private void zeroPadOtherResourcesIfNecessary(int requiredLength) {",
                "+        if (requiredLength > otherResources.length) {",
                "+            double[] newResources = new double[requiredLength];",
                "+            System.arraycopy(otherResources, 0, newResources, 0, otherResources.length);",
                "+            otherResources = newResources;",
                "+        }",
                "+    }",
                "@@ -155,8 +96,3 @@ public abstract class NormalizedResources {",
                "         int otherLength = resourceArray.length;",
                "-        int length = otherResources.length;",
                "-        if (otherLength > length) {",
                "-            double[] newResources = new double[otherLength];",
                "-            System.arraycopy(newResources, 0, otherResources, 0, length);",
                "-            otherResources = newResources;",
                "-        }",
                "+        zeroPadOtherResourcesIfNecessary(otherLength);",
                "         for (int i = 0; i < otherLength; i++) {",
                "@@ -179,5 +115,11 @@ public abstract class NormalizedResources {",
                "         cpu += workerNormalizedResources.getOrDefault(Constants.COMMON_CPU_RESOURCE_NAME, 0.0);",
                "-        add(makeArray(workerNormalizedResources));",
                "+        add(RESOURCE_MAP_ARRAY_BRIDGE.translateToResourceArray(workerNormalizedResources));",
                "     }",
                "+    public void throwBecauseResourceBecameNegative(String resourceName, double currentValue, double subtractedValue) {",
                "+        throw new IllegalArgumentException(String.format(\"Resource amounts should never be negative.\"",
                "+            + \" Resource '%s' with current value '%f' became negative because '%f' was removed.\",",
                "+            resourceName, currentValue, subtractedValue));",
                "+    }",
                "+    ",
                "     /**",
                "@@ -186,2 +128,3 @@ public abstract class NormalizedResources {",
                "      * @param other the resources we want removed.",
                "+     * @throws IllegalArgumentException if subtracting other from this would result in any resource amount becoming negative.",
                "      */",
                "@@ -189,13 +132,12 @@ public abstract class NormalizedResources {",
                "         this.cpu -= other.cpu;",
                "-        assert cpu >= 0.0;",
                "-        int otherLength = other.otherResources.length;",
                "-        int length = otherResources.length;",
                "-        if (otherLength > length) {",
                "-            double[] newResources = new double[otherLength];",
                "-            System.arraycopy(newResources, 0, otherResources, 0, length);",
                "-            otherResources = newResources;",
                "+        if (cpu < 0.0) {",
                "+            throwBecauseResourceBecameNegative(Constants.COMMON_CPU_RESOURCE_NAME, cpu, other.cpu);",
                "         }",
                "+        int otherLength = other.otherResources.length;",
                "+        zeroPadOtherResourcesIfNecessary(otherLength);",
                "         for (int i = 0; i < otherLength; i++) {",
                "             otherResources[i] -= other.otherResources[i];",
                "-            assert otherResources[i] >= 0.0;",
                "+            if (otherResources[i] < 0.0) {",
                "+                throwBecauseResourceBecameNegative(getResourceNameForResourceIndex(i), otherResources[i], other.otherResources[i]);",
                "+            }",
                "         }",
                "@@ -205,15 +147,3 @@ public abstract class NormalizedResources {",
                "     public String toString() {",
                "-        return \"CPU: \" + cpu + \" Other resources: \" + toNormalizedOtherResources();",
                "-    }",
                "-",
                "-    private Map<String, Double> toNormalizedOtherResources() {",
                "-        Map<String, Double> ret = new HashMap<>();",
                "-        int length = otherResources.length;",
                "-        for (Map.Entry<String, Integer> entry : resourceNames.entrySet()) {",
                "-            int index = entry.getValue();",
                "-            if (index < length) {",
                "-                ret.put(entry.getKey(), otherResources[index]);",
                "-            }",
                "-        }",
                "-        return ret;",
                "+        return \"Normalized resources: \" + toNormalizedMap();",
                "     }",
                "@@ -225,3 +155,3 @@ public abstract class NormalizedResources {",
                "     public Map<String, Double> toNormalizedMap() {",
                "-        Map<String, Double> ret = toNormalizedOtherResources();",
                "+        Map<String, Double> ret = RESOURCE_MAP_ARRAY_BRIDGE.translateFromResourceArray(otherResources);",
                "         ret.put(Constants.COMMON_CPU_RESOURCE_NAME, cpu);",
                "@@ -242,5 +172,7 @@ public abstract class NormalizedResources {",
                "      * @param other the resources that we want to check if they would fit in this.",
                "+     * @param thisTotalMemoryMb The total memory in MB of this",
                "+     * @param otherTotalMemoryMb The total memory in MB of other",
                "      * @return true if it might fit, else false if it could not possibly fit.",
                "      */",
                "-    public boolean couldHoldIgnoringSharedMemory(NormalizedResources other) {",
                "+    public boolean couldHoldIgnoringSharedMemory(NormalizedResources other, double thisTotalMemoryMb, double otherTotalMemoryMb) {",
                "         if (this.cpu < other.getTotalCpu()) {",
                "@@ -255,24 +187,19 @@ public abstract class NormalizedResources {",
                "-        if (this.getTotalMemoryMb() < other.getTotalMemoryMb()) {",
                "-            return false;",
                "-        }",
                "-        return true;",
                "+        return thisTotalMemoryMb >= otherTotalMemoryMb;",
                "     }",
                "-    private void throwBecauseResourceIsMissingFromTotal(int resourceIndex) {",
                "-        String resourceName = null;",
                "-        for (Map.Entry<String, Integer> entry : resourceNames.entrySet()) {",
                "+    private String getResourceNameForResourceIndex(int resourceIndex) {",
                "+        for (Map.Entry<String, Integer> entry : RESOURCE_MAP_ARRAY_BRIDGE.getResourceNamesToArrayIndex().entrySet()) {",
                "             int index = entry.getValue();",
                "             if (index == resourceIndex) {",
                "-                resourceName = entry.getKey();",
                "-                break;",
                "+                return entry.getKey();",
                "             }",
                "         }",
                "-        if (resourceName == null) {",
                "-            throw new IllegalStateException(\"Array index \" + resourceIndex + \" is not mapped in the resource names map.\"",
                "-                + \" This should not be possible, and is likely a bug in the Storm code.\");",
                "-        }",
                "-        throw new IllegalArgumentException(\"Total resources does not contain resource '\"",
                "-            + resourceName",
                "-            + \"'. All resources should be represented in the total. This is likely a bug in the Storm code\");",
                "+        return null;",
                "+    }",
                "+",
                "+    private void throwBecauseUsedIsNotSubsetOfTotal(NormalizedResources used, double totalMemoryMb, double usedMemoryMb) {",
                "+        throw new IllegalArgumentException(String.format(\"The used resources must be a subset of the total resources.\"",
                "+            + \" Used: '%s', Total: '%s', Used Mem: '%f', Total Mem: '%f'\",",
                "+            used.toNormalizedMap(), this.toNormalizedMap(), usedMemoryMb, totalMemoryMb));",
                "     }",
                "@@ -280,13 +207,27 @@ public abstract class NormalizedResources {",
                "     /**",
                "-     * Calculate the average resource usage percentage with this being the total resources and used being the amounts used.",
                "+     * Calculate the average resource usage percentage with this being the total resources and used being the amounts used. Used must be a",
                "+     * subset of the total resources. If a resource in the total has a value of zero, it will be skipped in the calculation to avoid",
                "+     * division by 0. If all resources are skipped the result is defined to be 100.0.",
                "      *",
                "      * @param used the amount of resources used.",
                "-     * @return the average percentage used 0.0 to 100.0. Clamps to 100.0 in case there are no available resources in the total",
                "+     * @param totalMemoryMb The total memory in MB",
                "+     * @param usedMemoryMb The used memory in MB",
                "+     * @return the average percentage used 0.0 to 100.0.",
                "+     * @throws IllegalArgumentException if any resource in used has a greater value than the same resource in the total, or used has generic",
                "+     *     resources that are not present in the total.",
                "      */",
                "-    public double calculateAveragePercentageUsedBy(NormalizedResources used) {",
                "+    public double calculateAveragePercentageUsedBy(NormalizedResources used, double totalMemoryMb, double usedMemoryMb) {",
                "+        if (LOG.isTraceEnabled()) {",
                "+            LOG.trace(\"Calculating avg percentage used by. Used Mem: {} Total Mem: {}\"",
                "+                + \" Used Normalized Resources: {} Total Normalized Resources: {}\", totalMemoryMb, usedMemoryMb,",
                "+                toNormalizedMap(), used.toNormalizedMap());",
                "+        }",
                "+",
                "         int skippedResourceTypes = 0;",
                "         double total = 0.0;",
                "-        double totalMemory = getTotalMemoryMb();",
                "-        if (totalMemory != 0.0) {",
                "-            total += used.getTotalMemoryMb() / totalMemory;",
                "+        if (usedMemoryMb > totalMemoryMb) {",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+        }",
                "+        if (totalMemoryMb != 0.0) {",
                "+            total += usedMemoryMb / totalMemoryMb;",
                "         } else {",
                "@@ -295,2 +236,5 @@ public abstract class NormalizedResources {",
                "         double totalCpu = getTotalCpu();",
                "+        if (used.getTotalCpu() > getTotalCpu()) {",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+        }",
                "         if (totalCpu != 0.0) {",
                "@@ -300,10 +244,5 @@ public abstract class NormalizedResources {",
                "         }",
                "-        if (LOG.isTraceEnabled()) {",
                "-            LOG.trace(\"Calculating avg percentage used by. Used CPU: {} Total CPU: {} Used Mem: {} Total Mem: {}\"",
                "-                + \" Other Used: {} Other Total: {}\", totalCpu, used.getTotalCpu(), totalMemory, used.getTotalMemoryMb(),",
                "-                this.toNormalizedOtherResources(), used.toNormalizedOtherResources());",
                "-        }",
                "         if (used.otherResources.length > otherResources.length) {",
                "-            throwBecauseResourceIsMissingFromTotal(used.otherResources.length);",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "         }",
                "@@ -312,8 +251,2 @@ public abstract class NormalizedResources {",
                "             double totalValue = otherResources[i];",
                "-            if (totalValue == 0.0) {",
                "-                //Skip any resources where the total is 0, the percent used for this resource isn't meaningful.",
                "-                //We fall back to prioritizing by cpu, memory and any other resources by ignoring this value",
                "-                skippedResourceTypes++;",
                "-                continue;",
                "-            }",
                "             double usedValue;",
                "@@ -325,2 +258,12 @@ public abstract class NormalizedResources {",
                "             }",
                "+            if (usedValue > totalValue) {",
                "+                throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+            }",
                "+            if (totalValue == 0.0) {",
                "+                //Skip any resources where the total is 0, the percent used for this resource isn't meaningful.",
                "+                //We fall back to prioritizing by cpu, memory and any other resources by ignoring this value",
                "+                skippedResourceTypes++;",
                "+                continue;",
                "+            }",
                "+",
                "             total += usedValue / totalValue;",
                "@@ -330,5 +273,5 @@ public abstract class NormalizedResources {",
                "         if (divisor == 0) {",
                "-            /*This is an arbitrary choice to make the result consistent with calculateMin.",
                "-             Any value would be valid here, becase there are no (non-zero) resources in the total set of resources,",
                "-             so we're trying to average 0 values.",
                "+            /*",
                "+             * This is an arbitrary choice to make the result consistent with calculateMin. Any value would be valid here, becase there are",
                "+             * no (non-zero) resources in the total set of resources, so we're trying to average 0 values.",
                "              */",
                "@@ -341,32 +284,30 @@ public abstract class NormalizedResources {",
                "     /**",
                "-     * Calculate the minimum resource usage percentage with this being the total resources and used being the amounts used.",
                "+     * Calculate the minimum resource usage percentage with this being the total resources and used being the amounts used. Used must be a",
                "+     * subset of the total resources. If a resource in the total has a value of zero, it will be skipped in the calculation to avoid",
                "+     * division by 0. If all resources are skipped the result is defined to be 100.0.",
                "      *",
                "      * @param used the amount of resources used.",
                "-     * @return the minimum percentage used 0.0 to 100.0. Clamps to 100.0 in case there are no available resources in the total.",
                "+     * @param totalMemoryMb The total memory in MB",
                "+     * @param usedMemoryMb The used memory in MB",
                "+     * @return the minimum percentage used 0.0 to 100.0.",
                "+     * @throws IllegalArgumentException if any resource in used has a greater value than the same resource in the total, or used has generic",
                "+     *     resources that are not present in the total.",
                "      */",
                "-    public double calculateMinPercentageUsedBy(NormalizedResources used) {",
                "-        double totalMemory = getTotalMemoryMb();",
                "-        double totalCpu = getTotalCpu();",
                "-",
                "+    public double calculateMinPercentageUsedBy(NormalizedResources used, double totalMemoryMb, double usedMemoryMb) {",
                "         if (LOG.isTraceEnabled()) {",
                "-            LOG.trace(\"Calculating min percentage used by. Used CPU: {} Total CPU: {} Used Mem: {} Total Mem: {}\"",
                "-                + \" Other Used: {} Other Total: {}\", totalCpu, used.getTotalCpu(), totalMemory, used.getTotalMemoryMb(),",
                "-                toNormalizedOtherResources(), used.toNormalizedOtherResources());",
                "+            LOG.trace(\"Calculating min percentage used by. Used Mem: {} Total Mem: {}\"",
                "+                + \" Used Normalized Resources: {} Total Normalized Resources: {}\", totalMemoryMb, usedMemoryMb,",
                "+                toNormalizedMap(), used.toNormalizedMap());",
                "         }",
                "-        if (used.otherResources.length > otherResources.length) {",
                "-            throwBecauseResourceIsMissingFromTotal(used.otherResources.length);",
                "+        double min = 1.0;",
                "+        if (usedMemoryMb > totalMemoryMb) {",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "         }",
                "-",
                "-        if (used.otherResources.length != otherResources.length",
                "-            || totalMemory == 0.0",
                "-            || totalCpu == 0.0) {",
                "-            //If the lengths don't match one of the resources will be 0, which means we would calculate the percentage to be 0.0",
                "-            // and so the min would be 0.0 (assuming that we can never go negative on a resource being used.",
                "-            return 0.0;",
                "+        if (totalMemoryMb != 0.0) {",
                "+            min = Math.min(min, usedMemoryMb / totalMemoryMb);",
                "         }",
                "-",
                "-        double min = 100.0;",
                "-        if (totalMemory != 0.0) {",
                "-            min = Math.min(min, used.getTotalMemoryMb() / totalMemory);",
                "+        double totalCpu = getTotalCpu();",
                "+        if (used.getTotalCpu() > totalCpu) {",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "         }",
                "@@ -376,2 +317,6 @@ public abstract class NormalizedResources {",
                "+        if (used.otherResources.length > otherResources.length) {",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+        }",
                "+        ",
                "         for (int i = 0; i < otherResources.length; i++) {",
                "@@ -386,2 +331,5 @@ public abstract class NormalizedResources {",
                "             }",
                "+            if (used.otherResources[i] > otherResources[i]) {",
                "+                throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+            }",
                "             min = Math.min(min, used.otherResources[i] / otherResources[i]);",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourcesWithMemory.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourcesWithMemory.java",
                "new file mode 100644",
                "index 000000000..640233ac9",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourcesWithMemory.java",
                "@@ -0,0 +1,28 @@",
                "+/*",
                "+ * Copyright 2018 The Apache Software Foundation.",
                "+ *",
                "+ * Licensed under the Apache License, Version 2.0 (the \"License\");",
                "+ * you may not use this file except in compliance with the License.",
                "+ * You may obtain a copy of the License at",
                "+ *",
                "+ *      http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource;",
                "+",
                "+/**",
                "+ * Intended for {@link NormalizedResources} wrappers that handle memory.",
                "+ */",
                "+public interface NormalizedResourcesWithMemory {",
                "+",
                "+    NormalizedResources getNormalizedResources();",
                "+",
                "+    double getTotalMemoryMb();",
                "+",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "index 535000576..db9ca7364 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "@@ -27,4 +27,2 @@ import java.util.Map.Entry;",
                " import java.util.Set;",
                "-",
                "-import org.apache.storm.Constants;",
                " import org.apache.storm.scheduler.Cluster;",
                "@@ -34,3 +32,2 @@ import org.apache.storm.scheduler.TopologyDetails;",
                " import org.apache.storm.scheduler.WorkerSlot;",
                "-import org.apache.storm.utils.ObjectReader;",
                " import org.slf4j.Logger;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Nodes.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Nodes.java",
                "index 6c70ff1dc..2bd2b863c 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Nodes.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Nodes.java",
                "@@ -24,3 +24,2 @@ import java.util.LinkedList;",
                " import java.util.Map;",
                "-",
                " import org.apache.storm.scheduler.Cluster;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "index dc557ffc5..e73070578 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "@@ -32,3 +32,2 @@ import org.apache.storm.scheduler.IScheduler;",
                " import org.apache.storm.scheduler.SchedulerAssignment;",
                "-import org.apache.storm.scheduler.SchedulerAssignmentImpl;",
                " import org.apache.storm.scheduler.SingleTopologyCluster;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceMapArrayBridge.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceMapArrayBridge.java",
                "new file mode 100644",
                "index 000000000..cf4f80b38",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceMapArrayBridge.java",
                "@@ -0,0 +1,89 @@",
                "+/*",
                "+ * Copyright 2018 The Apache Software Foundation.",
                "+ *",
                "+ * Licensed under the Apache License, Version 2.0 (the \"License\");",
                "+ * you may not use this file except in compliance with the License.",
                "+ * You may obtain a copy of the License at",
                "+ *",
                "+ *      http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource;",
                "+",
                "+import java.util.Collections;",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+import java.util.concurrent.ConcurrentHashMap;",
                "+import java.util.concurrent.ConcurrentMap;",
                "+import java.util.concurrent.atomic.AtomicInteger;",
                "+import org.apache.storm.Constants;",
                "+",
                "+/**",
                "+ * Provides translation between normalized resource maps and resource value arrays. Some operations use resource value arrays instead of the",
                "+ * full normalized resource map as an optimization. See {@link NormalizedResources}.",
                "+ */",
                "+public class ResourceMapArrayBridge {",
                "+",
                "+    private final ConcurrentMap<String, Integer> resourceNamesToArrayIndex = new ConcurrentHashMap<>();",
                "+    private final AtomicInteger counter = new AtomicInteger(0);",
                "+",
                "+    /**",
                "+     * Translates a normalized resource map to an array of resource values. Each resource name will be assigned an index in the array, which",
                "+     * is guaranteed to be consistent with subsequent invocations of this method. Note that CPU and memory resources are not translated by",
                "+     * this method, as they are expected to be captured elsewhere.",
                "+     *",
                "+     * @param normalizedResources The resources to translate to an array",
                "+     * @return The array of resource values",
                "+     */",
                "+    public double[] translateToResourceArray(Map<String, Double> normalizedResources) {",
                "+        //To avoid locking we will go through the map twice.  It should be small so it is probably not a big deal",
                "+        for (String key : normalizedResources.keySet()) {",
                "+            //We are going to skip over CPU and Memory, because they are captured elsewhere",
                "+            if (!Constants.COMMON_CPU_RESOURCE_NAME.equals(key)",
                "+                && !Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME.equals(key)",
                "+                && !Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME.equals(key)",
                "+                && !Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME.equals(key)) {",
                "+                resourceNamesToArrayIndex.computeIfAbsent(key, (k) -> counter.getAndIncrement());",
                "+            }",
                "+        }",
                "+        //By default all of the values are 0",
                "+        double[] ret = new double[counter.get()];",
                "+        for (Map.Entry<String, Double> entry : normalizedResources.entrySet()) {",
                "+            Integer index = resourceNamesToArrayIndex.get(entry.getKey());",
                "+            if (index != null) {",
                "+                //index == null if it is memory or CPU",
                "+                ret[index] = entry.getValue();",
                "+            }",
                "+        }",
                "+        return ret;",
                "+    }",
                "+",
                "+    /**",
                "+     * Translates an array of resource values to a normalized resource map.",
                "+     *",
                "+     * @param resources The resource array to translate",
                "+     * @return The normalized resource map",
                "+     */",
                "+    public Map<String, Double> translateFromResourceArray(double[] resources) {",
                "+        Map<String, Double> ret = new HashMap<>();",
                "+        int length = resources.length;",
                "+        for (Map.Entry<String, Integer> entry : resourceNamesToArrayIndex.entrySet()) {",
                "+            int index = entry.getValue();",
                "+            if (index < length) {",
                "+                ret.put(entry.getKey(), resources[index]);",
                "+            }",
                "+        }",
                "+        return ret;",
                "+    }",
                "+",
                "+    public Map<String, Integer> getResourceNamesToArrayIndex() {",
                "+        return Collections.unmodifiableMap(resourceNamesToArrayIndex);",
                "+    }",
                "+",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceNameNormalizer.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceNameNormalizer.java",
                "new file mode 100644",
                "index 000000000..d59ba927d",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceNameNormalizer.java",
                "@@ -0,0 +1,65 @@",
                "+/*",
                "+ * Copyright 2018 The Apache Software Foundation.",
                "+ *",
                "+ * Licensed under the Apache License, Version 2.0 (the \"License\");",
                "+ * you may not use this file except in compliance with the License.",
                "+ * You may obtain a copy of the License at",
                "+ *",
                "+ *      http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource;",
                "+",
                "+import java.util.Collections;",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+import java.util.stream.Collectors;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.Constants;",
                "+",
                "+/**",
                "+ * Provides resource name normalization for resource maps.",
                "+ */",
                "+public class ResourceNameNormalizer {",
                "+",
                "+    private final Map<String, String> resourceNameMapping;",
                "+",
                "+    public ResourceNameNormalizer() {",
                "+        Map<String, String> tmp = new HashMap<>();",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, Constants.COMMON_CPU_RESOURCE_NAME);",
                "+        tmp.put(Config.SUPERVISOR_CPU_CAPACITY, Constants.COMMON_CPU_RESOURCE_NAME);",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "+        tmp.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME);",
                "+        resourceNameMapping = Collections.unmodifiableMap(tmp);",
                "+    }",
                "+",
                "+    /**",
                "+     * Normalizes a supervisor resource map or topology details map's keys to universal resource names.",
                "+     *",
                "+     * @param resourceMap resource map of either Supervisor or Topology",
                "+     * @return the resource map with common resource names",
                "+     */",
                "+    public Map<String, Double> normalizedResourceMap(Map<String, ? extends Number> resourceMap) {",
                "+        if (resourceMap == null) {",
                "+            return new HashMap<>();",
                "+        }",
                "+        return new HashMap<>(resourceMap.entrySet().stream()",
                "+            .collect(Collectors.toMap(",
                "+                //Map the key if needed",
                "+                (e) -> resourceNameMapping.getOrDefault(e.getKey(), e.getKey()),",
                "+                //Map the value",
                "+                (e) -> e.getValue().doubleValue())));",
                "+    }",
                "+",
                "+    public Map<String, String> getResourceNameMapping() {",
                "+        return resourceNameMapping;",
                "+    }",
                "+",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "index db9dbfabb..ca4ea6337 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "@@ -22,5 +22,3 @@ import java.util.HashMap;",
                " import java.util.Map;",
                "-",
                " import org.apache.storm.Config;",
                "-import org.apache.storm.Constants;",
                " import org.apache.storm.generated.Bolt;",
                "@@ -35,4 +33,2 @@ import org.slf4j.LoggerFactory;",
                "-import static org.apache.storm.scheduler.resource.NormalizedResources.normalizedResourceMap;",
                "-",
                " public class ResourceUtils {",
                "@@ -97,3 +93,4 @@ public class ResourceUtils {",
                "                     ComponentCommon spoutCommon = spoutSpec.get_common();",
                "-                    Map<String, Double> resourcesUpdate = normalizedResourceMap(resourceUpdatesMap.get(spoutName));",
                "+                    Map<String, Double> resourcesUpdate = NormalizedResources",
                "+                        .RESOURCE_NAME_NORMALIZER.normalizedResourceMap(resourceUpdatesMap.get(spoutName));",
                "                     String newJsonConf = getJsonWithUpdatedResources(spoutCommon.get_json_conf(), resourcesUpdate);",
                "@@ -112,3 +109,4 @@ public class ResourceUtils {",
                "                     ComponentCommon boltCommon = boltObj.get_common();",
                "-                    Map<String, Double> resourcesUpdate = normalizedResourceMap(resourceUpdatesMap.get(boltName));",
                "+                    Map<String, Double> resourcesUpdate = NormalizedResources",
                "+                        .RESOURCE_NAME_NORMALIZER.normalizedResourceMap(resourceUpdatesMap.get(boltName));",
                "                     String newJsonConf = getJsonWithUpdatedResources(boltCommon.get_json_conf(), resourceUpdatesMap.get(boltName));",
                "@@ -130,3 +128,3 @@ public class ResourceUtils {",
                "     public static String getCorrespondingLegacyResourceName(String normalizedResourceName) {",
                "-        for(Map.Entry<String, String> entry : NormalizedResources.RESOURCE_NAME_MAPPING.entrySet()) {",
                "+        for(Map.Entry<String, String> entry : NormalizedResources.RESOURCE_NAME_NORMALIZER.getResourceNameMapping().entrySet()) {",
                "             if (entry.getValue().equals(normalizedResourceName)) {",
                "@@ -150,3 +148,3 @@ public class ResourceUtils {",
                "             for (Map.Entry<String, Double> resourceUpdateEntry : resourceUpdates.entrySet()) {",
                "-                if (NormalizedResources.RESOURCE_NAME_MAPPING.containsValue(resourceUpdateEntry.getKey())) {",
                "+                if (NormalizedResources.RESOURCE_NAME_NORMALIZER.getResourceNameMapping().containsValue(resourceUpdateEntry.getKey())) {",
                "                     // if there will be legacy values they will be in the outer conf",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "index 67bc86743..73e1aa0dc 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "@@ -21,3 +21,2 @@ package org.apache.storm.scheduler.resource.strategies.scheduling;",
                " import com.google.common.annotations.VisibleForTesting;",
                "-",
                " import java.util.ArrayList;",
                "@@ -33,4 +32,2 @@ import java.util.Set;",
                " import java.util.TreeSet;",
                "-",
                "-import org.apache.storm.executor.Executor;",
                " import org.apache.storm.generated.ComponentType;",
                "@@ -41,6 +38,5 @@ import org.apache.storm.scheduler.TopologyDetails;",
                " import org.apache.storm.scheduler.WorkerSlot;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                " import org.apache.storm.scheduler.resource.RAS_Node;",
                " import org.apache.storm.scheduler.resource.RAS_Nodes;",
                "-import org.apache.storm.scheduler.resource.ResourceUtils;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                " import org.slf4j.Logger;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "index e0dc8818d..efa3ecfb4 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "@@ -25,3 +25,2 @@ import java.util.List;",
                " import java.util.TreeSet;",
                "-",
                " import org.apache.storm.Config;",
                "@@ -31,3 +30,2 @@ import org.apache.storm.scheduler.ExecutorDetails;",
                " import org.apache.storm.scheduler.TopologyDetails;",
                "-",
                " import org.apache.storm.scheduler.resource.SchedulingResult;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "index 108dc497a..2b0ca407e 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "@@ -23,5 +23,3 @@ import java.util.Collection;",
                " import java.util.HashSet;",
                "-import java.util.LinkedList;",
                " import java.util.List;",
                "-import java.util.Map;",
                " import java.util.TreeSet;",
                "@@ -32,3 +30,2 @@ import org.apache.storm.scheduler.ExecutorDetails;",
                " import org.apache.storm.scheduler.TopologyDetails;",
                "-import org.apache.storm.scheduler.resource.ResourceUtils;",
                " import org.apache.storm.scheduler.resource.SchedulingResult;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "index a64c9b4ed..6f3325140 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "@@ -27,4 +27,4 @@ import java.io.FileOutputStream;",
                " import java.io.FileWriter;",
                "-import java.io.InputStream;",
                " import java.io.IOException;",
                "+import java.io.InputStream;",
                " import java.io.OutputStream;",
                "@@ -32,4 +32,4 @@ import java.io.PrintStream;",
                " import java.io.RandomAccessFile;",
                "-import java.nio.file.Files;",
                " import java.nio.file.FileSystems;",
                "+import java.nio.file.Files;",
                " import java.nio.file.Paths;",
                "@@ -40,6 +40,6 @@ import java.util.Enumeration;",
                " import java.util.HashMap;",
                "-import java.util.jar.JarEntry;",
                "-import java.util.jar.JarFile;",
                " import java.util.List;",
                " import java.util.Map;",
                "+import java.util.jar.JarEntry;",
                "+import java.util.jar.JarFile;",
                " import java.util.zip.GZIPInputStream;",
                "@@ -48,3 +48,2 @@ import java.util.zip.ZipFile;",
                " import javax.security.auth.Subject;",
                "-",
                " import org.apache.commons.compress.archivers.tar.TarArchiveEntry;",
                "@@ -56,2 +55,4 @@ import org.apache.commons.io.IOUtils;",
                " import org.apache.commons.lang.StringUtils;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.DaemonConfig;",
                " import org.apache.storm.blobstore.BlobStore;",
                "@@ -62,6 +63,3 @@ import org.apache.storm.blobstore.LocalFsBlobStore;",
                " import org.apache.storm.blobstore.LocalModeClientBlobStore;",
                "-import org.apache.storm.Config;",
                "-import org.apache.storm.Constants;",
                " import org.apache.storm.daemon.StormCommon;",
                "-import org.apache.storm.DaemonConfig;",
                " import org.apache.storm.generated.AccessControl;",
                "@@ -75,4 +73,4 @@ import org.apache.storm.generated.StormTopology;",
                " import org.apache.storm.nimbus.NimbusInfo;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                " import org.apache.storm.scheduler.resource.ResourceUtils;",
                "+import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                " import org.apache.storm.security.auth.SingleUserPrincipal;"
            ],
            "changed_files": [
                "storm-server/pom.xml",
                "storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java",
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourcesWithMemory.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Nodes.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceMapArrayBridge.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceNameNormalizer.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/GenericResourceAwareStrategy.java",
                "storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                }
            ]
        },
        {
            "commit_id": "9636d0cdeb13683d43de90d9ba5904a494887c44",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514929415,
            "hunks": 2,
            "message": "Merge branch 'STORM-2856' of https://github.com/srdo/storm into STORM-2856 STORM-2856: Make Storm build work on post-2017Q4 Travis Trusty image This closes #2486",
            "diff": [
                "diff --git a/.travis.yml b/.travis.yml",
                "index eefc0720c..597cf87da 100644",
                "--- a/.travis.yml",
                "+++ b/.travis.yml",
                "@@ -28,5 +28,6 @@ jdk:",
                " before_install:",
                "-  - rvm use 2.1.5 --install",
                "-  - nvm install 0.12.2",
                "-  - nvm use 0.12.2",
                "+  - rvm reload",
                "+  - rvm use 2.4.2 --install",
                "+  - nvm install 8.9.3",
                "+  - nvm use 8.9.3",
                " install: /bin/bash ./dev-tools/travis/travis-install.sh `pwd`",
                "@@ -39,2 +40,2 @@ cache:",
                "     - \"$HOME/.rvm\"",
                "-    - \"$NVM_DIR\"",
                "+    - \"$NVM_DIR\"",
                "\\ No newline at end of file"
            ],
            "changed_files": [
                ".travis.yml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2856": ""
            },
            "ghissue_refs": {
                "2486": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2856",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2486",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6292f51d66fe987b9316932046643f6d2f061209",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516377109,
            "hunks": 2,
            "message": "STORM-2903: Fix possible NullPointerException in AbstractAutoCreds logs and doc cleanups",
            "diff": [
                "diff --git a/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java b/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java",
                "index 551f6b3f6..eb383f4f9 100644",
                "--- a/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java",
                "+++ b/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java",
                "@@ -217,5 +217,10 @@ public abstract class AbstractAutoCreds implements IAutoCredentials, ICredential",
                "                             try {",
                "+",
                "+                                if (token == null) {",
                "+                                    LOG.debug(\"Ignoring null token\");",
                "+                                    continue;",
                "+                                }",
                "+",
                "                                 LOG.debug(\"Current user: {}\", UserGroupInformation.getCurrentUser());",
                "-                                LOG.debug(\"Token from credential: {} / {}\", token.toString(),",
                "-                                        token.decodeIdentifier().getUser());",
                "+                                LOG.debug(\"Token from Credentials : {}\", token);"
            ],
            "changed_files": [
                "external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractAutoCreds.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2903": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "a0437f73edd4df320f60e11929cb40aef5ea35ad"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2903",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "c460369305f61bb1bc7aa5f20590f907ffd401dc",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510171375,
            "hunks": 2,
            "message": "STORM-2807: Shut down integration test topologies immediately after tests, retry killing each topology for a minute and throw an error if it can't be done",
            "diff": [
                "diff --git a/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java b/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java",
                "index a77836f86..7cadd1869 100644",
                "--- a/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java",
                "+++ b/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java",
                "@@ -57,5 +57,5 @@ public class TumblingTimeCorrectness implements TestableTopology {",
                "-    public TumblingTimeCorrectness(int timbleSec) {",
                "-        this.tumbleSec = timbleSec;",
                "-        final String prefix = this.getClass().getSimpleName() + \"-timbleSec\" + timbleSec;",
                "+    public TumblingTimeCorrectness(int tumbleSec) {",
                "+        this.tumbleSec = tumbleSec;",
                "+        final String prefix = this.getClass().getSimpleName() + \"-tumbleSec\" + tumbleSec;",
                "         spoutName = prefix + \"IncrementingSpout\";",
                "diff --git a/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java b/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java",
                "index 22c6d7564..adcb9ddbf 100644",
                "--- a/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java",
                "+++ b/integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java",
                "@@ -56,3 +56,3 @@ public class TumblingWindowCorrectness implements TestableTopology {",
                "         this.tumbleSize = tumbleSize;",
                "-        final String prefix = this.getClass().getSimpleName() + \"-tubleSize\" + tumbleSize;",
                "+        final String prefix = this.getClass().getSimpleName() + \"-tumbleSize\" + tumbleSize;",
                "         spoutName = prefix + \"IncrementingSpout\";"
            ],
            "changed_files": [
                "integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingTimeCorrectness.java",
                "integration-test/src/main/java/org/apache/storm/st/topology/window/TumblingWindowCorrectness.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2807": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "e672295c905fa772cb16c18ed3c10007c7678a9a"
                ],
                [
                    "no-tag",
                    "3cb52398a3242ce6d1e01113aefee1402bd034a8"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2807",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8e42652ef40aede81c05c6e8a83da11889f9f54b",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512930134,
            "hunks": 18,
            "message": "STORM-2851: Fix ConcurrentModificationException in doSeekRetriablePartitions",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 9d12307e8..030735bb0 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -36,2 +36,3 @@ import java.util.Set;",
                " import java.util.concurrent.TimeUnit;",
                "+import java.util.stream.Collectors;",
                " import org.apache.commons.lang.Validate;",
                "@@ -231,3 +232,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-                if (commit()) {",
                "+                if (shouldCommit()) {",
                "                     commitOffsetsForAckedTuples();",
                "@@ -235,6 +236,6 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-                Set<TopicPartition> pollablePartitions = poll();",
                "-                if (!pollablePartitions.isEmpty()) {",
                "+                PollablePartitionsInfo pollablePartitionsInfo = getPollablePartitionsInfo();",
                "+                if (pollablePartitionsInfo.shouldPoll()) {",
                "                     try {",
                "-                        setWaitingToEmit(pollKafkaBroker(pollablePartitions));",
                "+                        setWaitingToEmit(pollKafkaBroker(pollablePartitionsInfo));",
                "                     } catch (RetriableException e) {",
                "@@ -259,19 +260,20 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    private boolean commit() {",
                "+    private boolean shouldCommit() {",
                "         return isAtLeastOnceProcessing() && commitTimer.isExpiredResetOnTrue();    // timer != null for non auto commit mode",
                "     }",
                "-",
                "-    private Set<TopicPartition> poll() {",
                "-        final int maxUncommittedOffsets = kafkaSpoutConfig.getMaxUncommittedOffsets();",
                "-",
                "+    ",
                "+    private PollablePartitionsInfo getPollablePartitionsInfo() {",
                "         if (isWaitingToEmit()) {",
                "             LOG.debug(\"Not polling. Tuples waiting to be emitted.\");",
                "-            return Collections.emptySet();",
                "+            return new PollablePartitionsInfo(Collections.emptySet(), Collections.emptyMap());",
                "         }",
                "+        ",
                "         Set<TopicPartition> assignment = kafkaConsumer.assignment();",
                "         if (!isAtLeastOnceProcessing()) {",
                "-            return assignment;",
                "+            return new PollablePartitionsInfo(assignment, Collections.emptyMap());",
                "         }",
                "+        ",
                "         Map<TopicPartition, Long> earliestRetriableOffsets = retryService.earliestRetriableOffsets();",
                "         Set<TopicPartition> pollablePartitions = new HashSet<>();",
                "+        final int maxUncommittedOffsets = kafkaSpoutConfig.getMaxUncommittedOffsets();",
                "         for (TopicPartition tp : assignment) {",
                "@@ -294,3 +296,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         }",
                "-        return pollablePartitions;",
                "+        return new PollablePartitionsInfo(pollablePartitions, earliestRetriableOffsets);",
                "     }",
                "@@ -310,6 +312,6 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     // ======== poll =========",
                "-    private ConsumerRecords<K, V> pollKafkaBroker(Set<TopicPartition> pollablePartitions) {",
                "-        final Map<TopicPartition, Long> retriableOffsets = doSeekRetriableTopicPartitions(pollablePartitions);",
                "+    private ConsumerRecords<K, V> pollKafkaBroker(PollablePartitionsInfo pollablePartitionsInfo) {",
                "+        doSeekRetriableTopicPartitions(pollablePartitionsInfo.pollableEarliestRetriableOffsets);",
                "         Set<TopicPartition> pausedPartitions = new HashSet<>(kafkaConsumer.assignment());",
                "-        pausedPartitions.removeIf(pollablePartitions::contains);",
                "+        pausedPartitions.removeIf(pollablePartitionsInfo.pollablePartitions::contains);",
                "         try {",
                "@@ -317,3 +319,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             final ConsumerRecords<K, V> consumerRecords = kafkaConsumer.poll(kafkaSpoutConfig.getPollTimeoutMs());",
                "-            ackRetriableOffsetsIfCompactedAway(retriableOffsets, consumerRecords);",
                "+            ackRetriableOffsetsIfCompactedAway(pollablePartitionsInfo.pollableEarliestRetriableOffsets, consumerRecords);",
                "             final int numPolledRecords = consumerRecords.count();",
                "@@ -331,10 +333,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    private Map<TopicPartition, Long> doSeekRetriableTopicPartitions(Set<TopicPartition> pollablePartitions) {",
                "-        final Map<TopicPartition, Long> retriableTopicPartitions = retryService.earliestRetriableOffsets();",
                "-        for (TopicPartition tp : retriableTopicPartitions.keySet()) {",
                "-            if (!pollablePartitions.contains(tp)) {",
                "-                retriableTopicPartitions.remove(tp);",
                "-            }",
                "-        }",
                "-        for (Entry<TopicPartition, Long> retriableTopicPartitionAndOffset : retriableTopicPartitions.entrySet()) {",
                "+    private void doSeekRetriableTopicPartitions(Map<TopicPartition, Long> pollableEarliestRetriableOffsets) {",
                "+        for (Entry<TopicPartition, Long> retriableTopicPartitionAndOffset : pollableEarliestRetriableOffsets.entrySet()) {",
                "             //Seek directly to the earliest retriable message for each retriable topic partition",
                "@@ -342,3 +338,2 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         }",
                "-        return retriableTopicPartitions;",
                "     }",
                "@@ -627,2 +622,19 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     }",
                "+    ",
                "+    private static class PollablePartitionsInfo {",
                "+        private final Set<TopicPartition> pollablePartitions;",
                "+        //The subset of earliest retriable offsets that are on pollable partitions",
                "+        private final Map<TopicPartition, Long> pollableEarliestRetriableOffsets;",
                "+        ",
                "+        public PollablePartitionsInfo(Set<TopicPartition> pollablePartitions, Map<TopicPartition, Long> earliestRetriableOffsets) {",
                "+            this.pollablePartitions = pollablePartitions;",
                "+            this.pollableEarliestRetriableOffsets = earliestRetriableOffsets.entrySet().stream()",
                "+                .filter(entry -> pollablePartitions.contains(entry.getKey()))",
                "+                .collect(Collectors.toMap(entry -> entry.getKey(), entry -> entry.getValue()));",
                "+        }",
                "+        ",
                "+        public boolean shouldPoll() {",
                "+            return !this.pollablePartitions.isEmpty();",
                "+        }",
                "+    }",
                " }"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2851": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "9913d81e705a69ac329c0565b99106eb96f7d4c0"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2851",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "06599b6ab6311446c68dea6f35aab16e529e164f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508211648,
            "hunks": 1,
            "message": "STORM-2779 NPE on shutting down WindowedBoltExecutor * waterMarkEventGenerator could be null when timestamp field is not specified",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java b/storm-core/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java",
                "index 26c69a263..c9afc671d 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java",
                "@@ -301,3 +301,5 @@ public class WindowedBoltExecutor implements IRichBolt {",
                "     public void cleanup() {",
                "-        waterMarkEventGenerator.shutdown();",
                "+        if (waterMarkEventGenerator != null) {",
                "+            waterMarkEventGenerator.shutdown();",
                "+        }",
                "         windowManager.shutdown();"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2779": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "e69fbc51ada2fd542d717bbd7623c39f4f6a1cb8"
                ],
                [
                    "no-tag",
                    "40384cf5f21337cc496660c1bf0c59ad41d30788"
                ]
            ],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2779",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "78e7077861e2f43b0d846665b1569e3c565fc8fa",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513234385,
            "hunks": 24,
            "message": "STORM-2854 Expose IEventLogger to make event logging pluggable * expose option to register IEventLogger similar to metrics consumer * change the interface of IEventLogger slightly   * allow argument   * the change is technically not backward compatible but in real we can treat it's OK     * cause we don't provide a chance to implement custom IEventLogger and plug to topology * Fix EventInfo to contain origin type of values instead of String-converted values * open possibility to extend FileBasedEventLogger and provide different format of log message * document the change * address review comments",
            "diff": [
                "diff --git a/conf/storm.yaml.example b/conf/storm.yaml.example",
                "index 8d54e1946..911ad07ba 100644",
                "--- a/conf/storm.yaml.example",
                "+++ b/conf/storm.yaml.example",
                "@@ -74,2 +74,9 @@",
                " #",
                "-# storm.cluster.metrics.consumer.publish.interval.secs: 60",
                "\\ No newline at end of file",
                "+# storm.cluster.metrics.consumer.publish.interval.secs: 60",
                "+",
                "+# Event Logger",
                "+# topology.event.logger.register:",
                "+#   - class: \"org.apache.storm.metric.FileBasedEventLogger\"",
                "+#   - class: \"org.mycompany.MyEventLogger\"",
                "+#     arguments:",
                "+#       endpoint: \"event-logger.mycompany.org\"",
                "\\ No newline at end of file",
                "diff --git a/storm-core/src/jvm/org/apache/storm/Config.java b/storm-core/src/jvm/org/apache/storm/Config.java",
                "index d50fe3479..6b0c86865 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/Config.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/Config.java",
                "@@ -22,2 +22,3 @@ import org.apache.storm.scheduler.resource.strategies.priority.ISchedulingPriori",
                " import org.apache.storm.scheduler.resource.strategies.scheduling.IStrategy;",
                "+import org.apache.storm.metric.IEventLogger;",
                " import org.apache.storm.serialization.IKryoDecorator;",
                "@@ -326,3 +327,2 @@ public class Config extends HashMap<String, Object> {",
                "      * the need for native dependencies, which can be difficult to install.",
                "-     *",
                "      * Defaults to false.",
                "@@ -1705,2 +1705,14 @@ public class Config extends HashMap<String, Object> {",
                "+    /**",
                "+     * A list of classes implementing IEventLogger (See storm.yaml.example for exact config format).",
                "+     * Each listed class will be routed all the events sampled from emitting tuples.",
                "+     * If there's no class provided to the option, default event logger will be initialized and used",
                "+     * unless you disable event logger executor.",
                "+     *",
                "+     * Note that EventLoggerBolt takes care of all the implementations of IEventLogger, hence registering",
                "+     * many implementations (especially they're implemented as 'blocking' manner) would slow down overall topology.",
                "+     */",
                "+    @isListEntryCustom(entryValidatorClasses={EventLoggerRegistryValidator.class})",
                "+    public static final String TOPOLOGY_EVENT_LOGGER_REGISTER = \"topology.event.logger.register\";",
                "+",
                "     /**",
                "@@ -2364,2 +2376,28 @@ public class Config extends HashMap<String, Object> {",
                "+    public void registerEventLogger(Class<? extends IEventLogger> klass, Map<String, Object> argument) {",
                "+        registerEventLogger(this, klass, argument);",
                "+    }",
                "+",
                "+    public void registerEventLogger(Class<? extends IEventLogger> klass) {",
                "+        registerEventLogger(this, klass, null);",
                "+    }",
                "+",
                "+    public static void registerEventLogger(Map<String, Object> conf, Class<? extends IEventLogger> klass, Map<String, Object> argument) {",
                "+        Map<String, Object> m = new HashMap<>();",
                "+        m.put(\"class\", klass.getCanonicalName());",
                "+        m.put(\"arguments\", argument);",
                "+",
                "+        List<Map<String, Object>> l = (List<Map<String, Object>>)conf.get(TOPOLOGY_EVENT_LOGGER_REGISTER);",
                "+        if (l == null) {",
                "+            l = new ArrayList<>();",
                "+        }",
                "+        l.add(m);",
                "+",
                "+        conf.put(TOPOLOGY_EVENT_LOGGER_REGISTER, l);",
                "+    }",
                "+",
                "+    public static void registerEventLogger(Map<String, Object> conf, Class<? extends IEventLogger> klass) {",
                "+        registerEventLogger(conf, klass, null);",
                "+    }",
                "+",
                "     public static void registerMetricsConsumer(Map conf, Class klass, Object argument, long parallelismHint) {",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metric/EventLoggerBolt.java b/storm-core/src/jvm/org/apache/storm/metric/EventLoggerBolt.java",
                "index 2950e080a..03eb9df3a 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metric/EventLoggerBolt.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metric/EventLoggerBolt.java",
                "@@ -17,4 +17,12 @@",
                "  */",
                "+",
                " package org.apache.storm.metric;",
                "+import static org.apache.storm.metric.IEventLogger.EventInfo;",
                "+",
                "+import java.util.ArrayList;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+",
                "+import org.apache.storm.Config;",
                " import org.apache.storm.task.IBolt;",
                "@@ -26,5 +34,2 @@ import org.slf4j.LoggerFactory;",
                "-import java.util.Map;",
                "-import static org.apache.storm.metric.IEventLogger.EventInfo;",
                "-",
                " public class EventLoggerBolt implements IBolt {",
                "@@ -40,4 +45,6 @@ public class EventLoggerBolt implements IBolt {",
                "     public static final String FIELD_MESSAGE_ID = \"message-id\";",
                "+    public static final String TOPOLOGY_EVENT_LOGGER_CLASS = \"class\";",
                "+    public static final String TOPOLOGY_EVENT_LOGGER_ARGUMENTS = \"arguments\";",
                "-    private IEventLogger eventLogger;",
                "+    private List<IEventLogger> eventLoggers;",
                "@@ -46,4 +53,9 @@ public class EventLoggerBolt implements IBolt {",
                "         LOG.info(\"EventLoggerBolt prepare called\");",
                "-        eventLogger = new FileBasedEventLogger();",
                "-        eventLogger.prepare(stormConf, context);",
                "+        eventLoggers = new ArrayList<>();",
                "+        List<Map<String, Object>> registerInfo = (List<Map<String, Object>>) stormConf.get(Config.TOPOLOGY_EVENT_LOGGER_REGISTER);",
                "+        if (registerInfo != null && !registerInfo.isEmpty()) {",
                "+            initializeEventLoggers(stormConf, context, registerInfo);",
                "+        } else {",
                "+            initializeDefaultEventLogger(stormConf, context);",
                "+        }",
                "     }",
                "@@ -55,7 +67,8 @@ public class EventLoggerBolt implements IBolt {",
                "         Object msgId = input.getValueByField(FIELD_MESSAGE_ID);",
                "-        EventInfo eventInfo = new EventInfo(input.getValueByField(FIELD_TS).toString(), input.getSourceComponent(),",
                "-                                            String.valueOf(input.getSourceTask()), msgId == null ? \"\" : msgId.toString(),",
                "-                                            input.getValueByField(FIELD_VALUES).toString());",
                "+        EventInfo eventInfo = new EventInfo(input.getLongByField(FIELD_TS), input.getSourceComponent(),",
                "+                                            input.getSourceTask(), msgId, (List<Object>) input.getValueByField(FIELD_VALUES));",
                "-        eventLogger.log(eventInfo);",
                "+        for (IEventLogger eventLogger : eventLoggers) {",
                "+            eventLogger.log(eventInfo);",
                "+        }",
                "     }",
                "@@ -64,3 +77,29 @@ public class EventLoggerBolt implements IBolt {",
                "     public void cleanup() {",
                "-        eventLogger.close();",
                "+        for (IEventLogger eventLogger : eventLoggers) {",
                "+            eventLogger.close();",
                "+        }",
                "+    }",
                "+",
                "+    private void initializeEventLoggers(Map<String, Object> topoConf, TopologyContext context, List<Map<String, Object>> registerInfo) {",
                "+        for (Map<String, Object> info : registerInfo) {",
                "+            String className = (String) info.get(TOPOLOGY_EVENT_LOGGER_CLASS);",
                "+            Map<String, Object> argument = (Map<String, Object>) info.get(TOPOLOGY_EVENT_LOGGER_ARGUMENTS);",
                "+",
                "+            IEventLogger eventLogger;",
                "+            try {",
                "+                eventLogger = (IEventLogger) Class.forName(className).newInstance();",
                "+            } catch (Exception e) {",
                "+                throw new RuntimeException(\"Could not instantiate a class listed in config under section \"",
                "+                        + Config.TOPOLOGY_EVENT_LOGGER_REGISTER + \" with fully qualified name \" + className, e);",
                "+            }",
                "+",
                "+            eventLogger.prepare(topoConf, argument, context);",
                "+            eventLoggers.add(eventLogger);",
                "+        }",
                "+    }",
                "+",
                "+    private void initializeDefaultEventLogger(Map<String, Object> topoConf, TopologyContext context) {",
                "+        FileBasedEventLogger eventLogger = new FileBasedEventLogger();",
                "+        eventLogger.prepare(topoConf, null, context);",
                "+        eventLoggers.add(eventLogger);",
                "     }",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metric/FileBasedEventLogger.java b/storm-core/src/jvm/org/apache/storm/metric/FileBasedEventLogger.java",
                "index 6a07c0ece..7697bf4d3 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metric/FileBasedEventLogger.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metric/FileBasedEventLogger.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.metric;",
                "@@ -66,3 +67,3 @@ public class FileBasedEventLogger implements IEventLogger {",
                "                 try {",
                "-                    if(dirty) {",
                "+                    if (dirty) {",
                "                         eventLogWriter.flush();",
                "@@ -103,3 +104,3 @@ public class FileBasedEventLogger implements IEventLogger {",
                "     @Override",
                "-    public void prepare(Map stormConf, TopologyContext context) {",
                "+    public void prepare(Map<String, Object> stormConf, Map<String, Object> arguments, TopologyContext context) {",
                "         String workersArtifactDir; // workers artifact directory",
                "@@ -131,3 +132,3 @@ public class FileBasedEventLogger implements IEventLogger {",
                "             //TODO: file rotation",
                "-            eventLogWriter.write(event.toString());",
                "+            eventLogWriter.write(buildLogMessage(event));",
                "             eventLogWriter.newLine();",
                "@@ -140,2 +141,6 @@ public class FileBasedEventLogger implements IEventLogger {",
                "+    protected String buildLogMessage(EventInfo event) {",
                "+        return event.toString();",
                "+    }",
                "+",
                "     @Override",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metric/IEventLogger.java b/storm-core/src/jvm/org/apache/storm/metric/IEventLogger.java",
                "index ce8160237..e310d164c 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metric/IEventLogger.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metric/IEventLogger.java",
                "@@ -22,2 +22,3 @@ import org.apache.storm.task.TopologyContext;",
                " import java.util.Date;",
                "+import java.util.List;",
                " import java.util.Map;",
                "@@ -33,9 +34,10 @@ public interface IEventLogger {",
                "      */",
                "-    public static class EventInfo {",
                "-        String ts;",
                "-        String component;",
                "-        String task;",
                "-        String messageId;",
                "-        String values;",
                "-        EventInfo(String ts, String component, String task, String messageId, String values) {",
                "+    class EventInfo {",
                "+        private long ts;",
                "+        private String component;",
                "+        private int task;",
                "+        private Object messageId;",
                "+        private List<Object> values;",
                "+",
                "+        public EventInfo(long ts, String component, int task, Object messageId, List<Object> values) {",
                "             this.ts = ts;",
                "@@ -47,2 +49,22 @@ public interface IEventLogger {",
                "+        public long getTs() {",
                "+            return ts;",
                "+        }",
                "+",
                "+        public String getComponent() {",
                "+            return component;",
                "+        }",
                "+",
                "+        public int getTask() {",
                "+            return task;",
                "+        }",
                "+",
                "+        public Object getMessageId() {",
                "+            return messageId;",
                "+        }",
                "+",
                "+        public List<Object> getValues() {",
                "+            return values;",
                "+        }",
                "+",
                "         /**",
                "@@ -54,3 +76,4 @@ public interface IEventLogger {",
                "         public String toString() {",
                "-            return new Date(Long.parseLong(ts)).toString() + \",\" + component + \",\" + task + \",\" + messageId + \",\" + values;",
                "+            return new Date(ts).toString() + \",\" + component + \",\" + String.valueOf(task) + \",\"",
                "+                    + (messageId == null ? \"\" : messageId.toString()) + \",\" + values.toString();",
                "         }",
                "@@ -58,3 +81,3 @@ public interface IEventLogger {",
                "-    void prepare(Map stormConf, TopologyContext context);",
                "+    void prepare(Map<String, Object> conf, Map<String, Object> arguments, TopologyContext context);",
                "diff --git a/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java b/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "index fb220dd6e..0602dbf47 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "@@ -495,2 +495,22 @@ public class ConfigValidation {",
                "+    public static class EventLoggerRegistryValidator extends Validator {",
                "+",
                "+        @Override",
                "+        public void validateField(String name, Object o) {",
                "+            if(o == null) {",
                "+                return;",
                "+            }",
                "+            SimpleTypeValidator.validateField(name, Map.class, o);",
                "+            if(!((Map<?, ?>) o).containsKey(\"class\") ) {",
                "+                throw new IllegalArgumentException( \"Field \" + name + \" must have map entry with key: class\");",
                "+            }",
                "+",
                "+            SimpleTypeValidator.validateField(name, String.class, ((Map<?, ?>) o).get(\"class\"));",
                "+",
                "+            if(((Map<?, ?>) o).containsKey(\"arguments\") ) {",
                "+                SimpleTypeValidator.validateField(name, Map.class, ((Map<?, ?>) o).get(\"arguments\"));",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "     public static class MapOfStringToMapOfStringToObjectValidator extends Validator {"
            ],
            "changed_files": [
                "conf/storm.yaml.example",
                "storm-core/src/jvm/org/apache/storm/Config.java",
                "storm-core/src/jvm/org/apache/storm/metric/EventLoggerBolt.java",
                "storm-core/src/jvm/org/apache/storm/metric/FileBasedEventLogger.java",
                "storm-core/src/jvm/org/apache/storm/metric/IEventLogger.java",
                "storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2854": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "6bf3094c60f0af665f0129bb9f68c0ec7836b05f"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2854",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "39e12aa221edc3e815c52622258f8d3ceb3a2891",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507834110,
            "hunks": 2,
            "message": "STORM-2775 Update kafkaPartition metrics to match the same format as kafkaOffset STORM-2775 Send kafkaPartition metrics in both the \"old\" and \"new\" format for backwards compatibility STORM-2775 Update kafkaPartition metrics to match the same format as kafkaOffset",
            "diff": [
                "diff --git a/external/storm-kafka/src/jvm/org/apache/storm/kafka/PartitionManager.java b/external/storm-kafka/src/jvm/org/apache/storm/kafka/PartitionManager.java",
                "index 5420887fd..c025e3b3f 100644",
                "--- a/external/storm-kafka/src/jvm/org/apache/storm/kafka/PartitionManager.java",
                "+++ b/external/storm-kafka/src/jvm/org/apache/storm/kafka/PartitionManager.java",
                "@@ -168,9 +168,18 @@ public class PartitionManager {",
                "     public Map getMetricsDataMap() {",
                "+        String[] metricPrefixes = new String[] {",
                "+            _partition.getId(),     // Correct metric prefix, see STORM-2775",
                "+            _partition.toString()   // Old prefix, kept for backwards compatibility",
                "+        };",
                "+",
                "         Map<String, Object> ret = new HashMap<>();",
                "-        ret.put(_partition + \"/fetchAPILatencyMax\", _fetchAPILatencyMax.getValueAndReset());",
                "-        ret.put(_partition + \"/fetchAPILatencyMean\", _fetchAPILatencyMean.getValueAndReset());",
                "-        ret.put(_partition + \"/fetchAPICallCount\", _fetchAPICallCount.getValueAndReset());",
                "-        ret.put(_partition + \"/fetchAPIMessageCount\", _fetchAPIMessageCount.getValueAndReset());",
                "-        ret.put(_partition + \"/lostMessageCount\", _lostMessageCount.getValueAndReset());",
                "-        ret.put(_partition + \"/messageIneligibleForRetryCount\", _messageIneligibleForRetryCount.getValueAndReset());",
                "+",
                "+        for (String metricPrefix : metricPrefixes) {",
                "+            ret.put(metricPrefix + \"/fetchAPILatencyMax\", _fetchAPILatencyMax.getValueAndReset());",
                "+            ret.put(metricPrefix + \"/fetchAPILatencyMean\", _fetchAPILatencyMean.getValueAndReset());",
                "+            ret.put(metricPrefix + \"/fetchAPICallCount\", _fetchAPICallCount.getValueAndReset());",
                "+            ret.put(metricPrefix + \"/fetchAPIMessageCount\", _fetchAPIMessageCount.getValueAndReset());",
                "+            ret.put(metricPrefix + \"/lostMessageCount\", _lostMessageCount.getValueAndReset());",
                "+            ret.put(metricPrefix + \"/messageIneligibleForRetryCount\", _messageIneligibleForRetryCount.getValueAndReset());",
                "+        }",
                "+",
                "         return ret;"
            ],
            "changed_files": [
                "external/storm-kafka/src/jvm/org/apache/storm/kafka/PartitionManager.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2775": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "c1504b18dd5337597ddf512da36050f35ce85930"
                ],
                [
                    "no-tag",
                    "f9747ef0d74db9567a5f2d73560c09f344c94d7e"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2775",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8363c40dc96f820440506804f692792f3d7fbdd9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515164769,
            "hunks": 7,
            "message": "Merge branch 'STORM-2876' of https://github.com/revans2/incubator-storm into STORM-2876 STORM-2876: Work around memory leak, and try to speed up tests This closes #2500",
            "diff": [
                "diff --git a/external/storm-hdfs/pom.xml b/external/storm-hdfs/pom.xml",
                "index 281499602..53572d565 100644",
                "--- a/external/storm-hdfs/pom.xml",
                "+++ b/external/storm-hdfs/pom.xml",
                "@@ -224,2 +224,10 @@",
                "         <plugins>",
                "+            <plugin>",
                "+                <groupId>org.apache.maven.plugins</groupId>",
                "+                <artifactId>maven-surefire-plugin</artifactId>",
                "+\t\t<configuration>",
                "+                    <reuseForks>false</reuseForks>",
                "+                    <forkCount>1</forkCount>",
                "+                </configuration>",
                "+            </plugin>",
                "             <plugin>",
                "diff --git a/external/storm-hive/pom.xml b/external/storm-hive/pom.xml",
                "index 78751d769..b7dbd942f 100644",
                "--- a/external/storm-hive/pom.xml",
                "+++ b/external/storm-hive/pom.xml",
                "@@ -168,2 +168,10 @@",
                "     <plugins>",
                "+      <plugin>",
                "+        <groupId>org.apache.maven.plugins</groupId>",
                "+        <artifactId>maven-surefire-plugin</artifactId>",
                "+        <configuration>",
                "+          <reuseForks>true</reuseForks>",
                "+          <forkCount>1</forkCount>",
                "+        </configuration>",
                "+      </plugin>",
                "       <plugin>",
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index 0f5b2f9f8..51bb79771 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -141,2 +141,10 @@",
                "         <plugins>",
                "+            <plugin>",
                "+                <groupId>org.apache.maven.plugins</groupId>",
                "+                <artifactId>maven-surefire-plugin</artifactId>",
                "+                <configuration>",
                "+                    <reuseForks>true</reuseForks>",
                "+                    <forkCount>1</forkCount>",
                "+                </configuration>",
                "+            </plugin>",
                "             <plugin>",
                "diff --git a/external/storm-mqtt/pom.xml b/external/storm-mqtt/pom.xml",
                "index 5546a97a6..5754b0b7a 100644",
                "--- a/external/storm-mqtt/pom.xml",
                "+++ b/external/storm-mqtt/pom.xml",
                "@@ -88,3 +88,2 @@",
                "                 <configuration>",
                "-                    <forkMode>perTest</forkMode>",
                "                     <enableAssertions>false</enableAssertions>",
                "@@ -103,3 +102,2 @@",
                "                 <configuration>",
                "-                    <forkMode>perTest</forkMode>",
                "                     <enableAssertions>false</enableAssertions>",
                "diff --git a/integration-test/pom.xml b/integration-test/pom.xml",
                "index bcf688d82..55c9d0e75 100755",
                "--- a/integration-test/pom.xml",
                "+++ b/integration-test/pom.xml",
                "@@ -166,2 +166,4 @@",
                "                     </properties>",
                "+                    <reuseForks>true</reuseForks>",
                "+                    <forkCount>1</forkCount>",
                "                     <systemPropertyVariables>",
                "diff --git a/pom.xml b/pom.xml",
                "index 24027163e..65dfdbf32 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -1035,2 +1035,4 @@",
                "                         <argLine>-Xmx1536m</argLine>",
                "+                        <forkCount>1.0C</forkCount>",
                "+                        <reuseForks>true</reuseForks>",
                "                     </configuration>"
            ],
            "changed_files": [
                "external/storm-hdfs/pom.xml",
                "external/storm-hive/pom.xml",
                "external/storm-kafka-client/pom.xml",
                "external/storm-mqtt/pom.xml",
                "integration-test/pom.xml",
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2876": ""
            },
            "ghissue_refs": {
                "2500": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2876",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2500",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1496d73b3ff21f40d678e2c38956d1d858e8c124",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515164769,
            "hunks": 7,
            "message": "Merge branch 'STORM-2876' of https://github.com/revans2/incubator-storm into STORM-2876 STORM-2876: Work around memory leak, and try to speed up tests This closes #2500",
            "diff": [
                "diff --git a/external/storm-hdfs/pom.xml b/external/storm-hdfs/pom.xml",
                "index 543599174..321569909 100644",
                "--- a/external/storm-hdfs/pom.xml",
                "+++ b/external/storm-hdfs/pom.xml",
                "@@ -219,2 +219,10 @@",
                "         <plugins>",
                "+            <plugin>",
                "+                <groupId>org.apache.maven.plugins</groupId>",
                "+                <artifactId>maven-surefire-plugin</artifactId>",
                "+\t\t<configuration>",
                "+                    <reuseForks>false</reuseForks>",
                "+                    <forkCount>1</forkCount>",
                "+                </configuration>",
                "+            </plugin>",
                "             <plugin>",
                "diff --git a/external/storm-hive/pom.xml b/external/storm-hive/pom.xml",
                "index 858c9f800..706e8f843 100644",
                "--- a/external/storm-hive/pom.xml",
                "+++ b/external/storm-hive/pom.xml",
                "@@ -157,2 +157,10 @@",
                "     <plugins>",
                "+      <plugin>",
                "+        <groupId>org.apache.maven.plugins</groupId>",
                "+        <artifactId>maven-surefire-plugin</artifactId>",
                "+        <configuration>",
                "+          <reuseForks>true</reuseForks>",
                "+          <forkCount>1</forkCount>",
                "+        </configuration>",
                "+      </plugin>",
                "       <plugin>",
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index a854f43ab..304e703ef 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -133,2 +133,10 @@",
                "         <plugins>",
                "+            <plugin>",
                "+                <groupId>org.apache.maven.plugins</groupId>",
                "+                <artifactId>maven-surefire-plugin</artifactId>",
                "+                <configuration>",
                "+                    <reuseForks>true</reuseForks>",
                "+                    <forkCount>1</forkCount>",
                "+                </configuration>",
                "+            </plugin>",
                "             <plugin>",
                "diff --git a/external/storm-mqtt/pom.xml b/external/storm-mqtt/pom.xml",
                "index bf006a2f3..9a3056c58 100644",
                "--- a/external/storm-mqtt/pom.xml",
                "+++ b/external/storm-mqtt/pom.xml",
                "@@ -88,3 +88,2 @@",
                "                 <configuration>",
                "-                    <forkMode>perTest</forkMode>",
                "                     <enableAssertions>false</enableAssertions>",
                "@@ -103,3 +102,2 @@",
                "                 <configuration>",
                "-                    <forkMode>perTest</forkMode>",
                "                     <enableAssertions>false</enableAssertions>",
                "diff --git a/integration-test/pom.xml b/integration-test/pom.xml",
                "index bcf688d82..55c9d0e75 100755",
                "--- a/integration-test/pom.xml",
                "+++ b/integration-test/pom.xml",
                "@@ -166,2 +166,4 @@",
                "                     </properties>",
                "+                    <reuseForks>true</reuseForks>",
                "+                    <forkCount>1</forkCount>",
                "                     <systemPropertyVariables>",
                "diff --git a/pom.xml b/pom.xml",
                "index 0fefe0690..d7fd43b2e 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -1034,2 +1034,4 @@",
                "                         <argLine>-Xmx1536m</argLine>",
                "+                        <forkCount>1.0C</forkCount>",
                "+                        <reuseForks>true</reuseForks>",
                "                     </configuration>"
            ],
            "changed_files": [
                "external/storm-hdfs/pom.xml",
                "external/storm-hive/pom.xml",
                "external/storm-kafka-client/pom.xml",
                "external/storm-mqtt/pom.xml",
                "integration-test/pom.xml",
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2876": ""
            },
            "ghissue_refs": {
                "2500": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2876",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2500",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "37403d17ddf4f762fa61aac9feb93143ae854645",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508161736,
            "hunks": 0,
            "message": "Merge branch 'branch_4' of https://github.com/httfighter/storm into STORM-2772 STORM-2772: In the DRPCSpout class, when the fetch from the DRPC server fails,the log should return to get the DRPC request failed instead of getting the DRPC result failed This closes #2364",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2772": ""
            },
            "ghissue_refs": {
                "2364": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2772",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2364",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "43a8429549e42c1123ba3ffcea68f2e2de2efa2f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507909520,
            "hunks": 5,
            "message": "Merge branch 'STORM-2771' of https://github.com/revans2/incubator-storm into STORM-2771 STORM-2771: By default don't run any tests as integration tests This closes #2368",
            "diff": [
                "diff --git a/pom.xml b/pom.xml",
                "index 759050034..c979a3356 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -283,2 +283,3 @@",
                "         <java.unit.test.include>**/Test*.java, **/*Test.java, **/*TestCase.java</java.unit.test.include>    <!--maven surefire plugin default test list-->",
                "+        <java.integration.test.include>no.tests</java.integration.test.include>",
                "         <!-- by default the clojure test set are all clojure tests that are not integration tests. This property is overridden in the profiles -->",
                "@@ -554,3 +555,2 @@",
                "                 <java.integration.test.include>**/*.java</java.integration.test.include>",
                "-                <java.integration.test.group>org.apache.storm.testing.IntegrationTest</java.integration.test.group>",
                "                 <clojure.test.set>*.*</clojure.test.set>",
                "@@ -564,3 +564,2 @@",
                "                 <java.integration.test.include>**/*.java</java.integration.test.include>",
                "-                <java.integration.test.group>org.apache.storm.testing.IntegrationTest</java.integration.test.group>",
                "                 <!--Clojure-->",
                "@@ -1033,2 +1032,3 @@",
                "                     <configuration>",
                "+                        <redirectTestOutputToFile>true</redirectTestOutputToFile>",
                "                         <includes>",
                "@@ -1036,3 +1036,3 @@",
                "                         </includes>",
                "-                        <groups>${java.integration.test.group}</groups>  <!--set in integration-test the profile-->",
                "+                        <groups>org.apache.storm.testing.IntegrationTest</groups>",
                "                         <argLine>-Xmx1536m</argLine>"
            ],
            "changed_files": [
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2771": ""
            },
            "ghissue_refs": {
                "2368": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2771",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2368",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "88533346dbfb318d8bf623703f6a16a1eab19534",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515345175,
            "hunks": 18,
            "message": "Move some resource normalization classes to a new package since the resource package was getting crowded",
            "diff": [
                "diff --git a/storm-server/pom.xml b/storm-server/pom.xml",
                "index ff917caaf..7c301a4df 100644",
                "--- a/storm-server/pom.xml",
                "+++ b/storm-server/pom.xml",
                "@@ -132,3 +132,3 @@",
                "                 <configuration>",
                "-                    <maxAllowedViolations>2620</maxAllowedViolations>",
                "+                    <maxAllowedViolations>2617</maxAllowedViolations>",
                "                 </configuration>",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index 2c785a5f9..e66dbe581 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -152,3 +152,3 @@ import org.apache.storm.scheduler.resource.ResourceAwareScheduler;",
                " import org.apache.storm.scheduler.resource.ResourceUtils;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;",
                " import org.apache.storm.security.INimbusCredentialPlugin;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "index 3faf1e4b4..2be241a48 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "@@ -29,3 +29,3 @@ import org.apache.storm.daemon.supervisor.Supervisor;",
                " import org.apache.storm.generated.SupervisorInfo;",
                "-import org.apache.storm.scheduler.resource.NormalizedResources;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;",
                " import org.apache.storm.utils.ObjectReader;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "index 1ed88c738..625fe6fdb 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "@@ -37,5 +37,5 @@ import org.apache.storm.networktopography.DNSToSwitchMapping;",
                " import org.apache.storm.networktopography.DefaultRackDNSToSwitchMapping;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                "-import org.apache.storm.scheduler.resource.NormalizedResources;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceOffer;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;",
                " import org.apache.storm.utils.ConfigUtils;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java b/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "index 7175b7e72..187a03c53 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "@@ -24,7 +24,6 @@ import java.util.Map;",
                " import java.util.Set;",
                "-",
                " import org.apache.storm.daemon.nimbus.TopologyResources;",
                " import org.apache.storm.generated.WorkerResources;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceOffer;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/SupervisorDetails.java b/storm-server/src/main/java/org/apache/storm/scheduler/SupervisorDetails.java",
                "index 3d087155a..242b54c43 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/SupervisorDetails.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/SupervisorDetails.java",
                "@@ -25,3 +25,3 @@ import java.util.Set;",
                " import org.apache.storm.Constants;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceOffer;",
                " import org.slf4j.Logger;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java b/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "index 8e4c1d532..41e7edfab 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "@@ -37,3 +37,3 @@ import org.apache.storm.generated.SpoutSpec;",
                " import org.apache.storm.generated.StormTopology;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;",
                " import org.apache.storm.utils.ObjectReader;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "index db9ca7364..84a95a782 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "@@ -32,2 +32,3 @@ import org.apache.storm.scheduler.TopologyDetails;",
                " import org.apache.storm.scheduler.WorkerSlot;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceOffer;",
                " import org.slf4j.Logger;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "index ca4ea6337..2f9ab4cdd 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "@@ -27,2 +27,4 @@ import org.apache.storm.generated.SpoutSpec;",
                " import org.apache.storm.generated.StormTopology;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResources;",
                " import org.json.simple.JSONObject;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourceOffer.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourceOffer.java",
                "new file mode 100644",
                "index 000000000..417dca9db",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourceOffer.java",
                "@@ -0,0 +1,114 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource.normalization;",
                "+",
                "+import java.util.Map;",
                "+import org.apache.storm.Constants;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * An offer of resources with normalized resource names.",
                "+ */",
                "+public class NormalizedResourceOffer implements NormalizedResourcesWithMemory {",
                "+",
                "+    private static final Logger LOG = LoggerFactory.getLogger(NormalizedResourceOffer.class);",
                "+    private final NormalizedResources normalizedResources;",
                "+    private double totalMemoryMb;",
                "+",
                "+    /**",
                "+     * Create a new normalized resource offer.",
                "+     *",
                "+     * @param resources the resources to be normalized.",
                "+     */",
                "+    public NormalizedResourceOffer(Map<String, ? extends Number> resources) {",
                "+        Map<String, Double> normalizedResourceMap = NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(resources);",
                "+        totalMemoryMb = normalizedResourceMap.getOrDefault(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, 0.0);",
                "+        this.normalizedResources = new NormalizedResources(normalizedResourceMap);",
                "+    }",
                "+",
                "+    public NormalizedResourceOffer() {",
                "+        this((Map<String, ? extends Number>) null);",
                "+    }",
                "+",
                "+    public NormalizedResourceOffer(NormalizedResourceOffer other) {",
                "+        this.totalMemoryMb = other.totalMemoryMb;",
                "+        this.normalizedResources = new NormalizedResources(other.normalizedResources);",
                "+    }",
                "+",
                "+    @Override",
                "+    public double getTotalMemoryMb() {",
                "+        return totalMemoryMb;",
                "+    }",
                "+",
                "+    public Map<String, Double> toNormalizedMap() {",
                "+        Map<String, Double> ret = normalizedResources.toNormalizedMap();",
                "+        ret.put(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, totalMemoryMb);",
                "+        return ret;",
                "+    }",
                "+",
                "+    public void add(NormalizedResourcesWithMemory other) {",
                "+        normalizedResources.add(other.getNormalizedResources());",
                "+        totalMemoryMb += other.getTotalMemoryMb();",
                "+    }",
                "+",
                "+    public void remove(NormalizedResourcesWithMemory other) {",
                "+        normalizedResources.remove(other.getNormalizedResources());",
                "+        totalMemoryMb -= other.getTotalMemoryMb();",
                "+        if (totalMemoryMb < 0.0) {",
                "+            normalizedResources.throwBecauseResourceBecameNegative(",
                "+                Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, totalMemoryMb, other.getTotalMemoryMb());",
                "+        };",
                "+    }",
                "+",
                "+    /**",
                "+     * @see NormalizedResources#calculateAveragePercentageUsedBy(org.apache.storm.scheduler.resource.normalization.NormalizedResources,",
                "+     * double, double).",
                "+     */",
                "+    public double calculateAveragePercentageUsedBy(NormalizedResourceOffer used) {",
                "+        return normalizedResources.calculateAveragePercentageUsedBy(",
                "+            used.getNormalizedResources(), getTotalMemoryMb(), used.getTotalMemoryMb());",
                "+    }",
                "+",
                "+    /**",
                "+     * @see NormalizedResources#calculateMinPercentageUsedBy(org.apache.storm.scheduler.resource.normalization.NormalizedResources, double,",
                "+     * double)",
                "+     */",
                "+    public double calculateMinPercentageUsedBy(NormalizedResourceOffer used) {",
                "+        return normalizedResources.calculateMinPercentageUsedBy(used.getNormalizedResources(), getTotalMemoryMb(), used.getTotalMemoryMb());",
                "+    }",
                "+",
                "+    /**",
                "+     * @see NormalizedResources#couldHoldIgnoringSharedMemory(org.apache.storm.scheduler.resource.normalization.NormalizedResources, double,",
                "+     * double).",
                "+     */",
                "+    public boolean couldHoldIgnoringSharedMemory(NormalizedResourcesWithMemory other) {",
                "+        return normalizedResources.couldHoldIgnoringSharedMemory(",
                "+            other.getNormalizedResources(), getTotalMemoryMb(), other.getTotalMemoryMb());",
                "+    }",
                "+",
                "+    public double getTotalCpu() {",
                "+        return normalizedResources.getTotalCpu();",
                "+    }",
                "+",
                "+    @Override",
                "+    public NormalizedResources getNormalizedResources() {",
                "+        return normalizedResources;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourceRequest.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourceRequest.java",
                "new file mode 100644",
                "index 000000000..6627bb5ff",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourceRequest.java",
                "@@ -0,0 +1,190 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource.normalization;",
                "+",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.Constants;",
                "+import org.apache.storm.generated.ComponentCommon;",
                "+import org.apache.storm.generated.WorkerResources;",
                "+import org.apache.storm.utils.ObjectReader;",
                "+import org.json.simple.JSONObject;",
                "+import org.json.simple.parser.JSONParser;",
                "+import org.json.simple.parser.ParseException;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * A resource request with normalized resource names.",
                "+ */",
                "+public class NormalizedResourceRequest implements NormalizedResourcesWithMemory {",
                "+",
                "+    private static final Logger LOG = LoggerFactory.getLogger(NormalizedResourceRequest.class);",
                "+",
                "+    private static void putIfMissing(Map<String, Double> dest, String destKey, Map<String, Object> src, String srcKey) {",
                "+        if (!dest.containsKey(destKey)) {",
                "+            Number value = (Number) src.get(srcKey);",
                "+            if (value != null) {",
                "+                dest.put(destKey, value.doubleValue());",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    private static Map<String, Double> getDefaultResources(Map<String, Object> topoConf) {",
                "+        Map<String, Double> ret = NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap((Map<String, Number>) topoConf.getOrDefault(",
                "+            Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<>()));",
                "+        putIfMissing(ret, Constants.COMMON_CPU_RESOURCE_NAME, topoConf, Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT);",
                "+        putIfMissing(ret, Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, topoConf, Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB);",
                "+        putIfMissing(ret, Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, topoConf, Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB);",
                "+        return ret;",
                "+    }",
                "+",
                "+    private static Map<String, Double> parseResources(String input) {",
                "+        Map<String, Double> topologyResources = new HashMap<>();",
                "+        JSONParser parser = new JSONParser();",
                "+        LOG.debug(\"Input to parseResources {}\", input);",
                "+        try {",
                "+            if (input != null) {",
                "+                Object obj = parser.parse(input);",
                "+                JSONObject jsonObject = (JSONObject) obj;",
                "+",
                "+                // Legacy resource parsing",
                "+                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB)) {",
                "+                    Double topoMemOnHeap = ObjectReader",
                "+                        .getDouble(jsonObject.get(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB), null);",
                "+                    topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, topoMemOnHeap);",
                "+                }",
                "+                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB)) {",
                "+                    Double topoMemOffHeap = ObjectReader",
                "+                        .getDouble(jsonObject.get(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB), null);",
                "+                    topologyResources.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, topoMemOffHeap);",
                "+                }",
                "+                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT)) {",
                "+                    Double topoCpu = ObjectReader.getDouble(jsonObject.get(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT),",
                "+                        null);",
                "+                    topologyResources.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, topoCpu);",
                "+                }",
                "+",
                "+                // If resource is also present in resources map will overwrite the above",
                "+                if (jsonObject.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "+                    Map<String, Number> rawResourcesMap =",
                "+                        (Map<String, Number>) jsonObject.computeIfAbsent(",
                "+                            Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "+",
                "+                    for (Map.Entry<String, Number> stringNumberEntry : rawResourcesMap.entrySet()) {",
                "+                        topologyResources.put(",
                "+                            stringNumberEntry.getKey(), stringNumberEntry.getValue().doubleValue());",
                "+                    }",
                "+",
                "+                }",
                "+            }",
                "+        } catch (ParseException e) {",
                "+            LOG.error(\"Failed to parse component resources is:\" + e.toString(), e);",
                "+            return null;",
                "+        }",
                "+        return topologyResources;",
                "+    }",
                "+",
                "+    private final NormalizedResources normalizedResources;",
                "+    private double onHeap;",
                "+    private double offHeap;",
                "+",
                "+    private NormalizedResourceRequest(Map<String, ? extends Number> resources,",
                "+        Map<String, Double> defaultResources) {",
                "+        Map<String, Double> normalizedResourceMap = NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(defaultResources);",
                "+        normalizedResourceMap.putAll(NormalizedResources.RESOURCE_NAME_NORMALIZER.normalizedResourceMap(resources));",
                "+        onHeap = normalizedResourceMap.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+        offHeap = normalizedResourceMap.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+        normalizedResources = new NormalizedResources(normalizedResourceMap);",
                "+    }",
                "+",
                "+    public NormalizedResourceRequest(ComponentCommon component, Map<String, Object> topoConf) {",
                "+        this(parseResources(component.get_json_conf()), getDefaultResources(topoConf));",
                "+    }",
                "+",
                "+    public NormalizedResourceRequest(Map<String, Object> topoConf) {",
                "+        this((Map<String, ? extends Number>) null, getDefaultResources(topoConf));",
                "+    }",
                "+",
                "+    public NormalizedResourceRequest() {",
                "+        this((Map<String, ? extends Number>) null, null);",
                "+    }",
                "+",
                "+    public Map<String, Double> toNormalizedMap() {",
                "+        Map<String, Double> ret = this.normalizedResources.toNormalizedMap();",
                "+        ret.put(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, offHeap);",
                "+        ret.put(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, onHeap);",
                "+        return ret;",
                "+    }",
                "+",
                "+    public double getOnHeapMemoryMb() {",
                "+        return onHeap;",
                "+    }",
                "+",
                "+    public void addOnHeap(final double onHeap) {",
                "+        this.onHeap += onHeap;",
                "+    }",
                "+",
                "+    public double getOffHeapMemoryMb() {",
                "+        return offHeap;",
                "+    }",
                "+",
                "+    public void addOffHeap(final double offHeap) {",
                "+        this.offHeap += offHeap;",
                "+    }",
                "+",
                "+    /**",
                "+     * Add the resources in other to this.",
                "+     *",
                "+     * @param other the other Request to add to this.",
                "+     */",
                "+    public void add(NormalizedResourceRequest other) {",
                "+        this.normalizedResources.add(other.normalizedResources);",
                "+        onHeap += other.onHeap;",
                "+        offHeap += other.offHeap;",
                "+    }",
                "+",
                "+    public void add(WorkerResources value) {",
                "+        this.normalizedResources.add(value);",
                "+        //The resources are already normalized",
                "+        Map<String, Double> resources = value.get_resources();",
                "+        onHeap += resources.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+        offHeap += resources.getOrDefault(Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "+    }",
                "+",
                "+    @Override",
                "+    public double getTotalMemoryMb() {",
                "+        return onHeap + offHeap;",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        return super.toString() + \" onHeap: \" + onHeap + \" offHeap: \" + offHeap;",
                "+    }",
                "+",
                "+    public double getTotalCpu() {",
                "+        return this.normalizedResources.getTotalCpu();",
                "+    }",
                "+",
                "+    @Override",
                "+    public NormalizedResources getNormalizedResources() {",
                "+        return this.normalizedResources;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResources.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResources.java",
                "new file mode 100644",
                "index 000000000..76d5ce21f",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResources.java",
                "@@ -0,0 +1,343 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource.normalization;",
                "+",
                "+import com.google.common.annotations.VisibleForTesting;",
                "+import java.util.Arrays;",
                "+import java.util.Map;",
                "+import org.apache.storm.Constants;",
                "+import org.apache.storm.generated.WorkerResources;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * Resources that have been normalized. This class is intended as a delegate for more specific types of normalized resource set, since it",
                "+ * does not keep track of memory as a resource.",
                "+ */",
                "+public class NormalizedResources {",
                "+",
                "+    private static final Logger LOG = LoggerFactory.getLogger(NormalizedResources.class);",
                "+",
                "+    public static ResourceNameNormalizer RESOURCE_NAME_NORMALIZER;",
                "+    private static ResourceMapArrayBridge RESOURCE_MAP_ARRAY_BRIDGE;",
                "+",
                "+    static {",
                "+        resetResourceNames();",
                "+    }",
                "+",
                "+    private double cpu;",
                "+    private double[] otherResources;",
                "+",
                "+    /**",
                "+     * This is for testing only. It allows a test to reset the static state relating to resource names. We reset the mapping because some",
                "+     * algorithms sadly have different behavior if a resource exists or not.",
                "+     */",
                "+    @VisibleForTesting",
                "+    public static void resetResourceNames() {",
                "+        RESOURCE_NAME_NORMALIZER = new ResourceNameNormalizer();",
                "+        RESOURCE_MAP_ARRAY_BRIDGE = new ResourceMapArrayBridge();",
                "+    }",
                "+",
                "+    /**",
                "+     * Copy constructor.",
                "+     */",
                "+    public NormalizedResources(NormalizedResources other) {",
                "+        cpu = other.cpu;",
                "+        otherResources = Arrays.copyOf(other.otherResources, other.otherResources.length);",
                "+    }",
                "+",
                "+    /**",
                "+     * Create a new normalized set of resources. Note that memory is not managed by this class, as it is not consistent in requests vs",
                "+     * offers because of how on heap vs off heap is used.",
                "+     *",
                "+     * @param normalizedResources the normalized resource map",
                "+     */",
                "+    public NormalizedResources(Map<String, Double> normalizedResources) {",
                "+        cpu = normalizedResources.getOrDefault(Constants.COMMON_CPU_RESOURCE_NAME, 0.0);",
                "+        otherResources = RESOURCE_MAP_ARRAY_BRIDGE.translateToResourceArray(normalizedResources);",
                "+    }",
                "+",
                "+    /**",
                "+     * Get the total amount of cpu.",
                "+     *",
                "+     * @return the amount of cpu.",
                "+     */",
                "+    public double getTotalCpu() {",
                "+        return cpu;",
                "+    }",
                "+    ",
                "+    private void zeroPadOtherResourcesIfNecessary(int requiredLength) {",
                "+        if (requiredLength > otherResources.length) {",
                "+            double[] newResources = new double[requiredLength];",
                "+            System.arraycopy(otherResources, 0, newResources, 0, otherResources.length);",
                "+            otherResources = newResources;",
                "+        }",
                "+    }",
                "+",
                "+    private void add(double[] resourceArray) {",
                "+        int otherLength = resourceArray.length;",
                "+        zeroPadOtherResourcesIfNecessary(otherLength);",
                "+        for (int i = 0; i < otherLength; i++) {",
                "+            otherResources[i] += resourceArray[i];",
                "+        }",
                "+    }",
                "+",
                "+    public void add(NormalizedResources other) {",
                "+        this.cpu += other.cpu;",
                "+        add(other.otherResources);",
                "+    }",
                "+",
                "+    /**",
                "+     * Add the resources from a worker to this.",
                "+     *",
                "+     * @param value the worker resources that should be added to this.",
                "+     */",
                "+    public void add(WorkerResources value) {",
                "+        Map<String, Double> workerNormalizedResources = value.get_resources();",
                "+        cpu += workerNormalizedResources.getOrDefault(Constants.COMMON_CPU_RESOURCE_NAME, 0.0);",
                "+        add(RESOURCE_MAP_ARRAY_BRIDGE.translateToResourceArray(workerNormalizedResources));",
                "+    }",
                "+",
                "+    /**",
                "+     * Throw an IllegalArgumentException because a resource became negative during remove.",
                "+     * @param resourceName The name of the resource that became negative",
                "+     * @param currentValue The current value of the resource",
                "+     * @param subtractedValue The value that was subtracted to make the resource negative",
                "+     */",
                "+    public void throwBecauseResourceBecameNegative(String resourceName, double currentValue, double subtractedValue) {",
                "+        throw new IllegalArgumentException(String.format(\"Resource amounts should never be negative.\"",
                "+            + \" Resource '%s' with current value '%f' became negative because '%f' was removed.\",",
                "+            resourceName, currentValue, subtractedValue));",
                "+    }",
                "+    ",
                "+    /**",
                "+     * Remove the other resources from this. This is the same as subtracting the resources in other from this.",
                "+     *",
                "+     * @param other the resources we want removed.",
                "+     * @throws IllegalArgumentException if subtracting other from this would result in any resource amount becoming negative.",
                "+     */",
                "+    public void remove(NormalizedResources other) {",
                "+        this.cpu -= other.cpu;",
                "+        if (cpu < 0.0) {",
                "+            throwBecauseResourceBecameNegative(Constants.COMMON_CPU_RESOURCE_NAME, cpu, other.cpu);",
                "+        }",
                "+        int otherLength = other.otherResources.length;",
                "+        zeroPadOtherResourcesIfNecessary(otherLength);",
                "+        for (int i = 0; i < otherLength; i++) {",
                "+            otherResources[i] -= other.otherResources[i];",
                "+            if (otherResources[i] < 0.0) {",
                "+                throwBecauseResourceBecameNegative(getResourceNameForResourceIndex(i), otherResources[i], other.otherResources[i]);",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        return \"Normalized resources: \" + toNormalizedMap();",
                "+    }",
                "+",
                "+    /**",
                "+     * Return a Map of the normalized resource name to a double. This should only be used when returning thrift resource requests to the end",
                "+     * user.",
                "+     */",
                "+    public Map<String, Double> toNormalizedMap() {",
                "+        Map<String, Double> ret = RESOURCE_MAP_ARRAY_BRIDGE.translateFromResourceArray(otherResources);",
                "+        ret.put(Constants.COMMON_CPU_RESOURCE_NAME, cpu);",
                "+        return ret;",
                "+    }",
                "+",
                "+    private double getResourceAt(int index) {",
                "+        if (index >= otherResources.length) {",
                "+            return 0.0;",
                "+        }",
                "+        return otherResources[index];",
                "+    }",
                "+",
                "+    /**",
                "+     * A simple sanity check to see if all of the resources in this would be large enough to hold the resources in other ignoring memory. It",
                "+     * does not check memory because with shared memory it is beyond the scope of this.",
                "+     *",
                "+     * @param other the resources that we want to check if they would fit in this.",
                "+     * @param thisTotalMemoryMb The total memory in MB of this",
                "+     * @param otherTotalMemoryMb The total memory in MB of other",
                "+     * @return true if it might fit, else false if it could not possibly fit.",
                "+     */",
                "+    public boolean couldHoldIgnoringSharedMemory(NormalizedResources other, double thisTotalMemoryMb, double otherTotalMemoryMb) {",
                "+        if (this.cpu < other.getTotalCpu()) {",
                "+            return false;",
                "+        }",
                "+        int length = Math.max(this.otherResources.length, other.otherResources.length);",
                "+        for (int i = 0; i < length; i++) {",
                "+            if (getResourceAt(i) < other.getResourceAt(i)) {",
                "+                return false;",
                "+            }",
                "+        }",
                "+",
                "+        return thisTotalMemoryMb >= otherTotalMemoryMb;",
                "+    }",
                "+",
                "+    private String getResourceNameForResourceIndex(int resourceIndex) {",
                "+        for (Map.Entry<String, Integer> entry : RESOURCE_MAP_ARRAY_BRIDGE.getResourceNamesToArrayIndex().entrySet()) {",
                "+            int index = entry.getValue();",
                "+            if (index == resourceIndex) {",
                "+                return entry.getKey();",
                "+            }",
                "+        }",
                "+        return null;",
                "+    }",
                "+",
                "+    private void throwBecauseUsedIsNotSubsetOfTotal(NormalizedResources used, double totalMemoryMb, double usedMemoryMb) {",
                "+        throw new IllegalArgumentException(String.format(\"The used resources must be a subset of the total resources.\"",
                "+            + \" Used: '%s', Total: '%s', Used Mem: '%f', Total Mem: '%f'\",",
                "+            used.toNormalizedMap(), this.toNormalizedMap(), usedMemoryMb, totalMemoryMb));",
                "+    }",
                "+",
                "+    /**",
                "+     * Calculate the average resource usage percentage with this being the total resources and used being the amounts used. Used must be a",
                "+     * subset of the total resources. If a resource in the total has a value of zero, it will be skipped in the calculation to avoid",
                "+     * division by 0. If all resources are skipped the result is defined to be 100.0.",
                "+     *",
                "+     * @param used the amount of resources used.",
                "+     * @param totalMemoryMb The total memory in MB",
                "+     * @param usedMemoryMb The used memory in MB",
                "+     * @return the average percentage used 0.0 to 100.0.",
                "+     * @throws IllegalArgumentException if any resource in used has a greater value than the same resource in the total, or used has generic",
                "+     *     resources that are not present in the total.",
                "+     */",
                "+    public double calculateAveragePercentageUsedBy(NormalizedResources used, double totalMemoryMb, double usedMemoryMb) {",
                "+        if (LOG.isTraceEnabled()) {",
                "+            LOG.trace(\"Calculating avg percentage used by. Used Mem: {} Total Mem: {}\"",
                "+                + \" Used Normalized Resources: {} Total Normalized Resources: {}\", totalMemoryMb, usedMemoryMb,",
                "+                toNormalizedMap(), used.toNormalizedMap());",
                "+        }",
                "+",
                "+        int skippedResourceTypes = 0;",
                "+        double total = 0.0;",
                "+        if (usedMemoryMb > totalMemoryMb) {",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+        }",
                "+        if (totalMemoryMb != 0.0) {",
                "+            total += usedMemoryMb / totalMemoryMb;",
                "+        } else {",
                "+            skippedResourceTypes++;",
                "+        }",
                "+        double totalCpu = getTotalCpu();",
                "+        if (used.getTotalCpu() > getTotalCpu()) {",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+        }",
                "+        if (totalCpu != 0.0) {",
                "+            total += used.getTotalCpu() / getTotalCpu();",
                "+        } else {",
                "+            skippedResourceTypes++;",
                "+        }",
                "+",
                "+        if (used.otherResources.length > otherResources.length) {",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+        }",
                "+",
                "+        for (int i = 0; i < otherResources.length; i++) {",
                "+            double totalValue = otherResources[i];",
                "+            double usedValue;",
                "+            if (i >= used.otherResources.length) {",
                "+                //Resources missing from used are using none of that resource",
                "+                usedValue = 0.0;",
                "+            } else {",
                "+                usedValue = used.otherResources[i];",
                "+            }",
                "+            if (usedValue > totalValue) {",
                "+                throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+            }",
                "+            if (totalValue == 0.0) {",
                "+                //Skip any resources where the total is 0, the percent used for this resource isn't meaningful.",
                "+                //We fall back to prioritizing by cpu, memory and any other resources by ignoring this value",
                "+                skippedResourceTypes++;",
                "+                continue;",
                "+            }",
                "+",
                "+            total += usedValue / totalValue;",
                "+        }",
                "+        //Adjust the divisor for the average to account for any skipped resources (those where the total was 0)",
                "+        int divisor = 2 + otherResources.length - skippedResourceTypes;",
                "+        if (divisor == 0) {",
                "+            /*",
                "+             * This is an arbitrary choice to make the result consistent with calculateMin. Any value would be valid here, becase there are",
                "+             * no (non-zero) resources in the total set of resources, so we're trying to average 0 values.",
                "+             */",
                "+            return 100.0;",
                "+        } else {",
                "+            return (total * 100.0) / divisor;",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Calculate the minimum resource usage percentage with this being the total resources and used being the amounts used. Used must be a",
                "+     * subset of the total resources. If a resource in the total has a value of zero, it will be skipped in the calculation to avoid",
                "+     * division by 0. If all resources are skipped the result is defined to be 100.0.",
                "+     *",
                "+     * @param used the amount of resources used.",
                "+     * @param totalMemoryMb The total memory in MB",
                "+     * @param usedMemoryMb The used memory in MB",
                "+     * @return the minimum percentage used 0.0 to 100.0.",
                "+     * @throws IllegalArgumentException if any resource in used has a greater value than the same resource in the total, or used has generic",
                "+     *     resources that are not present in the total.",
                "+     */",
                "+    public double calculateMinPercentageUsedBy(NormalizedResources used, double totalMemoryMb, double usedMemoryMb) {",
                "+        if (LOG.isTraceEnabled()) {",
                "+            LOG.trace(\"Calculating min percentage used by. Used Mem: {} Total Mem: {}\"",
                "+                + \" Used Normalized Resources: {} Total Normalized Resources: {}\", totalMemoryMb, usedMemoryMb,",
                "+                toNormalizedMap(), used.toNormalizedMap());",
                "+        }",
                "+",
                "+        double min = 1.0;",
                "+        if (usedMemoryMb > totalMemoryMb) {",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+        }",
                "+        if (totalMemoryMb != 0.0) {",
                "+            min = Math.min(min, usedMemoryMb / totalMemoryMb);",
                "+        }",
                "+        double totalCpu = getTotalCpu();",
                "+        if (used.getTotalCpu() > totalCpu) {",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+        }",
                "+        if (totalCpu != 0.0) {",
                "+            min = Math.min(min, used.getTotalCpu() / totalCpu);",
                "+        }",
                "+",
                "+        if (used.otherResources.length > otherResources.length) {",
                "+            throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+        }",
                "+        ",
                "+        for (int i = 0; i < otherResources.length; i++) {",
                "+            if (otherResources[i] == 0.0) {",
                "+                //Skip any resources where the total is 0, the percent used for this resource isn't meaningful.",
                "+                //We fall back to prioritizing by cpu, memory and any other resources by ignoring this value",
                "+                continue;",
                "+            }",
                "+            if (i >= used.otherResources.length) {",
                "+                //Resources missing from used are using none of that resource",
                "+                return 0;",
                "+            }",
                "+            if (used.otherResources[i] > otherResources[i]) {",
                "+                throwBecauseUsedIsNotSubsetOfTotal(used, totalMemoryMb, usedMemoryMb);",
                "+            }",
                "+            min = Math.min(min, used.otherResources[i] / otherResources[i]);",
                "+        }",
                "+        return min * 100.0;",
                "+    }",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourcesWithMemory.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourcesWithMemory.java",
                "new file mode 100644",
                "index 000000000..5001645b6",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourcesWithMemory.java",
                "@@ -0,0 +1,28 @@",
                "+/*",
                "+ * Copyright 2018 The Apache Software Foundation.",
                "+ *",
                "+ * Licensed under the Apache License, Version 2.0 (the \"License\");",
                "+ * you may not use this file except in compliance with the License.",
                "+ * You may obtain a copy of the License at",
                "+ *",
                "+ *      http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource.normalization;",
                "+",
                "+/**",
                "+ * Intended for {@link NormalizedResources} wrappers that handle memory.",
                "+ */",
                "+public interface NormalizedResourcesWithMemory {",
                "+",
                "+    NormalizedResources getNormalizedResources();",
                "+",
                "+    double getTotalMemoryMb();",
                "+",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/ResourceMapArrayBridge.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/ResourceMapArrayBridge.java",
                "new file mode 100644",
                "index 000000000..2b07c6500",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/ResourceMapArrayBridge.java",
                "@@ -0,0 +1,89 @@",
                "+/*",
                "+ * Copyright 2018 The Apache Software Foundation.",
                "+ *",
                "+ * Licensed under the Apache License, Version 2.0 (the \"License\");",
                "+ * you may not use this file except in compliance with the License.",
                "+ * You may obtain a copy of the License at",
                "+ *",
                "+ *      http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource.normalization;",
                "+",
                "+import java.util.Collections;",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+import java.util.concurrent.ConcurrentHashMap;",
                "+import java.util.concurrent.ConcurrentMap;",
                "+import java.util.concurrent.atomic.AtomicInteger;",
                "+import org.apache.storm.Constants;",
                "+",
                "+/**",
                "+ * Provides translation between normalized resource maps and resource value arrays. Some operations use resource value arrays instead of the",
                "+ * full normalized resource map as an optimization. See {@link NormalizedResources}.",
                "+ */",
                "+public class ResourceMapArrayBridge {",
                "+",
                "+    private final ConcurrentMap<String, Integer> resourceNamesToArrayIndex = new ConcurrentHashMap<>();",
                "+    private final AtomicInteger counter = new AtomicInteger(0);",
                "+",
                "+    /**",
                "+     * Translates a normalized resource map to an array of resource values. Each resource name will be assigned an index in the array, which",
                "+     * is guaranteed to be consistent with subsequent invocations of this method. Note that CPU and memory resources are not translated by",
                "+     * this method, as they are expected to be captured elsewhere.",
                "+     *",
                "+     * @param normalizedResources The resources to translate to an array",
                "+     * @return The array of resource values",
                "+     */",
                "+    public double[] translateToResourceArray(Map<String, Double> normalizedResources) {",
                "+        //To avoid locking we will go through the map twice.  It should be small so it is probably not a big deal",
                "+        for (String key : normalizedResources.keySet()) {",
                "+            //We are going to skip over CPU and Memory, because they are captured elsewhere",
                "+            if (!Constants.COMMON_CPU_RESOURCE_NAME.equals(key)",
                "+                && !Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME.equals(key)",
                "+                && !Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME.equals(key)",
                "+                && !Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME.equals(key)) {",
                "+                resourceNamesToArrayIndex.computeIfAbsent(key, (k) -> counter.getAndIncrement());",
                "+            }",
                "+        }",
                "+        //By default all of the values are 0",
                "+        double[] ret = new double[counter.get()];",
                "+        for (Map.Entry<String, Double> entry : normalizedResources.entrySet()) {",
                "+            Integer index = resourceNamesToArrayIndex.get(entry.getKey());",
                "+            if (index != null) {",
                "+                //index == null if it is memory or CPU",
                "+                ret[index] = entry.getValue();",
                "+            }",
                "+        }",
                "+        return ret;",
                "+    }",
                "+",
                "+    /**",
                "+     * Translates an array of resource values to a normalized resource map.",
                "+     *",
                "+     * @param resources The resource array to translate",
                "+     * @return The normalized resource map",
                "+     */",
                "+    public Map<String, Double> translateFromResourceArray(double[] resources) {",
                "+        Map<String, Double> ret = new HashMap<>();",
                "+        int length = resources.length;",
                "+        for (Map.Entry<String, Integer> entry : resourceNamesToArrayIndex.entrySet()) {",
                "+            int index = entry.getValue();",
                "+            if (index < length) {",
                "+                ret.put(entry.getKey(), resources[index]);",
                "+            }",
                "+        }",
                "+        return ret;",
                "+    }",
                "+",
                "+    public Map<String, Integer> getResourceNamesToArrayIndex() {",
                "+        return Collections.unmodifiableMap(resourceNamesToArrayIndex);",
                "+    }",
                "+",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/ResourceNameNormalizer.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/ResourceNameNormalizer.java",
                "new file mode 100644",
                "index 000000000..fc9182dc2",
                "--- /dev/null",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/ResourceNameNormalizer.java",
                "@@ -0,0 +1,68 @@",
                "+/*",
                "+ * Copyright 2018 The Apache Software Foundation.",
                "+ *",
                "+ * Licensed under the Apache License, Version 2.0 (the \"License\");",
                "+ * you may not use this file except in compliance with the License.",
                "+ * You may obtain a copy of the License at",
                "+ *",
                "+ *      http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.scheduler.resource.normalization;",
                "+",
                "+import java.util.Collections;",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+import java.util.stream.Collectors;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.Constants;",
                "+",
                "+/**",
                "+ * Provides resource name normalization for resource maps.",
                "+ */",
                "+public class ResourceNameNormalizer {",
                "+",
                "+    private final Map<String, String> resourceNameMapping;",
                "+",
                "+    /**",
                "+     * Creates a new resource name normalizer.",
                "+     */",
                "+    public ResourceNameNormalizer() {",
                "+        Map<String, String> tmp = new HashMap<>();",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_CPU_PCORE_PERCENT, Constants.COMMON_CPU_RESOURCE_NAME);",
                "+        tmp.put(Config.SUPERVISOR_CPU_CAPACITY, Constants.COMMON_CPU_RESOURCE_NAME);",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_ONHEAP_MEMORY_MB, Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME);",
                "+        tmp.put(Config.TOPOLOGY_COMPONENT_RESOURCES_OFFHEAP_MEMORY_MB, Constants.COMMON_OFFHEAP_MEMORY_RESOURCE_NAME);",
                "+        tmp.put(Config.SUPERVISOR_MEMORY_CAPACITY_MB, Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME);",
                "+        resourceNameMapping = Collections.unmodifiableMap(tmp);",
                "+    }",
                "+",
                "+    /**",
                "+     * Normalizes a supervisor resource map or topology details map's keys to universal resource names.",
                "+     *",
                "+     * @param resourceMap resource map of either Supervisor or Topology",
                "+     * @return the resource map with common resource names",
                "+     */",
                "+    public Map<String, Double> normalizedResourceMap(Map<String, ? extends Number> resourceMap) {",
                "+        if (resourceMap == null) {",
                "+            return new HashMap<>();",
                "+        }",
                "+        return new HashMap<>(resourceMap.entrySet().stream()",
                "+            .collect(Collectors.toMap(",
                "+                //Map the key if needed",
                "+                (e) -> resourceNameMapping.getOrDefault(e.getKey(), e.getKey()),",
                "+                //Map the value",
                "+                (e) -> e.getValue().doubleValue())));",
                "+    }",
                "+",
                "+    public Map<String, String> getResourceNameMapping() {",
                "+        return resourceNameMapping;",
                "+    }",
                "+",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "index 73e1aa0dc..02d06a940 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "@@ -40,3 +40,3 @@ import org.apache.storm.scheduler.resource.RAS_Node;",
                " import org.apache.storm.scheduler.resource.RAS_Nodes;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceOffer;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceOffer;",
                " import org.slf4j.Logger;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "index 6f3325140..cd16259b9 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java",
                "@@ -74,3 +74,3 @@ import org.apache.storm.nimbus.NimbusInfo;",
                " import org.apache.storm.scheduler.resource.ResourceUtils;",
                "-import org.apache.storm.scheduler.resource.NormalizedResourceRequest;",
                "+import org.apache.storm.scheduler.resource.normalization.NormalizedResourceRequest;",
                " import org.apache.storm.security.auth.SingleUserPrincipal;"
            ],
            "changed_files": [
                "storm-server/pom.xml",
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/SupervisorHeartbeat.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/Cluster.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/ISchedulingState.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/SupervisorDetails.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/RAS_Node.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourceOffer.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourceRequest.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResources.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/NormalizedResourcesWithMemory.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/ResourceMapArrayBridge.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/normalization/ResourceNameNormalizer.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java",
                "storm-server/src/main/java/org/apache/storm/utils/ServerUtils.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                }
            ]
        },
        {
            "commit_id": "875761189ca387f0575aeaea33a769d70d36ae47",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508719494,
            "hunks": 36,
            "message": "STORM-2781: Refactor storm-kafka-client KafkaSpout Processing Guarantees  - Define processing guarantees as AT_LEAST_ONCE, AT_MOST_ONCE, NONE  - Refactor method name from setForceEnableTupleTracking to setTupleTrackingEnforced  - Throw IllegalStateException instead of IllegalArgumentException if spout attempts to emit an already committed message  - Update documentation to reflect these changes",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 9253a2df9..170c02568 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -80,3 +80,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private transient KafkaTupleListener tupleListener;",
                "-    // timer == null for modes other than at-least-once",
                "+    // timer == null if processing guarantee is none or at-most-once",
                "     private transient Timer commitTimer;",
                "@@ -87,6 +87,6 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     // Tuples that were successfully acked/emitted. These tuples will be committed periodically when the commit timer expires,",
                "-    //or after a consumer rebalance, or during close/deactivate. Always empty if not using at-least-once mode.",
                "+    // or after a consumer rebalance, or during close/deactivate. Always empty if processing guarantee is none or at-most-once.",
                "     private transient Map<TopicPartition, OffsetManager> offsetManagers;",
                "     // Tuples that have been emitted but that are \"on the wire\", i.e. pending being acked or failed.",
                "-    // Always empty if not using at-least-once mode.",
                "+    // Always empty if processing guarantee is none or at-most-once",
                "     private transient Set<KafkaSpoutMessageId> emitted;",
                "@@ -127,4 +127,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-        if (isAtLeastOnce()) {",
                "-            // Only used if the spout commits offsets for acked tuples",
                "+        if (isAtLeastOnceProcessing()) {",
                "+            // Only used if the spout should commit an offset to Kafka only after the corresponding tuple has been acked.",
                "             commitTimer = new Timer(TIMER_DELAY_MS, kafkaSpoutConfig.getOffsetsCommitPeriodMs(), TimeUnit.MILLISECONDS);",
                "@@ -142,3 +142,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    private boolean isAtLeastOnce() {",
                "+    private boolean isAtLeastOnceProcessing() {",
                "         return kafkaSpoutConfig.getProcessingGuarantee() == KafkaSpoutConfig.ProcessingGuarantee.AT_LEAST_ONCE;",
                "@@ -156,3 +156,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             previousAssignment = partitions;",
                "-            if (isAtLeastOnce() && initialized) {",
                "+            if (isAtLeastOnceProcessing() && initialized) {",
                "                 initialized = false;",
                "@@ -172,3 +172,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         private void initialize(Collection<TopicPartition> partitions) {",
                "-            if (isAtLeastOnce()) {",
                "+            if (isAtLeastOnceProcessing()) {",
                "                 // remove from acked all partitions that are no longer assigned to this spout",
                "@@ -190,3 +190,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 // If this partition was previously assigned to this spout, leave the acked offsets as they were to resume where it left off",
                "-                if (isAtLeastOnce() && !offsetManagers.containsKey(tp)) {",
                "+                if (isAtLeastOnceProcessing() && !offsetManagers.containsKey(tp)) {",
                "                     offsetManagers.put(tp, new OffsetManager(tp, fetchOffset));",
                "@@ -257,3 +257,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private boolean commit() {",
                "-        return isAtLeastOnce() && commitTimer.isExpiredResetOnTrue();    // timer != null for non auto commit mode",
                "+        return isAtLeastOnceProcessing() && commitTimer.isExpiredResetOnTrue();    // timer != null for non auto commit mode",
                "     }",
                "@@ -264,7 +264,6 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         final boolean poll = !waitingToEmit()",
                "-            //Check that the number of uncommitted, nonretriable tuples is less than the maxUncommittedOffsets limit",
                "-            //Accounting for retriable tuples this way still guarantees that the limit is followed on a per partition basis,",
                "-            //and prevents locking up the spout when there are too many retriable tuples",
                "-            && (numUncommittedOffsets - readyMessageCount < maxUncommittedOffsets",
                "-            || !isAtLeastOnce());",
                "+            // Check that the number of uncommitted, non-retriable tuples is less than the maxUncommittedOffsets limit.",
                "+            // Accounting for retriable tuples in this way still guarantees that the limit is followed on a per partition basis,",
                "+            // and prevents locking up the spout when there are too many retriable tuples",
                "+            && (numUncommittedOffsets - readyMessageCount < maxUncommittedOffsets || !isAtLeastOnceProcessing());",
                "@@ -276,3 +275,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-            if (numUncommittedOffsets >= maxUncommittedOffsets && isAtLeastOnce()) {",
                "+            if (numUncommittedOffsets >= maxUncommittedOffsets && isAtLeastOnceProcessing()) {",
                "                 LOG.debug(\"Not polling. [{}] uncommitted offsets across all topic partitions has reached the threshold of [{}]\",",
                "@@ -338,10 +337,12 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         final KafkaSpoutMessageId msgId = retryService.getMessageId(tp, record.offset());",
                "+",
                "         if (offsetManagers.containsKey(tp) && offsetManagers.get(tp).contains(msgId)) {   // has been acked",
                "             LOG.trace(\"Tuple for record [{}] has already been acked. Skipping\", record);",
                "-        } else if (emitted.contains(msgId)) {   // has been emitted and it's pending ack or fail",
                "+        } else if (emitted.contains(msgId)) {   // has been emitted and it is pending ack or fail",
                "             LOG.trace(\"Tuple for record [{}] has already been emitted. Skipping\", record);",
                "         } else {",
                "-            Validate.isTrue(kafkaConsumer.committed(tp) == null || kafkaConsumer.committed(tp).offset() < kafkaConsumer.position(tp),",
                "-                \"The spout is about to emit a message that has already been committed.\"",
                "-                + \" This should never occur, and indicates a bug in the spout\");",
                "+            if (kafkaConsumer.committed(tp) != null && (kafkaConsumer.committed(tp).offset() >= kafkaConsumer.position(tp))) {",
                "+                throw new IllegalStateException(\"Attempting to emit a message that has already been committed.\");",
                "+            }",
                "+",
                "             final List<Object> tuple = kafkaSpoutConfig.getTranslator().apply(record);",
                "@@ -351,5 +352,6 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 if (!isScheduled || retryService.isReady(msgId)) {",
                "-                    String stream = tuple instanceof KafkaTuple ? ((KafkaTuple) tuple).getStream() : Utils.DEFAULT_STREAM_ID;",
                "-                    if (!isAtLeastOnce()) {",
                "-                        if (kafkaSpoutConfig.getForceEnableTupleTracking()) {",
                "+                    final String stream = tuple instanceof KafkaTuple ? ((KafkaTuple) tuple).getStream() : Utils.DEFAULT_STREAM_ID;",
                "+",
                "+                    if (!isAtLeastOnceProcessing()) {",
                "+                        if (kafkaSpoutConfig.isTupleTrackingEnforced()) {",
                "                             collector.emit(stream, tuple, msgId);",
                "@@ -440,4 +442,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     public void ack(Object messageId) {",
                "-        if (!isAtLeastOnce()) {",
                "-            // Only need to keep track of acked tuples if commits are done based on acks",
                "+        if (!isAtLeastOnceProcessing()) {",
                "             return;",
                "@@ -445,2 +446,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "+        // Only need to keep track of acked tuples if commits to Kafka are controlled by",
                "+        // tuple acks, which happens only for at-least-once processing semantics",
                "         final KafkaSpoutMessageId msgId = (KafkaSpoutMessageId) messageId;",
                "@@ -466,7 +469,7 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     public void fail(Object messageId) {",
                "-        if (!isAtLeastOnce()) {",
                "-            // Only need to keep track of failed tuples if commits are done based on acks",
                "+        if (!isAtLeastOnceProcessing()) {",
                "             return;",
                "         }",
                "-",
                "+        // Only need to keep track of failed tuples if commits to Kafka are controlled by",
                "+        // tuple acks, which happens only for at-least-once processing semantics",
                "         final KafkaSpoutMessageId msgId = (KafkaSpoutMessageId) messageId;",
                "@@ -479,3 +482,5 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             + \" This should never occur barring errors in the RetryService implementation or the spout code.\");",
                "+",
                "         msgId.incrementNumFails();",
                "+",
                "         if (!retryService.schedule(msgId)) {",
                "@@ -528,3 +533,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         try {",
                "-            if (isAtLeastOnce()) {",
                "+            if (isAtLeastOnceProcessing()) {",
                "                 commitOffsetsForAckedTuples();",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "index f2116978d..6a693fe24 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "@@ -57,3 +57,5 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "     public static final long DEFAULT_PARTITION_REFRESH_PERIOD_MS = 2_000;",
                "+",
                "     public static final FirstPollOffsetStrategy DEFAULT_FIRST_POLL_OFFSET_STRATEGY = FirstPollOffsetStrategy.UNCOMMITTED_EARLIEST;",
                "+",
                "     public static final KafkaSpoutRetryService DEFAULT_RETRY_SERVICE =",
                "@@ -61,2 +63,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "             DEFAULT_MAX_RETRIES, TimeInterval.seconds(10));",
                "+",
                "     public static final ProcessingGuarantee DEFAULT_PROCESSING_GUARANTEE = ProcessingGuarantee.AT_LEAST_ONCE;",
                "@@ -80,3 +83,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "     private final ProcessingGuarantee processingGuarantee;",
                "-    private final boolean forceEnableTupleTracking;",
                "+    private final boolean tupleTrackingEnforced;",
                "@@ -101,3 +104,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         this.processingGuarantee = builder.processingGuarantee;",
                "-        this.forceEnableTupleTracking = builder.forceEnableTupleTracking;",
                "+        this.tupleTrackingEnforced = builder.tupleTrackingEnforced;",
                "     }",
                "@@ -128,18 +131,25 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "     /**",
                "-     * The processing guarantee supported by the spout. This parameter affects when the spout commits offsets to Kafka, marking them as",
                "-     * processed.",
                "+     * This enum controls when the tuple with the {@link ConsumerRecord} for an offset is marked as processed,",
                "+     * i.e. when the offset is committed to Kafka. For AT_LEAST_ONCE and AT_MOST_ONCE the spout controls when",
                "+     * the commit happens. When the guarantee is NONE Kafka controls when the commit happens.",
                "      *",
                "      * <ul>",
                "-     * <li>AT_LEAST_ONCE means that the Kafka spout considers an offset ready for commit once a tuple corresponding to that offset has been",
                "-     * acked on the spout. This corresponds to an at-least-once guarantee.</li>",
                "-     * <li>ANY_TIMES means that the Kafka spout may commit polled offsets at any time. This means the message may be processed any number of",
                "-     * times (including 0), and causes the spout to enable auto offset committing on the underlying consumer.</li>",
                "-     * <li>AT_MOST_ONCE means that the spout will commit polled offsets before emitting them to the topology. This guarantees at-most-once",
                "-     * processing.</li>",
                "+     * <li>AT_LEAST_ONCE - an offset is ready to commit only after the corresponding tuple has been processed (at-least-once)",
                "+     * and acked. If a tuple fails or times-out it will be re-emitted. A tuple can be processed more than once if for instance",
                "+     * the ack gets lost.</li>",
                "+     * <br/>",
                "+     * <li>AT_MOST_ONCE - every offset will be committed to Kafka right after being polled but before being emitted",
                "+     * to the downstream components of the topology. It guarantees that the offset is processed at-most-once because it",
                "+     * won't retry tuples that fail or timeout after the commit to Kafka has been done.</li>",
                "+     * <br/>",
                "+     * <li>NONE - the polled offsets are committed to Kafka periodically as controlled by the Kafka properties",
                "+     * \"enable.auto.commit\" and \"auto.commit.interval.ms\". Because the spout does not control when the commit happens",
                "+     * it cannot give any message processing guarantees, i.e. a message may be processed 0, 1 or more times.",
                "+     * This option requires \"enable.auto.commit=true\". If \"enable.auto.commit=false\" an exception will be thrown.</li>",
                "      * </ul>",
                "      */",
                "-    public static enum ProcessingGuarantee {",
                "+    public enum ProcessingGuarantee {",
                "         AT_LEAST_ONCE,",
                "-        ANY_TIMES,",
                "-        AT_MOST_ONCE",
                "+        AT_MOST_ONCE,",
                "+        NONE,",
                "     }",
                "@@ -160,3 +170,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         private ProcessingGuarantee processingGuarantee = DEFAULT_PROCESSING_GUARANTEE;",
                "-        private boolean forceEnableTupleTracking = false;",
                "+        private boolean tupleTrackingEnforced = false;",
                "@@ -371,6 +381,6 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "          *",
                "-         * @param forceEnableTupleTracking true if Storm should track emitted tuples, false otherwise",
                "+         * @param tupleTrackingEnforced true if Storm should track emitted tuples, false otherwise",
                "          */",
                "-        public Builder<K, V> setForceEnableTupleTracking(boolean forceEnableTupleTracking) {",
                "-            this.forceEnableTupleTracking = forceEnableTupleTracking;",
                "+        public Builder<K, V> setTupleTrackingEnforced(boolean tupleTrackingEnforced) {",
                "+            this.tupleTrackingEnforced = tupleTrackingEnforced;",
                "             return this;",
                "@@ -427,3 +437,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         }",
                "-        if (builder.processingGuarantee == ProcessingGuarantee.ANY_TIMES) {",
                "+        if (builder.processingGuarantee == ProcessingGuarantee.NONE) {",
                "             builder.kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"true\");",
                "@@ -463,4 +473,4 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "-    public boolean getForceEnableTupleTracking() {",
                "-        return forceEnableTupleTracking;",
                "+    public boolean isTupleTrackingEnforced() {",
                "+        return tupleTrackingEnforced;",
                "     }"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2781": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "ac16fe1ee9d974af64a30769819561c0abae23af"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2781",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "cce4a5b4d9dc9715ceb632adfa93540e81d22134",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516247018,
            "hunks": 2,
            "message": "STORM-2900 Always return non null collection for config keys in AbstractHadoopNimbusPluginAutoCreds.",
            "diff": [
                "diff --git a/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractHadoopNimbusPluginAutoCreds.java b/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractHadoopNimbusPluginAutoCreds.java",
                "index 6f76b9f3e..83e6d9b83 100644",
                "--- a/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractHadoopNimbusPluginAutoCreds.java",
                "+++ b/external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractHadoopNimbusPluginAutoCreds.java",
                "@@ -33,2 +33,3 @@ import java.nio.file.Paths;",
                " import java.util.ArrayList;",
                "+import java.util.Collections;",
                " import java.util.List;",
                "@@ -123,3 +124,4 @@ public abstract class AbstractHadoopNimbusPluginAutoCreds",
                "         String configKeyString = getConfigKeyString();",
                "-        return (List<String>) conf.get(configKeyString);",
                "+        List<String> configKeys = (List<String>) conf.get(configKeyString);",
                "+        return configKeys != null ? configKeys : Collections.emptyList();",
                "     }"
            ],
            "changed_files": [
                "external/storm-autocreds/src/main/java/org/apache/storm/common/AbstractHadoopNimbusPluginAutoCreds.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2900": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "fc7c1ef5f9a20b631dea8a9d04c5244a61ead310"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2900",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "a66c4a5effba66bf97e67dc55bfb79299ea02afe",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507660250,
            "hunks": 0,
            "message": "Merge branch 'STORM-2438' of https://github.com/revans2/incubator-storm into STORM-2438 STORM-2438: added in rebalance changes to support RAS This closes #2345",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2438": ""
            },
            "ghissue_refs": {
                "2345": "STORM-2759: Let users indicate if a blob should restart a worker #2363"
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2438",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2345",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "14cb3a94a65136d016da25973d82e7177b2538ce",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516729186,
            "hunks": 2,
            "message": "Use 1.7 compatible Long size",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/cluster.clj b/storm-core/src/clj/org/apache/storm/cluster.clj",
                "index eafa40b61..731a0b983 100644",
                "--- a/storm-core/src/clj/org/apache/storm/cluster.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/cluster.clj",
                "@@ -513,6 +513,6 @@",
                "             (if-not (<= timestamp 0)",
                "-              (let [bytes (.. (ByteBuffer/allocate (Long/BYTES)) (putLong timestamp) (array))]",
                "+              (let [bytes (.. (ByteBuffer/allocate (/ (Long/SIZE) 8)) (putLong timestamp) (array))]",
                "                 (.set_data cluster-state path bytes acls)))",
                "             (when timestamp",
                "-              (let [bytes (.. (ByteBuffer/allocate (Long/BYTES)) (putLong timestamp) (array))]",
                "+              (let [bytes (.. (ByteBuffer/allocate (/ (Long/SIZE) 8)) (putLong timestamp) (array))]",
                "                 (.set_ephemeral_node cluster-state path bytes acls)))))) ;; create the znode since worker is congested"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/cluster.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                }
            ]
        },
        {
            "commit_id": "c8b306dc33e606c209046374952b6c9f7cc69ca0",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511922319,
            "hunks": 7,
            "message": "STORM-2835: storm-kafka-client KafkaSpout can fail to remove all tuples from waitingToEmit",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 425681de6..9d12307e8 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -244,5 +244,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-                if (waitingToEmit()) {",
                "-                    emit();",
                "-                }",
                "+                emitIfWaitingNotEmitted();",
                "             } else {",
                "@@ -268,3 +266,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-        if (waitingToEmit()) {",
                "+        if (isWaitingToEmit()) {",
                "             LOG.debug(\"Not polling. Tuples waiting to be emitted.\");",
                "@@ -299,3 +297,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    private boolean waitingToEmit() {",
                "+    private boolean isWaitingToEmit() {",
                "         return waitingToEmit != null && waitingToEmit.hasNext();",
                "@@ -374,5 +372,9 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     // ======== emit  =========",
                "-    private void emit() {",
                "-        while (!emitTupleIfNotEmitted(waitingToEmit.next()) && waitingToEmit.hasNext()) {",
                "+    private void emitIfWaitingNotEmitted() {",
                "+        while (isWaitingToEmit()) {",
                "+            final boolean emitted = emitOrRetryTuple(waitingToEmit.next());",
                "             waitingToEmit.remove();",
                "+            if (emitted) {",
                "+                break;",
                "+            }",
                "         }",
                "@@ -381,3 +383,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     /**",
                "-     * Creates a tuple from the kafka record and emits it if it was not yet emitted.",
                "+     * Creates a tuple from the kafka record and emits it if it was never emitted or it is ready to be retried.",
                "      *",
                "@@ -386,3 +388,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "      */",
                "-    private boolean emitTupleIfNotEmitted(ConsumerRecord<K, V> record) {",
                "+    private boolean emitOrRetryTuple(ConsumerRecord<K, V> record) {",
                "         final TopicPartition tp = new TopicPartition(record.topic(), record.partition());"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2835": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "53c2c677e1066df1ac97b7a62a2d41cab484ecbd"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2835",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7f8012ae6fdf9aed7f92577bab6258a63ec39e60",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514318130,
            "hunks": 24,
            "message": "STORM-2869: Only discard outdated records when adjusting KafkaConsumer position during commit",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 07cfb52cc..7785e08d3 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -31,2 +31,3 @@ import com.google.common.annotations.VisibleForTesting;",
                " import java.io.IOException;",
                "+import java.util.ArrayList;",
                " import java.util.Collection;",
                "@@ -36,3 +37,2 @@ import java.util.HashSet;",
                " import java.util.Iterator;",
                "-import java.util.LinkedList;",
                " import java.util.List;",
                "@@ -102,3 +102,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     // Records that have been polled and are queued to be emitted in the nextTuple() call. One record is emitted per nextTuple()",
                "-    private transient Iterator<ConsumerRecord<K, V>> waitingToEmit;",
                "+    private transient Map<TopicPartition, List<ConsumerRecord<K, V>>> waitingToEmit;",
                "     // Triggers when a subscription should be refreshed",
                "@@ -142,3 +142,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         emitted = new HashSet<>();",
                "-        waitingToEmit = Collections.emptyListIterator();",
                "+        waitingToEmit = new HashMap<>();",
                "         setCommitMetadata(context);",
                "@@ -155,3 +155,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         } catch (JsonProcessingException e) {",
                "-            LOG.error(\"Failed to create Kafka commit metadata due to JSON serialization error\",e);",
                "+            LOG.error(\"Failed to create Kafka commit metadata due to JSON serialization error\", e);",
                "             throw new RuntimeException(e);",
                "@@ -168,3 +168,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         private Collection<TopicPartition> previousAssignment = new HashSet<>();",
                "-        ",
                "+",
                "         @Override",
                "@@ -208,2 +208,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             }",
                "+            waitingToEmit.keySet().retainAll(partitions);",
                "@@ -262,4 +263,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     /**",
                "-     * Checks If {@link OffsetAndMetadata} was committed by a {@link KafkaSpout} instance in this topology.",
                "-     * This info is used to decide if {@link FirstPollOffsetStrategy} should be applied",
                "+     * Checks if {@link OffsetAndMetadata} was committed by a {@link KafkaSpout} instance in this topology. This info is used to decide if",
                "+     * {@link FirstPollOffsetStrategy} should be applied",
                "      *",
                "@@ -281,3 +282,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 + \"Defaulting to behavior compatible with earlier version\", committedOffset);",
                "-            LOG.trace(\"\",e);",
                "+            LOG.trace(\"\", e);",
                "             return false;",
                "@@ -328,3 +329,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         }",
                "-        ",
                "+",
                "         Set<TopicPartition> assignment = kafkaConsumer.assignment();",
                "@@ -333,3 +334,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         }",
                "-        ",
                "+",
                "         Map<TopicPartition, Long> earliestRetriableOffsets = retryService.earliestRetriableOffsets();",
                "@@ -359,3 +360,8 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private boolean isWaitingToEmit() {",
                "-        return waitingToEmit != null && waitingToEmit.hasNext();",
                "+        for (List<ConsumerRecord<K, V>> value : waitingToEmit.values()) {",
                "+            if (!value.isEmpty()) {",
                "+                return true;",
                "+            }",
                "+        }",
                "+        return false;",
                "     }",
                "@@ -363,7 +369,5 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private void setWaitingToEmit(ConsumerRecords<K, V> consumerRecords) {",
                "-        List<ConsumerRecord<K, V>> waitingToEmitList = new LinkedList<>();",
                "         for (TopicPartition tp : consumerRecords.partitions()) {",
                "-            waitingToEmitList.addAll(consumerRecords.records(tp));",
                "+            waitingToEmit.put(tp, new ArrayList<>(consumerRecords.records(tp)));",
                "         }",
                "-        waitingToEmit = waitingToEmitList.iterator();",
                "     }",
                "@@ -432,8 +436,13 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private void emitIfWaitingNotEmitted() {",
                "-        while (isWaitingToEmit()) {",
                "-            final boolean emittedTuple = emitOrRetryTuple(waitingToEmit.next());",
                "-            waitingToEmit.remove();",
                "-            if (emittedTuple) {",
                "-                break;",
                "+        Iterator<List<ConsumerRecord<K, V>>> waitingToEmitIter = waitingToEmit.values().iterator();",
                "+        outerLoop:",
                "+        while (waitingToEmitIter.hasNext()) {",
                "+            List<ConsumerRecord<K, V>> waitingToEmitForTp = waitingToEmitIter.next();",
                "+            while (!waitingToEmitForTp.isEmpty()) {",
                "+                final boolean emittedTuple = emitOrRetryTuple(waitingToEmitForTp.remove(0));",
                "+                if (emittedTuple) {",
                "+                    break outerLoop;",
                "+                }",
                "             }",
                "+            waitingToEmitIter.remove();",
                "         }",
                "@@ -458,3 +467,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             if (committedOffset != null && isOffsetCommittedByThisTopology(tp, committedOffset)",
                "-                && committedOffset.offset() > kafkaConsumer.position(tp)) {",
                "+                && committedOffset.offset() > record.offset()) {",
                "                 // Ensures that after a topology with this id is started, the consumer fetch",
                "@@ -540,3 +549,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                      * The consumer may only be part way through \"catching up\" to where it was when it went back to retry the failed tuple. ",
                "-                     * Skip the consumer forward to the committed offset drop the current waiting to emit list,",
                "+                     * Skip the consumer forward to the committed offset and drop the current waiting to emit list,",
                "                      * since it'll likely contain committed offsets.",
                "@@ -546,5 +555,15 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                     kafkaConsumer.seek(tp, committedOffset);",
                "-                    waitingToEmit = null;",
                "+                    List<ConsumerRecord<K, V>> waitingToEmitForTp = waitingToEmit.get(tp);",
                "+                    if (waitingToEmitForTp != null) {",
                "+                        //Discard the pending records that are already committed",
                "+                        List<ConsumerRecord<K, V>> filteredRecords = new ArrayList<>();",
                "+                        for (ConsumerRecord<K, V> record : waitingToEmitForTp) {",
                "+                            if (record.offset() >= committedOffset) {",
                "+                                filteredRecords.add(record);",
                "+                            }",
                "+                        }",
                "+                        waitingToEmit.put(tp, filteredRecords);",
                "+                    }",
                "                 }",
                "-                ",
                "+",
                "                 final OffsetManager offsetManager = assignedOffsetManagers.get(tp);",
                "@@ -696,3 +715,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     }",
                "-    ",
                "+",
                "     private static class PollablePartitionsInfo {",
                "@@ -702,3 +721,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         private final Map<TopicPartition, Long> pollableEarliestRetriableOffsets;",
                "-        ",
                "+",
                "         public PollablePartitionsInfo(Set<TopicPartition> pollablePartitions, Map<TopicPartition, Long> earliestRetriableOffsets) {",
                "@@ -712,3 +731,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         }",
                "-        ",
                "+",
                "         public boolean shouldPoll() {"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2869": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "df5187e803d38f2cbb35ac9f1853ed7f2410ad0e"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2869",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b80c2a86f4993959bd4bfafa19126610f031b23d",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515198118,
            "hunks": 1,
            "message": "fix STORM-2879",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/daemon/supervisor/Slot.java b/storm-core/src/jvm/org/apache/storm/daemon/supervisor/Slot.java",
                "index 11f800a20..60bcef3fa 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/daemon/supervisor/Slot.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/daemon/supervisor/Slot.java",
                "@@ -640,3 +640,14 @@ public class Slot extends Thread implements AutoCloseable {",
                "             try {",
                "-                container = containerLauncher.recoverContainer(port, currentAssignment, localState);",
                "+                // For now we do not make a transaction when removing a topology assignment from local, an overdue",
                "+                // assignment may be left on local disk.",
                "+                // So we should check if the local disk assignment is valid when initializing:",
                "+                // if topology files does not exist, the worker[possibly alive] will be reassigned if it is timed-out;",
                "+                // if topology files exist but the topology id is invalid, just let Supervisor make a sync;",
                "+                // if topology files exist and topology files is valid, recover the container.",
                "+                if (SupervisorUtils.doRequiredTopoFilesExist(conf, currentAssignment.get_topology_id())) {",
                "+                    container = containerLauncher.recoverContainer(port, currentAssignment, localState);",
                "+                } else {",
                "+                    // Make the assignment null to let slot clean up the disk assignment.",
                "+                    currentAssignment = null;",
                "+                }",
                "             } catch (ContainerRecoveryException e) {"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/daemon/supervisor/Slot.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2879": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "501efb30a75916ca3edfb88d3a7bca21ae36c77e"
                ],
                [
                    "no-tag",
                    "9655d0dc8c4f01e17edc3ff823cf7446dbc9930e"
                ],
                [
                    "no-tag",
                    "6b939e80d7845f7288aa434469cb5768f30544d8"
                ]
            ],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2879",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "60dfa7d1b3c8e2500a64db37777de38393354046",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516682566,
            "hunks": 40,
            "message": "STORM-2901: Reuse ZK connection for Nimbus for 1.x-branch",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/command/shell_submission.clj b/storm-core/src/clj/org/apache/storm/command/shell_submission.clj",
                "index 887ab3b5c..3efcc14c7 100644",
                "--- a/storm-core/src/clj/org/apache/storm/command/shell_submission.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/command/shell_submission.clj",
                "@@ -25,3 +25,8 @@",
                "         ; since this is not a purpose to add to leader lock queue, passing nil as blob-store is ok",
                "-        zk-leader-elector (zk-leader-elector conf nil)",
                "+        zk (mk-client conf",
                "+                      (conf STORM-ZOOKEEPER-SERVERS)",
                "+                      (conf STORM-ZOOKEEPER-PORT)",
                "+                      :root (conf STORM-ZOOKEEPER-ROOT)",
                "+                      :auth-conf conf)",
                "+        zk-leader-elector (zk-leader-elector conf zk nil)",
                "         leader-nimbus (.getLeader zk-leader-elector)",
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj b/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj",
                "index 7607b1b76..bc72b29e6 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj",
                "@@ -135,2 +135,9 @@",
                "+(defn mk-zk-client [conf]",
                "+  (let [zk-servers (conf STORM-ZOOKEEPER-SERVERS)",
                "+        zk-port (conf STORM-ZOOKEEPER-PORT)",
                "+        zk-root (conf STORM-ZOOKEEPER-ROOT)]",
                "+    (if (and zk-servers zk-port)",
                "+      (mk-client conf zk-servers zk-port :root zk-root :auth-conf conf))))",
                "+",
                " (defmulti blob-sync cluster-mode)",
                "@@ -185,3 +192,4 @@",
                "   (let [forced-scheduler (.getForcedScheduler inimbus)",
                "-        blob-store (Utils/getNimbusBlobStore conf (NimbusInfo/fromConf conf))]",
                "+        blob-store (Utils/getNimbusBlobStore conf (NimbusInfo/fromConf conf))",
                "+        zk-client (mk-zk-client conf)]",
                "     {:conf conf",
                "@@ -215,3 +223,4 @@",
                "      :scheduler (mk-scheduler conf inimbus)",
                "-     :leader-elector (zk-leader-elector conf blob-store)",
                "+     :zk-client zk-client",
                "+     :leader-elector (zk-leader-elector conf zk-client blob-store)",
                "      :id->sched-status (atom {})",
                "@@ -454,5 +463,5 @@",
                "-(defn- get-version-for-key [key nimbus-host-port-info conf]",
                "+(defn- get-version-for-key [key nimbus-host-port-info zk-client]",
                "   (let [version (KeySequenceNumber. key nimbus-host-port-info)]",
                "-    (.getKeySequenceNumber version conf)))",
                "+    (.getKeySequenceNumber version zk-client)))",
                "@@ -466,2 +475,3 @@",
                "         blob-store (:blob-store nimbus)",
                "+        zk-client (:zk-client nimbus)",
                "         jar-key (master-stormjar-key storm-id)",
                "@@ -473,9 +483,9 @@",
                "       (if (instance? LocalFsBlobStore blob-store)",
                "-        (.setup-blobstore! storm-cluster-state jar-key nimbus-host-port-info (get-version-for-key jar-key nimbus-host-port-info conf))))",
                "+        (.setup-blobstore! storm-cluster-state jar-key nimbus-host-port-info (get-version-for-key jar-key nimbus-host-port-info zk-client))))",
                "     (.createBlob blob-store conf-key (Utils/toCompressedJsonConf storm-conf) (SettableBlobMeta. BlobStoreAclHandler/DEFAULT) subject)",
                "     (if (instance? LocalFsBlobStore blob-store)",
                "-      (.setup-blobstore! storm-cluster-state conf-key nimbus-host-port-info (get-version-for-key conf-key nimbus-host-port-info conf)))",
                "+      (.setup-blobstore! storm-cluster-state conf-key nimbus-host-port-info (get-version-for-key conf-key nimbus-host-port-info zk-client)))",
                "     (.createBlob blob-store code-key (Utils/serialize topology) (SettableBlobMeta. BlobStoreAclHandler/DEFAULT) subject)",
                "     (if (instance? LocalFsBlobStore blob-store)",
                "-      (.setup-blobstore! storm-cluster-state code-key nimbus-host-port-info (get-version-for-key code-key nimbus-host-port-info conf)))))",
                "+      (.setup-blobstore! storm-cluster-state code-key nimbus-host-port-info (get-version-for-key code-key nimbus-host-port-info zk-client)))))",
                "@@ -1117,3 +1127,3 @@",
                "   (try-cause",
                "-    (read-storm-conf-as-nimbus conf storm-id blob-store)",
                "+    (read-storm-conf-as-nimbus storm-id blob-store)",
                "     (catch KeyNotFoundException e",
                "@@ -1309,2 +1319,3 @@",
                "         blob-store (:blob-store nimbus)",
                "+        zk-client (:zk-client nimbus)",
                "         local-set-of-keys (set (get-key-seq-from-blob-store blob-store))",
                "@@ -1321,3 +1332,3 @@",
                "       (try",
                "-        (.setup-blobstore! storm-cluster-state key (:nimbus-host-port-info nimbus) (get-version-for-key key nimbus-host-port-info conf))",
                "+        (.setup-blobstore! storm-cluster-state key (:nimbus-host-port-info nimbus) (get-version-for-key key nimbus-host-port-info zk-client))",
                "         (catch KeyNotFoundException _",
                "@@ -1475,2 +1486,3 @@",
                "     (let [storm-cluster-state (:storm-cluster-state nimbus)",
                "+          zk-client (:zk-client nimbus)",
                "           nimbus-host-port-info (:nimbus-host-port-info nimbus)",
                "@@ -1483,3 +1495,4 @@",
                "                           (.setBlobStoreKeySet blob-store-key-set)",
                "-                          (.setZookeeperKeySet zk-key-set))]",
                "+                          (.setZookeeperKeySet zk-key-set)",
                "+                          (.setZkClient zk-client))]",
                "         (.syncBlobs sync-blobs)))))",
                "@@ -2098,2 +2111,3 @@",
                "             blob-store (:blob-store nimbus)",
                "+            zk-client (:zk-client nimbus)",
                "             nimbus-host-port-info (:nimbus-host-port-info nimbus)",
                "@@ -2101,3 +2115,3 @@",
                "         (if (instance? LocalFsBlobStore blob-store)",
                "-          (.setup-blobstore! storm-cluster-state blob-key nimbus-host-port-info (get-version-for-key blob-key nimbus-host-port-info conf)))",
                "+          (.setup-blobstore! storm-cluster-state blob-key nimbus-host-port-info (get-version-for-key blob-key nimbus-host-port-info zk-client)))",
                "         (log-debug \"Created state in zookeeper\" storm-cluster-state blob-store nimbus-host-port-info)))",
                "diff --git a/storm-core/src/clj/org/apache/storm/zookeeper.clj b/storm-core/src/clj/org/apache/storm/zookeeper.clj",
                "index ca4109337..12e8ad790 100644",
                "--- a/storm-core/src/clj/org/apache/storm/zookeeper.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/zookeeper.clj",
                "@@ -254,6 +254,4 @@",
                "   \"Zookeeper Implementation of ILeaderElector.\"",
                "-  [conf blob-store]",
                "-  (let [servers (conf STORM-ZOOKEEPER-SERVERS)",
                "-        zk (mk-client conf (conf STORM-ZOOKEEPER-SERVERS) (conf STORM-ZOOKEEPER-PORT) :auth-conf conf)",
                "-        leader-lock-path (str (conf STORM-ZOOKEEPER-ROOT) \"/leader-lock\")",
                "+  [conf zk blob-store]",
                "+  (let [leader-lock-path \"/leader-lock\"",
                "         id (.toHostPortString (NimbusInfo/fromConf conf))",
                "@@ -304,3 +302,3 @@",
                "       (^void close[this]",
                "-        (log-message \"closing zookeeper connection of leader elector.\")",
                "-        (.close zk)))))",
                "+        ;;Do nothing now.",
                "+        ))))",
                "diff --git a/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java b/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java",
                "index f1eb2f4f3..569aef27e 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java",
                "@@ -53,2 +53,6 @@ public class BlobStoreUtils {",
                "+    public static String getBlobStoreSubtree() {",
                "+        return BLOBSTORE_SUBTREE;",
                "+    }",
                "+",
                "     public static CuratorFramework createZKClient(Map conf) {",
                "@@ -287,4 +291,2 @@ public class BlobStoreUtils {",
                "     }",
                "-",
                "-",
                " }",
                "diff --git a/storm-core/src/jvm/org/apache/storm/blobstore/BlobSynchronizer.java b/storm-core/src/jvm/org/apache/storm/blobstore/BlobSynchronizer.java",
                "index f03570979..b581e1239 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/blobstore/BlobSynchronizer.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/blobstore/BlobSynchronizer.java",
                "@@ -19,2 +19,6 @@ package org.apache.storm.blobstore;",
                "+import java.util.HashSet;",
                "+import java.util.Map;",
                "+import java.util.Set;",
                "+",
                " import org.apache.storm.generated.KeyNotFoundException;",
                "@@ -25,6 +29,2 @@ import org.slf4j.LoggerFactory;",
                "-import java.util.HashSet;",
                "-import java.util.Map;",
                "-import java.util.Set;",
                "-",
                " /**",
                "@@ -59,2 +59,6 @@ public class BlobSynchronizer {",
                "+    public void setZkClient(CuratorFramework zkClient) {",
                "+        this.zkClient = zkClient;",
                "+    }",
                "+",
                "     public Set<String> getBlobStoreKeySet() {",
                "@@ -74,3 +78,2 @@ public class BlobSynchronizer {",
                "             LOG.debug(\"Sync blobs - blobstore keys {}, zookeeper keys {}\",getBlobStoreKeySet(), getZookeeperKeySet());",
                "-            zkClient = BlobStoreUtils.createZKClient(conf);",
                "             deleteKeySetFromBlobStoreNotOnZookeeper(getBlobStoreKeySet(), getZookeeperKeySet());",
                "@@ -91,5 +94,2 @@ public class BlobSynchronizer {",
                "             }",
                "-            if (zkClient !=null) {",
                "-                zkClient.close();",
                "-            }",
                "         } catch(InterruptedException exp) {",
                "diff --git a/storm-core/src/jvm/org/apache/storm/blobstore/KeySequenceNumber.java b/storm-core/src/jvm/org/apache/storm/blobstore/KeySequenceNumber.java",
                "index adbd4c4ab..570e0ad86 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/blobstore/KeySequenceNumber.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/blobstore/KeySequenceNumber.java",
                "@@ -20,6 +20,10 @@ package org.apache.storm.blobstore;",
                "+import java.nio.ByteBuffer;",
                "+import java.util.TreeSet;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+",
                "+import org.apache.curator.framework.CuratorFramework;",
                " import org.apache.storm.generated.KeyNotFoundException;",
                " import org.apache.storm.nimbus.NimbusInfo;",
                "-import org.apache.storm.utils.Utils;",
                "-import org.apache.curator.framework.CuratorFramework;",
                " import org.apache.zookeeper.CreateMode;",
                "@@ -30,7 +34,2 @@ import org.slf4j.LoggerFactory;",
                "-import java.nio.ByteBuffer;",
                "-import java.util.TreeSet;",
                "-import java.util.Map;",
                "-import java.util.List;",
                "-",
                " /**",
                "@@ -122,3 +121,2 @@ public class KeySequenceNumber {",
                "     private static final Logger LOG = LoggerFactory.getLogger(KeySequenceNumber.class);",
                "-    private final String BLOBSTORE_SUBTREE=\"/blobstore\";",
                "     private final String BLOBSTORE_MAX_KEY_SEQUENCE_SUBTREE=\"/blobstoremaxkeysequencenumber\";",
                "@@ -134,8 +132,7 @@ public class KeySequenceNumber {",
                "-    public synchronized int getKeySequenceNumber(Map conf) throws KeyNotFoundException {",
                "+    public synchronized int getKeySequenceNumber(CuratorFramework zkClient) throws KeyNotFoundException {",
                "         TreeSet<Integer> sequenceNumbers = new TreeSet<Integer>();",
                "-        CuratorFramework zkClient = BlobStoreUtils.createZKClient(conf);",
                "         try {",
                "             // Key has not been created yet and it is the first time it is being created",
                "-            if (zkClient.checkExists().forPath(BLOBSTORE_SUBTREE + \"/\" + key) == null) {",
                "+            if (zkClient.checkExists().forPath(BlobStoreUtils.getBlobStoreSubtree() + \"/\" + key) == null) {",
                "                 zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT)",
                "@@ -150,3 +147,3 @@ public class KeySequenceNumber {",
                "             // if all go down which is unlikely. Hence there might be a need to update the blob if all go down.",
                "-            List<String> stateInfoList = zkClient.getChildren().forPath(BLOBSTORE_SUBTREE + \"/\" + key);",
                "+            List<String> stateInfoList = zkClient.getChildren().forPath(BlobStoreUtils.getBlobStoreSubtree() + \"/\" + key);",
                "             LOG.debug(\"stateInfoList-size {} stateInfoList-data {}\", stateInfoList.size(), stateInfoList);",
                "@@ -210,6 +207,2 @@ public class KeySequenceNumber {",
                "             return INITIAL_SEQUENCE_NUMBER - 1;",
                "-        } finally {",
                "-            if (zkClient != null) {",
                "-                zkClient.close();",
                "-            }",
                "         }",
                "diff --git a/storm-core/src/jvm/org/apache/storm/zookeeper/LeaderElectorImp.java b/storm-core/src/jvm/org/apache/storm/zookeeper/LeaderElectorImp.java",
                "index 74816c210..2f61430d2 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/zookeeper/LeaderElectorImp.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/zookeeper/LeaderElectorImp.java",
                "@@ -120,4 +120,3 @@ public class LeaderElectorImp implements ILeaderElector {",
                "     public void close() {",
                "-        LOG.info(\"closing zookeeper connection of leader elector.\");",
                "-        zk.close();",
                "+        //Do nothing now.",
                "     }",
                "diff --git a/storm-core/src/jvm/org/apache/storm/zookeeper/Zookeeper.java b/storm-core/src/jvm/org/apache/storm/zookeeper/Zookeeper.java",
                "index a2ad797c5..b2e2236db 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/zookeeper/Zookeeper.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/zookeeper/Zookeeper.java",
                "@@ -348,3 +348,3 @@ public class Zookeeper {",
                "             public void isLeader() {",
                "-                Set<String> activeTopologyIds = new TreeSet<>(Zookeeper.getChildren(zk, conf.get(Config.STORM_ZOOKEEPER_ROOT) + ClusterUtils.STORMS_SUBTREE, false));",
                "+                Set<String> activeTopologyIds = new TreeSet<>(Zookeeper.getChildren(zk, ClusterUtils.STORMS_SUBTREE, false));",
                "@@ -456,11 +456,19 @@ public class Zookeeper {",
                "-    public static ILeaderElector zkLeaderElector(Map conf, BlobStore blobStore) throws UnknownHostException {",
                "-        return _instance.zkLeaderElectorImpl(conf, blobStore);",
                "+    /**",
                "+     * Get master leader elector.",
                "+     * @param conf Config.",
                "+     * @param zkClient ZkClient, the client must have a default Config.STORM_ZOOKEEPER_ROOT as root path.",
                "+     * @param blobStore {@link BlobStore}",
                "+     * @return Instance of {@link ILeaderElector}",
                "+     * @throws UnknownHostException",
                "+     */",
                "+    public static ILeaderElector zkLeaderElector(Map conf, CuratorFramework zkClient,",
                "+        BlobStore blobStore) throws UnknownHostException {",
                "+        return _instance.zkLeaderElectorImpl(conf, zkClient, blobStore);",
                "     }",
                "-    protected ILeaderElector zkLeaderElectorImpl(Map conf, BlobStore blobStore) throws UnknownHostException {",
                "+    protected ILeaderElector zkLeaderElectorImpl(Map conf, CuratorFramework zk, BlobStore blobStore)",
                "+        throws UnknownHostException {",
                "         List<String> servers = (List<String>) conf.get(Config.STORM_ZOOKEEPER_SERVERS);",
                "-        Object port = conf.get(Config.STORM_ZOOKEEPER_PORT);",
                "-        CuratorFramework zk = mkClientImpl(conf, servers, port, \"\", conf);",
                "-        String leaderLockPath = conf.get(Config.STORM_ZOOKEEPER_ROOT) + \"/leader-lock\";",
                "+        String leaderLockPath = \"/leader-lock\";",
                "         String id = NimbusInfo.fromConf(conf).toHostPortString();"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/command/shell_submission.clj",
                "storm-core/src/clj/org/apache/storm/daemon/nimbus.clj",
                "storm-core/src/clj/org/apache/storm/zookeeper.clj",
                "storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java",
                "storm-core/src/jvm/org/apache/storm/blobstore/BlobSynchronizer.java",
                "storm-core/src/jvm/org/apache/storm/blobstore/KeySequenceNumber.java",
                "storm-core/src/jvm/org/apache/storm/zookeeper/LeaderElectorImp.java",
                "storm-core/src/jvm/org/apache/storm/zookeeper/Zookeeper.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2901": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "348faf946864b86b1833ff1115d1c5706f638269"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2901",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "82f7450cc24f0953730d48dc0264a4df11283c57",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509377106,
            "hunks": 0,
            "message": "Merge branch 'STORM-2782-squashed' of https://github.com/kevpeek/storm into STORM-2782 STORM-2782: refactor partial key grouping This closes #2395",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2782": ""
            },
            "ghissue_refs": {
                "2395": "STORM-2782 - refactor partial key grouping to make it more flexible a\u2026 #2379"
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2782",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2395",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "01ea3d644c2bc6eabdfc7eca74fbd9c1b96d6443",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515019885,
            "hunks": 9,
            "message": "STORM-2877: Add an option to configure pagination in Storm UI",
            "diff": [
                "diff --git a/conf/defaults.yaml b/conf/defaults.yaml",
                "index 6821701c0..72278465f 100644",
                "--- a/conf/defaults.yaml",
                "+++ b/conf/defaults.yaml",
                "@@ -90,2 +90,3 @@ ui.header.buffer.bytes: 4096",
                " ui.http.creds.plugin: org.apache.storm.security.auth.DefaultHttpCredentialsPlugin",
                "+ui.pagination: 20",
                "diff --git a/storm-core/src/ui/public/component.html b/storm-core/src/ui/public/component.html",
                "index 6233c471c..cee4ce482 100644",
                "--- a/storm-core/src/ui/public/component.html",
                "+++ b/storm-core/src/ui/public/component.html",
                "@@ -145,12 +145,14 @@ $(document).ready(function() {",
                "-    $.extend( $.fn.dataTable.defaults, {",
                "-      stateSave: true,",
                "-      stateSaveCallback: function (oSettings, oData) {",
                "-        sessionStorage.setItem( oSettings.sTableId.concat(tableStateKey), JSON.stringify(oData) );",
                "-      },",
                "-      stateLoadCallback: function (oSettings) {",
                "-        return JSON.parse( sessionStorage.getItem(oSettings.sTableId.concat(tableStateKey)) );",
                "-      },",
                "-      lengthMenu: [[20,40,60,100,-1], [20, 40, 60, 100, \"All\"]],",
                "-      pageLength: 20",
                "+    $.getJSON(\"/api/v1/cluster/configuration\",function(response,status,jqXHR) {",
                "+        $.extend( $.fn.dataTable.defaults, {",
                "+          stateSave: true,",
                "+          stateSaveCallback: function (oSettings, oData) {",
                "+            sessionStorage.setItem( oSettings.sTableId.concat(tableStateKey), JSON.stringify(oData) );",
                "+          },",
                "+          stateLoadCallback: function (oSettings) {",
                "+            return JSON.parse( sessionStorage.getItem(oSettings.sTableId.concat(tableStateKey)) );",
                "+          },",
                "+          lengthMenu: [[20,40,60,100,-1], [20, 40, 60, 100, \"All\"]],",
                "+          pageLength: response[\"ui.pagination\"]",
                "+        });",
                "     });",
                "diff --git a/storm-core/src/ui/public/index.html b/storm-core/src/ui/public/index.html",
                "index 9de07222c..817eff666 100644",
                "--- a/storm-core/src/ui/public/index.html",
                "+++ b/storm-core/src/ui/public/index.html",
                "@@ -108,89 +108,89 @@ $(document).ajaxStart(function(){",
                " $(document).ready(function() {",
                "-    $.extend( $.fn.dataTable.defaults, {",
                "-      stateSave: true,",
                "-      lengthMenu: [[20,40,60,100,-1], [20, 40, 60, 100, \"All\"]],",
                "-      pageLength: 20",
                "-    });",
                "-",
                "-    $.ajaxSetup({",
                "-        \"error\":function(jqXHR,textStatus,response) {",
                "-            var errorJson = jQuery.parseJSON(jqXHR.responseText);",
                "-            getStatic(\"/templates/json-error-template.html\", function(template) {",
                "-                $(\"#json-response-error\").append(Mustache.render($(template).filter(\"#json-error-template\").html(),errorJson));",
                "-            });",
                "-        }",
                "-    });",
                "-    var uiUser = $(\"#ui-user\");",
                "-    var clusterSummary = $(\"#cluster-summary\");",
                "-    var clusterResources = $(\"#cluster-resources\");",
                "-    var nimbusSummary = $(\"#nimbus-summary\");",
                "-    var ownerSummary = $(\"#owner-summary\");",
                "-    var topologySummary = $(\"#topology-summary\");",
                "-    var supervisorSummary = $(\"#supervisor-summary\");",
                "-    var config = $(\"#nimbus-configuration\");",
                "-",
                "-    getStatic(\"/templates/index-page-template.html\", function(indexTemplate) {",
                "-        $.getJSON(\"/api/v1/cluster/summary\",function(response,status,jqXHR) {",
                "-            getStatic(\"/templates/user-template.html\", function(template) {",
                "-                uiUser.append(Mustache.render($(template).filter(\"#user-template\").html(),response));",
                "-                $('#ui-user [data-toggle=\"tooltip\"]').tooltip()",
                "-            });",
                "-",
                "-            clusterSummary.append(Mustache.render($(indexTemplate).filter(\"#cluster-summary-template\").html(),response));",
                "-            $('#cluster-summary [data-toggle=\"tooltip\"]').tooltip();",
                "-",
                "-            clusterResources.append(Mustache.render($(indexTemplate).filter(\"#cluster-resources-template\").html(),response));",
                "-            $('#cluster-resources [data-toggle=\"tooltip\"]').tooltip();",
                "+    $.getJSON(\"/api/v1/cluster/configuration\",function(responseClusterConfig,status,jqXHR) {",
                "+        $.extend( $.fn.dataTable.defaults, {",
                "+          stateSave: true,",
                "+          lengthMenu: [[20,40,60,100,-1], [20, 40, 60, 100, \"All\"]],",
                "+          pageLength: responseClusterConfig[\"ui.pagination\"]",
                "+        });",
                "-            var displayResource = response[\"schedulerDisplayResource\"];",
                "-            $('#cluster-resources [data-toggle=\"tooltip\"]').tooltip();",
                "-            if (!displayResource){",
                "-                $('#cluster-resources-header').hide();",
                "-                $('#cluster-resources').hide();",
                "-            };",
                "+        $.ajaxSetup({",
                "+            \"error\":function(jqXHR,textStatus,response) {",
                "+                var errorJson = jQuery.parseJSON(jqXHR.responseText);",
                "+                getStatic(\"/templates/json-error-template.html\", function(template) {",
                "+                    $(\"#json-response-error\").append(Mustache.render($(template).filter(\"#json-error-template\").html(),errorJson));",
                "+                });",
                "+            }",
                "         });",
                "+        var uiUser = $(\"#ui-user\");",
                "+        var clusterSummary = $(\"#cluster-summary\");",
                "+        var clusterResources = $(\"#cluster-resources\");",
                "+        var nimbusSummary = $(\"#nimbus-summary\");",
                "+        var ownerSummary = $(\"#owner-summary\");",
                "+        var topologySummary = $(\"#topology-summary\");",
                "+        var supervisorSummary = $(\"#supervisor-summary\");",
                "+        var config = $(\"#nimbus-configuration\");",
                "+",
                "+        getStatic(\"/templates/index-page-template.html\", function(indexTemplate) {",
                "+            $.getJSON(\"/api/v1/cluster/summary\",function(response,status,jqXHR) {",
                "+                getStatic(\"/templates/user-template.html\", function(template) {",
                "+                    uiUser.append(Mustache.render($(template).filter(\"#user-template\").html(),response));",
                "+                    $('#ui-user [data-toggle=\"tooltip\"]').tooltip()",
                "+                });",
                "+",
                "+                clusterSummary.append(Mustache.render($(indexTemplate).filter(\"#cluster-summary-template\").html(),response));",
                "+                $('#cluster-summary [data-toggle=\"tooltip\"]').tooltip();",
                "+",
                "+                clusterResources.append(Mustache.render($(indexTemplate).filter(\"#cluster-resources-template\").html(),response));",
                "+                $('#cluster-resources [data-toggle=\"tooltip\"]').tooltip();",
                "+",
                "+                var displayResource = response[\"schedulerDisplayResource\"];",
                "+                $('#cluster-resources [data-toggle=\"tooltip\"]').tooltip();",
                "+                if (!displayResource){",
                "+                    $('#cluster-resources-header').hide();",
                "+                    $('#cluster-resources').hide();",
                "+                };",
                "+            });",
                "-        $.getJSON(\"/api/v1/nimbus/summary\",function(response,status,jqXHR) {",
                "-            nimbusSummary.append(Mustache.render($(indexTemplate).filter(\"#nimbus-summary-template\").html(),response));",
                "-            //host, port, isLeader, version, uptime",
                "-            dtAutoPage(\"#nimbus-summary-table\", {",
                "-              columnDefs: [",
                "-                {type: \"num\", targets: [1]},",
                "-                {type: \"time-str\", targets: [4]}",
                "-              ]",
                "+            $.getJSON(\"/api/v1/nimbus/summary\",function(response,status,jqXHR) {",
                "+                nimbusSummary.append(Mustache.render($(indexTemplate).filter(\"#nimbus-summary-template\").html(),response));",
                "+                //host, port, isLeader, version, uptime",
                "+                dtAutoPage(\"#nimbus-summary-table\", {",
                "+                  columnDefs: [",
                "+                    {type: \"num\", targets: [1]},",
                "+                    {type: \"time-str\", targets: [4]}",
                "+                  ]",
                "+                });",
                "+                $('#nimbus-summary [data-toggle=\"tooltip\"]').tooltip();",
                "             });",
                "-            $('#nimbus-summary [data-toggle=\"tooltip\"]').tooltip();",
                "-        });",
                "-        $.getJSON(\"/api/v1/owner-resources\", function(response, status, jqXHR) {",
                "-            ownerSummary.append(Mustache.render($(indexTemplate).filter(\"#owner-summary-template\").html(), response));",
                "-            makeOwnerSummaryTable(response, '#owner-summary-table', '#owner-summary');",
                "-        });",
                "+            $.getJSON(\"/api/v1/owner-resources\", function(response, status, jqXHR) {",
                "+                ownerSummary.append(Mustache.render($(indexTemplate).filter(\"#owner-summary-template\").html(), response));",
                "+                makeOwnerSummaryTable(response, '#owner-summary-table', '#owner-summary');",
                "+            });",
                "-        $.getJSON(\"/api/v1/topology/summary\",function(response,status,jqXHR) {",
                "-            topologySummary.append(Mustache.render($(indexTemplate).filter(\"#topology-summary-template\").html(),response));",
                "-            //name, owner, status, uptime, num workers, num executors, num tasks, replication count, assigned total mem, assigned total cpu, scheduler info",
                "-            dtAutoPage(\"#topology-summary-table\", {",
                "-              columnDefs: [",
                "-                {type: \"num\", targets: [4, 5, 6, 7, 8, 9]},",
                "-                {type: \"time-str\", targets: [3]}",
                "-              ]",
                "+            $.getJSON(\"/api/v1/topology/summary\",function(response,status,jqXHR) {",
                "+                topologySummary.append(Mustache.render($(indexTemplate).filter(\"#topology-summary-template\").html(),response));",
                "+                //name, owner, status, uptime, num workers, num executors, num tasks, replication count, assigned total mem, assigned total cpu, scheduler info",
                "+                dtAutoPage(\"#topology-summary-table\", {",
                "+                  columnDefs: [",
                "+                    {type: \"num\", targets: [4, 5, 6, 7, 8, 9]},",
                "+                    {type: \"time-str\", targets: [3]}",
                "+                  ]",
                "+                });",
                "+                $('#topology-summary [data-toggle=\"tooltip\"]').tooltip();",
                "             });",
                "-            $('#topology-summary [data-toggle=\"tooltip\"]').tooltip();",
                "-        });",
                "-        $.getJSON(\"/api/v1/supervisor/summary\",function(response,status,jqXHR) {",
                "-            supervisorSummary.append(Mustache.render($(indexTemplate).filter(\"#supervisor-summary-template\").html(),response));",
                "-            //id, host, uptime, slots, used slots",
                "-            dtAutoPage(\"#supervisor-summary-table\", {",
                "-              columnDefs: [",
                "-                {type: \"num\", targets: [3, 4]},",
                "-                {type: \"time-str\", targets: [2]}",
                "-              ]",
                "+            $.getJSON(\"/api/v1/supervisor/summary\",function(response,status,jqXHR) {",
                "+                supervisorSummary.append(Mustache.render($(indexTemplate).filter(\"#supervisor-summary-template\").html(),response));",
                "+                //id, host, uptime, slots, used slots",
                "+                dtAutoPage(\"#supervisor-summary-table\", {",
                "+                  columnDefs: [",
                "+                    {type: \"num\", targets: [3, 4]},",
                "+                    {type: \"time-str\", targets: [2]}",
                "+                  ]",
                "+                });",
                "+                $('#supervisor-summary [data-toggle=\"tooltip\"]').tooltip();",
                "             });",
                "-            $('#supervisor-summary [data-toggle=\"tooltip\"]').tooltip();",
                "-        });",
                "-        $.getJSON(\"/api/v1/cluster/configuration\",function(response,status,jqXHR) {",
                "-          var formattedResponse = formatConfigData(response);",
                "+          var formattedResponse = formatConfigData(responseClusterConfig);",
                "           config.append(Mustache.render($(indexTemplate).filter(\"#configuration-template\").html(),formattedResponse));",
                "diff --git a/storm-core/src/ui/public/topology.html b/storm-core/src/ui/public/topology.html",
                "index 2eac3a402..02502423f 100644",
                "--- a/storm-core/src/ui/public/topology.html",
                "+++ b/storm-core/src/ui/public/topology.html",
                "@@ -253,12 +253,14 @@ $(document).ready(function() {",
                "     if(window) url += \"&window=\"+window;",
                "-    $.extend( $.fn.dataTable.defaults, {",
                "-      stateSave: true,",
                "-      stateSaveCallback: function (oSettings, oData) {",
                "-        sessionStorage.setItem( oSettings.sTableId.concat(tableStateKey), JSON.stringify(oData) );",
                "-      },",
                "-      stateLoadCallback: function (oSettings) {",
                "-        return JSON.parse( sessionStorage.getItem(oSettings.sTableId.concat(tableStateKey)) );",
                "-      },",
                "-      lengthMenu: [[20,40,60,100,-1], [20, 40, 60, 100, \"All\"]],",
                "-      pageLength: 20",
                "+    $.getJSON(\"/api/v1/cluster/configuration\",function(response,status,jqXHR) {",
                "+        $.extend( $.fn.dataTable.defaults, {",
                "+          stateSave: true,",
                "+          stateSaveCallback: function (oSettings, oData) {",
                "+            sessionStorage.setItem( oSettings.sTableId.concat(tableStateKey), JSON.stringify(oData) );",
                "+          },",
                "+          stateLoadCallback: function (oSettings) {",
                "+            return JSON.parse( sessionStorage.getItem(oSettings.sTableId.concat(tableStateKey)) );",
                "+          },",
                "+          lengthMenu: [[20,40,60,100,-1], [20, 40, 60, 100, \"All\"]],",
                "+          pageLength: response[\"ui.pagination\"]",
                "+        });",
                "     });",
                "diff --git a/storm-server/src/main/java/org/apache/storm/DaemonConfig.java b/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "index 4d2229948..f80656222 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "@@ -309,2 +309,9 @@ public class DaemonConfig implements Validated {",
                "+    /**",
                "+     * Storm UI drop-down pagination value. Set ui.pagination to be a positive integer",
                "+     * or -1 (displays all entries). Valid values: -1, 10, 20, 25 etc.",
                "+     */",
                "+    @isInteger",
                "+    public static final String UI_PAGINATION = \"ui.pagination\";",
                "+",
                "     /**"
            ],
            "changed_files": [
                "conf/defaults.yaml",
                "storm-core/src/ui/public/component.html",
                "storm-core/src/ui/public/index.html",
                "storm-core/src/ui/public/topology.html",
                "storm-server/src/main/java/org/apache/storm/DaemonConfig.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2877": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "f327c98349fbbacc90fbd4839f0fabe29d2cb562"
                ],
                [
                    "no-tag",
                    "70e111204227b351132a2924d72800a598147017"
                ],
                [
                    "no-tag",
                    "075f9e1666eac72534ba7fdf4fff718fd52b4c3d"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2877",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "06341d8de6ad5a71ec00eb1336ba86b4f02ea720",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510071530,
            "hunks": 0,
            "message": "Merge branch 'STORM-2792' of https://github.com/revans2/incubator-storm into STORM-2792 STORM-2792: Remove RAS EvictionPolicy and cleanup This closes #2400",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2792": ""
            },
            "ghissue_refs": {
                "2400": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2792",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2400",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "2846417c8a2b2d990146063bc30aef5acbb6933c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514927947,
            "hunks": 0,
            "message": "Merge branch 'YSTORM-4457-II' of https://github.com/govind-menon/storm into STORM-2872 STORM-2872: Fix for wouldFit and rebalance as part of GenericResourceAwareScheduling changes This closes #2456",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "YSTORM-4457": "",
                "STORM-2872": ""
            },
            "ghissue_refs": {
                "2456": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: YSTORM-4457, STORM-2872",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2456",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "a18657d0cd01601dfd193dccade1f601fbef94b4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515091953,
            "hunks": 0,
            "message": "Merge branch 'slot-bug-fix' of https://github.com/danny0405/storm into STORM-2879 STORM-2879: Supervisor collapse continuously when there is a expired assignment for overdue storm This closes #2493",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2879": ""
            },
            "ghissue_refs": {
                "2493": "[STORM-2879] Supervisor collapse continuously when there is a expired assignment for overdue storm [fix for 1.x-branch] #2503 [STORM-2879] Supervisor collapse continuously when there is a expired assignment for overdue storm [fix for 1.x-branch] #2506"
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2879",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2493",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "0b547d373abeaac721c409034e3ae4b7ff485dd6",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508914374,
            "hunks": 2,
            "message": "STORM-2787: storm-kafka-client KafkaSpout method onPartitionsRevoked(...) should set initialized flag independently of processing guarantees",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 170c02568..4e3090cb5 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -153,7 +153,9 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         public void onPartitionsRevoked(Collection<TopicPartition> partitions) {",
                "+            initialized = false;",
                "+            previousAssignment = partitions;",
                "+",
                "             LOG.info(\"Partitions revoked. [consumer-group={}, consumer={}, topic-partitions={}]\",",
                "                     kafkaSpoutConfig.getConsumerGroupId(), kafkaConsumer, partitions);",
                "-            previousAssignment = partitions;",
                "-            if (isAtLeastOnceProcessing() && initialized) {",
                "-                initialized = false;",
                "+",
                "+            if (isAtLeastOnceProcessing()) {",
                "                 commitOffsetsForAckedTuples();"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2787": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "7d4ac07684a7405d6539a3bd0cb7da985736bac7"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2787",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "58ae04b09995e3182f2005881e64eacf5737196a",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514929415,
            "hunks": 3,
            "message": "Merge branch 'STORM-2856' of https://github.com/srdo/storm into STORM-2856 STORM-2856: Make Storm build work on post-2017Q4 Travis Trusty image This closes #2486",
            "diff": [
                "diff --git a/.travis.yml b/.travis.yml",
                "index 5af427a50..597cf87da 100644",
                "--- a/.travis.yml",
                "+++ b/.travis.yml",
                "@@ -22,3 +22,2 @@ dist: trusty",
                " sudo: required",
                "-group: deprecated-2017Q4",
                "@@ -29,5 +28,6 @@ jdk:",
                " before_install:",
                "-  - rvm use 2.1.5 --install",
                "-  - nvm install 0.12.2",
                "-  - nvm use 0.12.2",
                "+  - rvm reload",
                "+  - rvm use 2.4.2 --install",
                "+  - nvm install 8.9.3",
                "+  - nvm use 8.9.3",
                " install: /bin/bash ./dev-tools/travis/travis-install.sh `pwd`",
                "@@ -40,2 +40,2 @@ cache:",
                "     - \"$HOME/.rvm\"",
                "-    - \"$NVM_DIR\"",
                "+    - \"$NVM_DIR\"",
                "\\ No newline at end of file"
            ],
            "changed_files": [
                ".travis.yml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2856": ""
            },
            "ghissue_refs": {
                "2486": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2856",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2486",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "dd04a5563317fa6f57d3d7ec32190940b98454d7",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516654062,
            "hunks": 19,
            "message": "Adding backpressure timeout, backpressure znodes cleanup, Do not delete backpressure ephemeral node frequently",
            "diff": [
                "diff --git a/conf/defaults.yaml b/conf/defaults.yaml",
                "index f89211b07..2bd785517 100644",
                "--- a/conf/defaults.yaml",
                "+++ b/conf/defaults.yaml",
                "@@ -196,2 +196,4 @@ backpressure.disruptor.high.watermark: 0.9",
                " backpressure.disruptor.low.watermark: 0.4",
                "+backpressure.znode.timeout.secs: 30",
                "+backpressure.znode.update.freq.secs: 15",
                "diff --git a/storm-core/src/clj/org/apache/storm/cluster.clj b/storm-core/src/clj/org/apache/storm/cluster.clj",
                "index 810b3c3c0..eafa40b61 100644",
                "--- a/storm-core/src/clj/org/apache/storm/cluster.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/cluster.clj",
                "@@ -20,3 +20,4 @@",
                "             LogConfig ProfileAction ProfileRequest NodeInfo]",
                "-           [java.io Serializable])",
                "+           [java.io Serializable]",
                "+           [java.nio ByteBuffer])",
                "   (:import [org.apache.zookeeper KeeperException KeeperException$NoNodeException ZooDefs ZooDefs$Ids ZooDefs$Perms])",
                "@@ -82,3 +83,3 @@",
                "   (worker-backpressure! [this storm-id node port info])",
                "-  (topology-backpressure [this storm-id callback])",
                "+  (topology-backpressure [this storm-id timeout-ms callback])",
                "   (setup-backpressure! [this storm-id])",
                "@@ -174,2 +175,6 @@",
                "+(defn backpressure-full-path",
                "+  [storm-id short-path]",
                "+  (str (backpressure-storm-root storm-id) \"/\" short-path))",
                "+",
                " (defn error-storm-root",
                "@@ -244,2 +249,16 @@",
                "+",
                "+(defn max-timestamp",
                "+  \"Reduces the timestamps (e.g. those set by worker-backpressure!)",
                "+  to the most recent timestamp\"",
                "+  [cluster-state storm-id paths]",
                "+  (reduce (fn [acc path]",
                "+            (let [data (.get_data cluster-state (backpressure-full-path storm-id path) false)",
                "+                  timestamp (if data",
                "+                              (.. (ByteBuffer/wrap data) (getLong))",
                "+                              0)]",
                "+              (Math/max acc timestamp)))",
                "+          0",
                "+          paths))",
                "+",
                " ;; Watches should be used for optimization. When ZK is reconnecting, they're not guaranteed to be called.",
                "@@ -485,5 +504,7 @@",
                "       (worker-backpressure!",
                "-        [this storm-id node port on?]",
                "-        \"if znode exists and to be not on?, delete; if exists and on?, do nothing;",
                "-        if not exists and to be on?, create; if not exists and not on?, do nothing\"",
                "+        [this storm-id node port timestamp]",
                "+        \"If znode exists and timestamp is non-positive, ignore;",
                "+         if exists and timestamp is larger than 0, update the timestamp;",
                "+         if not exists and timestamp is larger than 0, create the znode and set the timestamp;",
                "+         if not exists and timestamp is non-positive, do nothing.\"",
                "         (let [path (backpressure-path storm-id node port)",
                "@@ -491,10 +512,14 @@",
                "           (if existed",
                "-            (if (not on?)",
                "-              (.delete_node cluster-state path))   ;; delete the znode since the worker is not congested",
                "-            (if on?",
                "-              (.set_ephemeral_node cluster-state path nil acls))))) ;; create the znode since worker is congested",
                "+            (if-not (<= timestamp 0)",
                "+              (let [bytes (.. (ByteBuffer/allocate (Long/BYTES)) (putLong timestamp) (array))]",
                "+                (.set_data cluster-state path bytes acls)))",
                "+            (when timestamp",
                "+              (let [bytes (.. (ByteBuffer/allocate (Long/BYTES)) (putLong timestamp) (array))]",
                "+                (.set_ephemeral_node cluster-state path bytes acls)))))) ;; create the znode since worker is congested",
                "       (topology-backpressure",
                "-        [this storm-id callback]",
                "+        [this storm-id timeout-ms callback]",
                "         \"if the backpresure/storm-id dir is not empty, this topology has throttle-on, otherwise throttle-off.",
                "+         But if the backpresure/storm-id dir is not empty and has not been updated for more than timeoutMs, we treat it as throttle-off.",
                "+         This will prevent the spouts from getting stuck indefinitely if something wrong happens.",
                "          The backpressure/storm-id dir may not exist if nimbus has shutdown the topology\"",
                "@@ -504,4 +529,8 @@",
                "               children (if (.node_exists cluster-state path false)",
                "-                         (.get_children cluster-state path (not-nil? callback))) ]",
                "-              (> (count children) 0)))",
                "+                         (.get_children cluster-state path (not-nil? callback)))",
                "+              most-recent-backpressure (max-timestamp cluster-state storm-id children)",
                "+              current-time (System/currentTimeMillis)",
                "+              ret (> timeout-ms (- current-time most-recent-backpressure))]",
                "+          (log-debug \"topology backpressure is \" (if ret \"on\" \"off\"))",
                "+          ret))",
                "@@ -513,3 +542,6 @@",
                "         [this storm-id]",
                "-        (.delete_node cluster-state (backpressure-storm-root storm-id)))",
                "+        (try-cause",
                "+          (.delete_node cluster-state (backpressure-storm-root storm-id))",
                "+          (catch KeeperException e",
                "+            (log-warn-error e \"Could not teardown backpressure for \" storm-id))))",
                "@@ -520,3 +552,6 @@",
                "           (if existed",
                "-            (.delete_node cluster-state (backpressure-path storm-id node port)))))",
                "+            (try-cause",
                "+              (.delete_node cluster-state (backpressure-path storm-id node port))",
                "+              (catch KeeperException e",
                "+                (log-warn-error e \"Could not teardown backpressure for \" storm-id))))))",
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/worker.clj b/storm-core/src/clj/org/apache/storm/daemon/worker.clj",
                "index 6626272a5..633a61d62 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/worker.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/worker.clj",
                "@@ -133,26 +133,35 @@",
                "-(defn- mk-backpressure-handler [executors]",
                "-  \"make a handler that checks and updates worker's backpressure flag\"",
                "-  (disruptor/worker-backpressure-handler",
                "-    (fn [worker]",
                "-      (let [storm-id (:storm-id worker)",
                "-            assignment-id (:assignment-id worker)",
                "-            port (:port worker)",
                "-            storm-cluster-state (:storm-cluster-state worker)",
                "-            prev-backpressure-flag @(:backpressure worker)",
                "-            ;; the backpressure flag is true if at least one of the disruptor queues has throttle-on",
                "-            curr-backpressure-flag (if executors",
                "-                                     (or (.getThrottleOn (:transfer-queue worker))",
                "-                                       (reduce #(or %1 %2) (map #(.get-backpressure-flag %1) executors)))",
                "-                                     prev-backpressure-flag)]",
                "-        ;; update the worker's backpressure flag to zookeeper only when it has changed",
                "-        (when (not= prev-backpressure-flag curr-backpressure-flag)",
                "-          (try",
                "-            (log-debug \"worker backpressure flag changing from \" prev-backpressure-flag \" to \" curr-backpressure-flag)",
                "-            (.worker-backpressure! storm-cluster-state storm-id assignment-id port curr-backpressure-flag)",
                "-            ;; doing the local reset after the zk update succeeds is very important to avoid a bad state upon zk exception",
                "-            (reset! (:backpressure worker) curr-backpressure-flag)",
                "-            (catch Exception exc",
                "-              (log-error exc \"workerBackpressure update failed when connecting to ZK ... will retry\"))))",
                "-        ))))",
                "+(defn should-trigger-backpressure [executors worker]",
                "+  (or (.getThrottleOn (:transfer-queue worker))",
                "+      (reduce #(or %1 %2) (map #(.get-backpressure-flag %1) executors))))",
                "+",
                "+(defn- mk-backpressure-handler [executors topo-conf]",
                "+  \"make a handler that checks and updates worker's backpressure timestamp\"",
                "+  (let [update-freq-ms (* (topo-conf BACKPRESSURE-ZNODE-UPDATE-FREQ-SECS) 1000)]",
                "+    (disruptor/worker-backpressure-handler",
                "+      (if executors",
                "+        (fn [worker]",
                "+          (let [storm-id (:storm-id worker)",
                "+                assignment-id (:assignment-id worker)",
                "+                port (:port worker)",
                "+                storm-cluster-state (:storm-cluster-state worker)",
                "+                prev-backpressure-timestamp @(:backpressure worker)",
                "+                curr-timestamp (System/currentTimeMillis)",
                "+                ;; the backpressure flag is true if at least one of the disruptor queues has throttle-on",
                "+                curr-backpressure-timestamp (if (should-trigger-backpressure executors worker)",
                "+                                              ;; Update the backpressure timestamp every update-freq-ms seconds",
                "+                                              (if (> (- curr-timestamp (or prev-backpressure-timestamp 0)) update-freq-ms)",
                "+                                                curr-timestamp",
                "+                                                prev-backpressure-timestamp)",
                "+                                              0)]",
                "+            ;; update the worker's backpressure timestamp to zookeeper only when it has changed",
                "+            (when (not= prev-backpressure-timestamp curr-backpressure-timestamp)",
                "+              (try",
                "+                (log-debug \"worker backpressure timestamp changing from \" prev-backpressure-timestamp \" to \" curr-backpressure-timestamp)",
                "+                (.worker-backpressure! storm-cluster-state storm-id assignment-id port curr-backpressure-timestamp)",
                "+                ;; doing the local reset after the zk update succeeds is very important to avoid a bad state upon zk exception",
                "+                (reset! (:backpressure worker) curr-backpressure-timestamp)",
                "+                (catch Exception exc",
                "+                  (log-error exc \"workerBackpressure update failed when connecting to ZK ... will retry\"))))))",
                "+        (fn [workers])))))",
                "@@ -319,3 +328,3 @@",
                "       :assignment-versions assignment-versions",
                "-      :backpressure (atom false) ;; whether this worker is going slow",
                "+      :backpressure (atom 0) ;; whether this worker is going slow. non-positive means turning off backpressure",
                "       :backpressure-trigger (Object.) ;; a trigger for synchronization with executors",
                "@@ -649,3 +658,3 @@",
                "               (.setEnableBackpressure ((:storm-conf worker) TOPOLOGY-BACKPRESSURE-ENABLE)))",
                "-        backpressure-handler (mk-backpressure-handler @executors)        ",
                "+        backpressure-handler (mk-backpressure-handler @executors storm-conf)",
                "         backpressure-thread (WorkerBackpressureThread. (:backpressure-trigger worker) worker backpressure-handler)",
                "@@ -653,7 +662,10 @@",
                "             (.start backpressure-thread))",
                "+        ;; this callback is registered as a zk watch on topology's backpressure directory",
                "+        ;; which makes sure that the topology's backpressure status is updated to the worker's throttle-on",
                "+        backpressure-znode-timeout-ms (* (storm-conf BACKPRESSURE-ZNODE-TIMEOUT-SECS) 1000)",
                "         topology-backpressure-callback (fn cb [& ignored]",
                "-                   (let [throttle-on (.topology-backpressure storm-cluster-state storm-id cb)]",
                "+                   (let [throttle-on (.topology-backpressure storm-cluster-state storm-id backpressure-znode-timeout-ms cb)]",
                "                      (reset! (:throttle-on worker) throttle-on)))",
                "         _ (if ((:storm-conf worker) TOPOLOGY-BACKPRESSURE-ENABLE)",
                "-            (.topology-backpressure storm-cluster-state storm-id topology-backpressure-callback))",
                "+            (.topology-backpressure storm-cluster-state storm-id backpressure-znode-timeout-ms topology-backpressure-callback))",
                "diff --git a/storm-core/src/jvm/org/apache/storm/Config.java b/storm-core/src/jvm/org/apache/storm/Config.java",
                "index 6b0c86865..11f980ed9 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/Config.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/Config.java",
                "@@ -1562,2 +1562,19 @@ public class Config extends HashMap<String, Object> {",
                "+    /**",
                "+     * How long until the backpressure znode is invalid.",
                "+     * It's measured by the data (timestamp) of the znode, not the ctime (creation time) or mtime (modification time), etc.",
                "+     * This must be larger than BACKPRESSURE_ZNODE_UPDATE_FREQ_SECS.",
                "+     */",
                "+    @isInteger",
                "+    @isPositiveNumber",
                "+    public static final String BACKPRESSURE_ZNODE_TIMEOUT_SECS = \"backpressure.znode.timeout.secs\";",
                "+",
                "+    /**",
                "+     * How often will the data (timestamp) of backpressure znode be updated.",
                "+     * But if the worker backpressure status (on/off) changes, the znode will be updated anyway.",
                "+     */",
                "+    @isInteger",
                "+    @isPositiveNumber",
                "+    public static final String BACKPRESSURE_ZNODE_UPDATE_FREQ_SECS = \"backpressure.znode.update.freq.secs\";",
                "+",
                "     /**"
            ],
            "changed_files": [
                "conf/defaults.yaml",
                "storm-core/src/clj/org/apache/storm/cluster.clj",
                "storm-core/src/clj/org/apache/storm/daemon/worker.clj",
                "storm-core/src/jvm/org/apache/storm/Config.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: cluster",
                    "relevance": 4
                }
            ]
        },
        {
            "commit_id": "3225fd619f4e6201a49ff835235fa35d262cbda3",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508968642,
            "hunks": 3,
            "message": "Merge branch 'STORM-2786-1.x' of https://github.com/revans2/incubator-storm into STORM-2786 STORM-2786: Enable tick tuples for ackers This closes #2383",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index 52063fc78..66d18513b 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -349,2 +349,3 @@",
                "   (let [storm-conf (:storm-conf executor-data)",
                "+        comp-id (:component-id executor-data)",
                "         tick-time-secs (storm-conf TOPOLOGY-TICK-TUPLE-FREQ-SECS)",
                "@@ -353,13 +354,13 @@",
                "     (when tick-time-secs",
                "-      (if (or (Utils/isSystemId (:component-id executor-data))",
                "+      (if (or (and (not= \"__acker\" comp-id) (Utils/isSystemId comp-id))",
                "               (and (= false (storm-conf TOPOLOGY-ENABLE-MESSAGE-TIMEOUTS))",
                "                    (= :spout (:type executor-data))))",
                "-        (log-message \"Timeouts disabled for executor \" (:component-id executor-data) \":\" (:executor-id executor-data))",
                "-        (schedule-recurring",
                "-          (:user-timer worker)",
                "-          tick-time-secs",
                "-          tick-time-secs",
                "-          (fn []",
                "-            (let [val [(AddressedTuple. AddressedTuple/BROADCAST_DEST (TupleImpl. context [tick-time-secs] Constants/SYSTEM_TASK_ID Constants/SYSTEM_TICK_STREAM_ID))]]",
                "-              (disruptor/publish receive-queue val))))))))",
                "+        (log-message \"Timeouts disabled for executor \" comp-id \":\" (:executor-id executor-data))",
                "+        (let [val [(AddressedTuple. AddressedTuple/BROADCAST_DEST (TupleImpl. context [tick-time-secs] Constants/SYSTEM_TASK_ID Constants/SYSTEM_TICK_STREAM_ID))]]",
                "+          (schedule-recurring",
                "+            (:user-timer worker)",
                "+            tick-time-secs",
                "+            tick-time-secs",
                "+            (fn []",
                "+                (disruptor/publish receive-queue val))))))))"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2786": ""
            },
            "ghissue_refs": {
                "2383": "STORM-2786: Turn ticks back on for ackers (2.x) #2382"
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2786",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2383",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "bc6fb46eede327ddc61b72ccd9f3545769728b54",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507909520,
            "hunks": 5,
            "message": "Merge branch 'STORM-2771' of https://github.com/revans2/incubator-storm into STORM-2771 STORM-2771: By default don't run any tests as integration tests This closes #2368",
            "diff": [
                "diff --git a/pom.xml b/pom.xml",
                "index 5e05e8d4f..1936db05e 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -261,2 +261,3 @@",
                "         <java.unit.test.include>**/Test*.java, **/*Test.java, **/*TestCase.java</java.unit.test.include>    <!--maven surefire plugin default test list-->",
                "+        <java.integration.test.include>no.tests</java.integration.test.include>",
                "         <!-- by default the clojure test set are all clojure tests that are not integration tests. This property is overridden in the profiles -->",
                "@@ -478,3 +479,2 @@",
                "                 <java.integration.test.include>**/*.java</java.integration.test.include>",
                "-                <java.integration.test.group>org.apache.storm.testing.IntegrationTest</java.integration.test.group>",
                "                 <clojure.test.set>*.*</clojure.test.set>",
                "@@ -488,3 +488,2 @@",
                "                 <java.integration.test.include>**/*.java</java.integration.test.include>",
                "-                <java.integration.test.group>org.apache.storm.testing.IntegrationTest</java.integration.test.group>",
                "                 <!--Clojure-->",
                "@@ -941,2 +940,3 @@",
                "                     <configuration>",
                "+                        <redirectTestOutputToFile>true</redirectTestOutputToFile>",
                "                         <includes>",
                "@@ -944,3 +944,3 @@",
                "                         </includes>",
                "-                        <groups>${java.integration.test.group}</groups>  <!--set in integration-test the profile-->",
                "+                        <groups>org.apache.storm.testing.IntegrationTest</groups>",
                "                         <argLine>-Xmx1536m</argLine>"
            ],
            "changed_files": [
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2771": ""
            },
            "ghissue_refs": {
                "2368": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2771",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2368",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1b8f67c14970323667cd5a6ecb15bf0ee73f9c98",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509384572,
            "hunks": 0,
            "message": "Merge branch 'add-admin-groups' of https://github.com/kishorvpatil/incubator-storm into STORM-2790 STORM-2790: Add nimbus admins groups This closes #2390",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2790": ""
            },
            "ghissue_refs": {
                "2390": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2790",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2390",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d1e6bbb50ee5dbfb595d27e3630205d29ff975c2",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507584456,
            "hunks": 45,
            "message": "STORM-2483: Addressed review comments",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "index da6292534..4baff6636 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "@@ -117,4 +117,2 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "-    //TODO go through all of the state transitions and make sure we handle changingBlobs",
                "-    //TODO make sure to add in transition helpers that clean changingBlobs && pendingChangeingBlobs for not the current topology",
                "     static class DynamicState {",
                "@@ -128,5 +126,5 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "         public final Set<TopoProfileAction> pendingStopProfileActions;",
                "-        public final Set<BlobChangeing> changingBlobs;",
                "+        public final Set<BlobChanging> changingBlobs;",
                "         public final LocalAssignment pendingChangingBlobsAssignment;",
                "-        public final Set<Future<Void>> pendingChangeingBlobs;",
                "+        public final Set<Future<Void>> pendingChangingBlobs;",
                "@@ -158,3 +156,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "             this.pendingChangingBlobsAssignment = null;",
                "-            this.pendingChangeingBlobs = Collections.emptySet();",
                "+            this.pendingChangingBlobs = Collections.emptySet();",
                "         }",
                "@@ -166,3 +164,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                             final Set<TopoProfileAction> pendingStopProfileActions,",
                "-                            final Set<BlobChangeing> changingBlobs,",
                "+                            final Set<BlobChanging> changingBlobs,",
                "                             final Set<Future<Void>> pendingChangingBlobs, final LocalAssignment pendingChaningBlobsAssignment) {",
                "@@ -180,3 +178,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "             this.changingBlobs = changingBlobs;",
                "-            this.pendingChangeingBlobs = pendingChangingBlobs;",
                "+            this.pendingChangingBlobs = pendingChangingBlobs;",
                "             this.pendingChangingBlobsAssignment = pendingChaningBlobsAssignment;",
                "@@ -208,3 +206,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                 this.pendingStopProfileActions, this.changingBlobs,",
                "-                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "+                this.pendingChangingBlobs, this.pendingChangingBlobsAssignment);",
                "         }",
                "@@ -217,3 +215,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                 this.pendingStopProfileActions, this.changingBlobs,",
                "-                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "+                this.pendingChangingBlobs, this.pendingChangingBlobsAssignment);",
                "         }",
                "@@ -231,3 +229,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                 this.pendingStopProfileActions, this.changingBlobs,",
                "-                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "+                this.pendingChangingBlobs, this.pendingChangingBlobsAssignment);",
                "         }",
                "@@ -240,3 +238,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                 this.pendingStopProfileActions, this.changingBlobs,",
                "-                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "+                this.pendingChangingBlobs, this.pendingChangingBlobsAssignment);",
                "         }",
                "@@ -249,6 +247,6 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                 pendingStopProfileActions, this.changingBlobs,",
                "-                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "+                this.pendingChangingBlobs, this.pendingChangingBlobsAssignment);",
                "         }",
                "-        public DynamicState withChangingBlobs(Set<BlobChangeing> changingBlobs) {",
                "+        public DynamicState withChangingBlobs(Set<BlobChanging> changingBlobs) {",
                "             if (changingBlobs == this.changingBlobs) {",
                "@@ -261,7 +259,7 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                 this.pendingStopProfileActions, changingBlobs,",
                "-                this.pendingChangeingBlobs, this.pendingChangingBlobsAssignment);",
                "+                this.pendingChangingBlobs, this.pendingChangingBlobsAssignment);",
                "         }",
                "-        public DynamicState withPendingChangeingBlobs(Set<Future<Void>> pendingChangeingBlobs,",
                "-                                                      LocalAssignment pendingChangeingBlobsAssignment) {",
                "+        public DynamicState withPendingChangingBlobs(Set<Future<Void>> pendingChangingBlobs,",
                "+                                                     LocalAssignment pendingChangingBlobsAssignment) {",
                "             return new DynamicState(this.state, this.newAssignment,",
                "@@ -271,4 +269,4 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                 this.pendingStopProfileActions, this.changingBlobs,",
                "-                pendingChangeingBlobs,",
                "-                pendingChangeingBlobsAssignment);",
                "+                pendingChangingBlobs,",
                "+                pendingChangingBlobsAssignment);",
                "         }",
                "@@ -320,3 +318,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "      */",
                "-    static class BlobChangeing {",
                "+    static class BlobChanging {",
                "         private final LocalAssignment assignment;",
                "@@ -325,3 +323,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "-        public BlobChangeing(LocalAssignment assignment, LocallyCachedBlob blob, GoodToGo.GoodToGoLatch latch) {",
                "+        public BlobChanging(LocalAssignment assignment, LocallyCachedBlob blob, GoodToGo.GoodToGoLatch latch) {",
                "             this.assignment = assignment;",
                "@@ -503,3 +501,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "     /**",
                "-     * Drop all of the changingBlobs and pendingChangeingBlobs.",
                "+     * Drop all of the changingBlobs and pendingChangingBlobs.",
                "      * @param dynamicState current state.",
                "@@ -509,3 +507,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "         if (!dynamicState.changingBlobs.isEmpty()) {",
                "-            for (BlobChangeing rc : dynamicState.changingBlobs) {",
                "+            for (BlobChanging rc : dynamicState.changingBlobs) {",
                "                 rc.latch.countDown();",
                "@@ -515,4 +513,4 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "-        if (!dynamicState.pendingChangeingBlobs.isEmpty()) {",
                "-            dynamicState = dynamicState.withPendingChangeingBlobs(Collections.emptySet(), null);",
                "+        if (!dynamicState.pendingChangingBlobs.isEmpty()) {",
                "+            dynamicState = dynamicState.withPendingChangingBlobs(Collections.emptySet(), null);",
                "         }",
                "@@ -537,3 +535,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "             //We need to add the new futures to the existing ones",
                "-            futures.addAll(dynamicState.pendingChangeingBlobs);",
                "+            futures.addAll(dynamicState.pendingChangingBlobs);",
                "         }",
                "@@ -541,3 +539,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "-        for (BlobChangeing rc: dynamicState.changingBlobs) {",
                "+        for (BlobChanging rc: dynamicState.changingBlobs) {",
                "             futures.add(rc.latch.countDown());",
                "@@ -545,5 +543,5 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "-        LOG.debug(\"found changeing blobs {} moving them to pending...\", dynamicState.changingBlobs);",
                "+        LOG.debug(\"found changing blobs {} moving them to pending...\", dynamicState.changingBlobs);",
                "         return dynamicState.withChangingBlobs(Collections.emptySet())",
                "-            .withPendingChangeingBlobs(futures, assignment);",
                "+            .withPendingChangingBlobs(futures, assignment);",
                "     }",
                "@@ -563,4 +561,4 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "-        HashSet<BlobChangeing> savedBlobs = new HashSet<>(dynamicState.changingBlobs.size());",
                "-        for (BlobChangeing rc: dynamicState.changingBlobs) {",
                "+        HashSet<BlobChanging> savedBlobs = new HashSet<>(dynamicState.changingBlobs.size());",
                "+        for (BlobChanging rc: dynamicState.changingBlobs) {",
                "             if (forSameTopology(assignment, rc.assignment)) {",
                "@@ -604,3 +602,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                 staticState.localizer.releaseSlotFor(dynamicState.pendingLocalization, staticState.port);",
                "-                return prepareForNewAssignmentNoWorkersRunning(dynamicState.withPendingChangeingBlobs(Collections.emptySet(), null),",
                "+                return prepareForNewAssignmentNoWorkersRunning(dynamicState.withPendingChangingBlobs(Collections.emptySet(), null),",
                "                     staticState);",
                "@@ -608,3 +606,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "-            if (!dynamicState.pendingChangeingBlobs.isEmpty()) {",
                "+            if (!dynamicState.pendingChangingBlobs.isEmpty()) {",
                "                 LOG.info(\"There are pending changes, waiting for them to finish before launching container...\");",
                "@@ -646,3 +644,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "      * PRECONDITION: container is null",
                "-     * PRECONDITION: pendingChangeingBlobs is not empty (otherwise why did we go to this state)",
                "+     * PRECONDITION: pendingChangingBlobs is not empty (otherwise why did we go to this state)",
                "      * PRECONDITION: pendingChangingBlobsAssignment is not null.",
                "@@ -658,3 +656,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "         assert dynamicState.pendingChangingBlobsAssignment != null;",
                "-        assert !dynamicState.pendingChangeingBlobs.isEmpty();",
                "+        assert !dynamicState.pendingChangingBlobs.isEmpty();",
                "@@ -670,3 +668,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "             return prepareForNewAssignmentNoWorkersRunning(dynamicState.withCurrentAssignment(null, null)",
                "-                    .withPendingChangeingBlobs(Collections.emptySet(), null),",
                "+                    .withPendingChangingBlobs(Collections.emptySet(), null),",
                "                 staticState);",
                "@@ -682,3 +680,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "         try {",
                "-            for (Future<Void> pending: dynamicState.pendingChangeingBlobs) {",
                "+            for (Future<Void> pending: dynamicState.pendingChangingBlobs) {",
                "                 long now = Time.nanoTime();",
                "@@ -695,3 +693,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                 .withCurrentAssignment(c, dynamicState.pendingChangingBlobsAssignment).withState(MachineState.WAITING_FOR_WORKER_START)",
                "-                .withPendingChangeingBlobs(Collections.emptySet(), null);",
                "+                .withPendingChangingBlobs(Collections.emptySet(), null);",
                "         } catch (TimeoutException ex) {",
                "@@ -966,3 +964,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "     private final AtomicReference<Set<TopoProfileAction>> profiling = new AtomicReference<>(new HashSet<>());",
                "-    private final BlockingQueue<BlobChangeing> changingBlobs = new LinkedBlockingQueue<>();",
                "+    private final BlockingQueue<BlobChanging> changingBlobs = new LinkedBlockingQueue<>();",
                "     private final StaticState staticState;",
                "@@ -1042,3 +1040,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "         try {",
                "-            changingBlobs.put(new BlobChangeing(assignment, blob, go.getLatch()));",
                "+            changingBlobs.put(new BlobChanging(assignment, blob, go.getLatch()));",
                "         } catch (InterruptedException e) {",
                "@@ -1103,3 +1101,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "-                Set<BlobChangeing> changingResourcesToHandle = dynamicState.changingBlobs;",
                "+                Set<BlobChanging> changingResourcesToHandle = dynamicState.changingBlobs;",
                "                 if (!changingBlobs.isEmpty()) {",
                "@@ -1107,3 +1105,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                     changingBlobs.drainTo(changingResourcesToHandle);",
                "-                    Iterator<BlobChangeing> it = changingResourcesToHandle.iterator();",
                "+                    Iterator<BlobChanging> it = changingResourcesToHandle.iterator();",
                "@@ -1111,3 +1109,3 @@ public class Slot extends Thread implements AutoCloseable, BlobChangingCallback",
                "                     while(it.hasNext()) {",
                "-                        BlobChangeing rc = it.next();",
                "+                        BlobChanging rc = it.next();",
                "                         if (!forSameTopology(rc.assignment, dynamicState.currentAssignment) &&",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/GoodToGo.java b/storm-server/src/main/java/org/apache/storm/localizer/GoodToGo.java",
                "index 04c7a0656..2fffe215d 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/GoodToGo.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/GoodToGo.java",
                "@@ -31,8 +31,8 @@ public class GoodToGo {",
                "         private final CountDownLatch latch;",
                "-        private final Future<Void> doneChangeing;",
                "+        private final Future<Void> doneChanging;",
                "         private boolean wasCounted = false;",
                "-        public GoodToGoLatch(CountDownLatch latch, Future<Void> doneChangeing) {",
                "+        public GoodToGoLatch(CountDownLatch latch, Future<Void> doneChanging) {",
                "             this.latch = latch;",
                "-            this.doneChangeing = doneChangeing;",
                "+            this.doneChanging = doneChanging;",
                "         }",
                "@@ -44,3 +44,3 @@ public class GoodToGo {",
                "             }",
                "-            return doneChangeing;",
                "+            return doneChanging;",
                "         }",
                "@@ -51,4 +51,4 @@ public class GoodToGo {",
                "-    public GoodToGo(CountDownLatch latch, Future<Void> doneChangeing) {",
                "-        this.latch = new GoodToGoLatch(latch, doneChangeing);",
                "+    public GoodToGo(CountDownLatch latch, Future<Void> doneChanging) {",
                "+        this.latch = new GoodToGoLatch(latch, doneChanging);",
                "     }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java",
                "index c09108da1..a287e959d 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java",
                "@@ -158,3 +158,3 @@ public abstract class LocallyCachedBlob {",
                "     /**",
                "-     * Mark that a given port and assignemnt are using this.",
                "+     * Mark that a given port and assignment are using this.",
                "      * @param pna the slot and assignment that are using this blob.",
                "@@ -193,3 +193,2 @@ public abstract class LocallyCachedBlob {",
                "                 BlobChangingCallback cb = entry.getValue();",
                "-                //TODO we probably want to not use this, or make it just return something that has less power to modify things",
                "                 cb.blobChanging(pna.getAssignment(), pna.getPort(), this, gtg);",
                "@@ -202,3 +201,4 @@ public abstract class LocallyCachedBlob {",
                "         } catch (InterruptedException e) {",
                "-            //TODO need to think about error handling here in general.",
                "+            //Interrupted is thrown when we are shutting down.",
                "+            // So just ignore it for now...",
                "         }"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                "storm-server/src/main/java/org/apache/storm/localizer/GoodToGo.java",
                "storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedBlob.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2483": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "dcad8a6f7461785000c9fb765324e0a2ddb39770"
                ],
                [
                    "no-tag",
                    "5e973624f337aa06000180e95046b503599dbc83"
                ],
                [
                    "no-tag",
                    "77be31bc4533cb9437242261ff89d1a750eef5c4"
                ],
                [
                    "no-tag",
                    "14cf5e61a96775d1997ec5e79d22fb3a36f13b6b"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2483",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "9371685d8a7f6685822ab367078d7ab5e86a4c96",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510170959,
            "hunks": 0,
            "message": "Merge branch 'STORM-2803' of https://github.com/srdo/storm into STORM-2803 STORM-2803: Fix leaking threads from Nimbus/TimeCacheMap, slightly refactor Time to use more final fields, replaced uses of deprecated classes/methods and added a few tests. This closes #2403",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2803": ""
            },
            "ghissue_refs": {
                "2403": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2803",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2403",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "dc65d3bb9dceab31191ab49e365bd25b17f46377",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513764893,
            "hunks": 19,
            "message": "STORM-2862: Move multilang logging to a shared class and make this class configurable.",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/Config.java b/storm-core/src/jvm/org/apache/storm/Config.java",
                "index 4f3bd1c27..ba543beb7 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/Config.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/Config.java",
                "@@ -1605,2 +1605,10 @@ public class Config extends HashMap<String, Object> {",
                "+    /**",
                "+     * The fully qualified name of a {@link ShellLogHandler} to handle output",
                "+     * from non-JVM processes e.g. \"com.mycompany.CustomShellLogHandler\". If",
                "+     * not provided, org.apache.storm.utils.DefaultLogHandler will be used.",
                "+     */",
                "+    @isString",
                "+    public static final String TOPOLOGY_MULTILANG_LOG_HANDLER = \"topology.multilang.log.handler\";",
                "+",
                "     /**",
                "diff --git a/storm-core/src/jvm/org/apache/storm/spout/ShellSpout.java b/storm-core/src/jvm/org/apache/storm/spout/ShellSpout.java",
                "index a5ec72bac..afd816b88 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/spout/ShellSpout.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/spout/ShellSpout.java",
                "@@ -26,3 +26,5 @@ import org.apache.storm.multilang.SpoutMsg;",
                " import org.apache.storm.task.TopologyContext;",
                "+import org.apache.storm.utils.ShellLogHandler;",
                " import org.apache.storm.utils.ShellProcess;",
                "+import org.apache.storm.utils.ShellUtils;",
                "@@ -52,2 +54,3 @@ public class ShellSpout implements ISpout {",
                "     private Map<String, String> env = new HashMap<>();",
                "+    private ShellLogHandler _logHandler;",
                "     private ShellProcess _process;",
                "@@ -113,2 +116,5 @@ public class ShellSpout implements ISpout {",
                "+        _logHandler =  ShellUtils.getLogHandler(stormConf);",
                "+        _logHandler.setUpContext(ShellSpout.class, _process, _context);",
                "+",
                "         heartBeatExecutorService = MoreExecutors.getExitingScheduledExecutorService(new ScheduledThreadPoolExecutor(1));",
                "@@ -147,3 +153,2 @@ public class ShellSpout implements ISpout {",
                "-    ",
                "     private void handleMetrics(ShellMsg shellMsg) {",
                "@@ -193,3 +198,3 @@ public class ShellSpout implements ISpout {",
                "                 } else if (command.equals(\"log\")) {",
                "-                    handleLog(shellMsg);",
                "+                    _logHandler.log(shellMsg);",
                "                 } else if (command.equals(\"error\")) {",
                "@@ -223,30 +228,2 @@ public class ShellSpout implements ISpout {",
                "-",
                "-    private void handleLog(ShellMsg shellMsg) {",
                "-        String msg = shellMsg.getMsg();",
                "-        msg = \"ShellLog \" + _process.getProcessInfoString() + \" \" + msg;",
                "-        ShellMsg.ShellLogLevel logLevel = shellMsg.getLogLevel();",
                "-",
                "-        switch (logLevel) {",
                "-            case TRACE:",
                "-                LOG.trace(msg);",
                "-                break;",
                "-            case DEBUG:",
                "-                LOG.debug(msg);",
                "-                break;",
                "-            case INFO:",
                "-                LOG.info(msg);",
                "-                break;",
                "-            case WARN:",
                "-                LOG.warn(msg);",
                "-                break;",
                "-            case ERROR:",
                "-                LOG.error(msg);",
                "-                break;",
                "-            default:",
                "-                LOG.info(msg);",
                "-                break;",
                "-        }",
                "-    }",
                "-",
                "     private void handleError(String msg) {",
                "diff --git a/storm-core/src/jvm/org/apache/storm/task/ShellBolt.java b/storm-core/src/jvm/org/apache/storm/task/ShellBolt.java",
                "index 3d9f14169..d520d0733 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/task/ShellBolt.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/task/ShellBolt.java",
                "@@ -26,6 +26,8 @@ import org.apache.storm.multilang.BoltMsg;",
                " import org.apache.storm.multilang.ShellMsg;",
                "-import org.apache.storm.topology.ReportedFailedException;",
                " import org.apache.storm.tuple.Tuple;",
                " import org.apache.storm.utils.ShellBoltMessageQueue;",
                "+import org.apache.storm.utils.ShellLogHandler;",
                " import org.apache.storm.utils.ShellProcess;",
                "+import org.apache.storm.utils.ShellUtils;",
                "+",
                " import clojure.lang.RT;",
                "@@ -79,2 +81,3 @@ public class ShellBolt implements IBolt {",
                "     private Map<String, String> env = new HashMap<>();",
                "+    private ShellLogHandler _logHandler;",
                "     private ShellProcess _process;",
                "@@ -152,2 +155,5 @@ public class ShellBolt implements IBolt {",
                "+        _logHandler = ShellUtils.getLogHandler(stormConf);",
                "+        _logHandler.setUpContext(ShellBolt.class, _process, _context);",
                "+",
                "         // reader",
                "@@ -247,30 +253,2 @@ public class ShellBolt implements IBolt {",
                "-    private void handleLog(ShellMsg shellMsg) {",
                "-        String msg = shellMsg.getMsg();",
                "-        msg = \"ShellLog \" + _process.getProcessInfoString() + \" \" + msg;",
                "-        ShellMsg.ShellLogLevel logLevel = shellMsg.getLogLevel();",
                "-",
                "-        switch (logLevel) {",
                "-            case TRACE:",
                "-                LOG.trace(msg);",
                "-                break;",
                "-            case DEBUG:",
                "-                LOG.debug(msg);",
                "-                break;",
                "-            case INFO:",
                "-                LOG.info(msg);",
                "-                break;",
                "-            case WARN:",
                "-                LOG.warn(msg);",
                "-                break;",
                "-            case ERROR:",
                "-                LOG.error(msg);",
                "-                _collector.reportError(new ReportedFailedException(msg));",
                "-                break;",
                "-            default:",
                "-                LOG.info(msg);",
                "-                break;",
                "-        }",
                "-    }",
                "-",
                "     private void handleMetrics(ShellMsg shellMsg) {",
                "@@ -372,3 +350,3 @@ public class ShellBolt implements IBolt {",
                "                         case \"log\":",
                "-                            handleLog(shellMsg);",
                "+                            _logHandler.log(shellMsg);",
                "                             break;",
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java b/storm-core/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java",
                "new file mode 100644",
                "index 000000000..2004bc15d",
                "--- /dev/null",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java",
                "@@ -0,0 +1,113 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.utils;",
                "+",
                "+import org.apache.storm.multilang.ShellMsg;",
                "+import org.apache.storm.task.TopologyContext;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * Default implementation of {@link ShellLogHandler}.",
                "+ */",
                "+public class DefaultShellLogHandler implements ShellLogHandler {",
                "+",
                "+    private Logger log;",
                "+",
                "+    /**",
                "+     * Save information about the current process.",
                "+     */",
                "+    private ShellProcess process;",
                "+",
                "+    /**",
                "+     * Default constructor; used when loading with",
                "+     * Class.forName(...).newInstance().",
                "+     */",
                "+    public DefaultShellLogHandler() {",
                "+    }",
                "+",
                "+    private Logger getLogger(final Class<?> ownerCls) {",
                "+        return LoggerFactory.getLogger(",
                "+                ownerCls == null ? DefaultShellLogHandler.class : ownerCls);",
                "+    }",
                "+",
                "+    /**",
                "+     * This default implementation saves the {@link ShellProcess} so it can",
                "+     * output the process info string later.",
                "+     * @see {@link ShellLogHandler#setUpContext}",
                "+     *",
                "+     * @param ownerCls",
                "+     *            - the class which instantiated this ShellLogHandler.",
                "+     * @param process",
                "+     *            - the current {@link ShellProcess}.",
                "+     * @param context",
                "+     *            - the current {@link TopologyContext}.",
                "+     */",
                "+    public void setUpContext(final Class<?> ownerCls, final ShellProcess process,",
                "+            final TopologyContext context) {",
                "+        this.log = getLogger(ownerCls);",
                "+        this.process = process;",
                "+        // context is not used by the default implementation, but is included",
                "+        // in the interface in case it is useful to subclasses",
                "+    }",
                "+",
                "+    /**",
                "+     * Log the given message.",
                "+     * @see {@link ShellLogHandler#log}",
                "+     *",
                "+     * @param shellMsg",
                "+     *            - the {@link ShellMsg} to log.",
                "+     */",
                "+    public void log(final ShellMsg shellMsg) {",
                "+        if (shellMsg == null) {",
                "+            throw new IllegalArgumentException(\"shellMsg is required\");",
                "+        }",
                "+        String msg = shellMsg.getMsg();",
                "+        if (this.log == null) {",
                "+            this.log = getLogger(null);",
                "+        }",
                "+        if (this.process == null) {",
                "+            msg = \"ShellLog \" + msg;",
                "+        } else {",
                "+            msg = \"ShellLog \" + process.getProcessInfoString() + \" \" + msg;",
                "+        }",
                "+        ShellMsg.ShellLogLevel logLevel = shellMsg.getLogLevel();",
                "+",
                "+        switch (logLevel) {",
                "+            case TRACE:",
                "+                log.trace(msg);",
                "+                break;",
                "+            case DEBUG:",
                "+                log.debug(msg);",
                "+                break;",
                "+            case INFO:",
                "+                log.info(msg);",
                "+                break;",
                "+            case WARN:",
                "+                log.warn(msg);",
                "+                break;",
                "+            case ERROR:",
                "+                log.error(msg);",
                "+                break;",
                "+            default:",
                "+                log.info(msg);",
                "+                break;",
                "+        }",
                "+    }",
                "+}",
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/ShellLogHandler.java b/storm-core/src/jvm/org/apache/storm/utils/ShellLogHandler.java",
                "new file mode 100644",
                "index 000000000..8463a9e16",
                "--- /dev/null",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/ShellLogHandler.java",
                "@@ -0,0 +1,52 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.utils;",
                "+",
                "+import org.apache.storm.multilang.ShellMsg;",
                "+import org.apache.storm.task.TopologyContext;",
                "+",
                "+/**",
                "+ * Handle logging from non-JVM processes.",
                "+ */",
                "+public interface ShellLogHandler {",
                "+",
                "+    /**",
                "+     * Called at least once before {@link ShellLogHandler#log} for each",
                "+     * spout and bolt. Allows implementing classes to save information about",
                "+     * the current running context e.g. pid, thread, task.",
                "+     *",
                "+     * @param ownerCls",
                "+     *            - the class which instantiated this ShellLogHandler.",
                "+     * @param process",
                "+     *            - the current {@link ShellProcess}.",
                "+     * @param context",
                "+     *            - the current {@link TopologyContext}.",
                "+     */",
                "+    void setUpContext(Class<?> ownerCls, ShellProcess process,",
                "+            TopologyContext context);",
                "+",
                "+    /**",
                "+     * Called by spouts and bolts when they receive a 'log' command from a",
                "+     * multilang process.",
                "+     *",
                "+     * @param msg",
                "+     *            - the {@link ShellMsg} containing the message to log.",
                "+     */",
                "+    void log(ShellMsg msg);",
                "+}",
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/ShellUtils.java b/storm-core/src/jvm/org/apache/storm/utils/ShellUtils.java",
                "index ef869b055..3b0f934d7 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/utils/ShellUtils.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/ShellUtils.java",
                "@@ -30,2 +30,3 @@ import java.util.concurrent.atomic.AtomicBoolean;",
                "+import org.apache.storm.Config;",
                " import org.slf4j.Logger;",
                "@@ -504,2 +505,18 @@ abstract public class ShellUtils {",
                "+    public static ShellLogHandler getLogHandler(Map stormConf) {",
                "+        if (stormConf == null) {",
                "+            throw new IllegalArgumentException(\"Config is required\");",
                "+        }",
                "+",
                "+        String logHandlerClassName = null;",
                "+        if (stormConf.containsKey(Config.TOPOLOGY_MULTILANG_LOG_HANDLER)) {",
                "+            try {",
                "+                logHandlerClassName = stormConf.get(Config.TOPOLOGY_MULTILANG_LOG_HANDLER).toString();",
                "+                return (ShellLogHandler) Class.forName(logHandlerClassName).newInstance();",
                "+            } catch (ClassCastException | InstantiationException | IllegalAccessException | ClassNotFoundException e) {",
                "+                throw new RuntimeException(\"Error loading ShellLogHandler \" + logHandlerClassName, e);",
                "+            }",
                "+        }",
                "+        return new DefaultShellLogHandler();",
                "+    }",
                " }"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/Config.java",
                "storm-core/src/jvm/org/apache/storm/spout/ShellSpout.java",
                "storm-core/src/jvm/org/apache/storm/task/ShellBolt.java",
                "storm-core/src/jvm/org/apache/storm/utils/DefaultShellLogHandler.java",
                "storm-core/src/jvm/org/apache/storm/utils/ShellLogHandler.java",
                "storm-core/src/jvm/org/apache/storm/utils/ShellUtils.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2862": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "5f595cbcd07333c54cdbca5312dcf935e80e3ece"
                ],
                [
                    "no-tag",
                    "20044b0e004f9ea9f309228c747aa7aa10647396"
                ]
            ],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2862",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "abf0f8a22b665117c88e2b2dccdc588d72f12403",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517510646,
            "hunks": 0,
            "message": "Merge branch 'STORM-2910' of https://github.com/revans2/incubator-storm into STORM-2910 STORM-2910: Fix metrics reporting from supervisor This closes #2534",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2910": ""
            },
            "ghissue_refs": {
                "2534": "STORM-2912 Revert optimization of sharing tick tuple #2533"
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2910",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2534",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "502ed7e7a0a1b459848de69fb5d0b32cf6059082",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508968642,
            "hunks": 0,
            "message": "Merge branch 'STORM-2786-1.x' of https://github.com/revans2/incubator-storm into STORM-2786 STORM-2786: Enable tick tuples for ackers This closes #2383",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2786": ""
            },
            "ghissue_refs": {
                "2383": "STORM-2786: Turn ticks back on for ackers (2.x) #2382"
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2786",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2383",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "89b48d2a89874ab6656494fc7a9c27f861e50ba6",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509741049,
            "hunks": 0,
            "message": "Merge branch 'STORM-2795' of https://github.com/revans2/incubator-storm into STORM-2795 STORM-2795: Race in downloading resources can cause failure This closes #2398",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2795": ""
            },
            "ghissue_refs": {
                "2398": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2795",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2398",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "414f2b4a80546074465c5035360d4c78d2001d63",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507909520,
            "hunks": 0,
            "message": "Merge branch 'STORM-2771' of https://github.com/revans2/incubator-storm into STORM-2771 STORM-2771: By default don't run any tests as integration tests This closes #2368",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2771": ""
            },
            "ghissue_refs": {
                "2368": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2771",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2368",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6d96c401108127d020a5fe98a0ab98bd2077cd9a",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513713761,
            "hunks": 0,
            "message": "Merge branch 'STORM-2837' of https://github.com/revans2/incubator-storm into STORM-2837 STORM-2837: ConstraintSolverStrategy This closes #2442",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2837": ""
            },
            "ghissue_refs": {
                "2442": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2837",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2442",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1daaaefbd6c738ed3dc8a9ae20145956ca7a1a93",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508955672,
            "hunks": 13,
            "message": "Merge branch 'STORM-2706-1.x' of https://github.com/srdo/storm into STORM-2706 STORM-2706: Upgrade to Curator 4.0.0 (1.x) This closes #2384",
            "diff": [
                "diff --git a/external/storm-eventhubs/pom.xml b/external/storm-eventhubs/pom.xml",
                "index 1be88e9e4..9ce5e2de6 100755",
                "--- a/external/storm-eventhubs/pom.xml",
                "+++ b/external/storm-eventhubs/pom.xml",
                "@@ -69,13 +69,2 @@",
                "             <artifactId>curator-framework</artifactId>",
                "-            <version>${curator.version}</version>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "diff --git a/external/storm-kafka-monitor/pom.xml b/external/storm-kafka-monitor/pom.xml",
                "index e3fb8c3bd..475e0732d 100644",
                "--- a/external/storm-kafka-monitor/pom.xml",
                "+++ b/external/storm-kafka-monitor/pom.xml",
                "@@ -59,8 +59,2 @@",
                "             <artifactId>curator-framework</artifactId>",
                "-            <exclusions>",
                "-               <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "diff --git a/external/storm-kafka/pom.xml b/external/storm-kafka/pom.xml",
                "index a841efca7..28ec77ced 100644",
                "--- a/external/storm-kafka/pom.xml",
                "+++ b/external/storm-kafka/pom.xml",
                "@@ -79,13 +79,2 @@",
                "             <artifactId>curator-framework</artifactId>",
                "-            <version>${curator.version}</version>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "@@ -94,9 +83,2 @@",
                "             <artifactId>curator-recipes</artifactId>",
                "-            <version>${curator.version}</version>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "             <scope>test</scope>",
                "@@ -106,13 +88,2 @@",
                "             <artifactId>curator-test</artifactId>",
                "-            <version>${curator.version}</version>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.testng</groupId>",
                "-                    <artifactId>testng</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "             <scope>test</scope>",
                "diff --git a/external/storm-kinesis/pom.xml b/external/storm-kinesis/pom.xml",
                "index 2b997e508..013a6efc3 100644",
                "--- a/external/storm-kinesis/pom.xml",
                "+++ b/external/storm-kinesis/pom.xml",
                "@@ -47,13 +47,2 @@",
                "             <artifactId>curator-framework</artifactId>",
                "-            <version>${curator.version}</version>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "diff --git a/pom.xml b/pom.xml",
                "index c979a3356..fd61e85fe 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -231,3 +231,4 @@",
                "         <clj-time.version>0.8.0</clj-time.version>",
                "-        <curator.version>2.12.0</curator.version>",
                "+        <curator.version>4.0.0</curator.version>",
                "+        <curator-test.version>2.12.0</curator-test.version>",
                "         <json-simple.version>1.1</json-simple.version>",
                "@@ -702,6 +703,2 @@",
                "                 <exclusions>",
                "-                    <exclusion>",
                "-                        <groupId>log4j</groupId>",
                "-                        <artifactId>log4j</artifactId>",
                "-                    </exclusion>",
                "                     <exclusion>",
                "@@ -716,12 +713,2 @@",
                "                 <version>${curator.version}</version>",
                "-                <exclusions>",
                "-                    <exclusion>",
                "-                        <groupId>log4j</groupId>",
                "-                        <artifactId>log4j</artifactId>",
                "-                    </exclusion>",
                "-                    <exclusion>",
                "-                        <groupId>org.slf4j</groupId>",
                "-                        <artifactId>slf4j-log4j12</artifactId>",
                "-                    </exclusion>",
                "-                </exclusions>",
                "             </dependency>",
                "@@ -735,3 +722,6 @@",
                "                 <artifactId>curator-test</artifactId>",
                "-                <version>${curator.version}</version>",
                "+                <!-- curator-test is not compatible with Zookeeper 3.4.x.",
                "+                Curator works around this by using an older curator-test jar.",
                "+                See https://github.com/apache/curator/tree/6ba4de36d4e8b2b65d45c005a6a92dd85c3c497f/curator-test-zk34-->",
                "+                <version>${curator-test.version}</version>",
                "                 <scope>test</scope>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index 737ec20a6..f20b829ed 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -190,12 +190,2 @@",
                "             <scope>compile</scope>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "@@ -204,12 +194,2 @@",
                "             <artifactId>curator-recipes</artifactId>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>",
                "@@ -218,12 +198,2 @@",
                "             <artifactId>curator-test</artifactId>",
                "-            <exclusions>",
                "-                <exclusion>",
                "-                    <groupId>log4j</groupId>",
                "-                    <artifactId>log4j</artifactId>",
                "-                </exclusion>",
                "-                <exclusion>",
                "-                    <groupId>org.slf4j</groupId>",
                "-                    <artifactId>slf4j-log4j12</artifactId>",
                "-                </exclusion>",
                "-            </exclusions>",
                "         </dependency>"
            ],
            "changed_files": [
                "external/storm-eventhubs/pom.xml",
                "external/storm-kafka-monitor/pom.xml",
                "external/storm-kafka/pom.xml",
                "external/storm-kinesis/pom.xml",
                "pom.xml",
                "storm-core/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2706": ""
            },
            "ghissue_refs": {
                "2384": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2706",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2384",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "426246fd039c0c9f91d9d638ccd968299b00f3e9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508968642,
            "hunks": 3,
            "message": "Merge branch 'STORM-2786-1.x' of https://github.com/revans2/incubator-storm into STORM-2786 STORM-2786: Enable tick tuples for ackers This closes #2383",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index 335df3b37..a630dab90 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -349,2 +349,3 @@",
                "   (let [storm-conf (:storm-conf executor-data)",
                "+        comp-id (:component-id executor-data)",
                "         tick-time-secs (storm-conf TOPOLOGY-TICK-TUPLE-FREQ-SECS)",
                "@@ -353,13 +354,13 @@",
                "     (when tick-time-secs",
                "-      (if (or (Utils/isSystemId (:component-id executor-data))",
                "+      (if (or (and (not= \"__acker\" comp-id) (Utils/isSystemId comp-id))",
                "               (and (= false (storm-conf TOPOLOGY-ENABLE-MESSAGE-TIMEOUTS))",
                "                    (= :spout (:type executor-data))))",
                "-        (log-message \"Timeouts disabled for executor \" (:component-id executor-data) \":\" (:executor-id executor-data))",
                "-        (schedule-recurring",
                "-          (:user-timer worker)",
                "-          tick-time-secs",
                "-          tick-time-secs",
                "-          (fn []",
                "-            (let [val [(AddressedTuple. AddressedTuple/BROADCAST_DEST (TupleImpl. context [tick-time-secs] Constants/SYSTEM_TASK_ID Constants/SYSTEM_TICK_STREAM_ID))]]",
                "-              (disruptor/publish receive-queue val))))))))",
                "+        (log-message \"Timeouts disabled for executor \" comp-id \":\" (:executor-id executor-data))",
                "+        (let [val [(AddressedTuple. AddressedTuple/BROADCAST_DEST (TupleImpl. context [tick-time-secs] Constants/SYSTEM_TASK_ID Constants/SYSTEM_TICK_STREAM_ID))]]",
                "+          (schedule-recurring",
                "+            (:user-timer worker)",
                "+            tick-time-secs",
                "+            tick-time-secs",
                "+            (fn []",
                "+                (disruptor/publish receive-queue val))))))))"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2786": ""
            },
            "ghissue_refs": {
                "2383": "STORM-2786: Turn ticks back on for ackers (2.x) #2382"
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2786",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2383",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8e4a146542a5a3176d2e22bccc851558d57f219a",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510872575,
            "hunks": 0,
            "message": "Merge branch 'STORM-2820' of https://github.com/Ethanlm/storm into STORM-2820 STORM-2820: fix validateTopologyWorkerMaxHeapSizeConfigs function never picks up the value set by nimbus problem This closes #2424",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2820": ""
            },
            "ghissue_refs": {
                "2424": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2820",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2424",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "64615c9ac45a3906e48b1c44f2393f8bedbb2c72",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515164769,
            "hunks": 0,
            "message": "Merge branch 'STORM-2876' of https://github.com/revans2/incubator-storm into STORM-2876 STORM-2876: Work around memory leak, and try to speed up tests This closes #2500",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2876": ""
            },
            "ghissue_refs": {
                "2500": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2876",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2500",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "38e308e3ae79282c6efc7826fda7e3d37455820f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508955672,
            "hunks": 0,
            "message": "Merge branch 'STORM-2706-1.x' of https://github.com/srdo/storm into STORM-2706 STORM-2706: Upgrade to Curator 4.0.0 (1.x) This closes #2384",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2706": ""
            },
            "ghissue_refs": {
                "2384": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2706",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2384",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "2daa375cbd2ae490844503106140e4a20a0cca02",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514929415,
            "hunks": 3,
            "message": "Merge branch 'STORM-2856' of https://github.com/srdo/storm into STORM-2856 STORM-2856: Make Storm build work on post-2017Q4 Travis Trusty image This closes #2486",
            "diff": [
                "diff --git a/.travis.yml b/.travis.yml",
                "index 5af427a50..597cf87da 100644",
                "--- a/.travis.yml",
                "+++ b/.travis.yml",
                "@@ -22,3 +22,2 @@ dist: trusty",
                " sudo: required",
                "-group: deprecated-2017Q4",
                "@@ -29,5 +28,6 @@ jdk:",
                " before_install:",
                "-  - rvm use 2.1.5 --install",
                "-  - nvm install 0.12.2",
                "-  - nvm use 0.12.2",
                "+  - rvm reload",
                "+  - rvm use 2.4.2 --install",
                "+  - nvm install 8.9.3",
                "+  - nvm use 8.9.3",
                " install: /bin/bash ./dev-tools/travis/travis-install.sh `pwd`",
                "@@ -40,2 +40,2 @@ cache:",
                "     - \"$HOME/.rvm\"",
                "-    - \"$NVM_DIR\"",
                "+    - \"$NVM_DIR\"",
                "\\ No newline at end of file"
            ],
            "changed_files": [
                ".travis.yml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2856": ""
            },
            "ghissue_refs": {
                "2486": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2856",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2486",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8bf8a2f95df266fc6ca4024f6ce7cd0743e7ecc5",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510151641,
            "hunks": 0,
            "message": "Merge branch 'STORM-2800' of https://github.com/srdo/storm into STORM-2800 STORM-2800: Use Maven JAXB api artifact instead of assuming it is available This closes #2402",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2800": ""
            },
            "ghissue_refs": {
                "2402": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2800",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2402",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "914abe59b4c6e7e156e81a8e37a483cab6097f5a",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508253064,
            "hunks": 0,
            "message": "Merge branch 'STORM-2777' of https://github.com/Ethanlm/storm into STORM-2777 STORM-2777 The number of ackers should default to the number of workers This closes #2372",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2777": ""
            },
            "ghissue_refs": {
                "2372": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2777",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2372",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f867e3ae577b4ac61d6941af0373253accd38ff5",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507659021,
            "hunks": 0,
            "message": "Merge branch 'storm2770' of https://github.com/kishorvpatil/incubator-storm into STORM-2770 STORM-2770: Add fragmentation metrics for CPU and Memory This closes #2358",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2770": ""
            },
            "ghissue_refs": {
                "2358": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2770",
                    "relevance": 2
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2358",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1190336a3b9525c5cd1487a05e36892aee057afe",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516693374,
            "hunks": 56,
            "message": "[maven-release-plugin] prepare for next development iteration",
            "diff": [
                "diff --git a/examples/storm-elasticsearch-examples/pom.xml b/examples/storm-elasticsearch-examples/pom.xml",
                "index 3e3a9fea7..f76291f1f 100644",
                "--- a/examples/storm-elasticsearch-examples/pom.xml",
                "+++ b/examples/storm-elasticsearch-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hbase-examples/pom.xml b/examples/storm-hbase-examples/pom.xml",
                "index 48c0f0203..28670b3f1 100644",
                "--- a/examples/storm-hbase-examples/pom.xml",
                "+++ b/examples/storm-hbase-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hdfs-examples/pom.xml b/examples/storm-hdfs-examples/pom.xml",
                "index 261d333be..f8dca7fcd 100644",
                "--- a/examples/storm-hdfs-examples/pom.xml",
                "+++ b/examples/storm-hdfs-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hive-examples/pom.xml b/examples/storm-hive-examples/pom.xml",
                "index 9e903294d..5052343ef 100644",
                "--- a/examples/storm-hive-examples/pom.xml",
                "+++ b/examples/storm-hive-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jdbc-examples/pom.xml b/examples/storm-jdbc-examples/pom.xml",
                "index 60339a2c7..43c15d77b 100644",
                "--- a/examples/storm-jdbc-examples/pom.xml",
                "+++ b/examples/storm-jdbc-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jms-examples/pom.xml b/examples/storm-jms-examples/pom.xml",
                "index 9de6b93ca..37598b3c2 100644",
                "--- a/examples/storm-jms-examples/pom.xml",
                "+++ b/examples/storm-jms-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-client-examples/pom.xml b/examples/storm-kafka-client-examples/pom.xml",
                "index 1f862d759..3b8abfae2 100644",
                "--- a/examples/storm-kafka-client-examples/pom.xml",
                "+++ b/examples/storm-kafka-client-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-examples/pom.xml b/examples/storm-kafka-examples/pom.xml",
                "index e0c6888ff..1dd9f45f5 100644",
                "--- a/examples/storm-kafka-examples/pom.xml",
                "+++ b/examples/storm-kafka-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mongodb-examples/pom.xml b/examples/storm-mongodb-examples/pom.xml",
                "index f867b986e..b09f3c41a 100644",
                "--- a/examples/storm-mongodb-examples/pom.xml",
                "+++ b/examples/storm-mongodb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mqtt-examples/pom.xml b/examples/storm-mqtt-examples/pom.xml",
                "index ac94556d1..ff6bf71f9 100644",
                "--- a/examples/storm-mqtt-examples/pom.xml",
                "+++ b/examples/storm-mqtt-examples/pom.xml",
                "@@ -28,3 +28,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.2</version>",
                "+    <version>1.1.3-SNAPSHOT</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-opentsdb-examples/pom.xml b/examples/storm-opentsdb-examples/pom.xml",
                "index 2ce34ee77..ad6c41d1a 100644",
                "--- a/examples/storm-opentsdb-examples/pom.xml",
                "+++ b/examples/storm-opentsdb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-perf/pom.xml b/examples/storm-perf/pom.xml",
                "index 501be8f2b..3780e7646 100644",
                "--- a/examples/storm-perf/pom.xml",
                "+++ b/examples/storm-perf/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-pmml-examples/pom.xml b/examples/storm-pmml-examples/pom.xml",
                "index 0b0d133d5..e15001ab0 100644",
                "--- a/examples/storm-pmml-examples/pom.xml",
                "+++ b/examples/storm-pmml-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-redis-examples/pom.xml b/examples/storm-redis-examples/pom.xml",
                "index c0dde9df0..0af5a2041 100644",
                "--- a/examples/storm-redis-examples/pom.xml",
                "+++ b/examples/storm-redis-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-solr-examples/pom.xml b/examples/storm-solr-examples/pom.xml",
                "index b81420aeb..6d7877513 100644",
                "--- a/examples/storm-solr-examples/pom.xml",
                "+++ b/examples/storm-solr-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-starter/pom.xml b/examples/storm-starter/pom.xml",
                "index 672a52f0d..50bc34c8d 100644",
                "--- a/examples/storm-starter/pom.xml",
                "+++ b/examples/storm-starter/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.1.2</version>",
                "+      <version>1.1.3-SNAPSHOT</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-core/pom.xml b/external/flux/flux-core/pom.xml",
                "index c62989764..d69d9b297 100644",
                "--- a/external/flux/flux-core/pom.xml",
                "+++ b/external/flux/flux-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-wrappers/pom.xml b/external/flux/flux-wrappers/pom.xml",
                "index 76c56c6ed..5d4d73a10 100644",
                "--- a/external/flux/flux-wrappers/pom.xml",
                "+++ b/external/flux/flux-wrappers/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/pom.xml b/external/flux/pom.xml",
                "index aa8cd48ee..31b2a24bb 100644",
                "--- a/external/flux/pom.xml",
                "+++ b/external/flux/pom.xml",
                "@@ -28,3 +28,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/pom.xml b/external/sql/pom.xml",
                "index c3cea241d..8adac96ce 100644",
                "--- a/external/sql/pom.xml",
                "+++ b/external/sql/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-core/pom.xml b/external/sql/storm-sql-core/pom.xml",
                "index db99e28a3..375dafc07 100644",
                "--- a/external/sql/storm-sql-core/pom.xml",
                "+++ b/external/sql/storm-sql-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "index 5ca571d76..55c378fb1 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "index f0919389f..64b58fa19 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "index 3d4df119e..9a3d10b45 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-redis/pom.xml b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "index e61224fae..cc78b6363 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-runtime/pom.xml b/external/sql/storm-sql-runtime/pom.xml",
                "index b64de3491..db7f852a7 100644",
                "--- a/external/sql/storm-sql-runtime/pom.xml",
                "+++ b/external/sql/storm-sql-runtime/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/storm-cassandra/pom.xml b/external/storm-cassandra/pom.xml",
                "index 3619a86f4..a123e6c32 100644",
                "--- a/external/storm-cassandra/pom.xml",
                "+++ b/external/storm-cassandra/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-druid/pom.xml b/external/storm-druid/pom.xml",
                "index afed6dd93..24f71445a 100644",
                "--- a/external/storm-druid/pom.xml",
                "+++ b/external/storm-druid/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-elasticsearch/pom.xml b/external/storm-elasticsearch/pom.xml",
                "index 7b721674c..cfa49dd2f 100644",
                "--- a/external/storm-elasticsearch/pom.xml",
                "+++ b/external/storm-elasticsearch/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-eventhubs/pom.xml b/external/storm-eventhubs/pom.xml",
                "index 9e91f712d..6597833dd 100755",
                "--- a/external/storm-eventhubs/pom.xml",
                "+++ b/external/storm-eventhubs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hbase/pom.xml b/external/storm-hbase/pom.xml",
                "index 49436efb9..6eefa95c5 100644",
                "--- a/external/storm-hbase/pom.xml",
                "+++ b/external/storm-hbase/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hdfs/pom.xml b/external/storm-hdfs/pom.xml",
                "index 7d3c9cb1a..54899dc5c 100644",
                "--- a/external/storm-hdfs/pom.xml",
                "+++ b/external/storm-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hive/pom.xml b/external/storm-hive/pom.xml",
                "index 2fa04b027..a7800adda 100644",
                "--- a/external/storm-hive/pom.xml",
                "+++ b/external/storm-hive/pom.xml",
                "@@ -23,3 +23,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.2</version>",
                "+    <version>1.1.3-SNAPSHOT</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jdbc/pom.xml b/external/storm-jdbc/pom.xml",
                "index 251f153b7..e7113b878 100644",
                "--- a/external/storm-jdbc/pom.xml",
                "+++ b/external/storm-jdbc/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jms/pom.xml b/external/storm-jms/pom.xml",
                "index d71235502..6e1871873 100644",
                "--- a/external/storm-jms/pom.xml",
                "+++ b/external/storm-jms/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index 5e960ae2b..f29f2ba70 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -24,3 +24,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-monitor/pom.xml b/external/storm-kafka-monitor/pom.xml",
                "index 8e0363c2a..3af00251e 100644",
                "--- a/external/storm-kafka-monitor/pom.xml",
                "+++ b/external/storm-kafka-monitor/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka/pom.xml b/external/storm-kafka/pom.xml",
                "index 56d83fcdd..30fff9058 100644",
                "--- a/external/storm-kafka/pom.xml",
                "+++ b/external/storm-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kinesis/pom.xml b/external/storm-kinesis/pom.xml",
                "index 9ff4e96b8..2ef7659af 100644",
                "--- a/external/storm-kinesis/pom.xml",
                "+++ b/external/storm-kinesis/pom.xml",
                "@@ -19,3 +19,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-metrics/pom.xml b/external/storm-metrics/pom.xml",
                "index a35c8c689..77ded6aa9 100644",
                "--- a/external/storm-metrics/pom.xml",
                "+++ b/external/storm-metrics/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.1.2</version>",
                "+      <version>1.1.3-SNAPSHOT</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mongodb/pom.xml b/external/storm-mongodb/pom.xml",
                "index 5420f6125..8147ddba5 100644",
                "--- a/external/storm-mongodb/pom.xml",
                "+++ b/external/storm-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mqtt/pom.xml b/external/storm-mqtt/pom.xml",
                "index 069a0e01d..c02aaf60b 100644",
                "--- a/external/storm-mqtt/pom.xml",
                "+++ b/external/storm-mqtt/pom.xml",
                "@@ -27,3 +27,3 @@",
                "         <artifactId>storm</artifactId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-opentsdb/pom.xml b/external/storm-opentsdb/pom.xml",
                "index f4a4f369d..7cd005467 100644",
                "--- a/external/storm-opentsdb/pom.xml",
                "+++ b/external/storm-opentsdb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-pmml/pom.xml b/external/storm-pmml/pom.xml",
                "index f0a07cadd..54d6b3f68 100644",
                "--- a/external/storm-pmml/pom.xml",
                "+++ b/external/storm-pmml/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-redis/pom.xml b/external/storm-redis/pom.xml",
                "index fe641778e..210558789 100644",
                "--- a/external/storm-redis/pom.xml",
                "+++ b/external/storm-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-solr/pom.xml b/external/storm-solr/pom.xml",
                "index 2c4439e51..d5dd89fb2 100644",
                "--- a/external/storm-solr/pom.xml",
                "+++ b/external/storm-solr/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-submit-tools/pom.xml b/external/storm-submit-tools/pom.xml",
                "index 8d17cac11..460316c5a 100644",
                "--- a/external/storm-submit-tools/pom.xml",
                "+++ b/external/storm-submit-tools/pom.xml",
                "@@ -21,3 +21,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/pom.xml b/pom.xml",
                "index 7f4c4b52c..b7aecbdb0 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -29,3 +29,3 @@",
                "     <artifactId>storm</artifactId>",
                "-    <version>1.1.2</version>",
                "+    <version>1.1.3-SNAPSHOT</version>",
                "     <packaging>pom</packaging>",
                "@@ -204,3 +204,3 @@",
                "         <developerConnection>scm:git:https://git-wip-us.apache.org/repos/asf/storm.git</developerConnection>",
                "-        <tag>v1.1.2</tag>",
                "+        <tag>HEAD</tag>",
                "         <url>https://git-wip-us.apache.org/repos/asf/storm</url>",
                "diff --git a/storm-buildtools/maven-shade-clojure-transformer/pom.xml b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "index f3e2a75e7..c2011e7ab 100644",
                "--- a/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "+++ b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-buildtools/storm-maven-plugins/pom.xml b/storm-buildtools/storm-maven-plugins/pom.xml",
                "index 77cd804bf..a826f0b42 100644",
                "--- a/storm-buildtools/storm-maven-plugins/pom.xml",
                "+++ b/storm-buildtools/storm-maven-plugins/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index dcbcab4db..69883d080 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>..</relativePath>",
                "diff --git a/storm-multilang/javascript/pom.xml b/storm-multilang/javascript/pom.xml",
                "index 72e5fd9d3..4ca916889 100644",
                "--- a/storm-multilang/javascript/pom.xml",
                "+++ b/storm-multilang/javascript/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/python/pom.xml b/storm-multilang/python/pom.xml",
                "index bb3abc9b7..54bdeb193 100644",
                "--- a/storm-multilang/python/pom.xml",
                "+++ b/storm-multilang/python/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/ruby/pom.xml b/storm-multilang/ruby/pom.xml",
                "index a0555d724..955df5ad2 100644",
                "--- a/storm-multilang/ruby/pom.xml",
                "+++ b/storm-multilang/ruby/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.3-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-rename-hack/pom.xml b/storm-rename-hack/pom.xml",
                "index 3b392bc83..a194b984b 100644",
                "--- a/storm-rename-hack/pom.xml",
                "+++ b/storm-rename-hack/pom.xml",
                "@@ -22,3 +22,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.2</version>",
                "+    <version>1.1.3-SNAPSHOT</version>",
                "     <relativePath>../pom.xml</relativePath>"
            ],
            "changed_files": [
                "examples/storm-elasticsearch-examples/pom.xml",
                "examples/storm-hbase-examples/pom.xml",
                "examples/storm-hdfs-examples/pom.xml",
                "examples/storm-hive-examples/pom.xml",
                "examples/storm-jdbc-examples/pom.xml",
                "examples/storm-jms-examples/pom.xml",
                "examples/storm-kafka-client-examples/pom.xml",
                "examples/storm-kafka-examples/pom.xml",
                "examples/storm-mongodb-examples/pom.xml",
                "examples/storm-mqtt-examples/pom.xml",
                "examples/storm-opentsdb-examples/pom.xml",
                "examples/storm-perf/pom.xml",
                "examples/storm-pmml-examples/pom.xml",
                "examples/storm-redis-examples/pom.xml",
                "examples/storm-solr-examples/pom.xml",
                "examples/storm-starter/pom.xml",
                "external/flux/flux-core/pom.xml",
                "external/flux/flux-wrappers/pom.xml",
                "external/flux/pom.xml",
                "external/sql/pom.xml",
                "external/sql/storm-sql-core/pom.xml",
                "external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "external/sql/storm-sql-runtime/pom.xml",
                "external/storm-cassandra/pom.xml",
                "external/storm-druid/pom.xml",
                "external/storm-elasticsearch/pom.xml",
                "external/storm-eventhubs/pom.xml",
                "external/storm-hbase/pom.xml",
                "external/storm-hdfs/pom.xml",
                "external/storm-hive/pom.xml",
                "external/storm-jdbc/pom.xml",
                "external/storm-jms/pom.xml",
                "external/storm-kafka-client/pom.xml",
                "external/storm-kafka-monitor/pom.xml",
                "external/storm-kafka/pom.xml",
                "external/storm-kinesis/pom.xml",
                "external/storm-metrics/pom.xml",
                "external/storm-mongodb/pom.xml",
                "external/storm-mqtt/pom.xml",
                "external/storm-opentsdb/pom.xml",
                "external/storm-pmml/pom.xml",
                "external/storm-redis/pom.xml",
                "external/storm-solr/pom.xml",
                "external/storm-submit-tools/pom.xml",
                "pom.xml",
                "storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "storm-buildtools/storm-maven-plugins/pom.xml",
                "storm-core/pom.xml",
                "storm-multilang/javascript/pom.xml",
                "storm-multilang/python/pom.xml",
                "storm-multilang/ruby/pom.xml",
                "storm-rename-hack/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "c335ad31d0c7803d078f50930d029cd5a9064da6"
                ],
                [
                    "no-tag",
                    "c638b8c8b1c386aa637e3917ab7be806edd40552"
                ],
                [
                    "no-tag",
                    "ec6eef73f45fe5bdcc9630778777b37ea514c202"
                ],
                [
                    "no-tag",
                    "c34f1bf4c800715372f5772b68781b96b71c4218"
                ],
                [
                    "no-tag",
                    "c269e4090d3bf360c0492dad106e221502042e41"
                ],
                [
                    "no-tag",
                    "aa32bb742e163e9a6468a60510dc952ccdfb43ed"
                ],
                [
                    "no-tag",
                    "34a220c952f8db2b1e90297021511e27cfde8a4b"
                ]
            ],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d70bf4b9e82733b4d1d8f476f067c520533c133e",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513937101,
            "hunks": 0,
            "message": "Merge branch 'STORM-2690__1.1.x-branch' of https://github.com/erikdw/storm into STORM-2690-1.1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2690": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2690",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1c7361982873fc8a84ed2b217b39ff634b35375b",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517494555,
            "hunks": 0,
            "message": "Merge branch 'STORM-2906' of https://github.com/HeartSaVioR/storm into STORM-2906-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2906": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2906",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "2181fcde3f95302e1363d07a72c2b16c5332bd6c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513033785,
            "hunks": 0,
            "message": "Merge branch 'STORM-2850-1.x' of https://github.com/srdo/storm into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2850": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2850",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "689d31c6ea3e5c52fcc4c18b0b065d66f2d29915",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511084197,
            "hunks": 1,
            "message": "STORM-2825: Fix ClassCastException when storm-kafka-client uses consumer config with String-type 'enable.auto.commit'",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "index ffefa29cd..d89b674ec 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "@@ -720,3 +720,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "-            final boolean enableAutoCommit = (boolean)builder.kafkaProps.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG);",
                "+            final boolean enableAutoCommit = Boolean.parseBoolean(builder.kafkaProps.get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG).toString());",
                "             if(enableAutoCommit) {"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2825": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2825",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5be414d087af1143edc682d53d7ac4da084fe6ee",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517494017,
            "hunks": 0,
            "message": "Merge branch 'STORM-2903-1.x' of https://github.com/omkreddy/storm into STORM-2903-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2903": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2903",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e354ea86f2d72c35bba6ea5f6b4b3b81ce4a58a3",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516310983,
            "hunks": 0,
            "message": "Merge branch 'STORM-2900-1.x' of https://github.com/satishd/storm into STORM-2900-1.x",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2900": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2900",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f6907912e38388fc983254e204b46e8ecabf9b25",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517513661,
            "hunks": 58,
            "message": "Revert \"[maven-release-plugin] prepare release v1.1.2\" This reverts commit 5d2eecf3d282a535541ac7520a88b47f01153da1.",
            "diff": [
                "diff --git a/examples/storm-elasticsearch-examples/pom.xml b/examples/storm-elasticsearch-examples/pom.xml",
                "index 3e3a9fea7..348637aed 100644",
                "--- a/examples/storm-elasticsearch-examples/pom.xml",
                "+++ b/examples/storm-elasticsearch-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hbase-examples/pom.xml b/examples/storm-hbase-examples/pom.xml",
                "index 48c0f0203..3a11a9c6c 100644",
                "--- a/examples/storm-hbase-examples/pom.xml",
                "+++ b/examples/storm-hbase-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hdfs-examples/pom.xml b/examples/storm-hdfs-examples/pom.xml",
                "index 261d333be..8bab8fcf2 100644",
                "--- a/examples/storm-hdfs-examples/pom.xml",
                "+++ b/examples/storm-hdfs-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hive-examples/pom.xml b/examples/storm-hive-examples/pom.xml",
                "index 9e903294d..50bf51214 100644",
                "--- a/examples/storm-hive-examples/pom.xml",
                "+++ b/examples/storm-hive-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jdbc-examples/pom.xml b/examples/storm-jdbc-examples/pom.xml",
                "index 60339a2c7..f599de795 100644",
                "--- a/examples/storm-jdbc-examples/pom.xml",
                "+++ b/examples/storm-jdbc-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jms-examples/pom.xml b/examples/storm-jms-examples/pom.xml",
                "index 9de6b93ca..44452502c 100644",
                "--- a/examples/storm-jms-examples/pom.xml",
                "+++ b/examples/storm-jms-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-client-examples/pom.xml b/examples/storm-kafka-client-examples/pom.xml",
                "index 1f862d759..6c40475a5 100644",
                "--- a/examples/storm-kafka-client-examples/pom.xml",
                "+++ b/examples/storm-kafka-client-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-examples/pom.xml b/examples/storm-kafka-examples/pom.xml",
                "index e0c6888ff..658e8f760 100644",
                "--- a/examples/storm-kafka-examples/pom.xml",
                "+++ b/examples/storm-kafka-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mongodb-examples/pom.xml b/examples/storm-mongodb-examples/pom.xml",
                "index f867b986e..f4ad7f55e 100644",
                "--- a/examples/storm-mongodb-examples/pom.xml",
                "+++ b/examples/storm-mongodb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mqtt-examples/pom.xml b/examples/storm-mqtt-examples/pom.xml",
                "index ac94556d1..d32cb992f 100644",
                "--- a/examples/storm-mqtt-examples/pom.xml",
                "+++ b/examples/storm-mqtt-examples/pom.xml",
                "@@ -28,3 +28,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.2</version>",
                "+    <version>1.1.2-SNAPSHOT</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-opentsdb-examples/pom.xml b/examples/storm-opentsdb-examples/pom.xml",
                "index 2ce34ee77..63cde9167 100644",
                "--- a/examples/storm-opentsdb-examples/pom.xml",
                "+++ b/examples/storm-opentsdb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-perf/pom.xml b/examples/storm-perf/pom.xml",
                "index 501be8f2b..6203793c0 100644",
                "--- a/examples/storm-perf/pom.xml",
                "+++ b/examples/storm-perf/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-pmml-examples/pom.xml b/examples/storm-pmml-examples/pom.xml",
                "index 0b0d133d5..c13fa7a1b 100644",
                "--- a/examples/storm-pmml-examples/pom.xml",
                "+++ b/examples/storm-pmml-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-redis-examples/pom.xml b/examples/storm-redis-examples/pom.xml",
                "index c0dde9df0..28d51aaf5 100644",
                "--- a/examples/storm-redis-examples/pom.xml",
                "+++ b/examples/storm-redis-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-solr-examples/pom.xml b/examples/storm-solr-examples/pom.xml",
                "index b81420aeb..e830e7cb7 100644",
                "--- a/examples/storm-solr-examples/pom.xml",
                "+++ b/examples/storm-solr-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-starter/pom.xml b/examples/storm-starter/pom.xml",
                "index 672a52f0d..9d029ff83 100644",
                "--- a/examples/storm-starter/pom.xml",
                "+++ b/examples/storm-starter/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.1.2</version>",
                "+      <version>1.1.2-SNAPSHOT</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-core/pom.xml b/external/flux/flux-core/pom.xml",
                "index c62989764..adde30d02 100644",
                "--- a/external/flux/flux-core/pom.xml",
                "+++ b/external/flux/flux-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-wrappers/pom.xml b/external/flux/flux-wrappers/pom.xml",
                "index 76c56c6ed..61f258b9a 100644",
                "--- a/external/flux/flux-wrappers/pom.xml",
                "+++ b/external/flux/flux-wrappers/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/pom.xml b/external/flux/pom.xml",
                "index aa8cd48ee..cbc33657c 100644",
                "--- a/external/flux/pom.xml",
                "+++ b/external/flux/pom.xml",
                "@@ -28,3 +28,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/pom.xml b/external/sql/pom.xml",
                "index c3cea241d..aedc08ab8 100644",
                "--- a/external/sql/pom.xml",
                "+++ b/external/sql/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-core/pom.xml b/external/sql/storm-sql-core/pom.xml",
                "index db99e28a3..a7980084a 100644",
                "--- a/external/sql/storm-sql-core/pom.xml",
                "+++ b/external/sql/storm-sql-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "index 5ca571d76..a03e25925 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "index f0919389f..7aad20378 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "index 3d4df119e..4b6153fc5 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-redis/pom.xml b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "index e61224fae..5889e6a9a 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-runtime/pom.xml b/external/sql/storm-sql-runtime/pom.xml",
                "index b64de3491..a506e92b7 100644",
                "--- a/external/sql/storm-sql-runtime/pom.xml",
                "+++ b/external/sql/storm-sql-runtime/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/storm-cassandra/pom.xml b/external/storm-cassandra/pom.xml",
                "index 3619a86f4..580000f4f 100644",
                "--- a/external/storm-cassandra/pom.xml",
                "+++ b/external/storm-cassandra/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-druid/pom.xml b/external/storm-druid/pom.xml",
                "index afed6dd93..02748eec7 100644",
                "--- a/external/storm-druid/pom.xml",
                "+++ b/external/storm-druid/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-elasticsearch/pom.xml b/external/storm-elasticsearch/pom.xml",
                "index 7b721674c..380ce4e5e 100644",
                "--- a/external/storm-elasticsearch/pom.xml",
                "+++ b/external/storm-elasticsearch/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-eventhubs/pom.xml b/external/storm-eventhubs/pom.xml",
                "index 9e91f712d..9ce5e2de6 100755",
                "--- a/external/storm-eventhubs/pom.xml",
                "+++ b/external/storm-eventhubs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hbase/pom.xml b/external/storm-hbase/pom.xml",
                "index 49436efb9..da3a9b51b 100644",
                "--- a/external/storm-hbase/pom.xml",
                "+++ b/external/storm-hbase/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hdfs/pom.xml b/external/storm-hdfs/pom.xml",
                "index 7d3c9cb1a..321569909 100644",
                "--- a/external/storm-hdfs/pom.xml",
                "+++ b/external/storm-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hive/pom.xml b/external/storm-hive/pom.xml",
                "index 2fa04b027..706e8f843 100644",
                "--- a/external/storm-hive/pom.xml",
                "+++ b/external/storm-hive/pom.xml",
                "@@ -23,3 +23,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.2</version>",
                "+    <version>1.1.2-SNAPSHOT</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jdbc/pom.xml b/external/storm-jdbc/pom.xml",
                "index 251f153b7..ea5d735c2 100644",
                "--- a/external/storm-jdbc/pom.xml",
                "+++ b/external/storm-jdbc/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jms/pom.xml b/external/storm-jms/pom.xml",
                "index d71235502..7c31b37ff 100644",
                "--- a/external/storm-jms/pom.xml",
                "+++ b/external/storm-jms/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index 5e960ae2b..304e703ef 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -24,3 +24,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-monitor/pom.xml b/external/storm-kafka-monitor/pom.xml",
                "index 8e0363c2a..475e0732d 100644",
                "--- a/external/storm-kafka-monitor/pom.xml",
                "+++ b/external/storm-kafka-monitor/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka/pom.xml b/external/storm-kafka/pom.xml",
                "index 56d83fcdd..28ec77ced 100644",
                "--- a/external/storm-kafka/pom.xml",
                "+++ b/external/storm-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kinesis/pom.xml b/external/storm-kinesis/pom.xml",
                "index 9ff4e96b8..013a6efc3 100644",
                "--- a/external/storm-kinesis/pom.xml",
                "+++ b/external/storm-kinesis/pom.xml",
                "@@ -19,3 +19,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-metrics/pom.xml b/external/storm-metrics/pom.xml",
                "index a35c8c689..85139343e 100644",
                "--- a/external/storm-metrics/pom.xml",
                "+++ b/external/storm-metrics/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.1.2</version>",
                "+      <version>1.1.2-SNAPSHOT</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mongodb/pom.xml b/external/storm-mongodb/pom.xml",
                "index 5420f6125..5ad9958eb 100644",
                "--- a/external/storm-mongodb/pom.xml",
                "+++ b/external/storm-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mqtt/pom.xml b/external/storm-mqtt/pom.xml",
                "index 069a0e01d..9a3056c58 100644",
                "--- a/external/storm-mqtt/pom.xml",
                "+++ b/external/storm-mqtt/pom.xml",
                "@@ -27,3 +27,3 @@",
                "         <artifactId>storm</artifactId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-opentsdb/pom.xml b/external/storm-opentsdb/pom.xml",
                "index f4a4f369d..0be768f76 100644",
                "--- a/external/storm-opentsdb/pom.xml",
                "+++ b/external/storm-opentsdb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-pmml/pom.xml b/external/storm-pmml/pom.xml",
                "index f0a07cadd..73f76f54e 100644",
                "--- a/external/storm-pmml/pom.xml",
                "+++ b/external/storm-pmml/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-redis/pom.xml b/external/storm-redis/pom.xml",
                "index fe641778e..33418eb9d 100644",
                "--- a/external/storm-redis/pom.xml",
                "+++ b/external/storm-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-solr/pom.xml b/external/storm-solr/pom.xml",
                "index 2c4439e51..c5303f467 100644",
                "--- a/external/storm-solr/pom.xml",
                "+++ b/external/storm-solr/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-submit-tools/pom.xml b/external/storm-submit-tools/pom.xml",
                "index 8d17cac11..340061c30 100644",
                "--- a/external/storm-submit-tools/pom.xml",
                "+++ b/external/storm-submit-tools/pom.xml",
                "@@ -21,3 +21,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/pom.xml b/pom.xml",
                "index 7f4c4b52c..d7fd43b2e 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -29,3 +29,3 @@",
                "     <artifactId>storm</artifactId>",
                "-    <version>1.1.2</version>",
                "+    <version>1.1.2-SNAPSHOT</version>",
                "     <packaging>pom</packaging>",
                "@@ -204,3 +204,3 @@",
                "         <developerConnection>scm:git:https://git-wip-us.apache.org/repos/asf/storm.git</developerConnection>",
                "-        <tag>v1.1.2</tag>",
                "+        <tag>HEAD</tag>",
                "         <url>https://git-wip-us.apache.org/repos/asf/storm</url>",
                "diff --git a/storm-buildtools/maven-shade-clojure-transformer/pom.xml b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "index f3e2a75e7..9585b9d35 100644",
                "--- a/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "+++ b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-buildtools/storm-maven-plugins/pom.xml b/storm-buildtools/storm-maven-plugins/pom.xml",
                "index 77cd804bf..759e35bfd 100644",
                "--- a/storm-buildtools/storm-maven-plugins/pom.xml",
                "+++ b/storm-buildtools/storm-maven-plugins/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index dcbcab4db..61db89b77 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>..</relativePath>",
                "diff --git a/storm-dist/binary/pom.xml b/storm-dist/binary/pom.xml",
                "index 166d09346..5824ca77a 100644",
                "--- a/storm-dist/binary/pom.xml",
                "+++ b/storm-dist/binary/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-dist/source/pom.xml b/storm-dist/source/pom.xml",
                "index 22db90db0..a24674718 100644",
                "--- a/storm-dist/source/pom.xml",
                "+++ b/storm-dist/source/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/javascript/pom.xml b/storm-multilang/javascript/pom.xml",
                "index 72e5fd9d3..58f9d5334 100644",
                "--- a/storm-multilang/javascript/pom.xml",
                "+++ b/storm-multilang/javascript/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/python/pom.xml b/storm-multilang/python/pom.xml",
                "index bb3abc9b7..194bc22bd 100644",
                "--- a/storm-multilang/python/pom.xml",
                "+++ b/storm-multilang/python/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/ruby/pom.xml b/storm-multilang/ruby/pom.xml",
                "index a0555d724..d366ecc41 100644",
                "--- a/storm-multilang/ruby/pom.xml",
                "+++ b/storm-multilang/ruby/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-rename-hack/pom.xml b/storm-rename-hack/pom.xml",
                "index 3b392bc83..b27e4e897 100644",
                "--- a/storm-rename-hack/pom.xml",
                "+++ b/storm-rename-hack/pom.xml",
                "@@ -22,3 +22,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.2</version>",
                "+    <version>1.1.2-SNAPSHOT</version>",
                "     <relativePath>../pom.xml</relativePath>"
            ],
            "changed_files": [
                "examples/storm-elasticsearch-examples/pom.xml",
                "examples/storm-hbase-examples/pom.xml",
                "examples/storm-hdfs-examples/pom.xml",
                "examples/storm-hive-examples/pom.xml",
                "examples/storm-jdbc-examples/pom.xml",
                "examples/storm-jms-examples/pom.xml",
                "examples/storm-kafka-client-examples/pom.xml",
                "examples/storm-kafka-examples/pom.xml",
                "examples/storm-mongodb-examples/pom.xml",
                "examples/storm-mqtt-examples/pom.xml",
                "examples/storm-opentsdb-examples/pom.xml",
                "examples/storm-perf/pom.xml",
                "examples/storm-pmml-examples/pom.xml",
                "examples/storm-redis-examples/pom.xml",
                "examples/storm-solr-examples/pom.xml",
                "examples/storm-starter/pom.xml",
                "external/flux/flux-core/pom.xml",
                "external/flux/flux-wrappers/pom.xml",
                "external/flux/pom.xml",
                "external/sql/pom.xml",
                "external/sql/storm-sql-core/pom.xml",
                "external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "external/sql/storm-sql-runtime/pom.xml",
                "external/storm-cassandra/pom.xml",
                "external/storm-druid/pom.xml",
                "external/storm-elasticsearch/pom.xml",
                "external/storm-eventhubs/pom.xml",
                "external/storm-hbase/pom.xml",
                "external/storm-hdfs/pom.xml",
                "external/storm-hive/pom.xml",
                "external/storm-jdbc/pom.xml",
                "external/storm-jms/pom.xml",
                "external/storm-kafka-client/pom.xml",
                "external/storm-kafka-monitor/pom.xml",
                "external/storm-kafka/pom.xml",
                "external/storm-kinesis/pom.xml",
                "external/storm-metrics/pom.xml",
                "external/storm-mongodb/pom.xml",
                "external/storm-mqtt/pom.xml",
                "external/storm-opentsdb/pom.xml",
                "external/storm-pmml/pom.xml",
                "external/storm-redis/pom.xml",
                "external/storm-solr/pom.xml",
                "external/storm-submit-tools/pom.xml",
                "pom.xml",
                "storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "storm-buildtools/storm-maven-plugins/pom.xml",
                "storm-core/pom.xml",
                "storm-dist/binary/pom.xml",
                "storm-dist/source/pom.xml",
                "storm-multilang/javascript/pom.xml",
                "storm-multilang/python/pom.xml",
                "storm-multilang/ruby/pom.xml",
                "storm-rename-hack/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "5fa505bf9291ba57de7d6f5e7062c5594e196555"
                ],
                [
                    "no-tag",
                    "5c9599b4fa8b2d0e12846d6c48213bd1e982f23a"
                ],
                [
                    "no-tag",
                    "773cded3bdabc82e977c73e4137bfac4fca2192d"
                ],
                [
                    "no-tag",
                    "3623e6a6f4db4af32f33c39736928407a7d016d5"
                ],
                [
                    "no-tag",
                    "d4247d759e2c5f1b8f560c0436d300407bd8ce35"
                ],
                [
                    "no-tag",
                    "d243dc94edcd2e12f79b50f670b9899ab45e1f07"
                ]
            ],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "437e524602e3087568696cfda5cb8f115361f81f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513725432,
            "hunks": 9,
            "message": "Address handling activate/deactivate in multilang module files",
            "diff": [
                "diff --git a/examples/storm-starter/multilang/resources/randomsentence.js b/examples/storm-starter/multilang/resources/randomsentence.js",
                "index b12191543..7fcf5e12f 100644",
                "--- a/examples/storm-starter/multilang/resources/randomsentence.js",
                "+++ b/examples/storm-starter/multilang/resources/randomsentence.js",
                "@@ -64,2 +64,10 @@ RandomSentenceSpout.prototype.nextTuple = function(done) {",
                "+RandomSentenceSpout.prototype.activate = function(done) {",
                "+    done();",
                "+}",
                "+",
                "+RandomSentenceSpout.prototype.deactivate = function(done) {",
                "+    done();",
                "+}",
                "+",
                " RandomSentenceSpout.prototype.createNextTupleId = function() {",
                "diff --git a/external/flux/flux-wrappers/src/main/resources/resources/randomsentence.js b/external/flux/flux-wrappers/src/main/resources/resources/randomsentence.js",
                "index b12191543..7fcf5e12f 100644",
                "--- a/external/flux/flux-wrappers/src/main/resources/resources/randomsentence.js",
                "+++ b/external/flux/flux-wrappers/src/main/resources/resources/randomsentence.js",
                "@@ -64,2 +64,10 @@ RandomSentenceSpout.prototype.nextTuple = function(done) {",
                "+RandomSentenceSpout.prototype.activate = function(done) {",
                "+    done();",
                "+}",
                "+",
                "+RandomSentenceSpout.prototype.deactivate = function(done) {",
                "+    done();",
                "+}",
                "+",
                " RandomSentenceSpout.prototype.createNextTupleId = function() {",
                "diff --git a/storm-core/src/dev/resources/tester_spout.js b/storm-core/src/dev/resources/tester_spout.js",
                "index 269234fb0..e3658c3d5 100644",
                "--- a/storm-core/src/dev/resources/tester_spout.js",
                "+++ b/storm-core/src/dev/resources/tester_spout.js",
                "@@ -46,2 +46,10 @@ TesterSpout.prototype.nextTuple = function(done) {",
                "+TesterSpout.prototype.activate = function(done) {",
                "+    done();",
                "+}",
                "+",
                "+TesterSpout.prototype.deactivate = function(done) {",
                "+    done();",
                "+}",
                "+",
                " TesterSpout.prototype.createNextTupleId = function() {",
                "diff --git a/storm-multilang/javascript/src/main/resources/resources/storm.js b/storm-multilang/javascript/src/main/resources/resources/storm.js",
                "index 25e0a6de4..a131eed0c 100755",
                "--- a/storm-multilang/javascript/src/main/resources/resources/storm.js",
                "+++ b/storm-multilang/javascript/src/main/resources/resources/storm.js",
                "@@ -355,2 +355,14 @@ Spout.prototype.constructor = Spout;",
                "+/**",
                "+ * This method will be called when a spout has been activated out of a deactivated mode.",
                "+ * @param done Call this method when finished and ready to receive tuples.",
                "+ */",
                "+Spout.prototype.activate = function(done) {};",
                "+",
                "+/**",
                "+ * This method will be called when a spout has been deactivated.",
                "+ * @param done Call this method when finished.",
                "+ */",
                "+Spout.prototype.deactivate = function(done) {};",
                "+",
                " /**",
                "@@ -382,2 +394,10 @@ Spout.prototype.handleNewCommand = function(command) {",
                "+    if (command['command'] === 'activate') {",
                "+        this.activate(callback);",
                "+    }",
                "+",
                "+    if (command['command'] === 'deactivate') {",
                "+        this.deactivate(callback);",
                "+    }",
                "+",
                "     if (command['command'] === 'next') {",
                "diff --git a/storm-multilang/python/src/main/resources/resources/storm.py b/storm-multilang/python/src/main/resources/resources/storm.py",
                "index 224ed120b..42b637850 100755",
                "--- a/storm-multilang/python/src/main/resources/resources/storm.py",
                "+++ b/storm-multilang/python/src/main/resources/resources/storm.py",
                "@@ -234,2 +234,8 @@ class Spout(object):",
                "+    def activate(self):",
                "+        pass",
                "+",
                "+    def deactivate(self):",
                "+        pass",
                "+",
                "     def ack(self, id):",
                "@@ -251,2 +257,6 @@ class Spout(object):",
                "                 msg = readCommand()",
                "+                if msg[\"command\"] == \"activate\":",
                "+                    self.activate()",
                "+                if msg[\"command\"] == \"deactivate\":",
                "+                    self.deactivate()",
                "                 if msg[\"command\"] == \"next\":",
                "diff --git a/storm-multilang/ruby/src/main/resources/resources/storm.rb b/storm-multilang/ruby/src/main/resources/resources/storm.rb",
                "index 816694e8b..5e955cccd 100644",
                "--- a/storm-multilang/ruby/src/main/resources/resources/storm.rb",
                "+++ b/storm-multilang/ruby/src/main/resources/resources/storm.rb",
                "@@ -207,2 +207,6 @@ module Storm",
                "+    def activate; end",
                "+",
                "+    def deactivate; end",
                "+",
                "     def nextTuple; end",
                "@@ -221,2 +225,6 @@ module Storm",
                "           case msg['command']",
                "+            when 'activate'",
                "+              activate",
                "+            when 'deactivate'",
                "+              deactivate",
                "             when 'next'"
            ],
            "changed_files": [
                "examples/storm-starter/multilang/resources/randomsentence.js",
                "external/flux/flux-wrappers/src/main/resources/resources/randomsentence.js",
                "storm-core/src/dev/resources/tester_spout.js",
                "storm-multilang/javascript/src/main/resources/resources/storm.js",
                "storm-multilang/python/src/main/resources/resources/storm.py",
                "storm-multilang/ruby/src/main/resources/resources/storm.rb"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "adabfdd0789570aabc62a76f50f32e107118b4a1"
                ],
                [
                    "no-tag",
                    "fa753bffcf2168f4617e0d5630072cb4e0514824"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "894a0b0a0be34b69085eec5f5c42efd5cdc5eb83",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514513612,
            "hunks": 0,
            "message": "Merge branch 'STORM-2869-1.x' of https://github.com/srdo/storm into STORM-2869-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2869": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2869",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "3af8dda88aaff2e62be0d6aafd8bb1cbaaa07f8d",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513638175,
            "hunks": 0,
            "message": "Merge branch 'STORM-2854'",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2854": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2854",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "2657c25f232f9cf6f7d6580cd27c2d76a7b6c4c9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510393661,
            "hunks": 0,
            "message": "Merge branch 'STORM-2807' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2807": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2807",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b5eb86b0147879c7536c9ad43feaf2f8046dfe4c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509939664,
            "hunks": 0,
            "message": "Merge branch 'STORM-2793-merge'",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2793": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2793",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e296d65fdd17c8f8b1fde966e2d54fdad259a7bc",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513877305,
            "hunks": 0,
            "message": "Merge branch 'STORM-2847-1.x' of https://github.com/srdo/storm into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2847": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2847",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "474b9af67fccc0ef4418c9aa7f12b4483dc78733",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515820980,
            "hunks": 0,
            "message": "Merge branch 'kerberos-solr-bolt' of https://github.com/omkreddy/storm into STORM-2860-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2860": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2860",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "3badc161ea4e9d630207eddaf7eddd077e4674f4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510788184,
            "hunks": 0,
            "message": "Merge branch 'STORM-2814-1.x' of https://github.com/Ethanlm/storm into STORM-2814-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2814": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2814",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "99ba5e4d4531b7ce2524cb109053402a9d579e7b",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516987737,
            "hunks": 59,
            "message": "Revert \"[maven-release-plugin] prepare for next development iteration\" This reverts commit c335ad31d0c7803d078f50930d029cd5a9064da6.",
            "diff": [
                "diff --git a/examples/storm-elasticsearch-examples/pom.xml b/examples/storm-elasticsearch-examples/pom.xml",
                "index be12b2f7a..598c71f46 100644",
                "--- a/examples/storm-elasticsearch-examples/pom.xml",
                "+++ b/examples/storm-elasticsearch-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hbase-examples/pom.xml b/examples/storm-hbase-examples/pom.xml",
                "index 80b9ed1d2..eb5523163 100644",
                "--- a/examples/storm-hbase-examples/pom.xml",
                "+++ b/examples/storm-hbase-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hdfs-examples/pom.xml b/examples/storm-hdfs-examples/pom.xml",
                "index 868f9e5e5..ffe196c1b 100644",
                "--- a/examples/storm-hdfs-examples/pom.xml",
                "+++ b/examples/storm-hdfs-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hive-examples/pom.xml b/examples/storm-hive-examples/pom.xml",
                "index ddf2de3b3..ea240a5d7 100644",
                "--- a/examples/storm-hive-examples/pom.xml",
                "+++ b/examples/storm-hive-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jdbc-examples/pom.xml b/examples/storm-jdbc-examples/pom.xml",
                "index 850c43abb..78c2f0958 100644",
                "--- a/examples/storm-jdbc-examples/pom.xml",
                "+++ b/examples/storm-jdbc-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jms-examples/pom.xml b/examples/storm-jms-examples/pom.xml",
                "index 944f0dc58..fdd66e7d5 100644",
                "--- a/examples/storm-jms-examples/pom.xml",
                "+++ b/examples/storm-jms-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-client-examples/pom.xml b/examples/storm-kafka-client-examples/pom.xml",
                "index bb055d92d..62869b106 100644",
                "--- a/examples/storm-kafka-client-examples/pom.xml",
                "+++ b/examples/storm-kafka-client-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-examples/pom.xml b/examples/storm-kafka-examples/pom.xml",
                "index 39f790523..95c3b4aad 100644",
                "--- a/examples/storm-kafka-examples/pom.xml",
                "+++ b/examples/storm-kafka-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mongodb-examples/pom.xml b/examples/storm-mongodb-examples/pom.xml",
                "index 7cee2da52..ccab1263e 100644",
                "--- a/examples/storm-mongodb-examples/pom.xml",
                "+++ b/examples/storm-mongodb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mqtt-examples/pom.xml b/examples/storm-mqtt-examples/pom.xml",
                "index 9f004fd92..5f074147d 100644",
                "--- a/examples/storm-mqtt-examples/pom.xml",
                "+++ b/examples/storm-mqtt-examples/pom.xml",
                "@@ -28,3 +28,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.2.1-SNAPSHOT</version>",
                "+    <version>1.2.0</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-opentsdb-examples/pom.xml b/examples/storm-opentsdb-examples/pom.xml",
                "index 948673a23..3a9ecdd3f 100644",
                "--- a/examples/storm-opentsdb-examples/pom.xml",
                "+++ b/examples/storm-opentsdb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-perf/pom.xml b/examples/storm-perf/pom.xml",
                "index bc167780d..ff80366f8 100644",
                "--- a/examples/storm-perf/pom.xml",
                "+++ b/examples/storm-perf/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-pmml-examples/pom.xml b/examples/storm-pmml-examples/pom.xml",
                "index 9d9703c11..9333468ba 100644",
                "--- a/examples/storm-pmml-examples/pom.xml",
                "+++ b/examples/storm-pmml-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-redis-examples/pom.xml b/examples/storm-redis-examples/pom.xml",
                "index a01d91f21..34bf7dc64 100644",
                "--- a/examples/storm-redis-examples/pom.xml",
                "+++ b/examples/storm-redis-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-solr-examples/pom.xml b/examples/storm-solr-examples/pom.xml",
                "index 6f18ea7dc..f87464b5a 100644",
                "--- a/examples/storm-solr-examples/pom.xml",
                "+++ b/examples/storm-solr-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-starter/pom.xml b/examples/storm-starter/pom.xml",
                "index 63b35704a..2b044fd3d 100644",
                "--- a/examples/storm-starter/pom.xml",
                "+++ b/examples/storm-starter/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.2.1-SNAPSHOT</version>",
                "+      <version>1.2.0</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-core/pom.xml b/external/flux/flux-core/pom.xml",
                "index 0ed9bff90..870c3de98 100644",
                "--- a/external/flux/flux-core/pom.xml",
                "+++ b/external/flux/flux-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-wrappers/pom.xml b/external/flux/flux-wrappers/pom.xml",
                "index 34741d397..3dc8397ec 100644",
                "--- a/external/flux/flux-wrappers/pom.xml",
                "+++ b/external/flux/flux-wrappers/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/pom.xml b/external/flux/pom.xml",
                "index feca1deb5..125a38158 100644",
                "--- a/external/flux/pom.xml",
                "+++ b/external/flux/pom.xml",
                "@@ -28,3 +28,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/pom.xml b/external/sql/pom.xml",
                "index 86acdb3a3..c6afee63c 100644",
                "--- a/external/sql/pom.xml",
                "+++ b/external/sql/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-core/pom.xml b/external/sql/storm-sql-core/pom.xml",
                "index 84cfc81e4..75b83933f 100644",
                "--- a/external/sql/storm-sql-core/pom.xml",
                "+++ b/external/sql/storm-sql-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "index 00ff3b3bd..518ce77a1 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "index 9f2aaddb2..ffe4740a6 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "index c7bca5556..146959f6b 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-redis/pom.xml b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "index 70adce4a2..e4878acfb 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-runtime/pom.xml b/external/sql/storm-sql-runtime/pom.xml",
                "index 3780328f5..32acec5f0 100644",
                "--- a/external/sql/storm-sql-runtime/pom.xml",
                "+++ b/external/sql/storm-sql-runtime/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/storm-autocreds/pom.xml b/external/storm-autocreds/pom.xml",
                "index e69eea7c3..30c10e7d9 100644",
                "--- a/external/storm-autocreds/pom.xml",
                "+++ b/external/storm-autocreds/pom.xml",
                "@@ -21,3 +21,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-cassandra/pom.xml b/external/storm-cassandra/pom.xml",
                "index bb5ea9e7e..5926fd410 100644",
                "--- a/external/storm-cassandra/pom.xml",
                "+++ b/external/storm-cassandra/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-druid/pom.xml b/external/storm-druid/pom.xml",
                "index 9ab09e32d..bf8b1c35f 100644",
                "--- a/external/storm-druid/pom.xml",
                "+++ b/external/storm-druid/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-elasticsearch/pom.xml b/external/storm-elasticsearch/pom.xml",
                "index 0a2166cca..e82b43dc9 100644",
                "--- a/external/storm-elasticsearch/pom.xml",
                "+++ b/external/storm-elasticsearch/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-eventhubs/pom.xml b/external/storm-eventhubs/pom.xml",
                "index 94f1060fe..da1324c8e 100755",
                "--- a/external/storm-eventhubs/pom.xml",
                "+++ b/external/storm-eventhubs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hbase/pom.xml b/external/storm-hbase/pom.xml",
                "index be859c414..363ab59ae 100644",
                "--- a/external/storm-hbase/pom.xml",
                "+++ b/external/storm-hbase/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hdfs/pom.xml b/external/storm-hdfs/pom.xml",
                "index 715a88bba..f773a4db1 100644",
                "--- a/external/storm-hdfs/pom.xml",
                "+++ b/external/storm-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hive/pom.xml b/external/storm-hive/pom.xml",
                "index 980db6308..f920a0933 100644",
                "--- a/external/storm-hive/pom.xml",
                "+++ b/external/storm-hive/pom.xml",
                "@@ -23,3 +23,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.2.1-SNAPSHOT</version>",
                "+    <version>1.2.0</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jdbc/pom.xml b/external/storm-jdbc/pom.xml",
                "index 1be5ee37c..1b95f5e19 100644",
                "--- a/external/storm-jdbc/pom.xml",
                "+++ b/external/storm-jdbc/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jms/pom.xml b/external/storm-jms/pom.xml",
                "index 5400fe1ff..115e284b8 100644",
                "--- a/external/storm-jms/pom.xml",
                "+++ b/external/storm-jms/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index 9e05432ec..be2d0f75c 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -24,3 +24,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-monitor/pom.xml b/external/storm-kafka-monitor/pom.xml",
                "index 4230787e3..43da43b40 100644",
                "--- a/external/storm-kafka-monitor/pom.xml",
                "+++ b/external/storm-kafka-monitor/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka/pom.xml b/external/storm-kafka/pom.xml",
                "index d74e35fca..a4a149d0b 100644",
                "--- a/external/storm-kafka/pom.xml",
                "+++ b/external/storm-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kinesis/pom.xml b/external/storm-kinesis/pom.xml",
                "index 4bcc08524..d6fe8d138 100644",
                "--- a/external/storm-kinesis/pom.xml",
                "+++ b/external/storm-kinesis/pom.xml",
                "@@ -19,3 +19,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-metrics/pom.xml b/external/storm-metrics/pom.xml",
                "index d59f63533..af732e979 100644",
                "--- a/external/storm-metrics/pom.xml",
                "+++ b/external/storm-metrics/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.2.1-SNAPSHOT</version>",
                "+      <version>1.2.0</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mongodb/pom.xml b/external/storm-mongodb/pom.xml",
                "index b7da50822..eb8bd5e3f 100644",
                "--- a/external/storm-mongodb/pom.xml",
                "+++ b/external/storm-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mqtt/pom.xml b/external/storm-mqtt/pom.xml",
                "index 41c39edf3..b1b8ddfba 100644",
                "--- a/external/storm-mqtt/pom.xml",
                "+++ b/external/storm-mqtt/pom.xml",
                "@@ -27,3 +27,3 @@",
                "         <artifactId>storm</artifactId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-opentsdb/pom.xml b/external/storm-opentsdb/pom.xml",
                "index 28444290a..270524380 100644",
                "--- a/external/storm-opentsdb/pom.xml",
                "+++ b/external/storm-opentsdb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-pmml/pom.xml b/external/storm-pmml/pom.xml",
                "index 9e28c7bd5..e8853ba4d 100644",
                "--- a/external/storm-pmml/pom.xml",
                "+++ b/external/storm-pmml/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-redis/pom.xml b/external/storm-redis/pom.xml",
                "index 613073967..09334a100 100644",
                "--- a/external/storm-redis/pom.xml",
                "+++ b/external/storm-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-solr/pom.xml b/external/storm-solr/pom.xml",
                "index 210599c7c..5a3bef0fe 100644",
                "--- a/external/storm-solr/pom.xml",
                "+++ b/external/storm-solr/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-submit-tools/pom.xml b/external/storm-submit-tools/pom.xml",
                "index 8ffe10a99..3cd66576d 100644",
                "--- a/external/storm-submit-tools/pom.xml",
                "+++ b/external/storm-submit-tools/pom.xml",
                "@@ -21,3 +21,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/pom.xml b/pom.xml",
                "index 6668e41d0..154d92087 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -29,3 +29,3 @@",
                "     <artifactId>storm</artifactId>",
                "-    <version>1.2.1-SNAPSHOT</version>",
                "+    <version>1.2.0</version>",
                "     <packaging>pom</packaging>",
                "@@ -204,3 +204,3 @@",
                "         <developerConnection>scm:git:https://git-wip-us.apache.org/repos/asf/storm.git</developerConnection>",
                "-        <tag>HEAD</tag>",
                "+        <tag>v1.2.0</tag>",
                "         <url>https://git-wip-us.apache.org/repos/asf/storm</url>",
                "diff --git a/storm-buildtools/maven-shade-clojure-transformer/pom.xml b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "index 53a9de383..94cf5f20c 100644",
                "--- a/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "+++ b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-buildtools/storm-maven-plugins/pom.xml b/storm-buildtools/storm-maven-plugins/pom.xml",
                "index 9031c9d8d..f9a8ad156 100644",
                "--- a/storm-buildtools/storm-maven-plugins/pom.xml",
                "+++ b/storm-buildtools/storm-maven-plugins/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index c3af36dac..0a0ce3e74 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>..</relativePath>",
                "diff --git a/storm-dist/binary/pom.xml b/storm-dist/binary/pom.xml",
                "index b911f808a..4ca1a2e88 100644",
                "--- a/storm-dist/binary/pom.xml",
                "+++ b/storm-dist/binary/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-dist/source/pom.xml b/storm-dist/source/pom.xml",
                "index ad17a88a0..6933bf868 100644",
                "--- a/storm-dist/source/pom.xml",
                "+++ b/storm-dist/source/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/javascript/pom.xml b/storm-multilang/javascript/pom.xml",
                "index 9d286a305..24e553561 100644",
                "--- a/storm-multilang/javascript/pom.xml",
                "+++ b/storm-multilang/javascript/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/python/pom.xml b/storm-multilang/python/pom.xml",
                "index 974742ea3..660ec54ad 100644",
                "--- a/storm-multilang/python/pom.xml",
                "+++ b/storm-multilang/python/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/ruby/pom.xml b/storm-multilang/ruby/pom.xml",
                "index c74b5c8ab..a55d0ca75 100644",
                "--- a/storm-multilang/ruby/pom.xml",
                "+++ b/storm-multilang/ruby/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.1-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-rename-hack/pom.xml b/storm-rename-hack/pom.xml",
                "index 0d170239c..a33192708 100644",
                "--- a/storm-rename-hack/pom.xml",
                "+++ b/storm-rename-hack/pom.xml",
                "@@ -22,3 +22,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.2.1-SNAPSHOT</version>",
                "+    <version>1.2.0</version>",
                "     <relativePath>../pom.xml</relativePath>"
            ],
            "changed_files": [
                "examples/storm-elasticsearch-examples/pom.xml",
                "examples/storm-hbase-examples/pom.xml",
                "examples/storm-hdfs-examples/pom.xml",
                "examples/storm-hive-examples/pom.xml",
                "examples/storm-jdbc-examples/pom.xml",
                "examples/storm-jms-examples/pom.xml",
                "examples/storm-kafka-client-examples/pom.xml",
                "examples/storm-kafka-examples/pom.xml",
                "examples/storm-mongodb-examples/pom.xml",
                "examples/storm-mqtt-examples/pom.xml",
                "examples/storm-opentsdb-examples/pom.xml",
                "examples/storm-perf/pom.xml",
                "examples/storm-pmml-examples/pom.xml",
                "examples/storm-redis-examples/pom.xml",
                "examples/storm-solr-examples/pom.xml",
                "examples/storm-starter/pom.xml",
                "external/flux/flux-core/pom.xml",
                "external/flux/flux-wrappers/pom.xml",
                "external/flux/pom.xml",
                "external/sql/pom.xml",
                "external/sql/storm-sql-core/pom.xml",
                "external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "external/sql/storm-sql-runtime/pom.xml",
                "external/storm-autocreds/pom.xml",
                "external/storm-cassandra/pom.xml",
                "external/storm-druid/pom.xml",
                "external/storm-elasticsearch/pom.xml",
                "external/storm-eventhubs/pom.xml",
                "external/storm-hbase/pom.xml",
                "external/storm-hdfs/pom.xml",
                "external/storm-hive/pom.xml",
                "external/storm-jdbc/pom.xml",
                "external/storm-jms/pom.xml",
                "external/storm-kafka-client/pom.xml",
                "external/storm-kafka-monitor/pom.xml",
                "external/storm-kafka/pom.xml",
                "external/storm-kinesis/pom.xml",
                "external/storm-metrics/pom.xml",
                "external/storm-mongodb/pom.xml",
                "external/storm-mqtt/pom.xml",
                "external/storm-opentsdb/pom.xml",
                "external/storm-pmml/pom.xml",
                "external/storm-redis/pom.xml",
                "external/storm-solr/pom.xml",
                "external/storm-submit-tools/pom.xml",
                "pom.xml",
                "storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "storm-buildtools/storm-maven-plugins/pom.xml",
                "storm-core/pom.xml",
                "storm-dist/binary/pom.xml",
                "storm-dist/source/pom.xml",
                "storm-multilang/javascript/pom.xml",
                "storm-multilang/python/pom.xml",
                "storm-multilang/ruby/pom.xml",
                "storm-rename-hack/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "dcb2949bc13a2730dee73b17a8fe1235c0a4182d"
                ],
                [
                    "no-tag",
                    "45fe29e84796015f3cfd10f0c2a3f9ddd150fba2"
                ],
                [
                    "no-tag",
                    "fc81fa3e3433c5a34651c4315bc6d6ba0a971cee"
                ],
                [
                    "no-tag",
                    "f887e534e72fba8731493dd69cc97553f994a1fe"
                ],
                [
                    "no-tag",
                    "8f877a07cdaaa26ec32f939e91e632457ca9c07c"
                ],
                [
                    "no-tag",
                    "f59b131a0c05479f1230b7501a7b9abb9920724d"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e2e3f5d19a8671e3759a04b94135fd6643b3aa61",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512716823,
            "hunks": 0,
            "message": "Merge branch 'STORM-2796-1.x-merge' into 1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2796": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2796",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "c6324246f926ea622b2c53fca3b5354d334522fe",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510527845,
            "hunks": 0,
            "message": "Merge branch 'STORM-2535-1.x' of https://github.com/srdo/storm into STORM-2535-1.x",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2535": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2535",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "fea11b26780dfd781f7cdb530e8ae6508afc79b4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515703542,
            "hunks": 1,
            "message": "STORM-2153: add missing type hint",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/task.clj b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "index 26ce76c55..68af75bb6 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "@@ -165,3 +165,3 @@",
                "              (when (emit-sampler)",
                "-               (stats/emitted-tuple! executor-stats (.getEmitted (.get ^Map (:task-metrics executor-data) task-id) stream) stream)",
                "+               (stats/emitted-tuple! executor-stats (.getEmitted ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) stream) stream)",
                "                (stats/transferred-tuples! executor-stats (.getTransferred ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) stream) stream (count out-tasks)))"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/task.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "05c810af4042c8e5d36824f029f56251f4c0d4d6",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514922289,
            "hunks": 2,
            "message": "STORM-2840: Getting resource availability from pre-GRAS supervisors",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/ui/core.clj b/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "index 8db46e8a9..309788110 100644",
                "--- a/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "@@ -427,4 +427,4 @@",
                "                                (for [^SupervisorSummary s sups",
                "-                                     :let [sup-total-mem (get (.get_total_resources s) Constants/COMMON_TOTAL_MEMORY_RESOURCE_NAME)",
                "-                                           sup-total-cpu (get (.get_total_resources s) Constants/COMMON_CPU_RESOURCE_NAME)",
                "+                                     :let [sup-total-mem (get (.get_total_resources s) Constants/COMMON_TOTAL_MEMORY_RESOURCE_NAME (get (.get_total_resources s) Config/SUPERVISOR_MEMORY_CAPACITY_MB))",
                "+                                           sup-total-cpu (get (.get_total_resources s) Constants/COMMON_CPU_RESOURCE_NAME (get (.get_total_resources s) Config/SUPERVISOR_CPU_CAPACITY))",
                "                                            sup-avail-mem (max (- sup-total-mem (.get_used_mem s)) 0.0)",
                "@@ -520,4 +520,4 @@",
                "         slotsFree (max (- slotsTotal slotsUsed) 0)",
                "-        totalMem (get (.get_total_resources summary) Constants/COMMON_TOTAL_MEMORY_RESOURCE_NAME)",
                "-        totalCpu (get (.get_total_resources summary) Constants/COMMON_CPU_RESOURCE_NAME)",
                "+        totalMem (get (.get_total_resources summary) Constants/COMMON_TOTAL_MEMORY_RESOURCE_NAME (get (.get_total_resources summary) Config/SUPERVISOR_MEMORY_CAPACITY_MB))",
                "+        totalCpu (get (.get_total_resources summary) Constants/COMMON_CPU_RESOURCE_NAME (get (.get_total_resources summary) Config/SUPERVISOR_CPU_CAPACITY))",
                "         usedMem (.get_used_mem summary)"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/ui/core.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2840": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2840",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1c771c24370d57c271d5151c4712bfe085963598",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517497320,
            "hunks": 0,
            "message": "Merge branch 'STORM-2907' of https://github.com/arunmahadevan/storm into STORM-2907-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2907": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2907",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "774928028f2d06af5d68e13f0fb745561c2aa4a0",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511085637,
            "hunks": 2,
            "message": "MINOR: Use booleans instead of strings for 'enable.auto.commit' setting",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "index ca8dcee5b..6c792ab6a 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "@@ -443,3 +443,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         if (builder.processingGuarantee == ProcessingGuarantee.NONE) {",
                "-            builder.kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"true\");",
                "+            builder.kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);",
                "         } else {",
                "@@ -466,3 +466,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "             }",
                "-            builder.kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false\");",
                "+            builder.kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);",
                "         }"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "b24f5d87cbcd4faeed651e45a8e673db52469a46"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "4c31fa77130e7c75eaa20a8e67bb7538e9ba8868",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514435088,
            "hunks": 0,
            "message": "Merge branch 'STORM-2870-1.x' of https://github.com/HeartSaVioR/storm into STORM-2870-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2870": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2870",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d68416b2447756e2fdf31ef63cbd3428eb299899",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517498645,
            "hunks": 0,
            "message": "Merge branch 'STORM-2916' of https://github.com/Ethanlm/storm into STORM-2916-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2916": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2916",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8bf7252ebd36e540515fedd70fb8c2004c1e4364",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515609813,
            "hunks": 4,
            "message": "STORM-2153: Use StringBuilder instead of String.format for composing metric names",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "index 789367b98..aea453991 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "@@ -120,3 +120,2 @@ public class StormMetricRegistry {",
                "-",
                "     public static void stop(){",
                "@@ -128,19 +127,31 @@ public class StormMetricRegistry {",
                "     public static String metricName(String name, String stormId, String componentId, String streamId, String executorId, Integer workerPort){",
                "-        return String.format(\"storm.worker.%s.%s.%s.%s.%s.%s-%s\",",
                "-                stormId,",
                "-                hostName,",
                "-                dotToUnderScore(componentId),",
                "-                dotToUnderScore(streamId),",
                "-                dotToUnderScore(executorId),",
                "-                workerPort,",
                "-                name);",
                "+        StringBuilder sb = new StringBuilder(\"storm.worker.\");",
                "+        sb.append(stormId);",
                "+        sb.append(\".\");",
                "+        sb.append(hostName);",
                "+        sb.append(\".\");",
                "+        sb.append(dotToUnderScore(componentId));",
                "+        sb.append(\".\");",
                "+        sb.append(dotToUnderScore(streamId));",
                "+        sb.append(\".\");",
                "+        sb.append(dotToUnderScore(executorId));",
                "+        sb.append(\".\");",
                "+        sb.append(workerPort);",
                "+        sb.append(\"-\");",
                "+        sb.append(name);",
                "+        return sb.toString();",
                "     }",
                "-    public static String metricName(String name, String stormId, String componentId, Integer workerPort){",
                "-        return String.format(\"storm.worker.%s.%s.%s.%s-%s\",",
                "-                stormId,",
                "-                hostName,",
                "-                dotToUnderScore(componentId),",
                "-                workerPort,",
                "-                name);",
                "+    public static String metricName(String name, String stormId, String componentId, Integer workerPort) {",
                "+        StringBuilder sb = new StringBuilder(\"storm.worker.\");",
                "+        sb.append(stormId);",
                "+        sb.append(\".\");",
                "+        sb.append(hostName);",
                "+        sb.append(\".\");",
                "+        sb.append(dotToUnderScore(componentId));",
                "+        sb.append(\".\");",
                "+        sb.append(workerPort);",
                "+        sb.append(\"-\");",
                "+        sb.append(name);",
                "+        return sb.toString();",
                "     }",
                "@@ -148,8 +159,13 @@ public class StormMetricRegistry {",
                "     public static String metricName(String name, TopologyContext context){",
                "-        return String.format(\"storm.topology.%s.%s.%s.%s.%s-%s\",",
                "-                context.getStormId(),",
                "-                hostName,",
                "-                dotToUnderScore(context.getThisComponentId()),",
                "-                context.getThisWorkerPort(),",
                "-                name);",
                "+        StringBuilder sb = new StringBuilder(\"storm.topology.\");",
                "+        sb.append(context.getStormId());",
                "+        sb.append(\".\");",
                "+        sb.append(hostName);",
                "+        sb.append(\".\");",
                "+        sb.append(dotToUnderScore(context.getThisComponentId()));",
                "+        sb.append(\".\");",
                "+        sb.append(context.getThisWorkerPort());",
                "+        sb.append(\"-\");",
                "+        sb.append(name);",
                "+        return sb.toString();",
                "     }"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "4128c7318044e007828d22cf277953ffd144a1ec",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509384151,
            "hunks": 20,
            "message": "Fix checkstyle issues",
            "diff": [
                "diff --git a/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java b/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java",
                "index a39dd6d94..dcc48540c 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java",
                "@@ -44,3 +44,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "-    protected Set<String> _userCommands = new HashSet<>(Arrays.asList(",
                "+    protected Set<String> userCommands = new HashSet<>(Arrays.asList(",
                "             \"submitTopology\",",
                "@@ -51,4 +51,4 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "             \"getOwnerResourceSummaries\"));",
                "-    protected Set<String> _supervisorCommands = new HashSet<>(Arrays.asList(\"fileDownload\"));",
                "-    protected Set<String> _topoReadOnlyCommands = new HashSet<>(Arrays.asList(",
                "+    protected Set<String> supervisorCommands = new HashSet<>(Arrays.asList(\"fileDownload\"));",
                "+    protected Set<String> topoReadOnlyCommands = new HashSet<>(Arrays.asList(",
                "             \"getTopologyConf\",",
                "@@ -62,3 +62,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "             \"getLogConfig\"));",
                "-    protected Set<String> _topoCommands = new HashSet<>(Arrays.asList(",
                "+    protected Set<String> topoCommands = new HashSet<>(Arrays.asList(",
                "             \"killTopology\",",
                "@@ -78,12 +78,12 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "     {",
                "-        _topoCommands.addAll(_topoReadOnlyCommands);",
                "+        topoCommands.addAll(topoReadOnlyCommands);",
                "     }",
                "-    protected Set<String> _admins;",
                "-    protected Set<String> _adminsGroups;",
                "-    protected Set<String> _supervisors;",
                "-    protected Set<String> _nimbusUsers;",
                "-    protected Set<String> _nimbusGroups;",
                "-    protected IPrincipalToLocal _ptol;",
                "-    protected IGroupMappingServiceProvider _groupMappingProvider;",
                "+    protected Set<String> admins;",
                "+    protected Set<String> adminsGroups;",
                "+    protected Set<String> supervisors;",
                "+    protected Set<String> nimbusUsers;",
                "+    protected Set<String> nimbusGroups;",
                "+    protected IPrincipalToLocal ptol;",
                "+    protected IGroupMappingServiceProvider groupMappingServiceProvider;",
                "     /**",
                "@@ -94,10 +94,10 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "     public void prepare(Map<String, Object> conf) {",
                "-        _admins = new HashSet<>();",
                "-        _adminsGroups = new HashSet<>();",
                "-        _supervisors = new HashSet<>();",
                "-        _nimbusUsers = new HashSet<>();",
                "-        _nimbusGroups = new HashSet<>();",
                "+        admins = new HashSet<>();",
                "+        adminsGroups = new HashSet<>();",
                "+        supervisors = new HashSet<>();",
                "+        nimbusUsers = new HashSet<>();",
                "+        nimbusGroups = new HashSet<>();",
                "         if (conf.containsKey(Config.NIMBUS_ADMINS)) {",
                "-            _admins.addAll((Collection<String>)conf.get(Config.NIMBUS_ADMINS));",
                "+            admins.addAll((Collection<String>)conf.get(Config.NIMBUS_ADMINS));",
                "         }",
                "@@ -105,3 +105,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "         if (conf.containsKey(Config.NIMBUS_ADMINS_GROUPS)) {",
                "-            _adminsGroups.addAll((Collection<String>)conf.get(Config.NIMBUS_ADMINS_GROUPS));",
                "+            adminsGroups.addAll((Collection<String>)conf.get(Config.NIMBUS_ADMINS_GROUPS));",
                "         }",
                "@@ -109,3 +109,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "         if (conf.containsKey(Config.NIMBUS_SUPERVISOR_USERS)) {",
                "-            _supervisors.addAll((Collection<String>)conf.get(Config.NIMBUS_SUPERVISOR_USERS));",
                "+            supervisors.addAll((Collection<String>)conf.get(Config.NIMBUS_SUPERVISOR_USERS));",
                "         }",
                "@@ -113,3 +113,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "         if (conf.containsKey(Config.NIMBUS_USERS)) {",
                "-            _nimbusUsers.addAll((Collection<String>)conf.get(Config.NIMBUS_USERS));",
                "+            nimbusUsers.addAll((Collection<String>)conf.get(Config.NIMBUS_USERS));",
                "         }",
                "@@ -117,7 +117,7 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "         if (conf.containsKey(Config.NIMBUS_GROUPS)) {",
                "-            _nimbusGroups.addAll((Collection<String>)conf.get(Config.NIMBUS_GROUPS));",
                "+            nimbusGroups.addAll((Collection<String>)conf.get(Config.NIMBUS_GROUPS));",
                "         }",
                "-        _ptol = AuthUtils.GetPrincipalToLocalPlugin(conf);",
                "-        _groupMappingProvider = AuthUtils.GetGroupMappingServiceProviderPlugin(conf);",
                "+        ptol = AuthUtils.GetPrincipalToLocalPlugin(conf);",
                "+        groupMappingServiceProvider = AuthUtils.GetGroupMappingServiceProviderPlugin(conf);",
                "     }",
                "@@ -134,8 +134,8 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "         String principal = context.principal().getName();",
                "-        String user = _ptol.toLocal(context.principal());",
                "+        String user = ptol.toLocal(context.principal());",
                "         Set<String> userGroups = new HashSet<>();",
                "-        if (_groupMappingProvider != null) {",
                "+        if (groupMappingServiceProvider != null) {",
                "             try {",
                "-                userGroups = _groupMappingProvider.getGroups(user);",
                "+                userGroups = groupMappingServiceProvider.getGroups(user);",
                "             } catch(IOException e) {",
                "@@ -145,3 +145,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "-        if (_admins.contains(principal) || _admins.contains(user) || checkUserGroupAllowed(userGroups, _adminsGroups)) {",
                "+        if (admins.contains(principal) || admins.contains(user) || checkUserGroupAllowed(userGroups, adminsGroups)) {",
                "             return true;",
                "@@ -149,11 +149,11 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "-        if (_supervisors.contains(principal) || _supervisors.contains(user)) {",
                "-            return _supervisorCommands.contains(operation);",
                "+        if (supervisors.contains(principal) || supervisors.contains(user)) {",
                "+            return supervisorCommands.contains(operation);",
                "         }",
                "-        if (_userCommands.contains(operation)) {",
                "-            return _nimbusUsers.size() == 0 || _nimbusUsers.contains(user) || checkUserGroupAllowed(userGroups, _nimbusGroups);",
                "+        if (userCommands.contains(operation)) {",
                "+            return nimbusUsers.size() == 0 || nimbusUsers.contains(user) || checkUserGroupAllowed(userGroups, nimbusGroups);",
                "         }",
                "-        if (_topoCommands.contains(operation)) {",
                "+        if (topoCommands.contains(operation)) {",
                "             if (checkTopoPermission(principal, user, userGroups, topoConf, Config.TOPOLOGY_USERS, Config.TOPOLOGY_GROUPS)) {",
                "@@ -162,3 +162,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "-            if (_topoReadOnlyCommands.contains(operation) && checkTopoPermission(principal, user, userGroups,",
                "+            if (topoReadOnlyCommands.contains(operation) && checkTopoPermission(principal, user, userGroups,",
                "                     topoConf, Config.TOPOLOGY_READONLY_USERS, Config.TOPOLOGY_READONLY_GROUPS)) {"
            ],
            "changed_files": [
                "storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "171a14464075705b04494ea594f14e41d92bee4c"
                ]
            ],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "2bd8a1a1dc1aff169ab61ce98f1893d4911d2795",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517591992,
            "hunks": 4,
            "message": "[STORM-2932] the naming of topology localityaware configs are confusing",
            "diff": [
                "diff --git a/conf/defaults.yaml b/conf/defaults.yaml",
                "index e15a26545..243f210b2 100644",
                "--- a/conf/defaults.yaml",
                "+++ b/conf/defaults.yaml",
                "@@ -260,4 +260,4 @@ topology.disable.loadaware.messaging: false",
                " topology.state.checkpoint.interval.ms: 1000",
                "-topology.localityaware.higher.bound.percent: 0.8",
                "-topology.localityaware.lower.bound.percent: 0.2",
                "+topology.localityaware.higher.bound: 0.8",
                "+topology.localityaware.lower.bound: 0.2",
                " topology.serialized.message.size.metrics: false",
                "diff --git a/storm-client/src/jvm/org/apache/storm/Config.java b/storm-client/src/jvm/org/apache/storm/Config.java",
                "index 7014dfd53..000c9b50e 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/Config.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/Config.java",
                "@@ -75,3 +75,3 @@ public class Config extends HashMap<String, Object> {",
                "     @NotNull",
                "-    public static final String TOPOLOGY_LOCALITYAWARE_HIGHER_BOUND_PERCENT = \"topology.localityaware.higher.bound.percent\";",
                "+    public static final String TOPOLOGY_LOCALITYAWARE_HIGHER_BOUND = \"topology.localityaware.higher.bound\";",
                "@@ -84,3 +84,3 @@ public class Config extends HashMap<String, Object> {",
                "     @NotNull",
                "-    public static final String TOPOLOGY_LOCALITYAWARE_LOWER_BOUND_PERCENT = \"topology.localityaware.lower.bound.percent\";",
                "+    public static final String TOPOLOGY_LOCALITYAWARE_LOWER_BOUND = \"topology.localityaware.lower.bound\";",
                "diff --git a/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java b/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java",
                "index 3fd75e53b..2b0b481ff 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java",
                "@@ -91,4 +91,4 @@ public class LoadAwareShuffleGrouping implements LoadAwareCustomStreamGrouping,",
                "         currentScope = Scope.WORKER_LOCAL;",
                "-        higherBound = ObjectReader.getDouble(conf.get(Config.TOPOLOGY_LOCALITYAWARE_HIGHER_BOUND_PERCENT));",
                "-        lowerBound = ObjectReader.getDouble(conf.get(Config.TOPOLOGY_LOCALITYAWARE_LOWER_BOUND_PERCENT));",
                "+        higherBound = ObjectReader.getDouble(conf.get(Config.TOPOLOGY_LOCALITYAWARE_HIGHER_BOUND));",
                "+        lowerBound = ObjectReader.getDouble(conf.get(Config.TOPOLOGY_LOCALITYAWARE_LOWER_BOUND));"
            ],
            "changed_files": [
                "conf/defaults.yaml",
                "storm-client/src/jvm/org/apache/storm/Config.java",
                "storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2932": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2932",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "c155a7f01d700141c3464dc657df3def4cbfb029",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514321771,
            "hunks": 0,
            "message": "Merge branch 'STORM-2868-1.x-merge' into 1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2868": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2868",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b09e98698e298ad0edaf1f4916c663875d387c2e",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508776306,
            "hunks": 20,
            "message": "STORM-2786: Turn ticks back on for ackers (and optimizes ackers a bit)",
            "diff": [
                "diff --git a/storm-client/src/jvm/org/apache/storm/daemon/Acker.java b/storm-client/src/jvm/org/apache/storm/daemon/Acker.java",
                "index c41baeeeb..8675e391e 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/daemon/Acker.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/daemon/Acker.java",
                "@@ -24,2 +24,3 @@ import org.apache.storm.tuple.Tuple;",
                " import org.apache.storm.tuple.Values;",
                "+import org.apache.storm.utils.Time;",
                " import org.apache.storm.utils.Utils;",
                "@@ -48,7 +49,7 @@ public class Acker implements IBolt {",
                "-    private class AckObject {",
                "+    private static class AckObject {",
                "         public long val = 0L;",
                "-        public Integer spoutTask = null;",
                "+        public long startTime = Time.currentTimeMillis();",
                "+        public int spoutTask = -1;",
                "         public boolean failed = false;",
                "-        public long startTime = System.currentTimeMillis();",
                "@@ -63,3 +64,3 @@ public class Acker implements IBolt {",
                "         this.collector = collector;",
                "-        this.pending = new RotatingMap<Object, AckObject>(TIMEOUT_BUCKET_NUM);",
                "+        this.pending = new RotatingMap<>(TIMEOUT_BUCKET_NUM);",
                "     }",
                "@@ -74,2 +75,3 @@ public class Acker implements IBolt {",
                "+        boolean resetTimeout = false;",
                "         String streamId = input.getSourceStreamId();",
                "@@ -80,18 +82,12 @@ public class Acker implements IBolt {",
                "                 curr = new AckObject();",
                "-                curr.val = input.getLong(1);",
                "-                curr.spoutTask = input.getInteger(2);",
                "                 pending.put(id, curr);",
                "-            } else {",
                "-                // If receiving bolt's ack before the init message from spout, just update the xor value.",
                "-                curr.updateAck(input.getLong(1));",
                "-                curr.spoutTask = input.getInteger(2);",
                "             }",
                "+            curr.updateAck(input.getLong(1));",
                "+            curr.spoutTask = input.getInteger(2);",
                "         } else if (ACKER_ACK_STREAM_ID.equals(streamId)) {",
                "-            if (curr != null) {",
                "-                curr.updateAck(input.getLong(1));",
                "-            } else {",
                "+            if (curr == null) {",
                "                 curr = new AckObject();",
                "-                curr.val = input.getLong(1);",
                "                 pending.put(id, curr);",
                "             }",
                "+            curr.updateAck(input.getLong(1));",
                "         } else if (ACKER_FAIL_STREAM_ID.equals(streamId)) {",
                "@@ -104,6 +100,6 @@ public class Acker implements IBolt {",
                "         } else if (ACKER_RESET_TIMEOUT_STREAM_ID.equals(streamId)) {",
                "-            if (curr == null) {",
                "-                curr = new AckObject();",
                "-            }",
                "-            pending.put(id, curr);",
                "+            resetTimeout = true;",
                "+            if (curr != null) {",
                "+                pending.put(id, curr);",
                "+            } //else if it has not been added yet, there is no reason time it out later on",
                "         } else {",
                "@@ -113,4 +109,5 @@ public class Acker implements IBolt {",
                "-        Integer task = curr.spoutTask;",
                "-        if (curr != null && task != null) {",
                "+        int task = curr.spoutTask;",
                "+        if (curr != null && task >= 0",
                "+            && (curr.val == 0 || curr.failed || resetTimeout)) {",
                "             Values tuple = new Values(id, getTimeDeltaMillis(curr.startTime));",
                "@@ -122,4 +119,6 @@ public class Acker implements IBolt {",
                "                 collector.emitDirect(task, ACKER_FAIL_STREAM_ID, tuple);",
                "-            } else if(ACKER_RESET_TIMEOUT_STREAM_ID.equals(streamId)) {",
                "+            } else if(resetTimeout) {",
                "                 collector.emitDirect(task, ACKER_RESET_TIMEOUT_STREAM_ID, tuple);",
                "+            } else {",
                "+                throw new IllegalStateException(\"The checks are inconsistent we reach what should be unreachable code.\");",
                "             }",
                "@@ -136,3 +135,3 @@ public class Acker implements IBolt {",
                "     private long getTimeDeltaMillis(long startTimeMillis) {",
                "-        return System.currentTimeMillis() - startTimeMillis;",
                "+        return Time.currentTimeMillis() - startTimeMillis;",
                "     }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/executor/Executor.java b/storm-client/src/jvm/org/apache/storm/executor/Executor.java",
                "index e55aca0b2..3c39194da 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/executor/Executor.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/executor/Executor.java",
                "@@ -42,2 +42,3 @@ import org.apache.storm.cluster.DaemonType;",
                " import org.apache.storm.cluster.IStormClusterState;",
                "+import org.apache.storm.daemon.Acker;",
                " import org.apache.storm.daemon.GrouperFactory;",
                "@@ -383,16 +384,12 @@ public abstract class Executor implements Callable, EventHandler<Object> {",
                "         if (tickTimeSecs != null) {",
                "-            if (Utils.isSystemId(componentId) || (!enableMessageTimeout && isSpout)) {",
                "-                LOG.info(\"Timeouts disabled for executor \" + componentId + \":\" + executorId);",
                "+            if ((!Acker.ACKER_COMPONENT_ID.equals(componentId) && Utils.isSystemId(componentId))",
                "+                || (!enableMessageTimeout && isSpout)) {",
                "+                LOG.info(\"Timeouts disabled for executor {}:{}\", componentId, executorId);",
                "             } else {",
                "                 StormTimer timerTask = workerData.getUserTimer();",
                "-                timerTask.scheduleRecurring(tickTimeSecs, tickTimeSecs, new Runnable() {",
                "-                    @Override",
                "-                    public void run() {",
                "-                        TupleImpl tuple = new TupleImpl(workerTopologyContext, new Values(tickTimeSecs),",
                "-                                (int) Constants.SYSTEM_TASK_ID, Constants.SYSTEM_TICK_STREAM_ID);",
                "-                        List<AddressedTuple> tickTuple =",
                "-                                Lists.newArrayList(new AddressedTuple(AddressedTuple.BROADCAST_DEST, tuple));",
                "-                        receiveQueue.publish(tickTuple);",
                "-                    }",
                "-                });",
                "+                TupleImpl tuple = new TupleImpl(workerTopologyContext, new Values(tickTimeSecs),",
                "+                    (int) Constants.SYSTEM_TASK_ID, Constants.SYSTEM_TICK_STREAM_ID);",
                "+                final List<AddressedTuple> tickTuple =",
                "+                    Lists.newArrayList(new AddressedTuple(AddressedTuple.BROADCAST_DEST, tuple));",
                "+                timerTask.scheduleRecurring(tickTimeSecs, tickTimeSecs, () -> receiveQueue.publish(tickTuple));",
                "             }"
            ],
            "changed_files": [
                "storm-client/src/jvm/org/apache/storm/daemon/Acker.java",
                "storm-client/src/jvm/org/apache/storm/executor/Executor.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2786": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2786",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f04b7a3a74580bde2681504313aa158b5956e7de",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516679960,
            "hunks": 60,
            "message": "[maven-release-plugin] prepare release v1.2.0",
            "diff": [
                "diff --git a/examples/storm-elasticsearch-examples/pom.xml b/examples/storm-elasticsearch-examples/pom.xml",
                "index 0f4a51565..598c71f46 100644",
                "--- a/examples/storm-elasticsearch-examples/pom.xml",
                "+++ b/examples/storm-elasticsearch-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hbase-examples/pom.xml b/examples/storm-hbase-examples/pom.xml",
                "index 0bdbabef8..eb5523163 100644",
                "--- a/examples/storm-hbase-examples/pom.xml",
                "+++ b/examples/storm-hbase-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hdfs-examples/pom.xml b/examples/storm-hdfs-examples/pom.xml",
                "index da4c25743..ffe196c1b 100644",
                "--- a/examples/storm-hdfs-examples/pom.xml",
                "+++ b/examples/storm-hdfs-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hive-examples/pom.xml b/examples/storm-hive-examples/pom.xml",
                "index 4ef51dc90..ea240a5d7 100644",
                "--- a/examples/storm-hive-examples/pom.xml",
                "+++ b/examples/storm-hive-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jdbc-examples/pom.xml b/examples/storm-jdbc-examples/pom.xml",
                "index d790020cb..78c2f0958 100644",
                "--- a/examples/storm-jdbc-examples/pom.xml",
                "+++ b/examples/storm-jdbc-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jms-examples/pom.xml b/examples/storm-jms-examples/pom.xml",
                "index 0d8926c23..fdd66e7d5 100644",
                "--- a/examples/storm-jms-examples/pom.xml",
                "+++ b/examples/storm-jms-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-client-examples/pom.xml b/examples/storm-kafka-client-examples/pom.xml",
                "index 9be33cad0..62869b106 100644",
                "--- a/examples/storm-kafka-client-examples/pom.xml",
                "+++ b/examples/storm-kafka-client-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-examples/pom.xml b/examples/storm-kafka-examples/pom.xml",
                "index 21d72f29d..95c3b4aad 100644",
                "--- a/examples/storm-kafka-examples/pom.xml",
                "+++ b/examples/storm-kafka-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mongodb-examples/pom.xml b/examples/storm-mongodb-examples/pom.xml",
                "index 49b547987..ccab1263e 100644",
                "--- a/examples/storm-mongodb-examples/pom.xml",
                "+++ b/examples/storm-mongodb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mqtt-examples/pom.xml b/examples/storm-mqtt-examples/pom.xml",
                "index a96179912..5f074147d 100644",
                "--- a/examples/storm-mqtt-examples/pom.xml",
                "+++ b/examples/storm-mqtt-examples/pom.xml",
                "@@ -28,3 +28,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.2.0-SNAPSHOT</version>",
                "+    <version>1.2.0</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-opentsdb-examples/pom.xml b/examples/storm-opentsdb-examples/pom.xml",
                "index 117998f04..3a9ecdd3f 100644",
                "--- a/examples/storm-opentsdb-examples/pom.xml",
                "+++ b/examples/storm-opentsdb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-perf/pom.xml b/examples/storm-perf/pom.xml",
                "index afedb1f9d..ff80366f8 100644",
                "--- a/examples/storm-perf/pom.xml",
                "+++ b/examples/storm-perf/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-pmml-examples/pom.xml b/examples/storm-pmml-examples/pom.xml",
                "index 830775247..9333468ba 100644",
                "--- a/examples/storm-pmml-examples/pom.xml",
                "+++ b/examples/storm-pmml-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-redis-examples/pom.xml b/examples/storm-redis-examples/pom.xml",
                "index fa0a6d1d7..34bf7dc64 100644",
                "--- a/examples/storm-redis-examples/pom.xml",
                "+++ b/examples/storm-redis-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-solr-examples/pom.xml b/examples/storm-solr-examples/pom.xml",
                "index 4f44fffe1..f87464b5a 100644",
                "--- a/examples/storm-solr-examples/pom.xml",
                "+++ b/examples/storm-solr-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-starter/pom.xml b/examples/storm-starter/pom.xml",
                "index fa1ec05ef..2b044fd3d 100644",
                "--- a/examples/storm-starter/pom.xml",
                "+++ b/examples/storm-starter/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.2.0-SNAPSHOT</version>",
                "+      <version>1.2.0</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-core/pom.xml b/external/flux/flux-core/pom.xml",
                "index 3f7c9d2d4..870c3de98 100644",
                "--- a/external/flux/flux-core/pom.xml",
                "+++ b/external/flux/flux-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-wrappers/pom.xml b/external/flux/flux-wrappers/pom.xml",
                "index 847d84a16..3dc8397ec 100644",
                "--- a/external/flux/flux-wrappers/pom.xml",
                "+++ b/external/flux/flux-wrappers/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/pom.xml b/external/flux/pom.xml",
                "index f44cce043..125a38158 100644",
                "--- a/external/flux/pom.xml",
                "+++ b/external/flux/pom.xml",
                "@@ -28,3 +28,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/pom.xml b/external/sql/pom.xml",
                "index 9dea3fc2e..c6afee63c 100644",
                "--- a/external/sql/pom.xml",
                "+++ b/external/sql/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-core/pom.xml b/external/sql/storm-sql-core/pom.xml",
                "index a30e7de5c..75b83933f 100644",
                "--- a/external/sql/storm-sql-core/pom.xml",
                "+++ b/external/sql/storm-sql-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "index 46180b475..518ce77a1 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "index 39887c23b..ffe4740a6 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "index 1083a9472..146959f6b 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-redis/pom.xml b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "index 3535fb16e..e4878acfb 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-runtime/pom.xml b/external/sql/storm-sql-runtime/pom.xml",
                "index c83682fd3..32acec5f0 100644",
                "--- a/external/sql/storm-sql-runtime/pom.xml",
                "+++ b/external/sql/storm-sql-runtime/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/storm-autocreds/pom.xml b/external/storm-autocreds/pom.xml",
                "index 1dbdc0add..30c10e7d9 100644",
                "--- a/external/storm-autocreds/pom.xml",
                "+++ b/external/storm-autocreds/pom.xml",
                "@@ -17,5 +17,3 @@",
                " -->",
                "-<project xmlns=\"http://maven.apache.org/POM/4.0.0\"",
                "-         xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"",
                "-         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">",
                "+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">",
                "     <parent>",
                "@@ -23,3 +21,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-cassandra/pom.xml b/external/storm-cassandra/pom.xml",
                "index fbbf70842..5926fd410 100644",
                "--- a/external/storm-cassandra/pom.xml",
                "+++ b/external/storm-cassandra/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-druid/pom.xml b/external/storm-druid/pom.xml",
                "index 0e7084002..bf8b1c35f 100644",
                "--- a/external/storm-druid/pom.xml",
                "+++ b/external/storm-druid/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-elasticsearch/pom.xml b/external/storm-elasticsearch/pom.xml",
                "index 4900a1e4b..e82b43dc9 100644",
                "--- a/external/storm-elasticsearch/pom.xml",
                "+++ b/external/storm-elasticsearch/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-eventhubs/pom.xml b/external/storm-eventhubs/pom.xml",
                "index e13f3059c..da1324c8e 100755",
                "--- a/external/storm-eventhubs/pom.xml",
                "+++ b/external/storm-eventhubs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hbase/pom.xml b/external/storm-hbase/pom.xml",
                "index dd8cd0b21..363ab59ae 100644",
                "--- a/external/storm-hbase/pom.xml",
                "+++ b/external/storm-hbase/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hdfs/pom.xml b/external/storm-hdfs/pom.xml",
                "index 53572d565..f773a4db1 100644",
                "--- a/external/storm-hdfs/pom.xml",
                "+++ b/external/storm-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hive/pom.xml b/external/storm-hive/pom.xml",
                "index b7dbd942f..f920a0933 100644",
                "--- a/external/storm-hive/pom.xml",
                "+++ b/external/storm-hive/pom.xml",
                "@@ -23,3 +23,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.2.0-SNAPSHOT</version>",
                "+    <version>1.2.0</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jdbc/pom.xml b/external/storm-jdbc/pom.xml",
                "index cbf2bb0db..1b95f5e19 100644",
                "--- a/external/storm-jdbc/pom.xml",
                "+++ b/external/storm-jdbc/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jms/pom.xml b/external/storm-jms/pom.xml",
                "index 774b11880..115e284b8 100644",
                "--- a/external/storm-jms/pom.xml",
                "+++ b/external/storm-jms/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index 51bb79771..be2d0f75c 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -24,3 +24,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-monitor/pom.xml b/external/storm-kafka-monitor/pom.xml",
                "index 76e7c8715..43da43b40 100644",
                "--- a/external/storm-kafka-monitor/pom.xml",
                "+++ b/external/storm-kafka-monitor/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka/pom.xml b/external/storm-kafka/pom.xml",
                "index 3fa532ce7..a4a149d0b 100644",
                "--- a/external/storm-kafka/pom.xml",
                "+++ b/external/storm-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kinesis/pom.xml b/external/storm-kinesis/pom.xml",
                "index 88c67d234..d6fe8d138 100644",
                "--- a/external/storm-kinesis/pom.xml",
                "+++ b/external/storm-kinesis/pom.xml",
                "@@ -19,3 +19,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-metrics/pom.xml b/external/storm-metrics/pom.xml",
                "index 06bd71024..af732e979 100644",
                "--- a/external/storm-metrics/pom.xml",
                "+++ b/external/storm-metrics/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.2.0-SNAPSHOT</version>",
                "+      <version>1.2.0</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mongodb/pom.xml b/external/storm-mongodb/pom.xml",
                "index 822b94856..eb8bd5e3f 100644",
                "--- a/external/storm-mongodb/pom.xml",
                "+++ b/external/storm-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mqtt/pom.xml b/external/storm-mqtt/pom.xml",
                "index 5754b0b7a..b1b8ddfba 100644",
                "--- a/external/storm-mqtt/pom.xml",
                "+++ b/external/storm-mqtt/pom.xml",
                "@@ -27,3 +27,3 @@",
                "         <artifactId>storm</artifactId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-opentsdb/pom.xml b/external/storm-opentsdb/pom.xml",
                "index 45e133ff3..270524380 100644",
                "--- a/external/storm-opentsdb/pom.xml",
                "+++ b/external/storm-opentsdb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-pmml/pom.xml b/external/storm-pmml/pom.xml",
                "index 24af7cfdc..e8853ba4d 100644",
                "--- a/external/storm-pmml/pom.xml",
                "+++ b/external/storm-pmml/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-redis/pom.xml b/external/storm-redis/pom.xml",
                "index 3facb1587..09334a100 100644",
                "--- a/external/storm-redis/pom.xml",
                "+++ b/external/storm-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-solr/pom.xml b/external/storm-solr/pom.xml",
                "index 6d6cf249a..5a3bef0fe 100644",
                "--- a/external/storm-solr/pom.xml",
                "+++ b/external/storm-solr/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-submit-tools/pom.xml b/external/storm-submit-tools/pom.xml",
                "index 48bc32fb3..3cd66576d 100644",
                "--- a/external/storm-submit-tools/pom.xml",
                "+++ b/external/storm-submit-tools/pom.xml",
                "@@ -21,3 +21,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/pom.xml b/pom.xml",
                "index c1511a428..154d92087 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -29,3 +29,3 @@",
                "     <artifactId>storm</artifactId>",
                "-    <version>1.2.0-SNAPSHOT</version>",
                "+    <version>1.2.0</version>",
                "     <packaging>pom</packaging>",
                "@@ -204,3 +204,3 @@",
                "         <developerConnection>scm:git:https://git-wip-us.apache.org/repos/asf/storm.git</developerConnection>",
                "-        <tag>HEAD</tag>",
                "+        <tag>v1.2.0</tag>",
                "         <url>https://git-wip-us.apache.org/repos/asf/storm</url>",
                "diff --git a/storm-buildtools/maven-shade-clojure-transformer/pom.xml b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "index acedaf738..94cf5f20c 100644",
                "--- a/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "+++ b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-buildtools/storm-maven-plugins/pom.xml b/storm-buildtools/storm-maven-plugins/pom.xml",
                "index 503a37305..f9a8ad156 100644",
                "--- a/storm-buildtools/storm-maven-plugins/pom.xml",
                "+++ b/storm-buildtools/storm-maven-plugins/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index 00326f3c8..0a0ce3e74 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>..</relativePath>",
                "diff --git a/storm-dist/binary/pom.xml b/storm-dist/binary/pom.xml",
                "index 3945f7868..4ca1a2e88 100644",
                "--- a/storm-dist/binary/pom.xml",
                "+++ b/storm-dist/binary/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-dist/source/pom.xml b/storm-dist/source/pom.xml",
                "index fbf7b2883..6933bf868 100644",
                "--- a/storm-dist/source/pom.xml",
                "+++ b/storm-dist/source/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/javascript/pom.xml b/storm-multilang/javascript/pom.xml",
                "index d96d9fb88..24e553561 100644",
                "--- a/storm-multilang/javascript/pom.xml",
                "+++ b/storm-multilang/javascript/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/python/pom.xml b/storm-multilang/python/pom.xml",
                "index 8d56edf09..660ec54ad 100644",
                "--- a/storm-multilang/python/pom.xml",
                "+++ b/storm-multilang/python/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/ruby/pom.xml b/storm-multilang/ruby/pom.xml",
                "index 841555772..a55d0ca75 100644",
                "--- a/storm-multilang/ruby/pom.xml",
                "+++ b/storm-multilang/ruby/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.2.0-SNAPSHOT</version>",
                "+        <version>1.2.0</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-rename-hack/pom.xml b/storm-rename-hack/pom.xml",
                "index dbbc8ef5a..a33192708 100644",
                "--- a/storm-rename-hack/pom.xml",
                "+++ b/storm-rename-hack/pom.xml",
                "@@ -22,3 +22,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.2.0-SNAPSHOT</version>",
                "+    <version>1.2.0</version>",
                "     <relativePath>../pom.xml</relativePath>"
            ],
            "changed_files": [
                "examples/storm-elasticsearch-examples/pom.xml",
                "examples/storm-hbase-examples/pom.xml",
                "examples/storm-hdfs-examples/pom.xml",
                "examples/storm-hive-examples/pom.xml",
                "examples/storm-jdbc-examples/pom.xml",
                "examples/storm-jms-examples/pom.xml",
                "examples/storm-kafka-client-examples/pom.xml",
                "examples/storm-kafka-examples/pom.xml",
                "examples/storm-mongodb-examples/pom.xml",
                "examples/storm-mqtt-examples/pom.xml",
                "examples/storm-opentsdb-examples/pom.xml",
                "examples/storm-perf/pom.xml",
                "examples/storm-pmml-examples/pom.xml",
                "examples/storm-redis-examples/pom.xml",
                "examples/storm-solr-examples/pom.xml",
                "examples/storm-starter/pom.xml",
                "external/flux/flux-core/pom.xml",
                "external/flux/flux-wrappers/pom.xml",
                "external/flux/pom.xml",
                "external/sql/pom.xml",
                "external/sql/storm-sql-core/pom.xml",
                "external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "external/sql/storm-sql-runtime/pom.xml",
                "external/storm-autocreds/pom.xml",
                "external/storm-cassandra/pom.xml",
                "external/storm-druid/pom.xml",
                "external/storm-elasticsearch/pom.xml",
                "external/storm-eventhubs/pom.xml",
                "external/storm-hbase/pom.xml",
                "external/storm-hdfs/pom.xml",
                "external/storm-hive/pom.xml",
                "external/storm-jdbc/pom.xml",
                "external/storm-jms/pom.xml",
                "external/storm-kafka-client/pom.xml",
                "external/storm-kafka-monitor/pom.xml",
                "external/storm-kafka/pom.xml",
                "external/storm-kinesis/pom.xml",
                "external/storm-metrics/pom.xml",
                "external/storm-mongodb/pom.xml",
                "external/storm-mqtt/pom.xml",
                "external/storm-opentsdb/pom.xml",
                "external/storm-pmml/pom.xml",
                "external/storm-redis/pom.xml",
                "external/storm-solr/pom.xml",
                "external/storm-submit-tools/pom.xml",
                "pom.xml",
                "storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "storm-buildtools/storm-maven-plugins/pom.xml",
                "storm-core/pom.xml",
                "storm-dist/binary/pom.xml",
                "storm-dist/source/pom.xml",
                "storm-multilang/javascript/pom.xml",
                "storm-multilang/python/pom.xml",
                "storm-multilang/ruby/pom.xml",
                "storm-rename-hack/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "458aa1cb696097cf07d4466aa7417c7b89662221"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "af1fd216a623e142f18248cd4d40ca1fbc19a9c4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509157949,
            "hunks": 6,
            "message": "Deprecated FixedTupleSpout constructor with String. Added constructor with Fields so more than one output field can be specified.",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/testing/FixedTupleSpout.java b/storm-core/src/jvm/org/apache/storm/testing/FixedTupleSpout.java",
                "index 87fc52d1f..5234d32d4 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/testing/FixedTupleSpout.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/testing/FixedTupleSpout.java",
                "@@ -61,9 +61,17 @@ public class FixedTupleSpout implements IRichSpout {",
                "     private String _id;",
                "-    private String _fieldName;",
                "+    private Fields _fields;",
                "     public FixedTupleSpout(List tuples) {",
                "-        this(tuples, null);",
                "+        this(tuples, (Fields) null);",
                "     }",
                "+    /**",
                "+     * @deprecated please use {@link #FixedTupleSpout(List, Fields)}",
                "+     */",
                "+    @Deprecated",
                "     public FixedTupleSpout(List tuples, String fieldName) {",
                "+        this(tuples, new Fields(fieldName));",
                "+    }",
                "+",
                "+    public FixedTupleSpout(List tuples, Fields fields) {",
                "         _id = UUID.randomUUID().toString();",
                "@@ -85,3 +93,3 @@ public class FixedTupleSpout implements IRichSpout {",
                "         }",
                "-        _fieldName = fieldName;",
                "+        _fields = fields;",
                "     }",
                "@@ -169,4 +177,4 @@ public class FixedTupleSpout implements IRichSpout {",
                "     public void declareOutputFields(OutputFieldsDeclarer declarer) { ",
                "-        if (_fieldName != null) {",
                "-            declarer.declare(new Fields(_fieldName));",
                "+        if (_fields != null) {",
                "+            declarer.declare(_fields);",
                "         }"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/testing/FixedTupleSpout.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "2060664018a90cc341025800cc644474ed586cb1"
                ]
            ],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5e11867cfa39baa9058e4e81a400752954c678c3",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515012767,
            "hunks": 1,
            "message": "STORM-2881: Explicitly specify the curator dependencies in storm-druid pom",
            "diff": [
                "diff --git a/external/storm-druid/pom.xml b/external/storm-druid/pom.xml",
                "index b670a1d47..0e7084002 100644",
                "--- a/external/storm-druid/pom.xml",
                "+++ b/external/storm-druid/pom.xml",
                "@@ -83,2 +83,19 @@",
                "+        <!-- tranquility library depends on 2.6.0 version of curator -->",
                "+        <dependency>",
                "+            <groupId>org.apache.curator</groupId>",
                "+            <artifactId>curator-framework</artifactId>",
                "+            <version>2.6.0</version>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.apache.curator</groupId>",
                "+            <artifactId>curator-client</artifactId>",
                "+            <version>2.6.0</version>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.apache.curator</groupId>",
                "+            <artifactId>curator-recipes</artifactId>",
                "+            <version>2.6.0</version>",
                "+        </dependency>",
                "+",
                "         <!--test dependencies -->"
            ],
            "changed_files": [
                "external/storm-druid/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2881": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2881",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "00a382b017c1e29863ac4d9a4449086ef79384e4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512005907,
            "hunks": 22,
            "message": "STORM-2153 New Metrics Reporting API * address missing sampling rate * rename field names cause we use Counter instead of Meter",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index 720bfa76d..0aca4bd7b 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -259,3 +259,3 @@",
                "      :suicide-fn (:suicide-fn worker)",
                "-     :storm-cluster-state (cluster/mk-storm-cluster-state (:cluster-state worker) ",
                "+     :storm-cluster-state (cluster/mk-storm-cluster-state (:cluster-state worker)",
                "                                                           :acls (Utils/getWorkerACL storm-conf)",
                "@@ -282,4 +282,4 @@",
                "      :sampler (mk-stats-sampler storm-conf)",
                "-     :failed-meter (StormMetricRegistry/counter \"failed\" worker-context component-id)",
                "-     :acked-meter (StormMetricRegistry/counter \"acked\" worker-context component-id)",
                "+     :failed-counter (StormMetricRegistry/counter \"failed\" worker-context component-id)",
                "+     :acked-counter (StormMetricRegistry/counter \"acked\" worker-context component-id)",
                "      :spout-throttling-metrics (if (= executor-type :spout)",
                "@@ -439,4 +439,3 @@",
                "         storm-conf (:storm-conf executor-data)",
                "-        task-id (:task-id task-data)",
                "-        failed-meter (:failed-meter executor-data)]",
                "+        task-id (:task-id task-data)]",
                "     ;;TODO: need to throttle these when there's lots of failures",
                "@@ -447,4 +446,3 @@",
                "     (when time-delta",
                "-      (.inc ^Counter failed-meter)",
                "-      (stats/spout-failed-tuple! (:stats executor-data) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-failed-tuple! (:stats executor-data) (:failed-counter executor-data) (:stream tuple-info) time-delta))))",
                "@@ -452,4 +450,3 @@",
                "   (let [^ISpout spout (:object task-data)",
                "-        task-id (:task-id task-data)",
                "-        acked-meter (:acked-meter executor-data)]",
                "+        task-id (:task-id task-data)]",
                "     (when debug? (log-message \"SPOUT Acking message \" id \" \" msg-id))",
                "@@ -458,4 +455,3 @@",
                "     (when time-delta",
                "-      (.inc ^Counter acked-meter)",
                "-      (stats/spout-acked-tuple! (:stats executor-data) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-acked-tuple! (:stats executor-data) (:acked-counter executor-data) (:stream tuple-info) time-delta))))",
                "@@ -827,4 +823,4 @@",
                "                            (when (<= 0 delta)",
                "-                             (.inc ^Counter (:acked-meter (:executor-data task-data)))",
                "                              (stats/bolt-acked-tuple! executor-stats",
                "+                                                      (:acked-counter (:executor-data task-data))",
                "                                                       (.getSourceComponent tuple)",
                "@@ -843,4 +839,4 @@",
                "                            (when (<= 0 delta)",
                "-                             (.inc  ^Counter (:failed-meter (:executor-data task-data)))",
                "                              (stats/bolt-failed-tuple! executor-stats",
                "+                                                       (:failed-counter (:executor-data task-data))",
                "                                                        (.getSourceComponent tuple)",
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/task.clj b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "index c43d20d97..a2f6c5450 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "@@ -133,3 +133,3 @@",
                "         debug? (= true (storm-conf TOPOLOGY-DEBUG))",
                "-        ^Counter emitted-meter (StormMetricRegistry/counter \"emitted\" worker-context component-id)]",
                "+        ^Counter emitted-counter (StormMetricRegistry/counter \"emitted\" worker-context component-id)]",
                "@@ -146,4 +146,3 @@",
                "             (when (emit-sampler)",
                "-              (.inc ^Counter emitted-meter)",
                "-              (stats/emitted-tuple! executor-stats stream)",
                "+              (stats/emitted-tuple! executor-stats emitted-counter stream)",
                "               (if out-task-id",
                "@@ -167,4 +166,3 @@",
                "              (when (emit-sampler)",
                "-               (.inc ^Counter emitted-meter)",
                "-               (stats/emitted-tuple! executor-stats stream)",
                "+               (stats/emitted-tuple! executor-stats emitted-counter stream)",
                "                (stats/transferred-tuples! executor-stats stream (count out-tasks)))",
                "diff --git a/storm-core/src/clj/org/apache/storm/stats.clj b/storm-core/src/clj/org/apache/storm/stats.clj",
                "index 17d021942..41aaf04ad 100644",
                "--- a/storm-core/src/clj/org/apache/storm/stats.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/stats.clj",
                "@@ -28,3 +28,4 @@",
                "   (:import [org.apache.storm.scheduler WorkerSlot])",
                "-  (:import [org.apache.storm.metric.internal MultiCountStatAndMetric MultiLatencyStatAndMetric])",
                "+  (:import [org.apache.storm.metric.internal MultiCountStatAndMetric MultiLatencyStatAndMetric]",
                "+           (com.codahale.metrics Counter))",
                "   (:use [org.apache.storm log util])",
                "@@ -119,5 +120,7 @@",
                " (defn emitted-tuple!",
                "-  [stats stream]",
                "-  (let [^MultiCountStatAndMetric emitted (stats-emitted stats)]",
                "-    (.incBy emitted ^Object stream ^long (stats-rate stats))))",
                "+  [stats ^Counter emitted-counter stream]",
                "+  (let [^MultiCountStatAndMetric emitted (stats-emitted stats)",
                "+        ^long rate (stats-rate stats)]",
                "+    (.incBy emitted ^Object stream rate)",
                "+    (.inc emitted-counter rate)))",
                "@@ -137,7 +140,9 @@",
                " (defn bolt-acked-tuple!",
                "-  [^BoltExecutorStats stats component stream latency-ms]",
                "+  [^BoltExecutorStats stats ^Counter acked-counter component stream latency-ms]",
                "   (let [key [component stream]",
                "         ^MultiCountStatAndMetric acked (stats-acked stats)",
                "-        ^MultiLatencyStatAndMetric process-lat (stats-process-latencies stats)]",
                "-    (.incBy acked key (stats-rate stats))",
                "+        ^MultiLatencyStatAndMetric process-lat (stats-process-latencies stats)",
                "+        ^long rate (stats-rate stats)]",
                "+    (.incBy acked key rate)",
                "+    (.inc acked-counter rate)",
                "     (.record process-lat key latency-ms)))",
                "@@ -145,12 +150,16 @@",
                " (defn bolt-failed-tuple!",
                "-  [^BoltExecutorStats stats component stream latency-ms]",
                "+  [^BoltExecutorStats stats ^Counter failed-counter component stream latency-ms]",
                "   (let [key [component stream]",
                "-        ^MultiCountStatAndMetric failed (stats-failed stats)]",
                "-    (.incBy failed key (stats-rate stats))))",
                "+        ^MultiCountStatAndMetric failed (stats-failed stats)",
                "+        ^long rate (stats-rate stats)]",
                "+    (.incBy failed key rate)",
                "+    (.inc failed-counter rate)))",
                " (defn spout-acked-tuple!",
                "-  [^SpoutExecutorStats stats stream latency-ms]",
                "+  [^SpoutExecutorStats stats ^Counter acked-counter stream latency-ms]",
                "   (let [^MultiCountStatAndMetric acked (stats-acked stats)",
                "-        ^MultiLatencyStatAndMetric complete-latencies (stats-complete-latencies stats)]",
                "-    (.incBy acked stream (stats-rate stats))",
                "+        ^MultiLatencyStatAndMetric complete-latencies (stats-complete-latencies stats)",
                "+        ^long rate (stats-rate stats)]",
                "+    (.incBy acked stream rate)",
                "+    (.inc acked-counter rate)",
                "     (.record complete-latencies stream latency-ms)))",
                "@@ -158,5 +167,7 @@",
                " (defn spout-failed-tuple!",
                "-  [^SpoutExecutorStats stats stream latency-ms]",
                "-  (let [^MultiCountStatAndMetric failed (stats-failed stats)]",
                "-    (.incBy failed stream (stats-rate stats))))",
                "+  [^SpoutExecutorStats stats ^Counter failed-counter stream latency-ms]",
                "+  (let [^MultiCountStatAndMetric failed (stats-failed stats)",
                "+        ^long rate (stats-rate stats)]",
                "+    (.incBy failed stream rate)",
                "+    (.inc failed-counter rate)))"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "storm-core/src/clj/org/apache/storm/stats.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "507e7ed8208f1bc8fd8be4210ce3e9adacb82931",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516578279,
            "hunks": 0,
            "message": "Merge branch 'master0111' of https://github.com/liuxianjiao/storm into STORM-2897-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2897": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2897",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "3414a8cad8e811faa7bcebcd0026ed9ebb3dab06",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509085487,
            "hunks": 0,
            "message": "Merge branch 'STORM-2784-1.x-merge' into 1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2784": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2784",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e7fdd67a6ad7b2fb9b30610a4613f0a132311cdb",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513331723,
            "hunks": 0,
            "message": "Merge branch 'STORM-2845'",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2845": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2845",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5ce45b72715da4478fddabdfde12becec1373a93",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513278380,
            "hunks": 6,
            "message": "STORM-2153: Add warnings for component names containing '.' to DefaultTopologyValidator; Add StrictTopologyValidator",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/nimbus/DefaultTopologyValidator.java b/storm-core/src/jvm/org/apache/storm/nimbus/DefaultTopologyValidator.java",
                "index 0626cb6f6..fc0dfacfa 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/nimbus/DefaultTopologyValidator.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/nimbus/DefaultTopologyValidator.java",
                "@@ -19,4 +19,9 @@ package org.apache.storm.nimbus;",
                "+import org.apache.storm.generated.Bolt;",
                " import org.apache.storm.generated.InvalidTopologyException;",
                "+import org.apache.storm.generated.SpoutSpec;",
                " import org.apache.storm.generated.StormTopology;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                " import java.util.Map;",
                "@@ -24,2 +29,3 @@ import java.util.Map;",
                " public class DefaultTopologyValidator implements ITopologyValidator {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(DefaultTopologyValidator.class);",
                "     @Override",
                "@@ -28,3 +34,31 @@ public class DefaultTopologyValidator implements ITopologyValidator {",
                "     @Override",
                "-    public void validate(String topologyName, Map topologyConf, StormTopology topology) throws InvalidTopologyException {        ",
                "+    public void validate(String topologyName, Map topologyConf, StormTopology topology) throws InvalidTopologyException {",
                "+        if(topologyName.contains(\".\")){",
                "+            LOG.warn(\"Metrics for topology name '{}' will be reported as '{}'.\", topologyName, topologyName.replace('.', '_') );",
                "+        }",
                "+        Map<String, SpoutSpec> spouts = topology.get_spouts();",
                "+        for(String spoutName : spouts.keySet()){",
                "+            if(spoutName.contains(\".\")){",
                "+                LOG.warn(\"Metrics for spout name '{}' will be reported as '{}'.\", spoutName, spoutName.replace('.', '_') );",
                "+            }",
                "+            SpoutSpec spoutSpec = spouts.get(spoutName);",
                "+            for(String streamName : spoutSpec.get_common().get_streams().keySet()){",
                "+                if(streamName.contains(\".\")){",
                "+                    LOG.warn(\"Metrics for stream name '{}' will be reported as '{}'.\", streamName, streamName.replace('.', '_') );",
                "+                }",
                "+            }",
                "+        }",
                "+",
                "+        Map<String, Bolt> bolts = topology.get_bolts();",
                "+        for(String boltName : bolts.keySet()){",
                "+            if(boltName.contains(\".\")){",
                "+                LOG.warn(\"Metrics for bolt name '{}' will be reported as '{}'.\", boltName, boltName.replace('.', '_') );",
                "+            }",
                "+            Bolt bolt = bolts.get(boltName);",
                "+            for(String streamName : bolt.get_common().get_streams().keySet()){",
                "+                if(streamName.contains(\".\")){",
                "+                    LOG.warn(\"Metrics for stream name '{}' will be reported as '{}'.\", streamName, streamName.replace('.', '_') );",
                "+                }",
                "+            }",
                "+        }",
                "     }    ",
                "diff --git a/storm-core/src/jvm/org/apache/storm/nimbus/StrictTopologyValidator.java b/storm-core/src/jvm/org/apache/storm/nimbus/StrictTopologyValidator.java",
                "new file mode 100644",
                "index 000000000..e081a4d23",
                "--- /dev/null",
                "+++ b/storm-core/src/jvm/org/apache/storm/nimbus/StrictTopologyValidator.java",
                "@@ -0,0 +1,65 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+package org.apache.storm.nimbus;",
                "+",
                "+import org.apache.storm.generated.Bolt;",
                "+import org.apache.storm.generated.InvalidTopologyException;",
                "+import org.apache.storm.generated.SpoutSpec;",
                "+import org.apache.storm.generated.StormTopology;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+import java.util.Map;",
                "+",
                "+public class StrictTopologyValidator implements ITopologyValidator {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(StrictTopologyValidator.class);",
                "+    @Override",
                "+    public void prepare(Map StormConf){",
                "+    }",
                "+    @Override",
                "+    public void validate(String topologyName, Map topologyConf, StormTopology topology) throws InvalidTopologyException {",
                "+        if(topologyName.contains(\".\")){",
                "+            throw new InvalidTopologyException(String.format(\"Topology name '%s' contains illegal character '.'\", topologyName));",
                "+        }",
                "+        Map<String, SpoutSpec> spouts = topology.get_spouts();",
                "+        for(String spoutName : spouts.keySet()){",
                "+            if(spoutName.contains(\".\")){",
                "+                throw new InvalidTopologyException(String.format(\"Spout name '%s' contains illegal character '.'\", spoutName));",
                "+            }",
                "+            SpoutSpec spoutSpec = spouts.get(spoutName);",
                "+            for(String streamName : spoutSpec.get_common().get_streams().keySet()){",
                "+                if(streamName.contains(\".\")){",
                "+                    throw new InvalidTopologyException(String.format(\"Stream name '%s' contains illegal character '.'\", streamName));",
                "+                }",
                "+            }",
                "+        }",
                "+",
                "+        Map<String, Bolt> bolts = topology.get_bolts();",
                "+        for(String boltName : bolts.keySet()){",
                "+            if(boltName.contains(\".\")){",
                "+                throw new InvalidTopologyException(String.format(\"Bolt name '%s' contains illegal character '.'\", boltName));",
                "+            }",
                "+            Bolt bolt = bolts.get(boltName);",
                "+            for(String streamName : bolt.get_common().get_streams().keySet()){",
                "+                if(streamName.contains(\".\")){",
                "+                    throw new InvalidTopologyException(String.format(\"Stream name '%s' contains illegal character '.'\", streamName));",
                "+                }",
                "+            }",
                "+        }",
                "+    }    ",
                "+}"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/nimbus/DefaultTopologyValidator.java",
                "storm-core/src/jvm/org/apache/storm/nimbus/StrictTopologyValidator.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "c36f9efad43677c8e82fbac306bbd6575ba46443",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515722693,
            "hunks": 2,
            "message": "[STORM-2897]Optimize defaults.yaml",
            "diff": [
                "diff --git a/conf/defaults.yaml b/conf/defaults.yaml",
                "index 6821701c0..51c2fc66f 100644",
                "--- a/conf/defaults.yaml",
                "+++ b/conf/defaults.yaml",
                "@@ -195,7 +195,3 @@ backpressure.znode.update.freq.secs: 15",
                "-zmq.threads: 1",
                "-zmq.linger.millis: 5000",
                "-zmq.hwm: 0",
                "-",
                "-",
                "+# Used by workers to communicate",
                " storm.messaging.netty.server_worker_threads: 1",
                "@@ -331,2 +327,3 @@ worker.metrics:",
                "+# The number of buckets for running statistics",
                " num.stat.buckets: 20"
            ],
            "changed_files": [
                "conf/defaults.yaml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2897": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2897",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d42e186e3ea1241ed2f79548002bbf53e306f71e",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511977328,
            "hunks": 0,
            "message": "Merge branch 'STORM-2830' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2830": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2830",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "767c7f3be8699099ad9b2ffbdfc8baab36dd14f0",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508166702,
            "hunks": 1,
            "message": "[STORM-2777] The number of ackers should default to the number of workers",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index 180abfd9b..3b914baa8 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -2675,3 +2675,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "             Map<String, Object> totalConfToSave = Utils.merge(otherConf, topoConf);",
                "-            Map<String, Object> totalConf = Utils.merge(totalConfToSave, conf);",
                "+            Map<String, Object> totalConf = Utils.merge(conf, totalConfToSave);",
                "             //When reading the conf in nimbus we want to fall back to our own settings"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2777": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2777",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5077228df5f2f047f991d2a026a3217fd904eec5",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509084690,
            "hunks": 0,
            "message": "Merge branch 'Apache_master_STORM-2784_KTLOPR' of https://github.com/hmcl/storm-apache into STORM-2784-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2784": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2784",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b25d580780d31598f568bbe8dde800fe4b0d6f61",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510356186,
            "hunks": 4,
            "message": "STORM-2808: Fix for NPE in UI",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/ui/core.clj b/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "index 247ff4a48..d12ff7498 100644",
                "--- a/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "@@ -48,3 +48,3 @@",
                "   (:import [org.apache.storm.utils VersionInfo ConfigUtils Utils WebAppUtils])",
                "-  (:import [org.apache.storm Config])",
                "+  (:import [org.apache.storm Config Constants])",
                "   (:import [java.io File])",
                "@@ -427,4 +427,4 @@",
                "                                (for [^SupervisorSummary s sups",
                "-                                     :let [sup-total-mem (get (.get_total_resources s) Config/SUPERVISOR_MEMORY_CAPACITY_MB)",
                "-                                           sup-total-cpu (get (.get_total_resources s) Config/SUPERVISOR_CPU_CAPACITY)",
                "+                                     :let [sup-total-mem (get (.get_total_resources s) Constants/COMMON_TOTAL_MEMORY_RESOURCE_NAME)",
                "+                                           sup-total-cpu (get (.get_total_resources s) Constants/COMMON_CPU_RESOURCE_NAME)",
                "                                            sup-avail-mem (max (- sup-total-mem (.get_used_mem s)) 0.0)",
                "@@ -520,4 +520,4 @@",
                "         slotsFree (max (- slotsTotal slotsUsed) 0)",
                "-        totalMem (get (.get_total_resources summary) Config/SUPERVISOR_MEMORY_CAPACITY_MB)",
                "-        totalCpu (get (.get_total_resources summary) Config/SUPERVISOR_CPU_CAPACITY)",
                "+        totalMem (get (.get_total_resources summary) Constants/COMMON_TOTAL_MEMORY_RESOURCE_NAME)",
                "+        totalCpu (get (.get_total_resources summary) Constants/COMMON_CPU_RESOURCE_NAME)",
                "         usedMem (.get_used_mem summary)",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index 2da7b1939..f878cf052 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -960,4 +960,4 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "             metrics.add(new DataPoint(\"slotsUsed\", sup.get_num_used_workers()));",
                "-            metrics.add(new DataPoint(\"totalMem\", sup.get_total_resources().get(Config.SUPERVISOR_MEMORY_CAPACITY_MB)));",
                "-            metrics.add(new DataPoint(\"totalCpu\", sup.get_total_resources().get(Config.SUPERVISOR_CPU_CAPACITY)));",
                "+            metrics.add(new DataPoint(\"totalMem\", sup.get_total_resources().get(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME)));",
                "+            metrics.add(new DataPoint(\"totalCpu\", sup.get_total_resources().get(Constants.COMMON_CPU_RESOURCE_NAME)));",
                "             metrics.add(new DataPoint(\"usedMem\", sup.get_used_mem()));"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/ui/core.clj",
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2808": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2808",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "2709a1cda9bd1bcc5982bd73243bbe2a5fd0112c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510597569,
            "hunks": 0,
            "message": "Merge branch 'STORM-2811' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2811": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2811",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "ea233a908fd3124f2460353c5a6622a8aed98519",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511217125,
            "hunks": 5,
            "message": "[STORM-2827] fix Logviewer search returning incorrect logviewerUrl problem",
            "diff": [
                "diff --git a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java",
                "index 9e9ad92f1..a418da3d1 100644",
                "--- a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java",
                "+++ b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java",
                "@@ -157,3 +157,3 @@ public class LogviewerServer implements AutoCloseable {",
                "         Utils.setupDefaultUncaughtExceptionHandler();",
                "-        Map<String, Object> conf = Utils.readStormConfig();",
                "+        Map<String, Object> conf = ConfigUtils.readStormConfig();",
                "diff --git a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java",
                "index 3ac225267..710db6b2a 100644",
                "--- a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java",
                "+++ b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java",
                "@@ -84,2 +84,3 @@ public class LogviewerLogSearchHandler {",
                "     private final Integer logviewerPort;",
                "+    private final String scheme;",
                "@@ -99,4 +100,10 @@ public class LogviewerLogSearchHandler {",
                "         this.resourceAuthorizer = resourceAuthorizer;",
                "-",
                "-        this.logviewerPort = ObjectReader.getInt(stormConf.get(DaemonConfig.LOGVIEWER_PORT));",
                "+        Object httpsPort = stormConf.get(DaemonConfig.LOGVIEWER_HTTPS_PORT);",
                "+        if (httpsPort == null) {",
                "+            this.logviewerPort = ObjectReader.getInt(stormConf.get(DaemonConfig.LOGVIEWER_PORT));",
                "+            this.scheme = \"http\";",
                "+        } else {",
                "+            this.logviewerPort = ObjectReader.getInt(httpsPort);",
                "+            this.scheme = \"https\";",
                "+        }",
                "     }",
                "@@ -664,3 +671,3 @@ public class LogviewerLogSearchHandler {",
                "-        return UrlBuilder.build(String.format(\"http://%s:%d/api/v1/log\", host, port), parameters);",
                "+        return UrlBuilder.build(String.format(this.scheme + \"://%s:%d/api/v1/log\", host, port), parameters);",
                "     }",
                "@@ -677,3 +684,3 @@ public class LogviewerLogSearchHandler {",
                "-        return UrlBuilder.build(String.format(\"http://%s:%d/api/v1/daemonlog\", host, port), parameters);",
                "+        return UrlBuilder.build(String.format(this.scheme + \"://%s:%d/api/v1/daemonlog\", host, port), parameters);",
                "     }"
            ],
            "changed_files": [
                "storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/LogviewerServer.java",
                "storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2827": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2827",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "879cb5b8f2d9f869d2f90b0968fba1e08b04cd07",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516620523,
            "hunks": 0,
            "message": "Merge branch 'reuse-zk' of https://github.com/danny0405/storm into STORM-2901-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2901": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2901",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "3da2a496613e2ba1ae128fa925c07b838cefd28f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517498247,
            "hunks": 0,
            "message": "Merge branch 'STORM-2917-1.x-merge' into 1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2917": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2917",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e91362357179466ee92737a1a5f17f28d1c71493",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516727581,
            "hunks": 58,
            "message": "[maven-release-plugin] prepare release v1.1.2",
            "diff": [
                "diff --git a/examples/storm-elasticsearch-examples/pom.xml b/examples/storm-elasticsearch-examples/pom.xml",
                "index 348637aed..3e3a9fea7 100644",
                "--- a/examples/storm-elasticsearch-examples/pom.xml",
                "+++ b/examples/storm-elasticsearch-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hbase-examples/pom.xml b/examples/storm-hbase-examples/pom.xml",
                "index 3a11a9c6c..48c0f0203 100644",
                "--- a/examples/storm-hbase-examples/pom.xml",
                "+++ b/examples/storm-hbase-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hdfs-examples/pom.xml b/examples/storm-hdfs-examples/pom.xml",
                "index 8bab8fcf2..261d333be 100644",
                "--- a/examples/storm-hdfs-examples/pom.xml",
                "+++ b/examples/storm-hdfs-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hive-examples/pom.xml b/examples/storm-hive-examples/pom.xml",
                "index 50bf51214..9e903294d 100644",
                "--- a/examples/storm-hive-examples/pom.xml",
                "+++ b/examples/storm-hive-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jdbc-examples/pom.xml b/examples/storm-jdbc-examples/pom.xml",
                "index f599de795..60339a2c7 100644",
                "--- a/examples/storm-jdbc-examples/pom.xml",
                "+++ b/examples/storm-jdbc-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jms-examples/pom.xml b/examples/storm-jms-examples/pom.xml",
                "index 44452502c..9de6b93ca 100644",
                "--- a/examples/storm-jms-examples/pom.xml",
                "+++ b/examples/storm-jms-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-client-examples/pom.xml b/examples/storm-kafka-client-examples/pom.xml",
                "index 6c40475a5..1f862d759 100644",
                "--- a/examples/storm-kafka-client-examples/pom.xml",
                "+++ b/examples/storm-kafka-client-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-examples/pom.xml b/examples/storm-kafka-examples/pom.xml",
                "index 658e8f760..e0c6888ff 100644",
                "--- a/examples/storm-kafka-examples/pom.xml",
                "+++ b/examples/storm-kafka-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mongodb-examples/pom.xml b/examples/storm-mongodb-examples/pom.xml",
                "index f4ad7f55e..f867b986e 100644",
                "--- a/examples/storm-mongodb-examples/pom.xml",
                "+++ b/examples/storm-mongodb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mqtt-examples/pom.xml b/examples/storm-mqtt-examples/pom.xml",
                "index d32cb992f..ac94556d1 100644",
                "--- a/examples/storm-mqtt-examples/pom.xml",
                "+++ b/examples/storm-mqtt-examples/pom.xml",
                "@@ -28,3 +28,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.2-SNAPSHOT</version>",
                "+    <version>1.1.2</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-opentsdb-examples/pom.xml b/examples/storm-opentsdb-examples/pom.xml",
                "index 63cde9167..2ce34ee77 100644",
                "--- a/examples/storm-opentsdb-examples/pom.xml",
                "+++ b/examples/storm-opentsdb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-perf/pom.xml b/examples/storm-perf/pom.xml",
                "index 6203793c0..501be8f2b 100644",
                "--- a/examples/storm-perf/pom.xml",
                "+++ b/examples/storm-perf/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-pmml-examples/pom.xml b/examples/storm-pmml-examples/pom.xml",
                "index c13fa7a1b..0b0d133d5 100644",
                "--- a/examples/storm-pmml-examples/pom.xml",
                "+++ b/examples/storm-pmml-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-redis-examples/pom.xml b/examples/storm-redis-examples/pom.xml",
                "index 28d51aaf5..c0dde9df0 100644",
                "--- a/examples/storm-redis-examples/pom.xml",
                "+++ b/examples/storm-redis-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-solr-examples/pom.xml b/examples/storm-solr-examples/pom.xml",
                "index e830e7cb7..b81420aeb 100644",
                "--- a/examples/storm-solr-examples/pom.xml",
                "+++ b/examples/storm-solr-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-starter/pom.xml b/examples/storm-starter/pom.xml",
                "index 9d029ff83..672a52f0d 100644",
                "--- a/examples/storm-starter/pom.xml",
                "+++ b/examples/storm-starter/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.1.2-SNAPSHOT</version>",
                "+      <version>1.1.2</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-core/pom.xml b/external/flux/flux-core/pom.xml",
                "index adde30d02..c62989764 100644",
                "--- a/external/flux/flux-core/pom.xml",
                "+++ b/external/flux/flux-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-wrappers/pom.xml b/external/flux/flux-wrappers/pom.xml",
                "index 61f258b9a..76c56c6ed 100644",
                "--- a/external/flux/flux-wrappers/pom.xml",
                "+++ b/external/flux/flux-wrappers/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/pom.xml b/external/flux/pom.xml",
                "index cbc33657c..aa8cd48ee 100644",
                "--- a/external/flux/pom.xml",
                "+++ b/external/flux/pom.xml",
                "@@ -28,3 +28,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/pom.xml b/external/sql/pom.xml",
                "index aedc08ab8..c3cea241d 100644",
                "--- a/external/sql/pom.xml",
                "+++ b/external/sql/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-core/pom.xml b/external/sql/storm-sql-core/pom.xml",
                "index a7980084a..db99e28a3 100644",
                "--- a/external/sql/storm-sql-core/pom.xml",
                "+++ b/external/sql/storm-sql-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "index a03e25925..5ca571d76 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "index 7aad20378..f0919389f 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "index 4b6153fc5..3d4df119e 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-redis/pom.xml b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "index 5889e6a9a..e61224fae 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-runtime/pom.xml b/external/sql/storm-sql-runtime/pom.xml",
                "index a506e92b7..b64de3491 100644",
                "--- a/external/sql/storm-sql-runtime/pom.xml",
                "+++ b/external/sql/storm-sql-runtime/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/storm-cassandra/pom.xml b/external/storm-cassandra/pom.xml",
                "index 580000f4f..3619a86f4 100644",
                "--- a/external/storm-cassandra/pom.xml",
                "+++ b/external/storm-cassandra/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-druid/pom.xml b/external/storm-druid/pom.xml",
                "index 02748eec7..afed6dd93 100644",
                "--- a/external/storm-druid/pom.xml",
                "+++ b/external/storm-druid/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-elasticsearch/pom.xml b/external/storm-elasticsearch/pom.xml",
                "index 380ce4e5e..7b721674c 100644",
                "--- a/external/storm-elasticsearch/pom.xml",
                "+++ b/external/storm-elasticsearch/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-eventhubs/pom.xml b/external/storm-eventhubs/pom.xml",
                "index 9ce5e2de6..9e91f712d 100755",
                "--- a/external/storm-eventhubs/pom.xml",
                "+++ b/external/storm-eventhubs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hbase/pom.xml b/external/storm-hbase/pom.xml",
                "index da3a9b51b..49436efb9 100644",
                "--- a/external/storm-hbase/pom.xml",
                "+++ b/external/storm-hbase/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hdfs/pom.xml b/external/storm-hdfs/pom.xml",
                "index 321569909..7d3c9cb1a 100644",
                "--- a/external/storm-hdfs/pom.xml",
                "+++ b/external/storm-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hive/pom.xml b/external/storm-hive/pom.xml",
                "index 706e8f843..2fa04b027 100644",
                "--- a/external/storm-hive/pom.xml",
                "+++ b/external/storm-hive/pom.xml",
                "@@ -23,3 +23,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.2-SNAPSHOT</version>",
                "+    <version>1.1.2</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jdbc/pom.xml b/external/storm-jdbc/pom.xml",
                "index ea5d735c2..251f153b7 100644",
                "--- a/external/storm-jdbc/pom.xml",
                "+++ b/external/storm-jdbc/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jms/pom.xml b/external/storm-jms/pom.xml",
                "index 7c31b37ff..d71235502 100644",
                "--- a/external/storm-jms/pom.xml",
                "+++ b/external/storm-jms/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index 304e703ef..5e960ae2b 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -24,3 +24,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-monitor/pom.xml b/external/storm-kafka-monitor/pom.xml",
                "index 475e0732d..8e0363c2a 100644",
                "--- a/external/storm-kafka-monitor/pom.xml",
                "+++ b/external/storm-kafka-monitor/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka/pom.xml b/external/storm-kafka/pom.xml",
                "index 28ec77ced..56d83fcdd 100644",
                "--- a/external/storm-kafka/pom.xml",
                "+++ b/external/storm-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kinesis/pom.xml b/external/storm-kinesis/pom.xml",
                "index 013a6efc3..9ff4e96b8 100644",
                "--- a/external/storm-kinesis/pom.xml",
                "+++ b/external/storm-kinesis/pom.xml",
                "@@ -19,3 +19,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-metrics/pom.xml b/external/storm-metrics/pom.xml",
                "index 85139343e..a35c8c689 100644",
                "--- a/external/storm-metrics/pom.xml",
                "+++ b/external/storm-metrics/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.1.2-SNAPSHOT</version>",
                "+      <version>1.1.2</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mongodb/pom.xml b/external/storm-mongodb/pom.xml",
                "index 5ad9958eb..5420f6125 100644",
                "--- a/external/storm-mongodb/pom.xml",
                "+++ b/external/storm-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mqtt/pom.xml b/external/storm-mqtt/pom.xml",
                "index 9a3056c58..069a0e01d 100644",
                "--- a/external/storm-mqtt/pom.xml",
                "+++ b/external/storm-mqtt/pom.xml",
                "@@ -27,3 +27,3 @@",
                "         <artifactId>storm</artifactId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-opentsdb/pom.xml b/external/storm-opentsdb/pom.xml",
                "index 0be768f76..f4a4f369d 100644",
                "--- a/external/storm-opentsdb/pom.xml",
                "+++ b/external/storm-opentsdb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-pmml/pom.xml b/external/storm-pmml/pom.xml",
                "index 73f76f54e..f0a07cadd 100644",
                "--- a/external/storm-pmml/pom.xml",
                "+++ b/external/storm-pmml/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-redis/pom.xml b/external/storm-redis/pom.xml",
                "index 33418eb9d..fe641778e 100644",
                "--- a/external/storm-redis/pom.xml",
                "+++ b/external/storm-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-solr/pom.xml b/external/storm-solr/pom.xml",
                "index c5303f467..2c4439e51 100644",
                "--- a/external/storm-solr/pom.xml",
                "+++ b/external/storm-solr/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-submit-tools/pom.xml b/external/storm-submit-tools/pom.xml",
                "index 340061c30..8d17cac11 100644",
                "--- a/external/storm-submit-tools/pom.xml",
                "+++ b/external/storm-submit-tools/pom.xml",
                "@@ -21,3 +21,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/pom.xml b/pom.xml",
                "index d7fd43b2e..7f4c4b52c 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -29,3 +29,3 @@",
                "     <artifactId>storm</artifactId>",
                "-    <version>1.1.2-SNAPSHOT</version>",
                "+    <version>1.1.2</version>",
                "     <packaging>pom</packaging>",
                "@@ -204,3 +204,3 @@",
                "         <developerConnection>scm:git:https://git-wip-us.apache.org/repos/asf/storm.git</developerConnection>",
                "-        <tag>HEAD</tag>",
                "+        <tag>v1.1.2</tag>",
                "         <url>https://git-wip-us.apache.org/repos/asf/storm</url>",
                "diff --git a/storm-buildtools/maven-shade-clojure-transformer/pom.xml b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "index 9585b9d35..f3e2a75e7 100644",
                "--- a/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "+++ b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-buildtools/storm-maven-plugins/pom.xml b/storm-buildtools/storm-maven-plugins/pom.xml",
                "index 759e35bfd..77cd804bf 100644",
                "--- a/storm-buildtools/storm-maven-plugins/pom.xml",
                "+++ b/storm-buildtools/storm-maven-plugins/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index 61db89b77..dcbcab4db 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>..</relativePath>",
                "diff --git a/storm-dist/binary/pom.xml b/storm-dist/binary/pom.xml",
                "index 5824ca77a..166d09346 100644",
                "--- a/storm-dist/binary/pom.xml",
                "+++ b/storm-dist/binary/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-dist/source/pom.xml b/storm-dist/source/pom.xml",
                "index a24674718..22db90db0 100644",
                "--- a/storm-dist/source/pom.xml",
                "+++ b/storm-dist/source/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/javascript/pom.xml b/storm-multilang/javascript/pom.xml",
                "index 58f9d5334..72e5fd9d3 100644",
                "--- a/storm-multilang/javascript/pom.xml",
                "+++ b/storm-multilang/javascript/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/python/pom.xml b/storm-multilang/python/pom.xml",
                "index 194bc22bd..bb3abc9b7 100644",
                "--- a/storm-multilang/python/pom.xml",
                "+++ b/storm-multilang/python/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/ruby/pom.xml b/storm-multilang/ruby/pom.xml",
                "index d366ecc41..a0555d724 100644",
                "--- a/storm-multilang/ruby/pom.xml",
                "+++ b/storm-multilang/ruby/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.2-SNAPSHOT</version>",
                "+        <version>1.1.2</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-rename-hack/pom.xml b/storm-rename-hack/pom.xml",
                "index b27e4e897..3b392bc83 100644",
                "--- a/storm-rename-hack/pom.xml",
                "+++ b/storm-rename-hack/pom.xml",
                "@@ -22,3 +22,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.2-SNAPSHOT</version>",
                "+    <version>1.1.2</version>",
                "     <relativePath>../pom.xml</relativePath>"
            ],
            "changed_files": [
                "examples/storm-elasticsearch-examples/pom.xml",
                "examples/storm-hbase-examples/pom.xml",
                "examples/storm-hdfs-examples/pom.xml",
                "examples/storm-hive-examples/pom.xml",
                "examples/storm-jdbc-examples/pom.xml",
                "examples/storm-jms-examples/pom.xml",
                "examples/storm-kafka-client-examples/pom.xml",
                "examples/storm-kafka-examples/pom.xml",
                "examples/storm-mongodb-examples/pom.xml",
                "examples/storm-mqtt-examples/pom.xml",
                "examples/storm-opentsdb-examples/pom.xml",
                "examples/storm-perf/pom.xml",
                "examples/storm-pmml-examples/pom.xml",
                "examples/storm-redis-examples/pom.xml",
                "examples/storm-solr-examples/pom.xml",
                "examples/storm-starter/pom.xml",
                "external/flux/flux-core/pom.xml",
                "external/flux/flux-wrappers/pom.xml",
                "external/flux/pom.xml",
                "external/sql/pom.xml",
                "external/sql/storm-sql-core/pom.xml",
                "external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "external/sql/storm-sql-runtime/pom.xml",
                "external/storm-cassandra/pom.xml",
                "external/storm-druid/pom.xml",
                "external/storm-elasticsearch/pom.xml",
                "external/storm-eventhubs/pom.xml",
                "external/storm-hbase/pom.xml",
                "external/storm-hdfs/pom.xml",
                "external/storm-hive/pom.xml",
                "external/storm-jdbc/pom.xml",
                "external/storm-jms/pom.xml",
                "external/storm-kafka-client/pom.xml",
                "external/storm-kafka-monitor/pom.xml",
                "external/storm-kafka/pom.xml",
                "external/storm-kinesis/pom.xml",
                "external/storm-metrics/pom.xml",
                "external/storm-mongodb/pom.xml",
                "external/storm-mqtt/pom.xml",
                "external/storm-opentsdb/pom.xml",
                "external/storm-pmml/pom.xml",
                "external/storm-redis/pom.xml",
                "external/storm-solr/pom.xml",
                "external/storm-submit-tools/pom.xml",
                "pom.xml",
                "storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "storm-buildtools/storm-maven-plugins/pom.xml",
                "storm-core/pom.xml",
                "storm-dist/binary/pom.xml",
                "storm-dist/source/pom.xml",
                "storm-multilang/javascript/pom.xml",
                "storm-multilang/python/pom.xml",
                "storm-multilang/ruby/pom.xml",
                "storm-rename-hack/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "5d2eecf3d282a535541ac7520a88b47f01153da1"
                ],
                [
                    "no-tag",
                    "f2b8cbfe58104129b225afa5fbd2f4fb0be2706e"
                ],
                [
                    "no-tag",
                    "279f16f2c17eafd0a0ab132bc42d7d3c64fdd3b2"
                ]
            ],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "05771bafaf6a11631867c457b8af53c8b9955734",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516613218,
            "hunks": 0,
            "message": "Merge branch 'STORM-2862' of https://github.com/hmcc/storm into STORM-2862-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2862": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2862",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "79626398f0e6db269bf6632c3e6e29a76ea1297b",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510846374,
            "hunks": 0,
            "message": "Merge branch 'STORM-2815-1.x' of https://github.com/Ethanlm/storm into STORM-2815-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2815": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2815",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "c91da676e7fc550ab44fd2d4d91dc95243059eb7",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513196756,
            "hunks": 11,
            "message": "STORM-2153: add streamId and executorId to metrics names; replace '.' with '_' in metrics names",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index 0aca4bd7b..fa7d44c56 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -282,4 +282,2 @@",
                "      :sampler (mk-stats-sampler storm-conf)",
                "-     :failed-counter (StormMetricRegistry/counter \"failed\" worker-context component-id)",
                "-     :acked-counter (StormMetricRegistry/counter \"acked\" worker-context component-id)",
                "      :spout-throttling-metrics (if (= executor-type :spout)",
                "@@ -446,3 +444,3 @@",
                "     (when time-delta",
                "-      (stats/spout-failed-tuple! (:stats executor-data) (:failed-counter executor-data) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-failed-tuple! (:stats executor-data) (StormMetricRegistry/counter \"failed\" worker-context (:component-id executor-data) (:executor-id executor-data) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "@@ -455,3 +453,3 @@",
                "     (when time-delta",
                "-      (stats/spout-acked-tuple! (:stats executor-data) (:acked-counter executor-data) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-acked-tuple! (:stats executor-data) (StormMetricRegistry/counter \"acked\" worker-context (:component-id executor-data) (:executor-id executor-data) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/task.clj b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "index a2f6c5450..edc144c7b 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "@@ -132,4 +132,3 @@",
                "         executor-stats (:stats executor-data)",
                "-        debug? (= true (storm-conf TOPOLOGY-DEBUG))",
                "-        ^Counter emitted-counter (StormMetricRegistry/counter \"emitted\" worker-context component-id)]",
                "+        debug? (= true (storm-conf TOPOLOGY-DEBUG))]",
                "@@ -146,3 +145,3 @@",
                "             (when (emit-sampler)",
                "-              (stats/emitted-tuple! executor-stats emitted-counter stream)",
                "+              (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id (:executor-id executor-data) stream) stream)",
                "               (if out-task-id",
                "@@ -166,3 +165,3 @@",
                "              (when (emit-sampler)",
                "-               (stats/emitted-tuple! executor-stats emitted-counter stream)",
                "+               (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id (:executor-id executor-data) stream) stream)",
                "                (stats/transferred-tuples! executor-stats stream (count out-tasks)))",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "index 200ddcf4b..2bab4e9fe 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "@@ -70,4 +70,4 @@ public class StormMetricRegistry {",
                "-    public static Meter meter(String name, WorkerTopologyContext context, String componentId){",
                "-        String metricName = metricName(name, context.getStormId(), componentId, context.getThisWorkerPort());",
                "+    public static Meter meter(String name, WorkerTopologyContext context, String componentId, String executorId, String streamId){",
                "+        String metricName = metricName(name, context.getStormId(), componentId, streamId,executorId, context.getThisWorkerPort());",
                "         return REGISTRY.meter(metricName);",
                "@@ -75,4 +75,4 @@ public class StormMetricRegistry {",
                "-    public static Counter counter(String name, WorkerTopologyContext context, String componentId){",
                "-        String metricName = metricName(name, context.getStormId(), componentId, context.getThisWorkerPort());",
                "+    public static Counter counter(String name, WorkerTopologyContext context, String componentId, String executorId, String streamId){",
                "+        String metricName = metricName(name, context.getStormId(), componentId, streamId,executorId, context.getThisWorkerPort());",
                "         return REGISTRY.counter(metricName);",
                "@@ -83,3 +83,3 @@ public class StormMetricRegistry {",
                "         try {",
                "-            hostName = Utils.localHostname();",
                "+            hostName = dotToUnderScore(Utils.localHostname());",
                "         } catch (UnknownHostException e) {",
                "@@ -132,7 +132,25 @@ public class StormMetricRegistry {",
                "-    public static String metricName(String name, String stormId, String componentId, Integer workerPort){",
                "-        return String.format(\"storm.worker.%s.%s.%s.%s-%s\", stormId, hostName, componentId, workerPort, name);",
                "+    public static String metricName(String name, String stormId, String componentId, String streamId, String executorId, Integer workerPort){",
                "+        return String.format(\"storm.worker.%s.%s.%s.%s.%s.%s-%s\",",
                "+                stormId,",
                "+                hostName,",
                "+                dotToUnderScore(componentId),",
                "+                dotToUnderScore(streamId),",
                "+                dotToUnderScore(executorId),",
                "+                workerPort,",
                "+                name);",
                "     }",
                "+    public static String metricName(String name, String stormId, String componentId, Integer workerPort){",
                "+        return String.format(\"storm.worker.%s.%s.%s.%s-%s\",",
                "+                stormId,",
                "+                hostName,",
                "+                dotToUnderScore(componentId),",
                "+                workerPort,",
                "+                name);",
                "+    }",
                "+    private static String dotToUnderScore(String str){",
                "+        return str.replace('.', '_');",
                "+    }",
                " }"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "29f845b4a5db3f8807f6a07ea3f80a4ebcf5bbc0",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509173576,
            "hunks": 0,
            "message": "Merge branch 'Apache_master_STORM-2781_KSProcGtees' of https://github.com/hmcl/storm-apache into STORM-2781-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2781": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2781",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "cef450064fa20e2194ef3f51a21c8e6693a285e3",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509079754,
            "hunks": 0,
            "message": "Merge branch 'STORM-2759-merge'",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2759": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2759",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8d53800f14ced3fd630a02dfd9537d5900979562",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515612350,
            "hunks": 11,
            "message": "STORM-2153: use taskId in metrics names instead of executorId",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index 993add643..e8d23e5e4 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -444,3 +444,3 @@",
                "     (when time-delta",
                "-      (stats/spout-failed-tuple! (:stats executor-data) (StormMetricRegistry/counter \"failed\" (:worker-context executor-data) (:component-id executor-data) (pr-str (:executor-id executor-data)) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-failed-tuple! (:stats executor-data) (StormMetricRegistry/counter \"failed\" (:worker-context executor-data) (:component-id executor-data) task-id (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "@@ -453,3 +453,3 @@",
                "     (when time-delta",
                "-      (stats/spout-acked-tuple! (:stats executor-data) (StormMetricRegistry/counter \"acked\" (:worker-context executor-data) (:component-id executor-data) (pr-str (:executor-id executor-data)) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-acked-tuple! (:stats executor-data) (StormMetricRegistry/counter \"acked\" (:worker-context executor-data) (:component-id executor-data) task-id (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "@@ -822,3 +822,3 @@",
                "                              (stats/bolt-acked-tuple! executor-stats",
                "-                                                      (StormMetricRegistry/counter \"acked\" worker-context  (:component-id executor-data) (pr-str (:executor-id executor-data)) (.getSourceStreamId tuple))",
                "+                                                      (StormMetricRegistry/counter \"acked\" worker-context  (:component-id executor-data) task-id (.getSourceStreamId tuple))",
                "                                                       (.getSourceComponent tuple)",
                "@@ -838,3 +838,3 @@",
                "                              (stats/bolt-failed-tuple! executor-stats",
                "-                                                       (StormMetricRegistry/counter \"failed\" worker-context (:component-id executor-data) (pr-str (:executor-id executor-data)) (.getSourceStreamId tuple))",
                "+                                                       (StormMetricRegistry/counter \"failed\" worker-context (:component-id executor-data) task-id (.getSourceStreamId tuple))",
                "                                                        (.getSourceComponent tuple)",
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/task.clj b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "index 7132fc12d..9e18331b7 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "@@ -145,5 +145,5 @@",
                "             (when (emit-sampler)",
                "-              (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id (pr-str (:executor-id executor-data)) stream) stream)",
                "+              (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id task-id stream) stream)",
                "               (if out-task-id",
                "-                (stats/transferred-tuples! executor-stats (StormMetricRegistry/counter \"transferred\" worker-context component-id (pr-str (:executor-id executor-data)) stream) stream 1)))",
                "+                (stats/transferred-tuples! executor-stats (StormMetricRegistry/counter \"transferred\" worker-context component-id task-id stream) stream 1)))",
                "             (if out-task-id [out-task-id])",
                "@@ -165,4 +165,4 @@",
                "              (when (emit-sampler)",
                "-               (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id (pr-str (:executor-id executor-data)) stream) stream)",
                "-               (stats/transferred-tuples! executor-stats (StormMetricRegistry/counter \"transferred\" worker-context component-id (pr-str (:executor-id executor-data)) stream) stream (count out-tasks)))",
                "+               (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id task-id stream) stream)",
                "+               (stats/transferred-tuples! executor-stats (StormMetricRegistry/counter \"transferred\" worker-context component-id task-id stream) stream (count out-tasks)))",
                "              out-tasks)))",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "index aea453991..e1305f990 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "@@ -70,4 +70,4 @@ public class StormMetricRegistry {",
                "-    public static Meter meter(String name, WorkerTopologyContext context, String componentId, String executorId, String streamId){",
                "-        String metricName = metricName(name, context.getStormId(), componentId, streamId,executorId, context.getThisWorkerPort());",
                "+    public static Meter meter(String name, WorkerTopologyContext context, String componentId, Integer taskId, String streamId){",
                "+        String metricName = metricName(name, context.getStormId(), componentId, streamId,taskId, context.getThisWorkerPort());",
                "         return REGISTRY.meter(metricName);",
                "@@ -75,4 +75,4 @@ public class StormMetricRegistry {",
                "-    public static Counter counter(String name, WorkerTopologyContext context, String componentId, String executorId, String streamId){",
                "-        String metricName = metricName(name, context.getStormId(), componentId, streamId,executorId, context.getThisWorkerPort());",
                "+    public static Counter counter(String name, WorkerTopologyContext context, String componentId, Integer taskId, String streamId){",
                "+        String metricName = metricName(name, context.getStormId(), componentId, streamId,taskId, context.getThisWorkerPort());",
                "         return REGISTRY.counter(metricName);",
                "@@ -126,3 +126,3 @@ public class StormMetricRegistry {",
                "-    public static String metricName(String name, String stormId, String componentId, String streamId, String executorId, Integer workerPort){",
                "+    public static String metricName(String name, String stormId, String componentId, String streamId, Integer taskId, Integer workerPort){",
                "         StringBuilder sb = new StringBuilder(\"storm.worker.\");",
                "@@ -136,3 +136,3 @@ public class StormMetricRegistry {",
                "         sb.append(\".\");",
                "-        sb.append(dotToUnderScore(executorId));",
                "+        sb.append(taskId);",
                "         sb.append(\".\");"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "32fefef835984663af2e799885351e7f3d9a067a",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510523874,
            "hunks": 0,
            "message": "Merge branch 'STORM-2808' of https://github.com/govind-menon/storm into STORM-2808-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2808": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2808",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e6a423dd8cada12997e2a9448a9944050cf23a1f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516656191,
            "hunks": 0,
            "message": "Merge branch 'agresch_rocksdb' of https://github.com/agresch/storm into STORM-2887-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2887": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2887",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8b9330ab055fc517209eb583b3b6fe7e78584e09",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514248890,
            "hunks": 0,
            "message": "Merge branch 'STORM-2690' of https://github.com/erikdw/storm into STORM-2690-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2690": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2690",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5885dc0e3b99bf8ef0432b1ea19b3cc5b44f9f91",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515371891,
            "hunks": 0,
            "message": "Merge branch 'STORM-2879-1.1.x' into 1.1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2879": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2879",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "3503dcea62c9bb9d004388773705ad362e7cc5dd",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507418366,
            "hunks": 35,
            "message": "STORM-2698: Upgrade Mockito and Hamcrest to latest versions",
            "diff": [
                "diff --git a/examples/storm-starter/pom.xml b/examples/storm-starter/pom.xml",
                "index e95958d65..d12062dae 100644",
                "--- a/examples/storm-starter/pom.xml",
                "+++ b/examples/storm-starter/pom.xml",
                "@@ -53,5 +53,8 @@",
                "     <dependency>",
                "-      <groupId>org.mockito</groupId>",
                "-      <artifactId>mockito-all</artifactId>",
                "-      <scope>test</scope>",
                "+        <groupId>org.mockito</groupId>",
                "+        <artifactId>mockito-core</artifactId>",
                "+    </dependency>",
                "+    <dependency>",
                "+        <groupId>org.hamcrest</groupId>",
                "+        <artifactId>java-hamcrest</artifactId>",
                "     </dependency>",
                "diff --git a/external/storm-cassandra/pom.xml b/external/storm-cassandra/pom.xml",
                "index 9a7d683a5..d98824c3e 100644",
                "--- a/external/storm-cassandra/pom.xml",
                "+++ b/external/storm-cassandra/pom.xml",
                "@@ -119,5 +119,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <version>1.10.19</version>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/external/storm-elasticsearch/pom.xml b/external/storm-elasticsearch/pom.xml",
                "index 72d2639c4..c6371a4e4 100644",
                "--- a/external/storm-elasticsearch/pom.xml",
                "+++ b/external/storm-elasticsearch/pom.xml",
                "@@ -87,8 +87,2 @@",
                "         </dependency>",
                "-        <dependency>",
                "-            <groupId>org.hamcrest</groupId>",
                "-            <artifactId>hamcrest-all</artifactId>",
                "-            <version>1.3</version>",
                "-            <scope>test</scope>",
                "-        </dependency>",
                "         <dependency>",
                "@@ -101,4 +95,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/external/storm-hbase/pom.xml b/external/storm-hbase/pom.xml",
                "index b4f558a16..2c97498e3 100644",
                "--- a/external/storm-hbase/pom.xml",
                "+++ b/external/storm-hbase/pom.xml",
                "@@ -111,4 +111,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/external/storm-hdfs/pom.xml b/external/storm-hdfs/pom.xml",
                "index 23b8ed7e8..03488a77d 100644",
                "--- a/external/storm-hdfs/pom.xml",
                "+++ b/external/storm-hdfs/pom.xml",
                "@@ -182,4 +182,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/external/storm-hive/pom.xml b/external/storm-hive/pom.xml",
                "index 24ca2cc11..62979f012 100644",
                "--- a/external/storm-hive/pom.xml",
                "+++ b/external/storm-hive/pom.xml",
                "@@ -137,6 +137,8 @@",
                "     <dependency>",
                "-      <groupId>org.mockito</groupId>",
                "-      <artifactId>mockito-all</artifactId>",
                "-      <version>1.9.0</version>",
                "-      <scope>test</scope>",
                "+        <groupId>org.mockito</groupId>",
                "+        <artifactId>mockito-core</artifactId>",
                "+    </dependency>",
                "+    <dependency>",
                "+        <groupId>org.hamcrest</groupId>",
                "+        <artifactId>java-hamcrest</artifactId>",
                "     </dependency>",
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index c3bd457e6..816f5dd11 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -81,8 +81,2 @@",
                "             <artifactId>mockito-core</artifactId>",
                "-            <scope>test</scope>",
                "-        </dependency>",
                "-        <dependency>",
                "-            <groupId>org.hamcrest</groupId>",
                "-            <artifactId>hamcrest-core</artifactId>",
                "-            <scope>test</scope>",
                "         </dependency>",
                "@@ -90,4 +84,3 @@",
                "             <groupId>org.hamcrest</groupId>",
                "-            <artifactId>hamcrest-library</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/external/storm-kafka/pom.xml b/external/storm-kafka/pom.xml",
                "index 14fd6093a..aa0454bb7 100644",
                "--- a/external/storm-kafka/pom.xml",
                "+++ b/external/storm-kafka/pom.xml",
                "@@ -74,5 +74,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <version>1.9.0</version>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/external/storm-kafka/src/jvm/org/apache/storm/kafka/bolt/KafkaBolt.java b/external/storm-kafka/src/jvm/org/apache/storm/kafka/bolt/KafkaBolt.java",
                "index 5cb6129ee..a4c255af0 100644",
                "--- a/external/storm-kafka/src/jvm/org/apache/storm/kafka/bolt/KafkaBolt.java",
                "+++ b/external/storm-kafka/src/jvm/org/apache/storm/kafka/bolt/KafkaBolt.java",
                "@@ -78,3 +78,3 @@ public class KafkaBolt<K, V> extends BaseTickTupleAwareRichBolt {",
                "     public KafkaBolt() {}",
                "-",
                "+    ",
                "     public KafkaBolt<K,V> withTupleToKafkaMapper(TupleToKafkaMapper<K,V> mapper) {",
                "diff --git a/external/storm-mongodb/pom.xml b/external/storm-mongodb/pom.xml",
                "index 886d213f8..b374957e3 100644",
                "--- a/external/storm-mongodb/pom.xml",
                "+++ b/external/storm-mongodb/pom.xml",
                "@@ -65,4 +65,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/external/storm-opentsdb/pom.xml b/external/storm-opentsdb/pom.xml",
                "index f99c6dccd..0f9e0c802 100644",
                "--- a/external/storm-opentsdb/pom.xml",
                "+++ b/external/storm-opentsdb/pom.xml",
                "@@ -92,4 +92,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/external/storm-redis/pom.xml b/external/storm-redis/pom.xml",
                "index 1023efd20..9abf9ab88 100644",
                "--- a/external/storm-redis/pom.xml",
                "+++ b/external/storm-redis/pom.xml",
                "@@ -81,4 +81,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/pom.xml b/pom.xml",
                "index bf6bf1c14..85d575fd3 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -272,3 +272,3 @@",
                "         <clojure-complete.version>0.2.3</clojure-complete.version>",
                "-        <mockito.version>1.9.5</mockito.version>",
                "+        <mockito.version>2.10.0</mockito.version>",
                "         <conjure.version>2.1.3</conjure.version>",
                "@@ -288,3 +288,3 @@",
                "         <hdrhistogram.version>2.1.7</hdrhistogram.version>",
                "-        <hamcrest.version>1.3</hamcrest.version>",
                "+        <hamcrest.version>2.0.0.0</hamcrest.version>",
                "         <cassandra.version>2.1.7</cassandra.version>",
                "@@ -992,3 +992,3 @@",
                "                 <groupId>org.hamcrest</groupId>",
                "-                <artifactId>hamcrest-core</artifactId>",
                "+                <artifactId>java-hamcrest</artifactId>",
                "                 <version>${hamcrest.version}</version>",
                "@@ -996,14 +996,2 @@",
                "             </dependency>",
                "-            <dependency>",
                "-                <groupId>org.hamcrest</groupId>",
                "-                <artifactId>hamcrest-library</artifactId>",
                "-                <version>${hamcrest.version}</version>",
                "-                <scope>test</scope>",
                "-            </dependency>",
                "-            <dependency>",
                "-                <groupId>org.mockito</groupId>",
                "-                <artifactId>mockito-all</artifactId>",
                "-                <version>${mockito.version}</version>",
                "-                <scope>test</scope>",
                "-            </dependency>",
                "             <dependency>",
                "diff --git a/sql/storm-sql-core/pom.xml b/sql/storm-sql-core/pom.xml",
                "index 8b3035c99..1723a5f5a 100644",
                "--- a/sql/storm-sql-core/pom.xml",
                "+++ b/sql/storm-sql-core/pom.xml",
                "@@ -95,4 +95,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/sql/storm-sql-external/storm-sql-hdfs/pom.xml b/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "index 1574ed2e2..4259951eb 100644",
                "--- a/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "+++ b/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "@@ -90,4 +90,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/sql/storm-sql-external/storm-sql-kafka/pom.xml b/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "index 5645da3ba..97bd87c47 100644",
                "--- a/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "+++ b/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "@@ -79,4 +79,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/sql/storm-sql-external/storm-sql-mongodb/pom.xml b/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "index 82a7036d4..8bd56519d 100644",
                "--- a/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "+++ b/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "@@ -70,4 +70,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/sql/storm-sql-external/storm-sql-redis/pom.xml b/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "index 82aa13b48..4627450bb 100644",
                "--- a/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "+++ b/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "@@ -62,4 +62,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/sql/storm-sql-runtime/pom.xml b/sql/storm-sql-runtime/pom.xml",
                "index f9969b2c9..77e22a492 100644",
                "--- a/sql/storm-sql-runtime/pom.xml",
                "+++ b/sql/storm-sql-runtime/pom.xml",
                "@@ -87,4 +87,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/storm-client/pom.xml b/storm-client/pom.xml",
                "index 538b7b910..b37f45879 100644",
                "--- a/storm-client/pom.xml",
                "+++ b/storm-client/pom.xml",
                "@@ -179,5 +179,2 @@",
                "         <!-- test -->",
                "-        <!-- hamcrest-core dependency is shaded inside the mockito-all and junit depends on newer version of hamcrest-core.",
                "-        To give higher precedence to classes from newer version of hamcrest-core, Junit has been placed above mockito.",
                "-         -->",
                "         <dependency>",
                "@@ -185,3 +182,2 @@",
                "             <artifactId>junit</artifactId>",
                "-            <scope>test</scope>",
                "         </dependency>",
                "@@ -190,3 +186,2 @@",
                "             <artifactId>mockito-core</artifactId>",
                "-            <scope>test</scope>",
                "         </dependency>",
                "@@ -194,4 +189,3 @@",
                "             <groupId>org.hamcrest</groupId>",
                "-            <artifactId>hamcrest-library</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index ad4972e14..727631d76 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -293,3 +293,2 @@",
                "             <artifactId>junit</artifactId>",
                "-            <scope>test</scope>",
                "         </dependency>",
                "@@ -297,4 +296,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/storm-server/pom.xml b/storm-server/pom.xml",
                "index 744925778..2e803d034 100644",
                "--- a/storm-server/pom.xml",
                "+++ b/storm-server/pom.xml",
                "@@ -86,3 +86,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java",
                "index 1e25e5063..16f157de8 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java",
                "@@ -271,3 +271,3 @@ public class BasicContainer extends Container {",
                "         if (env == null) {",
                "-            env = new HashMap<String, String>();",
                "+            env = new HashMap<>();",
                "         }",
                "diff --git a/storm-webapp/pom.xml b/storm-webapp/pom.xml",
                "index 7c2bcf2bf..28461462b 100644",
                "--- a/storm-webapp/pom.xml",
                "+++ b/storm-webapp/pom.xml",
                "@@ -63,5 +63,2 @@",
                "         </dependency>",
                "-        <!-- hamcrest-core dependency is shaded inside the mockito-all and junit depends on newer version of hamcrest-core.",
                "-        To give higher precedence to classes from newer version of hamcrest-core, Junit has been placed above mockito.",
                "-         -->",
                "         <dependency>",
                "@@ -69,3 +66,2 @@",
                "             <artifactId>junit</artifactId>",
                "-            <scope>test</scope>",
                "         </dependency>",
                "@@ -73,4 +69,7 @@",
                "             <groupId>org.mockito</groupId>",
                "-            <artifactId>mockito-all</artifactId>",
                "-            <scope>test</scope>",
                "+            <artifactId>mockito-core</artifactId>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.hamcrest</groupId>",
                "+            <artifactId>java-hamcrest</artifactId>",
                "         </dependency>"
            ],
            "changed_files": [
                "examples/storm-starter/pom.xml",
                "external/storm-cassandra/pom.xml",
                "external/storm-elasticsearch/pom.xml",
                "external/storm-hbase/pom.xml",
                "external/storm-hdfs/pom.xml",
                "external/storm-hive/pom.xml",
                "external/storm-kafka-client/pom.xml",
                "external/storm-kafka/pom.xml",
                "external/storm-kafka/src/jvm/org/apache/storm/kafka/bolt/KafkaBolt.java",
                "external/storm-mongodb/pom.xml",
                "external/storm-opentsdb/pom.xml",
                "external/storm-redis/pom.xml",
                "pom.xml",
                "sql/storm-sql-core/pom.xml",
                "sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "sql/storm-sql-external/storm-sql-redis/pom.xml",
                "sql/storm-sql-runtime/pom.xml",
                "storm-client/pom.xml",
                "storm-core/pom.xml",
                "storm-server/pom.xml",
                "storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java",
                "storm-webapp/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2698": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2698",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7e622d178bd98860f6722977f9d803c371bbe0f2",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511965700,
            "hunks": 0,
            "message": "Merge branch 'master' of https://github.com/MichealShin/storm Quick fix: typo in FAQ.md This closes #2439",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {
                "2439": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2439",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "644f5cb98d6d5b810ac23a04a01f62edd42c44b7",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507590643,
            "hunks": 0,
            "message": "Merge branch 'STORM-2666-clean-1.x' of https://github.com/srdo/storm into STORM-2666-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2666": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2666",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e99b53902d40fbcaff47bd876eba432de1d086d5",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511909396,
            "hunks": 0,
            "message": "Merge branch 'STORM-2833-merge'",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2833": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2833",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "44cd8ac3b4b55cfa86b63d45db2c0407f4c26417",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515528436,
            "hunks": 7,
            "message": "STORM-2152: address additional review comments",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "index 2bab4e9fe..789367b98 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "@@ -25,2 +25,3 @@ import org.apache.storm.cluster.DaemonType;",
                " import org.apache.storm.metrics2.reporters.StormReporter;",
                "+import org.apache.storm.task.TopologyContext;",
                " import org.apache.storm.task.WorkerTopologyContext;",
                "@@ -48,3 +49,2 @@ public class StormMetricRegistry {",
                "     public static <T> SimpleGauge<T>  gauge(T initialValue, String name, String topologyId, String componentId, Integer port){",
                "-        SimpleGauge<T> gauge = new SimpleGauge<>(initialValue);",
                "         String metricName = metricName(name, topologyId, componentId, port);",
                "@@ -53,3 +53,3 @@ public class StormMetricRegistry {",
                "         } else {",
                "-            return REGISTRY.register(metricName, gauge);",
                "+            return REGISTRY.register(metricName, new SimpleGauge<>(initialValue));",
                "         }",
                "@@ -81,3 +81,2 @@ public class StormMetricRegistry {",
                "     public static void start(Map<String, Object> stormConfig, DaemonType type){",
                "-        String localHost = \"localhost\";",
                "         try {",
                "@@ -112,7 +111,3 @@ public class StormMetricRegistry {",
                "         LOG.info(\"Attempting to instantiate reporter class: {}\", clazz);",
                "-        try{",
                "-            reporter = (StormReporter)Metrics2Utils.instantiate(clazz);",
                "-        } catch(Exception e){",
                "-            LOG.warn(\"Unable to instantiate metrics reporter class: {}. Will skip this reporter.\", clazz, e);",
                "-        }",
                "+        reporter = Utils.newInstance(clazz);",
                "         if(reporter != null){",
                "@@ -152,2 +147,11 @@ public class StormMetricRegistry {",
                "+    public static String metricName(String name, TopologyContext context){",
                "+        return String.format(\"storm.topology.%s.%s.%s.%s.%s-%s\",",
                "+                context.getStormId(),",
                "+                hostName,",
                "+                dotToUnderScore(context.getThisComponentId()),",
                "+                context.getThisWorkerPort(),",
                "+                name);",
                "+    }",
                "+",
                "     private static String dotToUnderScore(String str){",
                "diff --git a/storm-core/src/jvm/org/apache/storm/task/TopologyContext.java b/storm-core/src/jvm/org/apache/storm/task/TopologyContext.java",
                "index 444a8a73b..330fee123 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/task/TopologyContext.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/task/TopologyContext.java",
                "@@ -412,3 +412,3 @@ public class TopologyContext extends WorkerTopologyContext implements IMetricsCo",
                "     private String metricName(String name){",
                "-        return String.format(\"storm.topology.%s.%s.%s-%s\", getStormId(), getThisComponentId(), getThisWorkerPort(), name);",
                "+        return StormMetricRegistry.metricName(name, this);",
                "     }"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "storm-core/src/jvm/org/apache/storm/task/TopologyContext.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2152": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2152",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "ceac7b1e698c4eff6210853d16e19ac17acb1cf8",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514285443,
            "hunks": 0,
            "message": "Merge branch '1.x-branch_STORM-2844_ISEEarliest' of https://github.com/hmcl/storm-apache into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2844": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2844",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "06b48fe0454d756c81dbedf906b5ff084a00c233",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513411801,
            "hunks": 0,
            "message": "Merge branch 'STORM-2851' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2851": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2851",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "9aff9f91baafc6f849aa7febb7d0046162c34816",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508302770,
            "hunks": 0,
            "message": "Merge branch 'STORM-2779-1.x' of https://github.com/HeartSaVioR/storm into STORM-2779-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2779": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2779",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "19878bdb05e5eca4bf47012039243bbf9d14ec81",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515011704,
            "hunks": 7,
            "message": "STORM-2876: Work around memory leak, and try to speed up tests",
            "diff": [
                "diff --git a/external/storm-hdfs/pom.xml b/external/storm-hdfs/pom.xml",
                "index 95ce981f5..3fdc80172 100644",
                "--- a/external/storm-hdfs/pom.xml",
                "+++ b/external/storm-hdfs/pom.xml",
                "@@ -223,2 +223,10 @@",
                "         <plugins>",
                "+            <plugin>",
                "+                <groupId>org.apache.maven.plugins</groupId>",
                "+                <artifactId>maven-surefire-plugin</artifactId>",
                "+\t\t<configuration>",
                "+                    <reuseForks>false</reuseForks>",
                "+                    <forkCount>1</forkCount>",
                "+                </configuration>",
                "+            </plugin>",
                "             <plugin>",
                "diff --git a/external/storm-hive/pom.xml b/external/storm-hive/pom.xml",
                "index 62979f012..4637e7fe7 100644",
                "--- a/external/storm-hive/pom.xml",
                "+++ b/external/storm-hive/pom.xml",
                "@@ -158,2 +158,10 @@",
                "     <plugins>",
                "+      <plugin>",
                "+        <groupId>org.apache.maven.plugins</groupId>",
                "+        <artifactId>maven-surefire-plugin</artifactId>",
                "+        <configuration>",
                "+          <reuseForks>true</reuseForks>",
                "+          <forkCount>1</forkCount>",
                "+        </configuration>",
                "+      </plugin>",
                "       <plugin>",
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index 5ee1a6118..acb041bc8 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -135,2 +135,10 @@",
                "         <plugins>",
                "+            <plugin>",
                "+                <groupId>org.apache.maven.plugins</groupId>",
                "+                <artifactId>maven-surefire-plugin</artifactId>",
                "+                <configuration>",
                "+                    <reuseForks>true</reuseForks>",
                "+                    <forkCount>1</forkCount>",
                "+                </configuration>",
                "+            </plugin>",
                "             <plugin>",
                "diff --git a/external/storm-mqtt/pom.xml b/external/storm-mqtt/pom.xml",
                "index 64c1ca633..6c5c48c89 100644",
                "--- a/external/storm-mqtt/pom.xml",
                "+++ b/external/storm-mqtt/pom.xml",
                "@@ -94,3 +94,2 @@",
                "                 <configuration>",
                "-                    <forkMode>perTest</forkMode>",
                "                     <enableAssertions>false</enableAssertions>",
                "@@ -108,3 +107,2 @@",
                "                 <configuration>",
                "-                    <forkMode>perTest</forkMode>",
                "                     <enableAssertions>false</enableAssertions>",
                "diff --git a/integration-test/pom.xml b/integration-test/pom.xml",
                "index 59d1aa62e..236137622 100755",
                "--- a/integration-test/pom.xml",
                "+++ b/integration-test/pom.xml",
                "@@ -113,2 +113,4 @@",
                "                     </properties>",
                "+                    <reuseForks>true</reuseForks>",
                "+                    <forkCount>1</forkCount>",
                "                     <systemPropertyVariables>",
                "diff --git a/pom.xml b/pom.xml",
                "index 543c0d057..8a8c72fa0 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -1091,2 +1091,4 @@",
                "                         <trimStackTrace>false</trimStackTrace>",
                "+                        <forkCount>1.0C</forkCount>",
                "+                        <reuseForks>true</reuseForks>",
                "                     </configuration>"
            ],
            "changed_files": [
                "external/storm-hdfs/pom.xml",
                "external/storm-hive/pom.xml",
                "external/storm-kafka-client/pom.xml",
                "external/storm-mqtt/pom.xml",
                "integration-test/pom.xml",
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2876": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2876",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "fc4ac8e2a950a7f5e3447a29e395d325486a4698",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516771831,
            "hunks": 0,
            "message": "Merge branch 'fix-blob' of https://github.com/danny0405/storm into STORM-2901-FU-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2901": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2901",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "860fe46b96a078767f667560217a582962a3f3cd",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511300575,
            "hunks": 1,
            "message": "[STORM-2829] fix logviewer deepSearch direct self-reference leading to cycle exception",
            "diff": [
                "diff --git a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java",
                "index 24265ac98..3ac225267 100644",
                "--- a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java",
                "+++ b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java",
                "@@ -416,3 +416,3 @@ public class LogviewerLogSearchHandler {",
                "             }",
                "-            currentFileMatch.put(\"port\", truncatePathToLastElements(firstLogAbsPath, 2).getName(0));",
                "+            currentFileMatch.put(\"port\", truncatePathToLastElements(firstLogAbsPath, 2).getName(0).toString());",
                "             newMatches.add(currentFileMatch);"
            ],
            "changed_files": [
                "storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/handler/LogviewerLogSearchHandler.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2829": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2829",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b37d31a0240b4f3b64d5b74025a1649224eeb794",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509830698,
            "hunks": 4,
            "message": "STORM-2799: Exclude jdk.tools for JDK 9 compatibility",
            "diff": [
                "diff --git a/external/storm-hbase/pom.xml b/external/storm-hbase/pom.xml",
                "index 2c97498e3..47eb4481e 100644",
                "--- a/external/storm-hbase/pom.xml",
                "+++ b/external/storm-hbase/pom.xml",
                "@@ -57,2 +57,7 @@",
                "                 </exclusion>",
                "+                <exclusion>",
                "+                    <!-- This is leaking from hadoop-annotations. -->",
                "+                    <groupId>jdk.tools</groupId>",
                "+                    <artifactId>jdk.tools</artifactId>",
                "+                </exclusion>",
                "             </exclusions>",
                "@@ -72,2 +77,7 @@",
                "                 </exclusion>",
                "+                <exclusion>",
                "+                    <!-- This is leaking from hadoop-annotations. -->",
                "+                    <groupId>jdk.tools</groupId>",
                "+                    <artifactId>jdk.tools</artifactId>",
                "+                </exclusion>",
                "             </exclusions>",
                "diff --git a/external/storm-solr/pom.xml b/external/storm-solr/pom.xml",
                "index bd57e0559..f04520753 100644",
                "--- a/external/storm-solr/pom.xml",
                "+++ b/external/storm-solr/pom.xml",
                "@@ -73,2 +73,9 @@",
                "             <scope>test</scope>",
                "+            <exclusions>",
                "+                <exclusion>",
                "+                    <!-- This is leaking from hadoop-annotations. -->",
                "+                    <groupId>jdk.tools</groupId>",
                "+                    <artifactId>jdk.tools</artifactId>",
                "+                </exclusion>",
                "+            </exclusions>",
                "         </dependency>",
                "diff --git a/pom.xml b/pom.xml",
                "index 0b740c754..b3ad464e1 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -281,3 +281,3 @@",
                "         <hdfs.version>${hadoop.version}</hdfs.version>",
                "-        <hbase.version>1.1.0</hbase.version>",
                "+        <hbase.version>1.1.12</hbase.version>",
                "         <kryo.version>3.0.3</kryo.version>"
            ],
            "changed_files": [
                "external/storm-hbase/pom.xml",
                "external/storm-solr/pom.xml",
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2799": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2799",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "0ae5068a205becfafc75bd8e11c2c672c76e0f61",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510329794,
            "hunks": 0,
            "message": "Merge branch 'STORM-2549' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2549": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2549",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f94ecf9541984cab04641bf12c278494359ee0ed",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517498535,
            "hunks": 0,
            "message": "Merge branch 'STORM-2917-1.1.x-merge' into 1.1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2917": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2917",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "c1a1511f164d2f4fd27b4c54595086f4aec25ef2",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517494631,
            "hunks": 0,
            "message": "Merge branch 'STORM-2906-1.x-merge' into 1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2906": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2906",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "24ab227edf2309aaad1bb41145cc11861b8b46eb",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509253617,
            "hunks": 0,
            "message": "Merge branch 'STORM-2791' of https://github.com/danielsd/storm into STORM-2791-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2791": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2791",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "868de5b33b8145d787a9b3d08bdac6908591790d",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513971755,
            "hunks": 10,
            "message": "STORM-2153: address review comments",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/reporters/ScheduledStormReporter.java b/storm-core/src/jvm/org/apache/storm/metrics2/reporters/ScheduledStormReporter.java",
                "index e88b41b2a..b7ffa61fe 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/reporters/ScheduledStormReporter.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/reporters/ScheduledStormReporter.java",
                "@@ -83,3 +83,3 @@ public abstract class ScheduledStormReporter implements StormReporter{",
                "                 } catch (Exception e) {",
                "-                    LOG.warn(\"Unable to instantiate StormMetricsFilter class: {}\", clazz);",
                "+                    throw new RuntimeException(\"Unable to instantiate StormMetricsFilter class: \" + clazz);",
                "                 }",
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java b/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "index ca8568cea..d7cf401be 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "@@ -439,8 +439,13 @@ public class DisruptorQueue implements IStatefulObject {",
                "         _flusher.start();",
                "-        METRICS_TIMER.schedule(new TimerTask(){",
                "-            @Override",
                "-            public void run() {",
                "-                _disruptorMetrics.set(_metrics);",
                "-            }",
                "-        }, 15000, 15000);",
                "+        try {",
                "+            METRICS_TIMER.schedule(new TimerTask() {",
                "+                @Override",
                "+                public void run() {",
                "+                    _disruptorMetrics.set(_metrics);",
                "+                }",
                "+            }, 15000, 15000);",
                "+        } catch (IllegalStateException e){",
                "+            // Ignore. IllegalStateException is thrown by Timer.schedule() if the timer",
                "+            // has been cancelled. (This happens in unit tests)",
                "+        }",
                "     }",
                "@@ -460,2 +465,3 @@ public class DisruptorQueue implements IStatefulObject {",
                "             _metrics.close();",
                "+            METRICS_TIMER.cancel();",
                "         } catch (InsufficientCapacityException e) {",
                "diff --git a/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java b/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "index d7ca48d23..9d9db330e 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "@@ -496,2 +496,8 @@ public class ConfigValidation {",
                "     public static class MetricReportersValidator extends Validator {",
                "+        private static final String NIMBUS = \"nimbus\";",
                "+        private static final String SUPERVISOR = \"supervisor\";",
                "+        private static final String WORKER = \"worker\";",
                "+        private static final String CLASS = \"class\";",
                "+        private static final String FILTER = \"filter\";",
                "+        private static final String DAEMONS = \"daemons\";",
                "@@ -503,6 +509,6 @@ public class ConfigValidation {",
                "             SimpleTypeValidator.validateField(name, Map.class, o);",
                "-            if(!((Map) o).containsKey(\"class\") ) {",
                "+            if(!((Map) o).containsKey(CLASS) ) {",
                "                 throw new IllegalArgumentException( \"Field \" + name + \" must have map entry with key: class\");",
                "             }",
                "-            if(!((Map) o).containsKey(\"daemons\") ) {",
                "+            if(!((Map) o).containsKey(DAEMONS) ) {",
                "                 throw new IllegalArgumentException(\"Field \" + name + \" must have map entry with key: daemons\");",
                "@@ -510,4 +516,4 @@ public class ConfigValidation {",
                "                 // daemons can only be 'nimbus', 'supervisor', or 'worker'",
                "-                Object list = ((Map)o).get(\"daemons\");",
                "-                if(list == null || !(list instanceof List)){",
                "+                Object list = ((Map)o).get(DAEMONS);",
                "+                if(!(list instanceof List)){",
                "                     throw new IllegalArgumentException(\"Field 'daemons' must be a non-null list.\");",
                "@@ -517,5 +523,5 @@ public class ConfigValidation {",
                "                     if (string instanceof String &&",
                "-                            (((String) string).equals(\"nimbus\") ||",
                "-                                    ((String) string).equals(\"supervisor\") ||",
                "-                                    ((String) string).equals(\"worker\"))) {",
                "+                            (string.equals(NIMBUS) ||",
                "+                                    string.equals(SUPERVISOR) ||",
                "+                                    string.equals(WORKER))) {",
                "                         continue;",
                "@@ -527,7 +533,7 @@ public class ConfigValidation {",
                "             }",
                "-            if(((Map)o).containsKey(\"filter\")){",
                "-                Map filterMap = (Map)((Map)o).get(\"filter\");",
                "-                SimpleTypeValidator.validateField(\"class\", String.class, filterMap.get(\"class\"));",
                "+            if(((Map)o).containsKey(FILTER)){",
                "+                Map filterMap = (Map)((Map)o).get(FILTER);",
                "+                SimpleTypeValidator.validateField(CLASS, String.class, filterMap.get(CLASS));",
                "             }",
                "-            SimpleTypeValidator.validateField(name, String.class, ((Map) o).get(\"class\"));",
                "+            SimpleTypeValidator.validateField(name, String.class, ((Map) o).get(CLASS));"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/metrics2/reporters/ScheduledStormReporter.java",
                "storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7ecb3d73e8e909c01d39e03a7a7ed45a2fb81859",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515176629,
            "hunks": 0,
            "message": "Merge branch 'patch-1' of github.com:dvehar/storm Add anchor links to the README This closes #2501",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {
                "2501": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2501",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "163ea7186498c426260fcf1851e12d367ed74d37",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516574993,
            "hunks": 0,
            "message": "Merge branch 'AUTO-CREDS-CLEANUP-MASTER' of https://github.com/omkreddy/storm into STORM-2903-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2903": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2903",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "ee0c36dd3ed5b00e8dbe725d62945bf9702728ad",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516044613,
            "hunks": 0,
            "message": "Merge branch 'STORM-2894' of https://github.com/erikdw/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2894": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2894",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e756889aa712aee22c216bc99ee17b972abf886d",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517062545,
            "hunks": 21,
            "message": "STORM-2913: Add metadata to at-most-once and at-least-once commits",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index 84e785190..9d133a7ea 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -25,4 +25,2 @@ import static org.apache.storm.kafka.spout.KafkaSpoutConfig.FirstPollOffsetStrat",
                "-import com.fasterxml.jackson.core.JsonProcessingException;",
                "-import com.fasterxml.jackson.databind.ObjectMapper;",
                " import com.google.common.annotations.VisibleForTesting;",
                "@@ -55,2 +53,3 @@ import org.apache.storm.kafka.spout.KafkaSpoutConfig.ProcessingGuarantee;",
                " import org.apache.storm.kafka.spout.internal.CommitMetadata;",
                "+import org.apache.storm.kafka.spout.internal.CommitMetadataManager;",
                " import org.apache.storm.kafka.spout.internal.KafkaConsumerFactory;",
                "@@ -74,3 +73,2 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private static final Logger LOG = LoggerFactory.getLogger(KafkaSpout.class);",
                "-    private static final ObjectMapper JSON_MAPPER = new ObjectMapper();",
                "@@ -106,4 +104,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private transient TopologyContext context;",
                "-    // Metadata information to commit to Kafka. It is unique per spout per topology.",
                "-    private transient String commitMetadata;",
                "+    private transient CommitMetadataManager commitMetadataManager;",
                "     private transient KafkaOffsetMetric kafkaOffsetMetric;",
                "@@ -144,3 +141,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         waitingToEmit = new HashMap<>();",
                "-        setCommitMetadata(context);",
                "+        commitMetadataManager = new CommitMetadataManager(context, kafkaSpoutConfig.getProcessingGuarantee());",
                "@@ -156,3 +153,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         LOG.info(\"Registering Spout Metrics\");",
                "-        kafkaOffsetMetric = new KafkaOffsetMetric(() -> offsetManagers, () -> kafkaConsumer);",
                "+        kafkaOffsetMetric = new KafkaOffsetMetric(() -> Collections.unmodifiableMap(offsetManagers), () -> kafkaConsumer);",
                "         context.registerMetric(\"kafkaOffset\", kafkaOffsetMetric, kafkaSpoutConfig.getMetricsTimeBucketSizeInSecs());",
                "@@ -170,12 +167,2 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    private void setCommitMetadata(TopologyContext context) {",
                "-        try {",
                "-            commitMetadata = JSON_MAPPER.writeValueAsString(new CommitMetadata(",
                "-                context.getStormId(), context.getThisTaskId(), Thread.currentThread().getName()));",
                "-        } catch (JsonProcessingException e) {",
                "-            LOG.error(\"Failed to create Kafka commit metadata due to JSON serialization error\", e);",
                "-            throw new RuntimeException(e);",
                "-        }",
                "-    }",
                "-",
                "     private boolean isAtLeastOnceProcessing() {",
                "@@ -217,4 +204,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 /*",
                "-                 * Emitted messages for partitions that are no longer assigned to this spout can't",
                "-                 * be acked and should not be retried, hence remove them from emitted collection.",
                "+                 * Emitted messages for partitions that are no longer assigned to this spout can't be acked and should not be retried, hence",
                "+                 * remove them from emitted collection.",
                "                  */",
                "@@ -248,3 +235,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 // offset was previously committed for this consumer group and topic-partition, either by this or another topology.",
                "-                if (isOffsetCommittedByThisTopology(newTp, committedOffset)) {",
                "+                if (commitMetadataManager.isOffsetCommittedByThisTopology(newTp, committedOffset, Collections.unmodifiableMap(offsetManagers))) {",
                "                     // Another KafkaSpout instance (of this topology) already committed, therefore FirstPollOffsetStrategy does not apply.",
                "@@ -276,27 +263,2 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    /**",
                "-     * Checks if {@link OffsetAndMetadata} was committed by a {@link KafkaSpout} instance in this topology. This info is used to decide if",
                "-     * {@link FirstPollOffsetStrategy} should be applied",
                "-     *",
                "-     * @param tp topic-partition",
                "-     * @param committedOffset {@link OffsetAndMetadata} info committed to Kafka",
                "-     * @return true if this topology committed this {@link OffsetAndMetadata}, false otherwise",
                "-     */",
                "-    private boolean isOffsetCommittedByThisTopology(TopicPartition tp, OffsetAndMetadata committedOffset) {",
                "-        try {",
                "-            if (offsetManagers.containsKey(tp) && offsetManagers.get(tp).hasCommitted()) {",
                "-                return true;",
                "-            }",
                "-",
                "-            final CommitMetadata committedMetadata = JSON_MAPPER.readValue(committedOffset.metadata(), CommitMetadata.class);",
                "-            return committedMetadata.getTopologyId().equals(context.getStormId());",
                "-        } catch (IOException e) {",
                "-            LOG.warn(\"Failed to deserialize [{}]. Error likely occurred because the last commit \"",
                "-                + \"for this topic-partition was done using an earlier version of Storm. \"",
                "-                + \"Defaulting to behavior compatible with earlier version\", committedOffset);",
                "-            LOG.trace(\"\", e);",
                "-            return false;",
                "-        }",
                "-    }",
                "-",
                "     // ======== Next Tuple =======",
                "@@ -313,3 +275,6 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 } else if (kafkaSpoutConfig.getProcessingGuarantee() == ProcessingGuarantee.NO_GUARANTEE) {",
                "-                    commitFetchedOffsetsAsync(kafkaConsumer.assignment());",
                "+                    Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = ",
                "+                        createFetchedOffsetsMetadata(kafkaConsumer.assignment());",
                "+                    kafkaConsumer.commitAsync(offsetsToCommit, null);",
                "+                    LOG.debug(\"Committed offsets {} to Kafka\", offsetsToCommit);",
                "                 }",
                "@@ -398,3 +363,6 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                 //Commit polled records immediately to ensure delivery is at-most-once.",
                "-                kafkaConsumer.commitSync();",
                "+                Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = ",
                "+                    createFetchedOffsetsMetadata(kafkaConsumer.assignment());",
                "+                kafkaConsumer.commitSync(offsetsToCommit);",
                "+                LOG.debug(\"Committed offsets {} to Kafka\", offsetsToCommit);",
                "             }",
                "@@ -471,7 +439,10 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "             final OffsetAndMetadata committedOffset = kafkaConsumer.committed(tp);",
                "-            if (committedOffset != null && isOffsetCommittedByThisTopology(tp, committedOffset)",
                "-                && committedOffset.offset() > record.offset()) {",
                "+            if (isAtLeastOnceProcessing()",
                "+                && committedOffset != null ",
                "+                && committedOffset.offset() > record.offset()",
                "+                && commitMetadataManager.isOffsetCommittedByThisTopology(tp, committedOffset, Collections.unmodifiableMap(offsetManagers))) {",
                "                 // Ensures that after a topology with this id is started, the consumer fetch",
                "                 // position never falls behind the committed offset (STORM-2844)",
                "-                throw new IllegalStateException(\"Attempting to emit a message that has already been committed.\");",
                "+                throw new IllegalStateException(\"Attempting to emit a message that has already been committed.\"",
                "+                    + \" This should never occur when using the at-least-once processing guarantee.\");",
                "             }",
                "@@ -521,9 +492,8 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    private void commitFetchedOffsetsAsync(Set<TopicPartition> assignedPartitions) {",
                "+    private Map<TopicPartition, OffsetAndMetadata> createFetchedOffsetsMetadata(Set<TopicPartition> assignedPartitions) {",
                "         Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>();",
                "         for (TopicPartition tp : assignedPartitions) {",
                "-            offsetsToCommit.put(tp, new OffsetAndMetadata(kafkaConsumer.position(tp)));",
                "+            offsetsToCommit.put(tp, new OffsetAndMetadata(kafkaConsumer.position(tp), commitMetadataManager.getCommitMetadata()));",
                "         }",
                "-        kafkaConsumer.commitAsync(offsetsToCommit, null);",
                "-        LOG.debug(\"Committed offsets {} to Kafka\", offsetsToCommit);",
                "+        return offsetsToCommit;",
                "     }",
                "@@ -538,3 +508,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "         for (Map.Entry<TopicPartition, OffsetManager> tpOffset : assignedOffsetManagers.entrySet()) {",
                "-            final OffsetAndMetadata nextCommitOffset = tpOffset.getValue().findNextCommitOffset(commitMetadata);",
                "+            final OffsetAndMetadata nextCommitOffset = tpOffset.getValue().findNextCommitOffset(commitMetadataManager.getCommitMetadata());",
                "             if (nextCommitOffset != null) {",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "index c2305cb63..40e449a37 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "@@ -476,3 +476,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "                  */",
                "-                LOG.info(\"Setting consumer property '{}' to 'earliest' to ensure at-least-once processing\",",
                "+                LOG.info(\"Setting Kafka consumer property '{}' to 'earliest' to ensure at-least-once processing\",",
                "                     ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);",
                "@@ -490,3 +490,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         }",
                "-        LOG.info(\"Setting consumer property '{}' to 'false', because the spout does not support auto-commit\",",
                "+        LOG.info(\"Setting Kafka consumer property '{}' to 'false', because the spout does not support auto-commit\",",
                "             ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG);",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/CommitMetadataManager.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/CommitMetadataManager.java",
                "new file mode 100644",
                "index 000000000..a63619c6e",
                "--- /dev/null",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/CommitMetadataManager.java",
                "@@ -0,0 +1,91 @@",
                "+/*",
                "+ * Copyright 2018 The Apache Software Foundation.",
                "+ *",
                "+ * Licensed under the Apache License, Version 2.0 (the \"License\");",
                "+ * you may not use this file except in compliance with the License.",
                "+ * You may obtain a copy of the License at",
                "+ *",
                "+ *      http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.kafka.spout.internal;",
                "+",
                "+import com.fasterxml.jackson.core.JsonProcessingException;",
                "+import com.fasterxml.jackson.databind.ObjectMapper;",
                "+import java.io.IOException;",
                "+import java.util.Map;",
                "+import org.apache.kafka.clients.consumer.OffsetAndMetadata;",
                "+import org.apache.kafka.common.TopicPartition;",
                "+import org.apache.storm.kafka.spout.KafkaSpout;",
                "+import org.apache.storm.kafka.spout.KafkaSpoutConfig.ProcessingGuarantee;",
                "+import org.apache.storm.task.TopologyContext;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+/**",
                "+ * Generates and reads commit metadata.",
                "+ */",
                "+public final class CommitMetadataManager {",
                "+",
                "+    private static final ObjectMapper JSON_MAPPER = new ObjectMapper();",
                "+    private static final Logger LOG = LoggerFactory.getLogger(CommitMetadataManager.class);",
                "+    // Metadata information to commit to Kafka. It is unique per spout instance.",
                "+    private final String commitMetadata;",
                "+    private final ProcessingGuarantee processingGuarantee;",
                "+    private final TopologyContext context;",
                "+",
                "+    /**",
                "+     * Create a manager with the given context.",
                "+     */",
                "+    public CommitMetadataManager(TopologyContext context, ProcessingGuarantee processingGuarantee) {",
                "+        this.context = context;",
                "+        try {",
                "+            commitMetadata = JSON_MAPPER.writeValueAsString(new CommitMetadata(",
                "+                context.getStormId(), context.getThisTaskId(), Thread.currentThread().getName()));",
                "+            this.processingGuarantee = processingGuarantee;",
                "+        } catch (JsonProcessingException e) {",
                "+            LOG.error(\"Failed to create Kafka commit metadata due to JSON serialization error\", e);",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Checks if {@link OffsetAndMetadata} was committed by a {@link KafkaSpout} instance in this topology.",
                "+     *",
                "+     * @param tp The topic partition the commit metadata belongs to.",
                "+     * @param committedOffset {@link OffsetAndMetadata} info committed to Kafka",
                "+     * @param offsetManagers The offset managers.",
                "+     * @return true if this topology committed this {@link OffsetAndMetadata}, false otherwise",
                "+     */",
                "+    public boolean isOffsetCommittedByThisTopology(TopicPartition tp, OffsetAndMetadata committedOffset,",
                "+        Map<TopicPartition, OffsetManager> offsetManagers) {",
                "+        try {",
                "+            if (processingGuarantee == ProcessingGuarantee.AT_LEAST_ONCE",
                "+                && offsetManagers.containsKey(tp)",
                "+                && offsetManagers.get(tp).hasCommitted()) {",
                "+                return true;",
                "+            }",
                "+",
                "+            final CommitMetadata committedMetadata = JSON_MAPPER.readValue(committedOffset.metadata(), CommitMetadata.class);",
                "+            return committedMetadata.getTopologyId().equals(context.getStormId());",
                "+        } catch (IOException e) {",
                "+            LOG.warn(\"Failed to deserialize expected commit metadata [{}].\"",
                "+                + \" This error is expected to occur once per partition, if the last commit to each partition\"",
                "+                + \" was by an earlier version of the KafkaSpout, or by a process other than the KafkaSpout. \"",
                "+                + \"Defaulting to behavior compatible with earlier version\", committedOffset);",
                "+            LOG.trace(\"\", e);",
                "+            return false;",
                "+        }",
                "+    }",
                "+",
                "+    public String getCommitMetadata() {",
                "+        return commitMetadata;",
                "+    }",
                "+",
                "+}"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/CommitMetadataManager.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2913": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2913",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "566db743de29d8463c9a8c68a359cb34ce01d900",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510152924,
            "hunks": 0,
            "message": "Merge branch 'master' of https://github.com/lawrencecraft/storm Fixing markdown for HBase bolt README in Storm This closes #2396",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {
                "2396": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2396",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "becab7cc2de23bb7754a7bf438be2270101eecf0",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511568551,
            "hunks": 0,
            "message": "Merge branch 'STORM-2813' of https://github.com/revans2/incubator-storm into STORM-2813-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2813": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2813",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7e1771220969c68e20c4905d9c6afdfbf7a72bc8",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510597417,
            "hunks": 0,
            "message": "Merge branch 'STORM-2525-1.x' of https://github.com/srdo/storm into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2525": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2525",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5b507eaf03d9e5304e1025233b86766b27eadd78",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517545134,
            "hunks": 0,
            "message": "Merge branch 'STORM-2853-1.0.x-merge' into 1.0.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2853": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2853",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "deb960fd43e5d1be26fa68e6c4571d24848c587f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516745600,
            "hunks": 3,
            "message": "STORM-2907: Exclude curator dependencies from storm-core in storm-autocreds pom",
            "diff": [
                "diff --git a/external/storm-autocreds/pom.xml b/external/storm-autocreds/pom.xml",
                "index 30c10e7d9..81b869ef7 100644",
                "--- a/external/storm-autocreds/pom.xml",
                "+++ b/external/storm-autocreds/pom.xml",
                "@@ -42,2 +42,14 @@",
                "                 </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.apache.curator</groupId>",
                "+                    <artifactId>curator-client</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.apache.curator</groupId>",
                "+                    <artifactId>curator-recipes</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.apache.curator</groupId>",
                "+                    <artifactId>curator-framework</artifactId>",
                "+                </exclusion>",
                "             </exclusions>",
                "@@ -146,2 +158,2 @@",
                "     </build>",
                "-</project>",
                "\\ No newline at end of file",
                "+</project>"
            ],
            "changed_files": [
                "external/storm-autocreds/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2907": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2907",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f72ab36f62f9aae560035ffad6a25f7cba8d5efd",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510788229,
            "hunks": 0,
            "message": "Merge branch 'STORM-2814-1.1.x-merge' into 1.1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2814": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2814",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b257ba47aae42d5486901a1252cd9b5c0d9ad70e",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515707542,
            "hunks": 10,
            "message": "STORM-2153: move task-metrics from executor to task to avoid map lookup",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index 3dd7289e6..3af9b2c4d 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -267,3 +267,2 @@",
                "      :task->component (:task->component worker)",
                "-     :task-metrics (TaskMetrics/taskMetricsMap (first task-ids) (last task-ids) worker-context component-id)",
                "      :stream->component->grouper (outbound-components worker-context component-id storm-conf)",
                "@@ -445,3 +444,3 @@",
                "     (when time-delta",
                "-      (stats/spout-failed-tuple! (:stats executor-data)  (.getFailed ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-failed-tuple! (:stats executor-data)  (.getFailed ^TaskMetrics (:task-metrics task-data) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "@@ -454,3 +453,3 @@",
                "     (when time-delta",
                "-      (stats/spout-acked-tuple! (:stats executor-data) (.getAcked ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-acked-tuple! (:stats executor-data) (.getAcked ^TaskMetrics (:task-metrics task-data) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "@@ -823,3 +822,3 @@",
                "                              (stats/bolt-acked-tuple! executor-stats",
                "-                                                      (.getAcked ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) (.getSourceStreamId tuple))",
                "+                                                      (.getAcked ^TaskMetrics (:task-metrics task-data) (.getSourceStreamId tuple))",
                "                                                       (.getSourceComponent tuple)",
                "@@ -839,3 +838,3 @@",
                "                              (stats/bolt-failed-tuple! executor-stats",
                "-                                                       (.getFailed ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) (.getSourceStreamId tuple))",
                "+                                                       (.getFailed ^TaskMetrics (:task-metrics task-data) (.getSourceStreamId tuple))",
                "                                                        (.getSourceComponent tuple)",
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/task.clj b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "index 68af75bb6..82bd2c57e 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "@@ -145,5 +145,5 @@",
                "             (when (emit-sampler)",
                "-              (stats/emitted-tuple! executor-stats (.getEmitted ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) stream) stream)",
                "+              (stats/emitted-tuple! executor-stats (.getEmitted ^TaskMetrics (:task-metrics task-data) stream) stream)",
                "               (if out-task-id",
                "-                (stats/transferred-tuples! executor-stats (.getTransferred ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) stream) stream 1)))",
                "+                (stats/transferred-tuples! executor-stats (.getTransferred ^TaskMetrics (:task-metrics task-data) stream) stream 1)))",
                "             (if out-task-id [out-task-id])",
                "@@ -165,4 +165,4 @@",
                "              (when (emit-sampler)",
                "-               (stats/emitted-tuple! executor-stats (.getEmitted ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) stream) stream)",
                "-               (stats/transferred-tuples! executor-stats (.getTransferred ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) stream) stream (count out-tasks)))",
                "+               (stats/emitted-tuple! executor-stats (.getEmitted ^TaskMetrics (:task-metrics task-data) stream) stream)",
                "+               (stats/transferred-tuples! executor-stats (.getTransferred ^TaskMetrics (:task-metrics task-data) stream) stream (count out-tasks)))",
                "              out-tasks)))",
                "@@ -177,2 +177,3 @@",
                "     :builtin-metrics (builtin-metrics/make-data (:type executor-data) (:stats executor-data))",
                "+    :task-metrics (TaskMetrics. (:worker-context executor-data) (:component-id executor-data) task-id)",
                "     :tasks-fn (mk-tasks-fn <>)",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java b/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java",
                "index 550b17688..05c62daf5 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java",
                "@@ -80,10 +80,2 @@ public class TaskMetrics {",
                "     }",
                "-",
                "-    public static Map<Integer, TaskMetrics> taskMetricsMap(Integer startTaskId, Integer endTaskId, WorkerTopologyContext context, String componentId){",
                "-        Map<Integer, TaskMetrics> retval = new HashMap<>();",
                "-        for (int i = startTaskId; i < endTaskId + 1; i++) {",
                "-            retval.put(i, new TaskMetrics(context, componentId, i));",
                "-        }",
                "-        return retval;",
                "-    }",
                " }"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "c3ae8b97d2d11e008402f5e8d30692df830be8d4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511977392,
            "hunks": 0,
            "message": "Merge branch 'STORM-2827' of https://github.com/Ethanlm/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2827": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2827",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "3578cf8d4f4f541c1ba3e0e0d9410426908114f4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510172234,
            "hunks": 2,
            "message": "[STORM-2804] Fix TopoCache is not caching ACLs correctly problem",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/TopoCache.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/TopoCache.java",
                "index c7387f929..2b9de22f8 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/TopoCache.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/TopoCache.java",
                "@@ -126,4 +126,5 @@ public class TopoCache {",
                "         final List<AccessControl> acl = BlobStoreAclHandler.DEFAULT;",
                "-        store.createBlob(key, Utils.serialize(topo), new SettableBlobMeta(acl), who);",
                "-        topos.put(topoId, new WithAcl<>(acl, topo));",
                "+        SettableBlobMeta meta = new SettableBlobMeta(acl);",
                "+        store.createBlob(key, Utils.serialize(topo), meta, who);",
                "+        topos.put(topoId, new WithAcl<>(meta.get_acl(), topo));",
                "     }",
                "@@ -208,4 +209,5 @@ public class TopoCache {",
                "         final List<AccessControl> acl = BlobStoreAclHandler.DEFAULT;",
                "-        store.createBlob(key, Utils.toCompressedJsonConf(topoConf), new SettableBlobMeta(acl), who);",
                "-        confs.put(topoId, new WithAcl<>(acl, topoConf));",
                "+        SettableBlobMeta meta = new SettableBlobMeta(acl);",
                "+        store.createBlob(key, Utils.toCompressedJsonConf(topoConf), meta, who);",
                "+        confs.put(topoId, new WithAcl<>(meta.get_acl(), topoConf));",
                "     }"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/TopoCache.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2804": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2804",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d3a74eec58a918fbc9414f59edc1189e40d38650",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517493872,
            "hunks": 0,
            "message": "Merge branch 'STORM-2903' of https://github.com/omkreddy/storm into STORM-2903-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2903": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2903",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "2a01ac78203582774e45f46a874c7f1a8893e4b4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517498555,
            "hunks": 0,
            "message": "Merge branch 'STORM-2917-1.0.x-merge' into 1.0.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2917": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2917",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "a35dcc40cc9e66636a54bd02480ef1508e61f5d4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516987613,
            "hunks": 0,
            "message": "Merge branch 'STORM-2912-1.x' of github.com:HeartSaVioR/storm into 1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2912": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2912",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "25fa9dd7cc2bcb63460d37dcb317b1f55bf10496",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516613253,
            "hunks": 0,
            "message": "Merge branch 'STORM-2862-1.x' of https://github.com/hmcc/storm into STORM-2862-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2862": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2862",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1b0e88e2f5fc5919089cb613d16eb8ff3f0838d9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517498849,
            "hunks": 0,
            "message": "Merge branch 'STORM-2877-1.x' of https://github.com/srishtyagrawal/storm into STORM-2877-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2877": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2877",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "bce45993f8622e4d3e9ccba96cc78e4ef76e48ae",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516817327,
            "hunks": 40,
            "message": "[maven-release-plugin] prepare release v1.0.6",
            "diff": [
                "diff --git a/examples/storm-starter/pom.xml b/examples/storm-starter/pom.xml",
                "index 28a0036f2..d53be60e1 100644",
                "--- a/examples/storm-starter/pom.xml",
                "+++ b/examples/storm-starter/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.0.6-SNAPSHOT</version>",
                "+      <version>1.0.6</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-core/pom.xml b/external/flux/flux-core/pom.xml",
                "index 823a92199..c2a77df55 100644",
                "--- a/external/flux/flux-core/pom.xml",
                "+++ b/external/flux/flux-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-examples/pom.xml b/external/flux/flux-examples/pom.xml",
                "index ee7829716..b02c4fd09 100644",
                "--- a/external/flux/flux-examples/pom.xml",
                "+++ b/external/flux/flux-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-wrappers/pom.xml b/external/flux/flux-wrappers/pom.xml",
                "index 18cf9f7b8..0b5a18d89 100644",
                "--- a/external/flux/flux-wrappers/pom.xml",
                "+++ b/external/flux/flux-wrappers/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/pom.xml b/external/flux/pom.xml",
                "index 279fb113d..e290736b9 100644",
                "--- a/external/flux/pom.xml",
                "+++ b/external/flux/pom.xml",
                "@@ -28,3 +28,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/pom.xml b/external/sql/pom.xml",
                "index 7397a7ee4..0c7a06860 100644",
                "--- a/external/sql/pom.xml",
                "+++ b/external/sql/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-core/pom.xml b/external/sql/storm-sql-core/pom.xml",
                "index f2a167558..8ec571138 100644",
                "--- a/external/sql/storm-sql-core/pom.xml",
                "+++ b/external/sql/storm-sql-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-kafka/pom.xml b/external/sql/storm-sql-kafka/pom.xml",
                "index 6bb8ad814..1d4d3e50e 100644",
                "--- a/external/sql/storm-sql-kafka/pom.xml",
                "+++ b/external/sql/storm-sql-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-runtime/pom.xml b/external/sql/storm-sql-runtime/pom.xml",
                "index 3cb090444..717ff8b46 100644",
                "--- a/external/sql/storm-sql-runtime/pom.xml",
                "+++ b/external/sql/storm-sql-runtime/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/storm-cassandra/pom.xml b/external/storm-cassandra/pom.xml",
                "index 129ad0243..6a5377361 100644",
                "--- a/external/storm-cassandra/pom.xml",
                "+++ b/external/storm-cassandra/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-elasticsearch/pom.xml b/external/storm-elasticsearch/pom.xml",
                "index b972ea66f..163b0dedc 100644",
                "--- a/external/storm-elasticsearch/pom.xml",
                "+++ b/external/storm-elasticsearch/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-eventhubs/pom.xml b/external/storm-eventhubs/pom.xml",
                "index 7fdd2212a..8b28ffb50 100755",
                "--- a/external/storm-eventhubs/pom.xml",
                "+++ b/external/storm-eventhubs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "@@ -28,3 +28,3 @@",
                "     <artifactId>storm-eventhubs</artifactId>",
                "-    <version>1.0.6-SNAPSHOT</version>",
                "+    <version>1.0.6</version>",
                "     <packaging>jar</packaging>",
                "diff --git a/external/storm-hbase/pom.xml b/external/storm-hbase/pom.xml",
                "index 15a97bb6a..f563d6c50 100644",
                "--- a/external/storm-hbase/pom.xml",
                "+++ b/external/storm-hbase/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hdfs/pom.xml b/external/storm-hdfs/pom.xml",
                "index 75d73c21a..7e77daac1 100644",
                "--- a/external/storm-hdfs/pom.xml",
                "+++ b/external/storm-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hive/pom.xml b/external/storm-hive/pom.xml",
                "index b98aacfad..d4547bb0c 100644",
                "--- a/external/storm-hive/pom.xml",
                "+++ b/external/storm-hive/pom.xml",
                "@@ -23,3 +23,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.0.6-SNAPSHOT</version>",
                "+    <version>1.0.6</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jdbc/pom.xml b/external/storm-jdbc/pom.xml",
                "index 8c50fcb2e..e139dc67d 100644",
                "--- a/external/storm-jdbc/pom.xml",
                "+++ b/external/storm-jdbc/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jms/core/pom.xml b/external/storm-jms/core/pom.xml",
                "index 264535095..559350bc1 100644",
                "--- a/external/storm-jms/core/pom.xml",
                "+++ b/external/storm-jms/core/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <artifactId>storm-jms-parent</artifactId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/storm-jms/examples/pom.xml b/external/storm-jms/examples/pom.xml",
                "index 1b32272fc..35e79bea5 100644",
                "--- a/external/storm-jms/examples/pom.xml",
                "+++ b/external/storm-jms/examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>storm-jms-parent</artifactId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/storm-jms/pom.xml b/external/storm-jms/pom.xml",
                "index 95b493e11..823ae82dd 100644",
                "--- a/external/storm-jms/pom.xml",
                "+++ b/external/storm-jms/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index 14f6c5e8a..8c59e860a 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -24,3 +24,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka/pom.xml b/external/storm-kafka/pom.xml",
                "index 0881768df..4e89bf5cb 100644",
                "--- a/external/storm-kafka/pom.xml",
                "+++ b/external/storm-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-metrics/pom.xml b/external/storm-metrics/pom.xml",
                "index c7102402f..bca465fa6 100644",
                "--- a/external/storm-metrics/pom.xml",
                "+++ b/external/storm-metrics/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.0.6-SNAPSHOT</version>",
                "+      <version>1.0.6</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mongodb/pom.xml b/external/storm-mongodb/pom.xml",
                "index 238cb96be..adc9b32eb 100644",
                "--- a/external/storm-mongodb/pom.xml",
                "+++ b/external/storm-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mqtt/core/pom.xml b/external/storm-mqtt/core/pom.xml",
                "index 967e3a811..d45c12070 100644",
                "--- a/external/storm-mqtt/core/pom.xml",
                "+++ b/external/storm-mqtt/core/pom.xml",
                "@@ -27,3 +27,3 @@",
                "         <artifactId>storm-mqtt-parent</artifactId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/storm-mqtt/examples/pom.xml b/external/storm-mqtt/examples/pom.xml",
                "index 0c2850bab..e42e319b2 100644",
                "--- a/external/storm-mqtt/examples/pom.xml",
                "+++ b/external/storm-mqtt/examples/pom.xml",
                "@@ -28,3 +28,3 @@",
                "     <artifactId>storm-mqtt-parent</artifactId>",
                "-    <version>1.0.6-SNAPSHOT</version>",
                "+    <version>1.0.6</version>",
                "     <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/storm-mqtt/pom.xml b/external/storm-mqtt/pom.xml",
                "index be7e0285d..f5c9dae4f 100644",
                "--- a/external/storm-mqtt/pom.xml",
                "+++ b/external/storm-mqtt/pom.xml",
                "@@ -22,3 +22,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.0.6-SNAPSHOT</version>",
                "+    <version>1.0.6</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-redis/pom.xml b/external/storm-redis/pom.xml",
                "index abd28a2e7..8e2e02d0b 100644",
                "--- a/external/storm-redis/pom.xml",
                "+++ b/external/storm-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-solr/pom.xml b/external/storm-solr/pom.xml",
                "index 9d1158956..dbc26cdde 100644",
                "--- a/external/storm-solr/pom.xml",
                "+++ b/external/storm-solr/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/pom.xml b/pom.xml",
                "index 1936db05e..f9e3db3ac 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -29,3 +29,3 @@",
                "     <artifactId>storm</artifactId>",
                "-    <version>1.0.6-SNAPSHOT</version>",
                "+    <version>1.0.6</version>",
                "     <packaging>pom</packaging>",
                "@@ -186,3 +186,3 @@",
                "         <developerConnection>scm:git:https://git-wip-us.apache.org/repos/asf/storm.git</developerConnection>",
                "-        <tag>HEAD</tag>",
                "+        <tag>v1.0.6</tag>",
                "         <url>https://git-wip-us.apache.org/repos/asf/storm</url>",
                "diff --git a/storm-buildtools/maven-shade-clojure-transformer/pom.xml b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "index e01105ffe..b2315e469 100644",
                "--- a/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "+++ b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-buildtools/storm-maven-plugins/pom.xml b/storm-buildtools/storm-maven-plugins/pom.xml",
                "index 8837d3cb5..aeb357935 100644",
                "--- a/storm-buildtools/storm-maven-plugins/pom.xml",
                "+++ b/storm-buildtools/storm-maven-plugins/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index 7f01254bf..568020080 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>..</relativePath>",
                "diff --git a/storm-dist/binary/pom.xml b/storm-dist/binary/pom.xml",
                "index 9525ee216..d7690d6ca 100644",
                "--- a/storm-dist/binary/pom.xml",
                "+++ b/storm-dist/binary/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-dist/source/pom.xml b/storm-dist/source/pom.xml",
                "index be862cb52..01ced527e 100644",
                "--- a/storm-dist/source/pom.xml",
                "+++ b/storm-dist/source/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/javascript/pom.xml b/storm-multilang/javascript/pom.xml",
                "index e2aa1dd04..f09caad6a 100644",
                "--- a/storm-multilang/javascript/pom.xml",
                "+++ b/storm-multilang/javascript/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/python/pom.xml b/storm-multilang/python/pom.xml",
                "index 7d2718597..dc6b224e9 100644",
                "--- a/storm-multilang/python/pom.xml",
                "+++ b/storm-multilang/python/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/ruby/pom.xml b/storm-multilang/ruby/pom.xml",
                "index 0bceb68d1..c474e4cca 100644",
                "--- a/storm-multilang/ruby/pom.xml",
                "+++ b/storm-multilang/ruby/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.0.6-SNAPSHOT</version>",
                "+        <version>1.0.6</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-rename-hack/pom.xml b/storm-rename-hack/pom.xml",
                "index 834230beb..263df5c35 100644",
                "--- a/storm-rename-hack/pom.xml",
                "+++ b/storm-rename-hack/pom.xml",
                "@@ -22,3 +22,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.0.6-SNAPSHOT</version>",
                "+    <version>1.0.6</version>",
                "     <relativePath>../pom.xml</relativePath>"
            ],
            "changed_files": [
                "examples/storm-starter/pom.xml",
                "external/flux/flux-core/pom.xml",
                "external/flux/flux-examples/pom.xml",
                "external/flux/flux-wrappers/pom.xml",
                "external/flux/pom.xml",
                "external/sql/pom.xml",
                "external/sql/storm-sql-core/pom.xml",
                "external/sql/storm-sql-kafka/pom.xml",
                "external/sql/storm-sql-runtime/pom.xml",
                "external/storm-cassandra/pom.xml",
                "external/storm-elasticsearch/pom.xml",
                "external/storm-eventhubs/pom.xml",
                "external/storm-hbase/pom.xml",
                "external/storm-hdfs/pom.xml",
                "external/storm-hive/pom.xml",
                "external/storm-jdbc/pom.xml",
                "external/storm-jms/core/pom.xml",
                "external/storm-jms/examples/pom.xml",
                "external/storm-jms/pom.xml",
                "external/storm-kafka-client/pom.xml",
                "external/storm-kafka/pom.xml",
                "external/storm-metrics/pom.xml",
                "external/storm-mongodb/pom.xml",
                "external/storm-mqtt/core/pom.xml",
                "external/storm-mqtt/examples/pom.xml",
                "external/storm-mqtt/pom.xml",
                "external/storm-redis/pom.xml",
                "external/storm-solr/pom.xml",
                "pom.xml",
                "storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "storm-buildtools/storm-maven-plugins/pom.xml",
                "storm-core/pom.xml",
                "storm-dist/binary/pom.xml",
                "storm-dist/source/pom.xml",
                "storm-multilang/javascript/pom.xml",
                "storm-multilang/python/pom.xml",
                "storm-multilang/ruby/pom.xml",
                "storm-rename-hack/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "fe9819275e566a164d90cca6c46e7f06d4b7706c"
                ]
            ],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "a307ed58d7b103acf0757fbaa37ccd377bde43d1",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513331095,
            "hunks": 0,
            "message": "Merge branch 'STORM-2855-1.1.x-merge' into 1.1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2855": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2855",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8aae491b6b1e2be2a085f61d45d5d915203eb0ee",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513180051,
            "hunks": 58,
            "message": "STORM-2859: Fix a number of issues with NormalizedResources when resource totals are zero",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java b/storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java",
                "index 9aca91c22..d660ab010 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java",
                "@@ -198,6 +198,6 @@ public class BlobStoreUtils {",
                "                 // nimbodes might throw an exception.",
                "-                LOG.info(\"KeyNotFoundException {}\", knf);",
                "+                LOG.info(\"KeyNotFoundException\", knf);",
                "             } catch (Exception exp) {",
                "                 // Logging an exception while client is connecting",
                "-                LOG.error(\"Exception {}\", exp);",
                "+                LOG.error(\"Exception\", exp);",
                "             }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java",
                "index 262365552..58f12d047 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java",
                "@@ -36,3 +36,3 @@ public class NormalizedResourceOffer extends NormalizedResources {",
                "     /**",
                "-     * Create a new normalized set of resources.  Note that memory is not covered here becasue it is not consistent in requests vs offers",
                "+     * Create a new normalized set of resources.  Note that memory is not covered here because it is not consistent in requests vs offers",
                "      * because of how on heap vs off heap is used.",
                "@@ -43,2 +43,3 @@ public class NormalizedResourceOffer extends NormalizedResources {",
                "         super(resources, null);",
                "+        totalMemory = getNormalizedResources().getOrDefault(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, 0.0);",
                "     }",
                "@@ -46,3 +47,3 @@ public class NormalizedResourceOffer extends NormalizedResources {",
                "     public NormalizedResourceOffer() {",
                "-        super(null, null);",
                "+        this((Map<String, ? extends Number>)null);",
                "     }",
                "@@ -54,7 +55,2 @@ public class NormalizedResourceOffer extends NormalizedResources {",
                "-    @Override",
                "-    protected void initializeMemory(Map<String, Double> normalizedResources) {",
                "-        totalMemory = normalizedResources.getOrDefault(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME, 0.0);",
                "-    }",
                "-",
                "     @Override",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java",
                "index 926184c35..596375003 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java",
                "@@ -112,3 +112,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "     /**",
                "-     * Create a new normalized set of resources.  Note that memory is not covered here becasue it is not consistent in requests vs offers",
                "+     * Create a new normalized set of resources.  Note that memory is not covered here because it is not consistent in requests vs offers",
                "      * because of how on heap vs off heap is used.",
                "@@ -121,2 +121,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "         super(resources, getDefaultResources(topologyConf));",
                "+        initializeMemory(getNormalizedResources());",
                "     }",
                "@@ -133,2 +134,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "         super(null, null);",
                "+        initializeMemory(getNormalizedResources());",
                "     }",
                "@@ -143,4 +145,3 @@ public class NormalizedResourceRequest extends NormalizedResources {",
                "-    @Override",
                "-    protected void initializeMemory(Map<String, Double> normalizedResources) {",
                "+    private void initializeMemory(Map<String, Double> normalizedResources) {",
                "         onHeap = normalizedResources.getOrDefault(Constants.COMMON_ONHEAP_MEMORY_RESOURCE_NAME, 0.0);",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "index eea38cff9..846ab6945 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "@@ -39,2 +39,3 @@ import org.slf4j.LoggerFactory;",
                " public abstract class NormalizedResources {",
                "+",
                "     private static final Logger LOG = LoggerFactory.getLogger(NormalizedResources.class);",
                "@@ -64,3 +65,3 @@ public abstract class NormalizedResources {",
                "         //By default all of the values are 0",
                "-        double [] ret = new double[counter.get()];",
                "+        double[] ret = new double[counter.get()];",
                "         for (Map.Entry<String, Double> entry : normalizedResources.entrySet()) {",
                "@@ -79,7 +80,7 @@ public abstract class NormalizedResources {",
                "     private double[] otherResources;",
                "+    private final Map<String, Double> normalizedResources;",
                "     /**",
                "-     * This is for testing only.  It allows a test to reset the mapping of resource names in the array.",
                "-     * We reset the mapping because some algorithms sadly have different behavior if a resource exists",
                "-     * or not.",
                "+     * This is for testing only. It allows a test to reset the mapping of resource names in the array. We reset the mapping because some",
                "+     * algorithms sadly have different behavior if a resource exists or not.",
                "      */",
                "@@ -91,2 +92,5 @@ public abstract class NormalizedResources {",
                "+    /**",
                "+     * Copy constructor.",
                "+     */",
                "     public NormalizedResources(NormalizedResources other) {",
                "@@ -94,2 +98,3 @@ public abstract class NormalizedResources {",
                "         otherResources = Arrays.copyOf(other.otherResources, other.otherResources.length);",
                "+        normalizedResources = other.normalizedResources;",
                "     }",
                "@@ -97,5 +102,5 @@ public abstract class NormalizedResources {",
                "     /**",
                "-     * Create a new normalized set of resources.  Note that memory is not",
                "-     * covered here because it is not consistent in requests vs offers because",
                "-     * of how on heap vs off heap is used.",
                "+     * Create a new normalized set of resources. Note that memory is not covered here because it is not consistent in requests vs offers",
                "+     * because of how on heap vs off heap is used.",
                "+     *",
                "      * @param resources the resources to be normalized.",
                "@@ -104,3 +109,3 @@ public abstract class NormalizedResources {",
                "     public NormalizedResources(Map<String, ? extends Number> resources, Map<String, ? extends Number> defaults) {",
                "-        Map<String, Double> normalizedResources = normalizedResourceMap(defaults);",
                "+        normalizedResources = normalizedResourceMap(defaults);",
                "         normalizedResources.putAll(normalizedResourceMap(resources));",
                "@@ -108,10 +113,7 @@ public abstract class NormalizedResources {",
                "         otherResources = makeArray(normalizedResources);",
                "-        initializeMemory(normalizedResources);",
                "     }",
                "-    /**",
                "-     * Initialize any memory usage from the normalized map.",
                "-     * @param normalizedResources the normalized resource map.",
                "-     */",
                "-    protected abstract void initializeMemory(Map<String, Double> normalizedResources);",
                "+    protected final Map<String, Double> getNormalizedResources() {",
                "+        return this.normalizedResources;",
                "+    }",
                "@@ -119,2 +121,3 @@ public abstract class NormalizedResources {",
                "      * Normalizes a supervisor resource map or topology details map's keys to universal resource names.",
                "+     *",
                "      * @param resourceMap resource map of either Supervisor or Topology",
                "@@ -136,2 +139,3 @@ public abstract class NormalizedResources {",
                "      * Get the total amount of memory.",
                "+     *",
                "      * @return the total amount of memory requested or provided.",
                "@@ -142,2 +146,3 @@ public abstract class NormalizedResources {",
                "      * Get the total amount of cpu.",
                "+     *",
                "      * @return the amount of cpu.",
                "@@ -152,3 +157,3 @@ public abstract class NormalizedResources {",
                "         if (otherLength > length) {",
                "-            double [] newResources = new double[otherLength];",
                "+            double[] newResources = new double[otherLength];",
                "             System.arraycopy(newResources, 0, otherResources, 0, length);",
                "@@ -168,2 +173,3 @@ public abstract class NormalizedResources {",
                "      * Add the resources from a worker to this.",
                "+     *",
                "      * @param value the worker resources that should be added to this.",
                "@@ -171,5 +177,5 @@ public abstract class NormalizedResources {",
                "     public void add(WorkerResources value) {",
                "-        Map<String, Double> normalizedResources = value.get_resources();",
                "-        cpu += normalizedResources.getOrDefault(Constants.COMMON_CPU_RESOURCE_NAME, 0.0);",
                "-        add(makeArray(normalizedResources));",
                "+        Map<String, Double> workerNormalizedResources = value.get_resources();",
                "+        cpu += workerNormalizedResources.getOrDefault(Constants.COMMON_CPU_RESOURCE_NAME, 0.0);",
                "+        add(makeArray(workerNormalizedResources));",
                "     }",
                "@@ -177,3 +183,4 @@ public abstract class NormalizedResources {",
                "     /**",
                "-     * Remove the resources from other.  This is the same as subtracting the resources in other from this.",
                "+     * Remove the other resources from this. This is the same as subtracting the resources in other from this.",
                "+     *",
                "      * @param other the resources we want removed.",
                "@@ -186,3 +193,3 @@ public abstract class NormalizedResources {",
                "         if (otherLength > length) {",
                "-            double [] newResources = new double[otherLength];",
                "+            double[] newResources = new double[otherLength];",
                "             System.arraycopy(newResources, 0, otherResources, 0, length);",
                "@@ -190,3 +197,3 @@ public abstract class NormalizedResources {",
                "         }",
                "-        for (int i = 0; i < Math.min(length, otherLength); i++) {",
                "+        for (int i = 0; i < otherLength; i++) {",
                "             otherResources[i] -= other.otherResources[i];",
                "@@ -198,14 +205,9 @@ public abstract class NormalizedResources {",
                "     public String toString() {",
                "-        return \"CPU: \" + cpu;",
                "+        return \"CPU: \" + cpu + \" Other resources: \" + toNormalizedOtherResources();",
                "     }",
                "-    /**",
                "-     * Return a Map of the normalized resource name to a double.  This should only",
                "-     * be used when returning thrift resource requests to the end user.",
                "-     */",
                "-    public Map<String,Double> toNormalizedMap() {",
                "-        HashMap<String, Double> ret = new HashMap<>();",
                "-        ret.put(Constants.COMMON_CPU_RESOURCE_NAME, cpu);",
                "+    private Map<String, Double> toNormalizedOtherResources() {",
                "+        Map<String, Double> ret = new HashMap<>();",
                "         int length = otherResources.length;",
                "-        for (Map.Entry<String, Integer> entry: resourceNames.entrySet()) {",
                "+        for (Map.Entry<String, Integer> entry : resourceNames.entrySet()) {",
                "             int index = entry.getValue();",
                "@@ -218,2 +220,12 @@ public abstract class NormalizedResources {",
                "+    /**",
                "+     * Return a Map of the normalized resource name to a double. This should only be used when returning thrift resource requests to the end",
                "+     * user.",
                "+     */",
                "+    public Map<String, Double> toNormalizedMap() {",
                "+        Map<String, Double> ret = toNormalizedOtherResources();",
                "+        ret.put(Constants.COMMON_CPU_RESOURCE_NAME, cpu);",
                "+        return ret;",
                "+    }",
                "+",
                "     private double getResourceAt(int index) {",
                "@@ -226,4 +238,5 @@ public abstract class NormalizedResources {",
                "     /**",
                "-     * A simple sanity check to see if all of the resources in this would be large enough to hold the resources in other ignoring memory.",
                "-     * It does not check memory because with shared memory it is beyond the scope of this.",
                "+     * A simple sanity check to see if all of the resources in this would be large enough to hold the resources in other ignoring memory. It",
                "+     * does not check memory because with shared memory it is beyond the scope of this.",
                "+     *",
                "      * @param other the resources that we want to check if they would fit in this.",
                "@@ -248,9 +261,28 @@ public abstract class NormalizedResources {",
                "+    private void throwBecauseResourceIsMissingFromTotal(int resourceIndex) {",
                "+        String resourceName = null;",
                "+        for (Map.Entry<String, Integer> entry : resourceNames.entrySet()) {",
                "+            int index = entry.getValue();",
                "+            if (index == resourceIndex) {",
                "+                resourceName = entry.getKey();",
                "+                break;",
                "+            }",
                "+        }",
                "+        if (resourceName == null) {",
                "+            throw new IllegalStateException(\"Array index \" + resourceIndex + \" is not mapped in the resource names map.\"",
                "+                + \" This should not be possible, and is likely a bug in the Storm code.\");",
                "+        }",
                "+        throw new IllegalArgumentException(\"Total resources does not contain resource '\"",
                "+            + resourceName",
                "+            + \"'. All resources should be represented in the total. This is likely a bug in the Storm code\");",
                "+    }",
                "+",
                "     /**",
                "-     * Calculate the average resource usage percentage with this being the total resources and",
                "-     * used being the amounts used.",
                "+     * Calculate the average resource usage percentage with this being the total resources and used being the amounts used.",
                "+     *",
                "      * @param used the amount of resources used.",
                "-     * @return the average percentage used 0.0 to 100.0.",
                "+     * @return the average percentage used 0.0 to 100.0. Clamps to 100.0 in case there are no available resources in the total",
                "      */",
                "     public double calculateAveragePercentageUsedBy(NormalizedResources used) {",
                "+        int skippedResourceTypes = 0;",
                "         double total = 0.0;",
                "@@ -259,2 +291,4 @@ public abstract class NormalizedResources {",
                "             total += used.getTotalMemoryMb() / totalMemory;",
                "+        } else {",
                "+            skippedResourceTypes++;",
                "         }",
                "@@ -263,12 +297,43 @@ public abstract class NormalizedResources {",
                "             total += used.getTotalCpu() / getTotalCpu();",
                "+        } else {",
                "+            skippedResourceTypes++;",
                "         }",
                "-        //If total is 0 we add in a 0% used, so we can just skip over anything that is not in both.",
                "-        int length = Math.min(used.otherResources.length, otherResources.length);",
                "-        for (int i = 0; i < length; i++) {",
                "-            if (otherResources[i] != 0.0) {",
                "-                total += used.otherResources[i] / otherResources[i];",
                "+        if (LOG.isTraceEnabled()) {",
                "+            LOG.trace(\"Calculating avg percentage used by. Used CPU: {} Total CPU: {} Used Mem: {} Total Mem: {}\"",
                "+                + \" Other Used: {} Other Total: {}\", totalCpu, used.getTotalCpu(), totalMemory, used.getTotalMemoryMb(),",
                "+                this.toNormalizedOtherResources(), used.toNormalizedOtherResources());",
                "+        }",
                "+",
                "+        if (used.otherResources.length > otherResources.length) {",
                "+            throwBecauseResourceIsMissingFromTotal(used.otherResources.length);",
                "+        }",
                "+",
                "+        for (int i = 0; i < otherResources.length; i++) {",
                "+            double totalValue = otherResources[i];",
                "+            if (totalValue == 0.0) {",
                "+                //Skip any resources where the total is 0, the percent used for this resource isn't meaningful.",
                "+                //We fall back to prioritizing by cpu, memory and any other resources by ignoring this value",
                "+                skippedResourceTypes++;",
                "+                continue;",
                "             }",
                "+            double usedValue;",
                "+            if (i >= used.otherResources.length) {",
                "+                //Resources missing from used are using none of that resource",
                "+                usedValue = 0.0;",
                "+            } else {",
                "+                usedValue = used.otherResources[i];",
                "+            }",
                "+            total += usedValue / totalValue;",
                "+        }",
                "+        //Adjust the divisor for the average to account for any skipped resources (those where the total was 0)",
                "+        int divisor = 2 + otherResources.length - skippedResourceTypes;",
                "+        if (divisor == 0) {",
                "+            /*This is an arbitrary choice to make the result consistent with calculateMin.",
                "+             Any value would be valid here, becase there are no (non-zero) resources in the total set of resources,",
                "+             so we're trying to average 0 values.",
                "+             */",
                "+            return 100.0;",
                "+        } else {",
                "+            return (total * 100.0) / divisor;",
                "         }",
                "-        //To get the count we divide by we need to take the maximum length because we are doing an average.",
                "-        return (total * 100.0) / (2 + Math.max(otherResources.length, used.otherResources.length));",
                "     }",
                "@@ -276,6 +341,6 @@ public abstract class NormalizedResources {",
                "     /**",
                "-     * Calculate the minimum resource usage percentage with this being the total resources and",
                "-     * used being the amounts used.",
                "+     * Calculate the minimum resource usage percentage with this being the total resources and used being the amounts used.",
                "+     *",
                "      * @param used the amount of resources used.",
                "-     * @return the minimum percentage used 0.0 to 100.0.",
                "+     * @return the minimum percentage used 0.0 to 100.0. Clamps to 100.0 in case there are no available resources in the total.",
                "      */",
                "@@ -284,2 +349,13 @@ public abstract class NormalizedResources {",
                "         double totalCpu = getTotalCpu();",
                "+",
                "+        if (LOG.isTraceEnabled()) {",
                "+            LOG.trace(\"Calculating min percentage used by. Used CPU: {} Total CPU: {} Used Mem: {} Total Mem: {}\"",
                "+                + \" Other Used: {} Other Total: {}\", totalCpu, used.getTotalCpu(), totalMemory, used.getTotalMemoryMb(),",
                "+                toNormalizedOtherResources(), used.toNormalizedOtherResources());",
                "+        }",
                "+",
                "+        if (used.otherResources.length > otherResources.length) {",
                "+            throwBecauseResourceIsMissingFromTotal(used.otherResources.length);",
                "+        }",
                "+",
                "         if (used.otherResources.length != otherResources.length",
                "@@ -291,11 +367,22 @@ public abstract class NormalizedResources {",
                "         }",
                "-        double min = used.getTotalMemoryMb() / totalMemory;",
                "-        min = Math.min(min, used.getTotalCpu() / getTotalCpu());",
                "+",
                "+        double min = 100.0;",
                "+        if (totalMemory != 0.0) {",
                "+            min = Math.min(min, used.getTotalMemoryMb() / totalMemory);",
                "+        }",
                "+        if (totalCpu != 0.0) {",
                "+            min = Math.min(min, used.getTotalCpu() / totalCpu);",
                "+        }",
                "         for (int i = 0; i < otherResources.length; i++) {",
                "-            if (otherResources[i] != 0.0) {",
                "-                min = Math.min(min, used.otherResources[i] / otherResources[i]);",
                "-            } else {",
                "-                return 0.0; //0 will be the minimum, because we count values not in here as 0",
                "+            if (otherResources[i] == 0.0) {",
                "+                //Skip any resources where the total is 0, the percent used for this resource isn't meaningful.",
                "+                //We fall back to prioritizing by cpu, memory and any other resources by ignoring this value",
                "+                continue;",
                "+            }",
                "+            if (i >= used.otherResources.length) {",
                "+                //Resources missing from used are using none of that resource",
                "+                return 0;",
                "             }",
                "+            min = Math.min(min, used.otherResources[i] / otherResources[i]);",
                "         }",
                "@@ -303,2 +390,2 @@ public abstract class NormalizedResources {",
                "     }",
                "-}",
                "\\ No newline at end of file",
                "+}",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "index fc746b084..e0dc8818d 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java",
                "@@ -22,7 +22,4 @@ import java.util.ArrayList;",
                " import java.util.Collection;",
                "-import java.util.Collections;",
                " import java.util.HashSet;",
                "-import java.util.LinkedList;",
                " import java.util.List;",
                "-import java.util.Map;",
                " import java.util.TreeSet;",
                "@@ -35,3 +32,2 @@ import org.apache.storm.scheduler.TopologyDetails;",
                "-import org.apache.storm.scheduler.resource.ResourceUtils;",
                " import org.apache.storm.scheduler.resource.SchedulingResult;",
                "@@ -43,3 +39,3 @@ public class DefaultResourceAwareStrategy extends BaseResourceAwareStrategy impl",
                "-    private static final Logger LOG = LoggerFactory.getLogger(BaseResourceAwareStrategy.class);",
                "+    private static final Logger LOG = LoggerFactory.getLogger(DefaultResourceAwareStrategy.class);",
                "@@ -127,3 +123,3 @@ public class DefaultResourceAwareStrategy extends BaseResourceAwareStrategy impl",
                "             final ExistingScheduleFunc existingScheduleFunc) {",
                "-",
                "+        ",
                "         for (ObjectResources objectResources : allResources.objectResources) {",
                "@@ -131,2 +127,7 @@ public class DefaultResourceAwareStrategy extends BaseResourceAwareStrategy impl",
                "                 allResources.availableResourcesOverall.calculateMinPercentageUsedBy(objectResources.availableResources);",
                "+            if (LOG.isTraceEnabled()) {",
                "+                LOG.trace(\"Effective resources for {} is {}, and numExistingSchedule is {}\",",
                "+                    objectResources.id, objectResources.effectiveResources,",
                "+                    existingScheduleFunc.getNumExistingSchedule(objectResources.id));",
                "+            }",
                "         }"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/blobstore/BlobStoreUtils.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceOffer.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResourceRequest.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/DefaultResourceAwareStrategy.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2859": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2859",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "278f01bf09ee6d0bbe71315a69118961405b19fe",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516991067,
            "hunks": 0,
            "message": "Merge branch 'STORM-2912' of github.com:HeartSaVioR/storm",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2912": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2912",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b25d297ade58a996d54e1d0f0e9f12e1a3943f24",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515606906,
            "hunks": 0,
            "message": "Merge branch 'add-jira-search' of https://github.com/srdo/storm Add link to JIRA search showing open newbie labelled issues to contributing page. This closes #2509",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {
                "2509": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2509",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "00e303fc97c72bbd9cb68e779a54668eab83f1ab",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515371708,
            "hunks": 0,
            "message": "Merge branch 'fix-1.x' of https://github.com/danny0405/storm into STORM-2879-1.x",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2879": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2879",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "16a4cb8c4ed88124bd7476256b02ca41d9399ed0",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509248669,
            "hunks": 0,
            "message": "Merge branch 'Apache_master_STORM-2787_KSInitFlag' of https://github.com/hmcl/storm-apache into STORM-2787-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2787": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2787",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "4495f661ce80fb98aefee0aa843f58a72ecada89",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512457933,
            "hunks": 0,
            "message": "Merge branch 'STORM-2834' of https://github.com/Ethanlm/storm into STORM-2834-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2834": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2834",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b16a0deb0ceb7a730c7a5835b39881a5785ac388",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510099723,
            "hunks": 0,
            "message": "Merge branch 'STORM-2794' of https://github.com/Ethanlm/storm into STORM-2794-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2794": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2794",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "47be75c8d2fc5f9c7901595fecb385393f9fab8b",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513360088,
            "hunks": 4,
            "message": "STORM-2153: Add transferred metric",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/task.clj b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "index 8eb432eeb..7132fc12d 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "@@ -147,3 +147,3 @@",
                "               (if out-task-id",
                "-                (stats/transferred-tuples! executor-stats stream 1)))",
                "+                (stats/transferred-tuples! executor-stats (StormMetricRegistry/counter \"transferred\" worker-context component-id (pr-str (:executor-id executor-data)) stream) stream 1)))",
                "             (if out-task-id [out-task-id])",
                "@@ -166,3 +166,3 @@",
                "                (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id (pr-str (:executor-id executor-data)) stream) stream)",
                "-               (stats/transferred-tuples! executor-stats stream (count out-tasks)))",
                "+               (stats/transferred-tuples! executor-stats (StormMetricRegistry/counter \"transferred\" worker-context component-id (pr-str (:executor-id executor-data)) stream) stream (count out-tasks)))",
                "              out-tasks)))",
                "diff --git a/storm-core/src/clj/org/apache/storm/stats.clj b/storm-core/src/clj/org/apache/storm/stats.clj",
                "index 41aaf04ad..85c7bbe6b 100644",
                "--- a/storm-core/src/clj/org/apache/storm/stats.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/stats.clj",
                "@@ -127,4 +127,5 @@",
                " (defn transferred-tuples!",
                "-  [stats stream amt]",
                "+  [stats ^Counter transferred-counter stream amt]",
                "   (let [^MultiCountStatAndMetric transferred (stats-transferred stats)]",
                "+    (.inc transferred-counter amt)",
                "     (.incBy transferred ^Object stream ^long (* (stats-rate stats) amt))))"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "storm-core/src/clj/org/apache/storm/stats.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7fbe7a278f99e30ac856a21cfe9e6e5a2c16c8b1",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517545623,
            "hunks": 0,
            "message": "Merge branch 'STORM-2918' of https://github.com/dbist/storm into STORM-2918-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2918": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2918",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "3cf71d2c6fde34fb708b9f834135702b0adeb4b6",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511537994,
            "hunks": 0,
            "message": "Merge branch 'STORM-2829' of https://github.com/Ethanlm/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2829": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2829",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e906180cdd3b77d47458c22ca3120fbea3cb3b17",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517545743,
            "hunks": 0,
            "message": "Merge branch 'STORM-2918-1.1.x-merge' into 1.1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2918": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2918",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "31133fdcecb5e83317729de67ef7ea12a1bd9929",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512460856,
            "hunks": 0,
            "message": "Merge branch '1.x-branch_STORM-2784_KTLOPR' of https://github.com/hmcl/storm-apache into STORM-2784-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2784": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2784",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "352cd4665287712813d5f6f2af2537769a2cbb82",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507574389,
            "hunks": 0,
            "message": "Merge branch 'STORM-2698-2' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2698": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2698",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "2552c078427bbae54100093b0976dfff9d8c6d08",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511940271,
            "hunks": 0,
            "message": "Merge pull request #1 from MichealShin/MichealShin-patch-1 Quick fix: typo in FAQ.md",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {
                "1": "[STORM-2607] Offset consumer + 1 #2181 STORM-2538: New kafka spout emits duplicate tuples #2147"
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 1",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7b940aed63d05b17ce6fc18341af02748c548c7e",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508302741,
            "hunks": 0,
            "message": "Merge branch 'STORM-2779' of https://github.com/HeartSaVioR/storm into STORM-2779-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2779": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2779",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "51360afd2fbfb3ccfe01ba7eb97ffbeaec9a7620",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511909787,
            "hunks": 0,
            "message": "Merge branch 'STORM-2833-1.1.x-merge' into 1.1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2833": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2833",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6cb81fd7a911ddcb986d332ec7e793acd009f0a9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515704395,
            "hunks": 3,
            "message": "STORM-2153: remove unnecessary Metrics2Utils class",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/Metrics2Utils.java b/storm-core/src/jvm/org/apache/storm/metrics2/Metrics2Utils.java",
                "deleted file mode 100644",
                "index 716b8b719..000000000",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/Metrics2Utils.java",
                "+++ /dev/null",
                "@@ -1,28 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-package org.apache.storm.metrics2;",
                "-",
                "-public class Metrics2Utils {",
                "-    private Metrics2Utils(){}",
                "-",
                "-    public static Object instantiate(String klass) throws ClassNotFoundException, IllegalAccessException, InstantiationException {",
                "-        Class<?> c = Class.forName(klass);",
                "-        return  c.newInstance();",
                "-    }",
                "-",
                "-}",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/reporters/ScheduledStormReporter.java b/storm-core/src/jvm/org/apache/storm/metrics2/reporters/ScheduledStormReporter.java",
                "index b7ffa61fe..dccba0642 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/reporters/ScheduledStormReporter.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/reporters/ScheduledStormReporter.java",
                "@@ -20,3 +20,2 @@ package org.apache.storm.metrics2.reporters;",
                " import com.codahale.metrics.ScheduledReporter;",
                "-import org.apache.storm.metrics2.Metrics2Utils;",
                " import org.apache.storm.metrics2.filters.StormMetricsFilter;",
                "@@ -79,8 +78,4 @@ public abstract class ScheduledStormReporter implements StormReporter{",
                "             if (clazz != null) {",
                "-                try {",
                "-                    filter = (StormMetricsFilter) Metrics2Utils.instantiate(clazz);",
                "-                    filter.prepare(filterConf);",
                "-                } catch (Exception e) {",
                "-                    throw new RuntimeException(\"Unable to instantiate StormMetricsFilter class: \" + clazz);",
                "-                }",
                "+                filter = Utils.newInstance(clazz);",
                "+                filter.prepare(filterConf);",
                "             }"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/metrics2/Metrics2Utils.java",
                "storm-core/src/jvm/org/apache/storm/metrics2/reporters/ScheduledStormReporter.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "eb63a842c943762e3d3cb63c61471f2f4bdfe795",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517498993,
            "hunks": 0,
            "message": "Merge branch 'STORM-2877-1.1.x-merge' into 1.1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2877": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2877",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "9b6d157358410bdee46412ff17ab81ff1919e64c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516613281,
            "hunks": 0,
            "message": "Merge branch 'STORM-2862-1.1.x' of https://github.com/hmcc/storm into STORM-2862-1.1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2862": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2862",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6568fd066df04c18c42f39249cea9673f546c59a",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511473686,
            "hunks": 12,
            "message": "STORM-2830: Upgrade to Jackson 2.9.2. Switch to using the Jackson BOM to keep components in sync",
            "diff": [
                "diff --git a/external/storm-hbase/pom.xml b/external/storm-hbase/pom.xml",
                "index 47eb4481e..6d9662039 100644",
                "--- a/external/storm-hbase/pom.xml",
                "+++ b/external/storm-hbase/pom.xml",
                "@@ -107,3 +107,2 @@",
                "             <artifactId>jackson-core</artifactId>",
                "-            <version>${jackson.version}</version>",
                "         </dependency>",
                "@@ -112,3 +111,2 @@",
                "             <artifactId>jackson-databind</artifactId>",
                "-            <version>${jackson.version}</version>",
                "         </dependency>",
                "diff --git a/external/storm-opentsdb/pom.xml b/external/storm-opentsdb/pom.xml",
                "index 0f9e0c802..78afad669 100644",
                "--- a/external/storm-opentsdb/pom.xml",
                "+++ b/external/storm-opentsdb/pom.xml",
                "@@ -56,3 +56,2 @@",
                "             <artifactId>jackson-core</artifactId>",
                "-            <version>${jackson.version}</version>",
                "         </dependency>",
                "@@ -61,3 +60,2 @@",
                "             <artifactId>jackson-databind</artifactId>",
                "-            <version>${jackson.version}</version>",
                "         </dependency>",
                "diff --git a/external/storm-redis/pom.xml b/external/storm-redis/pom.xml",
                "index 9abf9ab88..ca70344c8 100644",
                "--- a/external/storm-redis/pom.xml",
                "+++ b/external/storm-redis/pom.xml",
                "@@ -66,3 +66,2 @@",
                "             <artifactId>jackson-core</artifactId>",
                "-            <version>${jackson.version}</version>",
                "         </dependency>",
                "@@ -71,3 +70,2 @@",
                "             <artifactId>jackson-databind</artifactId>",
                "-            <version>${jackson.version}</version>",
                "         </dependency>",
                "diff --git a/pom.xml b/pom.xml",
                "index aa4cf8d17..543c0d057 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -300,3 +300,3 @@",
                "-        <jackson.version>2.6.3</jackson.version>",
                "+        <jackson.version>2.9.2</jackson.version>",
                "         <!-- Kafka version used by old storm-kafka spout code -->",
                "@@ -743,10 +743,7 @@",
                "             <dependency>",
                "-                <groupId>com.fasterxml.jackson.core</groupId>",
                "-                <artifactId>jackson-core</artifactId>",
                "-                <version>${jackson.version}</version>",
                "-            </dependency>",
                "-            <dependency>",
                "-                <groupId>com.fasterxml.jackson.dataformat</groupId>",
                "-                <artifactId>jackson-dataformat-smile</artifactId>",
                "+                <groupId>com.fasterxml.jackson</groupId>",
                "+                <artifactId>jackson-bom</artifactId>",
                "                 <version>${jackson.version}</version>",
                "+                <scope>import</scope>",
                "+                <type>pom</type>",
                "             </dependency>",
                "@@ -1009,8 +1006,3 @@",
                "                 <version>${calcite.version}</version>",
                "-            </dependency>",
                "-            <dependency>",
                "-                <groupId>com.fasterxml.jackson.core</groupId>",
                "-                <artifactId>jackson-databind</artifactId>",
                "-                <version>${jackson.version}</version>",
                "-            </dependency>",
                "+            </dependency> ",
                "diff --git a/sql/storm-sql-core/pom.xml b/sql/storm-sql-core/pom.xml",
                "index 1723a5f5a..b58aa20f5 100644",
                "--- a/sql/storm-sql-core/pom.xml",
                "+++ b/sql/storm-sql-core/pom.xml",
                "@@ -77,3 +77,2 @@",
                "             <artifactId>jackson-annotations</artifactId>",
                "-            <version>${jackson.version}</version>",
                "         </dependency>",
                "diff --git a/storm-webapp/pom.xml b/storm-webapp/pom.xml",
                "index 28461462b..1c7ecfc36 100644",
                "--- a/storm-webapp/pom.xml",
                "+++ b/storm-webapp/pom.xml",
                "@@ -79,2 +79,6 @@",
                "         </dependency>",
                "+        <dependency>",
                "+            <groupId>com.fasterxml.jackson.core</groupId>",
                "+            <artifactId>jackson-databind</artifactId>",
                "+        </dependency>"
            ],
            "changed_files": [
                "external/storm-hbase/pom.xml",
                "external/storm-opentsdb/pom.xml",
                "external/storm-redis/pom.xml",
                "pom.xml",
                "sql/storm-sql-core/pom.xml",
                "storm-webapp/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2830": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2830",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "cd272c41521c17adde31b64471b748917dbe5ab0",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513203048,
            "hunks": 6,
            "message": "STORM-2153: fix test failures",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index fa7d44c56..993add643 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -444,3 +444,3 @@",
                "     (when time-delta",
                "-      (stats/spout-failed-tuple! (:stats executor-data) (StormMetricRegistry/counter \"failed\" worker-context (:component-id executor-data) (:executor-id executor-data) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-failed-tuple! (:stats executor-data) (StormMetricRegistry/counter \"failed\" (:worker-context executor-data) (:component-id executor-data) (pr-str (:executor-id executor-data)) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "@@ -453,3 +453,3 @@",
                "     (when time-delta",
                "-      (stats/spout-acked-tuple! (:stats executor-data) (StormMetricRegistry/counter \"acked\" worker-context (:component-id executor-data) (:executor-id executor-data) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-acked-tuple! (:stats executor-data) (StormMetricRegistry/counter \"acked\" (:worker-context executor-data) (:component-id executor-data) (pr-str (:executor-id executor-data)) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "@@ -822,3 +822,3 @@",
                "                              (stats/bolt-acked-tuple! executor-stats",
                "-                                                      (:acked-counter (:executor-data task-data))",
                "+                                                      (StormMetricRegistry/counter \"acked\" worker-context  (:component-id executor-data) (pr-str (:executor-id executor-data)) (.getSourceStreamId tuple))",
                "                                                       (.getSourceComponent tuple)",
                "@@ -838,3 +838,3 @@",
                "                              (stats/bolt-failed-tuple! executor-stats",
                "-                                                       (:failed-counter (:executor-data task-data))",
                "+                                                       (StormMetricRegistry/counter \"failed\" worker-context (:component-id executor-data) (pr-str (:executor-id executor-data)) (.getSourceStreamId tuple))",
                "                                                        (.getSourceComponent tuple)",
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/task.clj b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "index edc144c7b..8eb432eeb 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "@@ -145,3 +145,3 @@",
                "             (when (emit-sampler)",
                "-              (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id (:executor-id executor-data) stream) stream)",
                "+              (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id (pr-str (:executor-id executor-data)) stream) stream)",
                "               (if out-task-id",
                "@@ -165,3 +165,3 @@",
                "              (when (emit-sampler)",
                "-               (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id (:executor-id executor-data) stream) stream)",
                "+               (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id (pr-str (:executor-id executor-data)) stream) stream)",
                "                (stats/transferred-tuples! executor-stats stream (count out-tasks)))"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "storm-core/src/clj/org/apache/storm/daemon/task.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "9dc5414615e92aee969216519db7e7dd794575e2",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511291189,
            "hunks": 0,
            "message": "Merge branch 'STORM-2825' of https://github.com/srdo/storm into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2825": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2825",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "a5ed0f918af8aba055258ce783f54eb21586deab",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510614121,
            "hunks": 0,
            "message": "Merge branch 'STORM-2811-1.x' of https://github.com/srdo/storm into STORM-2811-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2811": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2811",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e13f903452585ab84e08d5a7ac3a79c43141f232",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515008922,
            "hunks": 6,
            "message": "STORM-2153: Replace timer with ScheduledThreadPoolExecutor",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java b/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "index d7cf401be..6ea3683a6 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "@@ -50,2 +50,3 @@ import java.util.concurrent.ConcurrentHashMap;",
                " import java.util.concurrent.ConcurrentLinkedQueue;",
                "+import java.util.concurrent.ScheduledThreadPoolExecutor;",
                " import java.util.concurrent.ThreadFactory;",
                "@@ -67,3 +68,3 @@ public class DisruptorQueue implements IStatefulObject {",
                "     private static final FlusherPool FLUSHER = new FlusherPool();",
                "-    private static final Timer METRICS_TIMER = new Timer(\"disruptor-metrics-timer\", true);",
                "+    private static final ScheduledThreadPoolExecutor METRICS_REPORTER_EXECUTOR = new ScheduledThreadPoolExecutor(1);",
                "@@ -439,4 +440,4 @@ public class DisruptorQueue implements IStatefulObject {",
                "         _flusher.start();",
                "-        try {",
                "-            METRICS_TIMER.schedule(new TimerTask() {",
                "+        if(!METRICS_REPORTER_EXECUTOR.isShutdown()) {",
                "+            METRICS_REPORTER_EXECUTOR.scheduleAtFixedRate(new Runnable() {",
                "                 @Override",
                "@@ -445,7 +446,5 @@ public class DisruptorQueue implements IStatefulObject {",
                "                 }",
                "-            }, 15000, 15000);",
                "-        } catch (IllegalStateException e){",
                "-            // Ignore. IllegalStateException is thrown by Timer.schedule() if the timer",
                "-            // has been cancelled. (This happens in unit tests)",
                "+            }, 15, 15, TimeUnit.SECONDS);",
                "         }",
                "+",
                "     }",
                "@@ -465,3 +464,3 @@ public class DisruptorQueue implements IStatefulObject {",
                "             _metrics.close();",
                "-            METRICS_TIMER.cancel();",
                "+            METRICS_REPORTER_EXECUTOR.shutdown();",
                "         } catch (InsufficientCapacityException e) {"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "a1ab78b2b0087096e2d019238c72c6e527c44082",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509185816,
            "hunks": 0,
            "message": "Merge branch 'STORM-1927' of https://github.com/Ethanlm/storm into STORM-1927-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-1927": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-1927",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6a26bef8f0373c26317b1aa5dc26a74f92b87016",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510640585,
            "hunks": 0,
            "message": "Merge branch 'STORM-2806' of https://github.com/Ethanlm/storm into STORM-2806-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2806": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2806",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e85e50fb86c03f0fe036d13d55209d80291b96d6",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510261294,
            "hunks": 0,
            "message": "Merge branch 'STORM-2804' of https://github.com/Ethanlm/storm into STORM-2804-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2804": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2804",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5bdf3dd674d0fdf4c5d6b2549da34e21e6c70d77",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513331036,
            "hunks": 0,
            "message": "Merge branch 'STORM-2855' of https://github.com/srdo/storm into STORM-2855-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2855": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2855",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d6f666785a1e78894b1e4e25080cf3f90a953121",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514262102,
            "hunks": 0,
            "message": "Merge branch 'STORM-2863' of https://github.com/revans2/incubator-storm into STORM-2863-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2863": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2863",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "49c2fc39fa62467d7fcc4f7e0c2eeec86e5e8d4c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517494886,
            "hunks": 0,
            "message": "Merge branch 'reuse-zk-1.x' of https://github.com/danny0405/storm into STORM-2901-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2901": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2901",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "eff32a32bb724c37f4eaca6a363665c55b17813c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517077327,
            "hunks": 30,
            "message": "STORM-2914: Implement ProcessingGuarantee.NONE in the spout instead of using enable.auto.commit",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "index c52309ecf..84e785190 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "@@ -41,3 +41,2 @@ import java.util.Set;",
                " import java.util.concurrent.TimeUnit;",
                "-import java.util.function.Supplier;",
                " import java.util.stream.Collectors;",
                "@@ -54,2 +53,3 @@ import org.apache.kafka.common.errors.RetriableException;",
                " import org.apache.storm.kafka.spout.KafkaSpoutConfig.FirstPollOffsetStrategy;",
                "+import org.apache.storm.kafka.spout.KafkaSpoutConfig.ProcessingGuarantee;",
                " import org.apache.storm.kafka.spout.internal.CommitMetadata;",
                "@@ -91,3 +91,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     private transient KafkaTupleListener tupleListener;",
                "-    // timer == null if processing guarantee is none or at-most-once",
                "+    // timer == null only if the processing guarantee is at-most-once",
                "     private transient Timer commitTimer;",
                "@@ -135,4 +135,4 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-        if (isAtLeastOnceProcessing()) {",
                "-            // Only used if the spout should commit an offset to Kafka only after the corresponding tuple has been acked.",
                "+        if (kafkaSpoutConfig.getProcessingGuarantee() != KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE) {",
                "+            // In at-most-once mode the offsets are committed after every poll, and not periodically as controlled by the timer",
                "             commitTimer = new Timer(TIMER_DELAY_MS, kafkaSpoutConfig.getOffsetsCommitPeriodMs(), TimeUnit.MILLISECONDS);",
                "@@ -309,4 +309,8 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-            if (shouldCommit()) {",
                "-                commitOffsetsForAckedTuples(kafkaConsumer.assignment());",
                "+            if (commitTimer != null && commitTimer.isExpiredResetOnTrue()) {",
                "+                if (isAtLeastOnceProcessing()) {",
                "+                    commitOffsetsForAckedTuples(kafkaConsumer.assignment());",
                "+                } else if (kafkaSpoutConfig.getProcessingGuarantee() == ProcessingGuarantee.NO_GUARANTEE) {",
                "+                    commitFetchedOffsetsAsync(kafkaConsumer.assignment());",
                "+                }",
                "             }",
                "@@ -334,6 +338,2 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "-    private boolean shouldCommit() {",
                "-        return isAtLeastOnceProcessing() && commitTimer.isExpiredResetOnTrue();    // timer != null for non auto commit mode",
                "-    }",
                "-",
                "     private PollablePartitionsInfo getPollablePartitionsInfo() {",
                "@@ -521,2 +521,11 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "+    private void commitFetchedOffsetsAsync(Set<TopicPartition> assignedPartitions) {",
                "+        Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>();",
                "+        for (TopicPartition tp : assignedPartitions) {",
                "+            offsetsToCommit.put(tp, new OffsetAndMetadata(kafkaConsumer.position(tp)));",
                "+        }",
                "+        kafkaConsumer.commitAsync(offsetsToCommit, null);",
                "+        LOG.debug(\"Committed offsets {} to Kafka\", offsetsToCommit);",
                "+    }",
                "+    ",
                "     private void commitOffsetsForAckedTuples(Set<TopicPartition> assignedPartitions) {",
                "@@ -548,7 +557,6 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "                     /*",
                "-                     * The position is behind the committed offset. This can happen in some cases, e.g. if a message failed,",
                "-                     * lots of (more than max.poll.records) later messages were acked, and the failed message then gets acked. ",
                "-                     * The consumer may only be part way through \"catching up\" to where it was when it went back to retry the failed tuple. ",
                "-                     * Skip the consumer forward to the committed offset and drop the current waiting to emit list,",
                "-                     * since it'll likely contain committed offsets.",
                "+                     * The position is behind the committed offset. This can happen in some cases, e.g. if a message failed, lots of (more",
                "+                     * than max.poll.records) later messages were acked, and the failed message then gets acked. The consumer may only be",
                "+                     * part way through \"catching up\" to where it was when it went back to retry the failed tuple. Skip the consumer forward",
                "+                     * to the committed offset and drop the current waiting to emit list, since it'll likely contain committed offsets.",
                "                      */",
                "@@ -734,3 +742,3 @@ public class KafkaSpout<K, V> extends BaseRichSpout {",
                "     KafkaOffsetMetric getKafkaOffsetMetric() {",
                "-        return  kafkaOffsetMetric;",
                "+        return kafkaOffsetMetric;",
                "     }",
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "index a063790b2..c2305cb63 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "@@ -33,2 +33,3 @@ import org.apache.kafka.common.serialization.StringDeserializer;",
                " import org.apache.storm.Config;",
                "+import org.apache.storm.annotation.InterfaceStability;",
                " import org.apache.storm.kafka.spout.KafkaSpoutRetryExponentialBackoff.TimeInterval;",
                "@@ -98,3 +99,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "     public KafkaSpoutConfig(Builder<K, V> builder) {",
                "-        setAutoCommitMode(builder);",
                "+        setKafkaPropsForProcessingGuarantee(builder);",
                "         this.kafkaProps = builder.kafkaProps;",
                "@@ -117,19 +118,22 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "      * Defines how the {@link KafkaSpout} seeks the offset to be used in the first poll to Kafka upon topology deployment.",
                "-     * By default this parameter is set to UNCOMMITTED_EARLIEST. If the strategy is set to:",
                "-     * <br/>",
                "-     * <ul>",
                "-     * <li>EARLIEST - the kafka spout polls records starting in the first offset of the partition, regardless",
                "-     * of previous commits. This setting only takes effect on topology deployment.</li>",
                "-     * <li>LATEST - the kafka spout polls records with offsets greater than the last offset in the partition,",
                "-     * regardless of previous commits. This setting only takes effect on topology deployment.</li>",
                "-     * <li>UNCOMMITTED_EARLIEST - the kafka spout polls records from the last committed offset, if any. If no offset has been",
                "-     * committed it behaves as EARLIEST.</li>",
                "-     * <li>UNCOMMITTED_LATEST - the kafka spout polls records from the last committed offset, if any. If no offset has been",
                "-     * committed it behaves as LATEST.</li>",
                "-     * </ul>",
                "+     * By default this parameter is set to UNCOMMITTED_EARLIEST. ",
                "      */",
                "     public enum FirstPollOffsetStrategy {",
                "+        /**",
                "+         * The kafka spout polls records starting in the first offset of the partition, regardless of previous commits. This setting only",
                "+         * takes effect on topology deployment",
                "+         */",
                "         EARLIEST,",
                "+        /**",
                "+         * The kafka spout polls records with offsets greater than the last offset in the partition, regardless of previous commits. This",
                "+         * setting only takes effect on topology deployment",
                "+         */",
                "         LATEST,",
                "+        /**",
                "+         * The kafka spout polls records from the last committed offset, if any. If no offset has been committed it behaves as EARLIEST",
                "+         */",
                "         UNCOMMITTED_EARLIEST,",
                "+        /**",
                "+         * The kafka spout polls records from the last committed offset, if any. If no offset has been committed it behaves as LATEST",
                "+         */",
                "         UNCOMMITTED_LATEST;",
                "@@ -144,24 +148,26 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "      * This enum controls when the tuple with the {@link ConsumerRecord} for an offset is marked as processed,",
                "-     * i.e. when the offset is committed to Kafka. For AT_LEAST_ONCE and AT_MOST_ONCE the spout controls when",
                "-     * the commit happens. When the guarantee is NONE Kafka controls when the commit happens.",
                "-     *",
                "-     * <ul>",
                "-     * <li>AT_LEAST_ONCE - an offset is ready to commit only after the corresponding tuple has been processed (at-least-once)",
                "-     * and acked. If a tuple fails or times-out it will be re-emitted. A tuple can be processed more than once if for instance",
                "-     * the ack gets lost.</li>",
                "-     * <br/>",
                "-     * <li>AT_MOST_ONCE - every offset will be committed to Kafka right after being polled but before being emitted",
                "-     * to the downstream components of the topology. It guarantees that the offset is processed at-most-once because it",
                "-     * won't retry tuples that fail or timeout after the commit to Kafka has been done.</li>",
                "-     * <br/>",
                "-     * <li>NONE - the polled offsets are committed to Kafka periodically as controlled by the Kafka properties",
                "-     * \"enable.auto.commit\" and \"auto.commit.interval.ms\". Because the spout does not control when the commit happens",
                "-     * it cannot give any message processing guarantees, i.e. a message may be processed 0, 1 or more times.",
                "-     * This option requires \"enable.auto.commit=true\". If \"enable.auto.commit=false\" an exception will be thrown.</li>",
                "-     * </ul>",
                "+     * i.e. when the offset can be committed to Kafka. The default value is AT_LEAST_ONCE.",
                "+     * The commit interval is controlled by {@link KafkaSpoutConfig#getOffsetsCommitPeriodMs() }, if the mode commits on an interval.",
                "+     * NO_GUARANTEE may be removed in a later release without warning, we're still evaluating whether it makes sense to keep.",
                "      */",
                "+    @InterfaceStability.Unstable",
                "     public enum ProcessingGuarantee {",
                "+        /**",
                "+         * An offset is ready to commit only after the corresponding tuple has been processed and acked (at least once). If a tuple fails or",
                "+         * times out it will be re-emitted, as controlled by the {@link KafkaSpoutRetryService}. Commits synchronously on the defined",
                "+         * interval.",
                "+         */",
                "         AT_LEAST_ONCE,",
                "+        /**",
                "+         * Every offset will be synchronously committed to Kafka right after being polled but before being emitted to the downstream",
                "+         * components of the topology. The commit interval is ignored. This mode guarantees that the offset is processed at most once by",
                "+         * ensuring the spout won't retry tuples that fail or time out after the commit to Kafka has been done",
                "+         */",
                "         AT_MOST_ONCE,",
                "-        NONE,",
                "+        /**",
                "+         * The polled offsets are ready to commit immediately after being polled. The offsets are committed periodically, i.e. a message may",
                "+         * be processed 0, 1 or more times. This behavior is similar to setting enable.auto.commit=true in the consumer, but allows the",
                "+         * spout to control when commits occur. Commits asynchronously on the defined interval.",
                "+         */",
                "+        NO_GUARANTEE,",
                "     }",
                "@@ -215,3 +221,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         /**",
                "-         * Set a {@link KafkaConsumer} property.",
                "+         * Set a {@link KafkaConsumer} property. ",
                "          */",
                "@@ -223,3 +229,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         /**",
                "-         * Set multiple {@link KafkaConsumer} properties.",
                "+         * Set multiple {@link KafkaConsumer} properties. ",
                "          */",
                "@@ -231,3 +237,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         /**",
                "-         * Set multiple {@link KafkaConsumer} properties.",
                "+         * Set multiple {@link KafkaConsumer} properties. ",
                "          */",
                "@@ -258,3 +264,4 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "          *",
                "-         * <p>This setting only has an effect if the configured {@link ProcessingGuarantee} is {@link ProcessingGuarantee#AT_LEAST_ONCE}.",
                "+         * <p>This setting only has an effect if the configured {@link ProcessingGuarantee} is {@link ProcessingGuarantee#AT_LEAST_ONCE} or",
                "+         * {@link ProcessingGuarantee#NO_GUARANTEE}.",
                "          *",
                "@@ -455,33 +462,33 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "-    private static void setAutoCommitMode(Builder<?, ?> builder) {",
                "+    private static void setKafkaPropsForProcessingGuarantee(Builder<?, ?> builder) {",
                "         if (builder.kafkaProps.containsKey(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG)) {",
                "-            throw new IllegalArgumentException(\"Do not set \" + ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG + \" manually.\"",
                "-                + \" Instead use KafkaSpoutConfig.Builder.setProcessingGuarantee\");",
                "+            throw new IllegalStateException(\"The KafkaConsumer \" + ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG",
                "+                + \" setting is not supported. You can configure similar behavior through KafkaSpoutConfig.Builder.setProcessingGuarantee\");",
                "         }",
                "-        if (builder.processingGuarantee == ProcessingGuarantee.NONE) {",
                "-            builder.kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);",
                "-        } else {",
                "-            String autoOffsetResetPolicy = (String)builder.kafkaProps.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);",
                "-            if (builder.processingGuarantee == ProcessingGuarantee.AT_LEAST_ONCE) {",
                "-                if (autoOffsetResetPolicy == null) {",
                "-                    /*",
                "-                    If the user wants to explicitly set an auto offset reset policy, we should respect it, but when the spout is configured",
                "-                    for at-least-once processing we should default to seeking to the earliest offset in case there's an offset out of range",
                "-                    error, rather than seeking to the latest (Kafka's default). This type of error will typically happen when the consumer ",
                "-                    requests an offset that was deleted.",
                "-                     */",
                "-                    builder.kafkaProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");",
                "-                } else if (!autoOffsetResetPolicy.equals(\"earliest\") && !autoOffsetResetPolicy.equals(\"none\")) {",
                "-                    LOG.warn(\"Cannot guarantee at-least-once processing with auto.offset.reset.policy other than 'earliest' or 'none'.\"",
                "-                        + \" Some messages may be skipped.\");",
                "-                }",
                "-            } else if (builder.processingGuarantee == ProcessingGuarantee.AT_MOST_ONCE) {",
                "-                if (autoOffsetResetPolicy != null",
                "-                    && (!autoOffsetResetPolicy.equals(\"latest\") && !autoOffsetResetPolicy.equals(\"none\"))) {",
                "-                    LOG.warn(\"Cannot guarantee at-most-once processing with auto.offset.reset.policy other than 'latest' or 'none'.\"",
                "-                        + \" Some messages may be processed more than once.\");",
                "-                }",
                "+        String autoOffsetResetPolicy = (String) builder.kafkaProps.get(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);",
                "+        if (builder.processingGuarantee == ProcessingGuarantee.AT_LEAST_ONCE) {",
                "+            if (autoOffsetResetPolicy == null) {",
                "+                /*",
                "+                 * If the user wants to explicitly set an auto offset reset policy, we should respect it, but when the spout is configured",
                "+                 * for at-least-once processing we should default to seeking to the earliest offset in case there's an offset out of range",
                "+                 * error, rather than seeking to the latest (Kafka's default). This type of error will typically happen when the consumer",
                "+                 * requests an offset that was deleted.",
                "+                 */",
                "+                LOG.info(\"Setting consumer property '{}' to 'earliest' to ensure at-least-once processing\",",
                "+                    ConsumerConfig.AUTO_OFFSET_RESET_CONFIG);",
                "+                builder.kafkaProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");",
                "+            } else if (!autoOffsetResetPolicy.equals(\"earliest\") && !autoOffsetResetPolicy.equals(\"none\")) {",
                "+                LOG.warn(\"Cannot guarantee at-least-once processing with auto.offset.reset.policy other than 'earliest' or 'none'.\"",
                "+                    + \" Some messages may be skipped.\");",
                "+            }",
                "+        } else if (builder.processingGuarantee == ProcessingGuarantee.AT_MOST_ONCE) {",
                "+            if (autoOffsetResetPolicy != null",
                "+                && (!autoOffsetResetPolicy.equals(\"latest\") && !autoOffsetResetPolicy.equals(\"none\"))) {",
                "+                LOG.warn(\"Cannot guarantee at-most-once processing with auto.offset.reset.policy other than 'latest' or 'none'.\"",
                "+                    + \" Some messages may be processed more than once.\");",
                "             }",
                "-            builder.kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);",
                "         }",
                "+        LOG.info(\"Setting consumer property '{}' to 'false', because the spout does not support auto-commit\",",
                "+            ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG);",
                "+        builder.kafkaProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);",
                "     }"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java",
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2914": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2914",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "ee1be2b73fffff05e32cfd15857561c24655d90c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510347234,
            "hunks": 0,
            "message": "Merge branch 'patch-1' of https://github.com/jacobtolar/storm Fix pom dependency indent This closes #2408",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {
                "2408": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 2408",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "44855ba60e616ebd35bef9f4b7a5ca9013bd5c7e",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508883176,
            "hunks": 38,
            "message": "[STORM-1927] upgrade to jetty 9",
            "diff": [
                "diff --git a/flux/flux-examples/pom.xml b/flux/flux-examples/pom.xml",
                "index 720427057..3a15f2e01 100644",
                "--- a/flux/flux-examples/pom.xml",
                "+++ b/flux/flux-examples/pom.xml",
                "@@ -88,2 +88,6 @@",
                "                 </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>javax.servlet</groupId>",
                "+                    <artifactId>javax.servlet-api</artifactId>",
                "+                </exclusion>",
                "             </exclusions>",
                "diff --git a/pom.xml b/pom.xml",
                "index 8b84873fc..0b740c754 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -252,5 +252,5 @@",
                "         <json-simple.version>1.1</json-simple.version>",
                "-        <ring.version>1.3.1</ring.version>",
                "-        <ring-json.version>0.3.1</ring-json.version>",
                "-        <jetty.version>7.6.13.v20130916</jetty.version>",
                "+        <ring.version>1.6.2</ring.version>",
                "+        <ring-json.version>0.4.0</ring-json.version>",
                "+        <jetty.version>9.4.7.v20170914</jetty.version>",
                "         <clojure.tools.logging.version>0.2.3</clojure.tools.logging.version>",
                "@@ -283,3 +283,3 @@",
                "         <kryo.version>3.0.3</kryo.version>",
                "-        <servlet.version>2.5</servlet.version>",
                "+        <servlet.version>3.1.0</servlet.version>",
                "         <joda-time.version>2.3</joda-time.version>",
                "@@ -728,3 +728,3 @@",
                "                 <groupId>javax.servlet</groupId>",
                "-                <artifactId>servlet-api</artifactId>",
                "+                <artifactId>javax.servlet-api</artifactId>",
                "                 <version>${servlet.version}</version>",
                "diff --git a/storm-client/pom.xml b/storm-client/pom.xml",
                "index b37f45879..851206b6d 100644",
                "--- a/storm-client/pom.xml",
                "+++ b/storm-client/pom.xml",
                "@@ -89,2 +89,6 @@",
                "                 </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>javax.servlet</groupId>",
                "+                    <artifactId>javax.servlet-api</artifactId>",
                "+                </exclusion>",
                "             </exclusions>",
                "@@ -160,3 +164,3 @@",
                "             <groupId>javax.servlet</groupId>",
                "-            <artifactId>servlet-api</artifactId>",
                "+            <artifactId>javax.servlet-api</artifactId>",
                "         </dependency>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index 7f5b6f2c3..70abfbced 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -188,2 +188,6 @@",
                "                 </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>javax.servlet</groupId>",
                "+                    <artifactId>javax.servlet-api</artifactId>",
                "+                </exclusion>",
                "             </exclusions>",
                "@@ -278,3 +282,3 @@",
                "             <groupId>javax.servlet</groupId>",
                "-            <artifactId>servlet-api</artifactId>",
                "+            <artifactId>javax.servlet-api</artifactId>",
                "         </dependency>",
                "diff --git a/storm-core/src/clj/org/apache/storm/ui/core.clj b/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "index 574c93410..247ff4a48 100644",
                "--- a/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/ui/core.clj",
                "@@ -1618,2 +1618,3 @@",
                "                                 https-port",
                "+                                header-buffer-size",
                "                                 (reify IConfigurator",
                "@@ -1630,5 +1631,4 @@",
                "                                       https-need-client-auth",
                "-                                      https-want-client-auth)",
                "-                                    (doseq [connector (.getConnectors server)]",
                "-                                      (.setRequestHeaderSize connector header-buffer-size))",
                "+                                      https-want-client-auth",
                "+                                      header-buffer-size)",
                "                                     (UIHelpers/configFilter server (ring.util.servlet/servlet app) filters-confs)))))",
                "diff --git a/storm-core/src/clj/org/apache/storm/ui/helpers.clj b/storm-core/src/clj/org/apache/storm/ui/helpers.clj",
                "index ac1ecd1b4..5764529c3 100644",
                "--- a/storm-core/src/clj/org/apache/storm/ui/helpers.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/ui/helpers.clj",
                "@@ -30,10 +30,2 @@",
                "            [java.net URLEncoder])",
                "-  (:import [org.eclipse.jetty.server Server]",
                "-           [org.eclipse.jetty.server.nio SelectChannelConnector]",
                "-           [org.eclipse.jetty.server.ssl SslSocketConnector]",
                "-           [org.eclipse.jetty.servlet ServletHolder FilterMapping]",
                "-           [org.eclipse.jetty.util.ssl SslContextFactory]",
                "-           [org.eclipse.jetty.server DispatcherType]",
                "-           [org.eclipse.jetty.servlets CrossOriginFilter]",
                "-           (org.json.simple JSONValue))",
                "   (:require [ring.util servlet]",
                "diff --git a/storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java b/storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java",
                "index d43873fe0..75559984d 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java",
                "@@ -21,3 +21,12 @@ import com.google.common.base.Joiner;",
                " import com.google.common.collect.ImmutableMap;",
                "-",
                "+import java.io.PrintWriter;",
                "+import java.io.StringWriter;",
                "+import java.net.URLEncoder;",
                "+import java.util.EnumSet;",
                "+import java.util.HashMap;",
                "+import java.util.LinkedList;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import javax.servlet.DispatcherType;",
                "+import javax.servlet.Servlet;",
                " import org.apache.storm.generated.ExecutorInfo;",
                "@@ -25,9 +34,10 @@ import org.apache.storm.logging.filters.AccessLoggingFilter;",
                " import org.apache.storm.utils.ObjectReader;",
                "-import org.eclipse.jetty.server.Connector;",
                "-import org.eclipse.jetty.server.DispatcherType;",
                "+import org.eclipse.jetty.http.HttpVersion;",
                "+import org.eclipse.jetty.server.HttpConfiguration;",
                "+import org.eclipse.jetty.server.HttpConnectionFactory;",
                "+import org.eclipse.jetty.server.SecureRequestCustomizer;",
                " import org.eclipse.jetty.server.Server;",
                "-import org.eclipse.jetty.server.nio.SelectChannelConnector;",
                "-import org.eclipse.jetty.server.ssl.SslSocketConnector;",
                "+import org.eclipse.jetty.server.ServerConnector;",
                "+import org.eclipse.jetty.server.SslConnectionFactory;",
                " import org.eclipse.jetty.servlet.FilterHolder;",
                "-import org.eclipse.jetty.servlet.FilterMapping;",
                " import org.eclipse.jetty.servlet.ServletContextHandler;",
                "@@ -38,8 +48,2 @@ import org.json.simple.JSONValue;",
                "-import javax.servlet.Servlet;",
                "-import java.io.PrintWriter;",
                "-import java.io.StringWriter;",
                "-import java.net.URLEncoder;",
                "-import java.util.*;",
                "-",
                " public class UIHelpers {",
                "@@ -112,5 +116,5 @@ public class UIHelpers {",
                "-    private static SslSocketConnector mkSslConnector(Integer port, String ksPath, String ksPassword, String ksType,",
                "+    private static ServerConnector mkSslConnector(Server server, Integer port, String ksPath, String ksPassword, String ksType,",
                "                                                      String keyPassword, String tsPath, String tsPassword, String tsType,",
                "-                                                     Boolean needClientAuth, Boolean wantClientAuth) {",
                "+                                                     Boolean needClientAuth, Boolean wantClientAuth, Integer headerBufferSize) {",
                "         SslContextFactory factory = new SslContextFactory();",
                "@@ -118,3 +122,3 @@ public class UIHelpers {",
                "         factory.setExcludeProtocols(\"SSLv3\");",
                "-        factory.setAllowRenegotiate(false);",
                "+        factory.setRenegotiationAllowed(false);",
                "         factory.setKeyStorePath(ksPath);",
                "@@ -125,3 +129,3 @@ public class UIHelpers {",
                "         if (tsPath != null && tsPassword != null && tsType != null) {",
                "-            factory.setTrustStore(tsPath);",
                "+            factory.setTrustStorePath(tsPath);",
                "             factory.setTrustStoreType(tsType);",
                "@@ -136,3 +140,10 @@ public class UIHelpers {",
                "-        SslSocketConnector sslConnector = new SslSocketConnector(factory);",
                "+        HttpConfiguration httpsConfig = new HttpConfiguration();",
                "+        httpsConfig.addCustomizer(new SecureRequestCustomizer());",
                "+        if (null != headerBufferSize) {",
                "+            httpsConfig.setRequestHeaderSize(headerBufferSize);",
                "+        }",
                "+        ServerConnector sslConnector = new ServerConnector(server,",
                "+                new SslConnectionFactory(factory, HttpVersion.HTTP_1_1.asString()),",
                "+                new HttpConnectionFactory(httpsConfig));",
                "         sslConnector.setPort(port);",
                "@@ -142,6 +153,14 @@ public class UIHelpers {",
                "     public static void configSsl(Server server, Integer port, String ksPath, String ksPassword, String ksType,",
                "-                                 String keyPassword, String tsPath, String tsPassword, String tsType, Boolean needClientAuth, Boolean wantClientAuth) {",
                "+                                 String keyPassword, String tsPath, String tsPassword, String tsType,",
                "+                                 Boolean needClientAuth, Boolean wantClientAuth) {",
                "+        configSsl(server, port, ksPath, ksPassword, ksType, keyPassword,",
                "+                tsPath, tsPassword, tsType, needClientAuth, wantClientAuth, null);",
                "+    }",
                "+",
                "+    public static void configSsl(Server server, Integer port, String ksPath, String ksPassword, String ksType,",
                "+                                 String keyPassword, String tsPath, String tsPassword, String tsType,",
                "+                                 Boolean needClientAuth, Boolean wantClientAuth, Integer headerBufferSize) {",
                "         if (port > 0) {",
                "-            server.addConnector(mkSslConnector(port, ksPath, ksPassword, ksType, keyPassword,",
                "-                    tsPath, tsPassword, tsType, needClientAuth, wantClientAuth));",
                "+            server.addConnector(mkSslConnector(server, port, ksPath, ksPassword, ksType, keyPassword,",
                "+                    tsPath, tsPassword, tsType, needClientAuth, wantClientAuth, headerBufferSize));",
                "         }",
                "@@ -199,3 +218,3 @@ public class UIHelpers {",
                "                 }",
                "-                context.addFilter(filterHolder, \"/*\", FilterMapping.ALL);",
                "+                context.addFilter(filterHolder, \"/*\", EnumSet.allOf(DispatcherType.class));",
                "             }",
                "@@ -204,11 +223,2 @@ public class UIHelpers {",
                "     }",
                "-    ",
                "-    private static Server removeNonSslConnector(Server server) {",
                "-        for (Connector c : server.getConnectors()) {",
                "-            if (c != null && !(c instanceof SslSocketConnector)) {",
                "-                server.removeConnector(c);",
                "-            }",
                "-        }",
                "-        return server;",
                "-    }",
                "@@ -218,14 +228,24 @@ public class UIHelpers {",
                "     public static Server jettyCreateServer(Integer port, String host, Integer httpsPort) {",
                "-        SelectChannelConnector connector = new SelectChannelConnector();",
                "-        connector.setPort(ObjectReader.getInt(port, 80));",
                "-        connector.setHost(host);",
                "-        connector.setMaxIdleTime(200000);",
                "+        return jettyCreateServer(port, host, httpsPort, null);",
                "+    }",
                "+    /**",
                "+     * Construct a Jetty Server instance.",
                "+     */",
                "+    public static Server jettyCreateServer(Integer port, String host, Integer httpsPort, Integer headerBufferSize) {",
                "         Server server = new Server();",
                "-        server.addConnector(connector);",
                "-        server.setSendDateHeader(true);",
                "-        if (httpsPort != null && httpsPort > 0) {",
                "-            removeNonSslConnector(server);",
                "+        if (httpsPort == null || httpsPort <= 0) {",
                "+            HttpConfiguration httpConfig = new HttpConfiguration();",
                "+            httpConfig.setSendDateHeader(true);",
                "+            if (null != headerBufferSize) {",
                "+                httpConfig.setRequestHeaderSize(headerBufferSize);",
                "+            }",
                "+            ServerConnector httpConnector = new ServerConnector(server, new HttpConnectionFactory(httpConfig));",
                "+            httpConnector.setPort(ObjectReader.getInt(port, 80));",
                "+            httpConnector.setIdleTimeout(200000);",
                "+            httpConnector.setHost(host);",
                "+            server.addConnector(httpConnector);",
                "         }",
                "+",
                "         return server;",
                "@@ -237,4 +257,4 @@ public class UIHelpers {",
                "      */",
                "-    public static void stormRunJetty(Integer port, String host, Integer httpsPort, IConfigurator configurator) throws Exception {",
                "-        Server s = jettyCreateServer(port, host, httpsPort);",
                "+    public static void stormRunJetty(Integer port, String host, Integer httpsPort, Integer headerBufferSize, IConfigurator configurator) throws Exception {",
                "+        Server s = jettyCreateServer(port, host, httpsPort, headerBufferSize);",
                "         if (configurator != null) {",
                "@@ -245,4 +265,4 @@ public class UIHelpers {",
                "-    public static void stormRunJetty(Integer port, IConfigurator configurator) throws Exception {",
                "-        stormRunJetty(port, null, null, configurator);",
                "+    public static void stormRunJetty(Integer port, Integer headerBufferSize, IConfigurator configurator) throws Exception {",
                "+        stormRunJetty(port, null, null, headerBufferSize, configurator);",
                "     }",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index 3b914baa8..73ceb42ef 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -187,3 +187,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "     private static final Logger LOG = LoggerFactory.getLogger(Nimbus.class);",
                "-    ",
                "+",
                "     //    Metrics",
                "diff --git a/storm-webapp/src/main/java/org/apache/storm/daemon/drpc/DRPCServer.java b/storm-webapp/src/main/java/org/apache/storm/daemon/drpc/DRPCServer.java",
                "index e3ef31cbe..2dc5434b6 100644",
                "--- a/storm-webapp/src/main/java/org/apache/storm/daemon/drpc/DRPCServer.java",
                "+++ b/storm-webapp/src/main/java/org/apache/storm/daemon/drpc/DRPCServer.java",
                "@@ -22,7 +22,7 @@ import com.codahale.metrics.Meter;",
                " import com.google.common.annotations.VisibleForTesting;",
                "-",
                " import java.util.Arrays;",
                "+import java.util.EnumSet;",
                " import java.util.List;",
                " import java.util.Map;",
                "-",
                "+import javax.servlet.DispatcherType;",
                " import org.apache.storm.Config;",
                "@@ -43,4 +43,4 @@ import org.apache.storm.utils.Utils;",
                " import org.eclipse.jetty.server.Server;",
                "+import org.eclipse.jetty.server.ServerConnector;",
                " import org.eclipse.jetty.servlet.FilterHolder;",
                "-import org.eclipse.jetty.servlet.FilterMapping;",
                " import org.eclipse.jetty.servlet.ServletContextHandler;",
                "@@ -66,3 +66,3 @@ public class DRPCServer implements AutoCloseable {",
                "         ReqContextFilter filter = new ReqContextFilter(auth);",
                "-        context.addFilter(new FilterHolder(filter), \"/*\", FilterMapping.ALL);",
                "+        context.addFilter(new FilterHolder(filter), \"/*\", EnumSet.allOf(DispatcherType.class));",
                "     }",
                "@@ -212,3 +212,3 @@ public class DRPCServer implements AutoCloseable {",
                "-        return httpServer.getConnectors()[0].getLocalPort();",
                "+        return ((ServerConnector) (httpServer.getConnectors()[0])).getLocalPort();",
                "     }"
            ],
            "changed_files": [
                "flux/flux-examples/pom.xml",
                "pom.xml",
                "storm-client/pom.xml",
                "storm-core/pom.xml",
                "storm-core/src/clj/org/apache/storm/ui/core.clj",
                "storm-core/src/clj/org/apache/storm/ui/helpers.clj",
                "storm-core/src/jvm/org/apache/storm/ui/UIHelpers.java",
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "storm-webapp/src/main/java/org/apache/storm/daemon/drpc/DRPCServer.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-1927": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-1927",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "a1ef0f6169761721a1c6f9247fdaf8544c2f3d8d",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510696361,
            "hunks": 0,
            "message": "Merge branch 'STORM-2809' of https://github.com/revans2/incubator-storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2809": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2809",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "3489dbce822c502046c204318b14412dee033653",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512639245,
            "hunks": 0,
            "message": "Merge branch 'STORM-2842' of https://github.com/vesense/storm",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2842": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2842",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "15845336fd878f45298cfb49069bdfda326fa5fb",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507963799,
            "hunks": 0,
            "message": "Merge branch 'STORM-2607-fixup' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2607": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2607",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "37d96e7adb4450357958211a5ef568ff84a99cfc",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513288371,
            "hunks": 3,
            "message": "[STORM-2857] Loosen some constraints to support running topologies of older version",
            "diff": [
                "diff --git a/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java b/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "index c78af226c..8c6659f99 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "@@ -564,4 +564,5 @@ public class ConfigValidation {",
                "             SimpleTypeValidator.validateField(name, String.class, o);",
                "+            String className = (String) o;",
                "             try {",
                "-                Class<?> objectClass = Class.forName((String) o);",
                "+                Class<?> objectClass = Class.forName(className);",
                "                 if (!this.classImplements.isAssignableFrom(objectClass)) {",
                "@@ -571,3 +572,12 @@ public class ConfigValidation {",
                "             } catch (ClassNotFoundException e) {",
                "-                throw new RuntimeException(e);",
                "+                //To support topologies of older version to run, we might have to loose the constraints so that",
                "+                //the configs of older version can pass the validation.",
                "+                if (className.startsWith(\"backtype.storm\")) {",
                "+                    LOG.error(\"ClassNotFoundException: {}\", className);",
                "+                    LOG.warn(\"Replace backtype.storm with org.apache.storm and try to validate again\");",
                "+                    LOG.warn(\"We loosen some constraints here to support topologies of older version running on the current version\");",
                "+                    validateField(name, className.replace(\"backtype.storm\", \"org.apache.storm\"));",
                "+                } else {",
                "+                    throw new RuntimeException(e);",
                "+                }",
                "             }"
            ],
            "changed_files": [
                "storm-client/src/jvm/org/apache/storm/validation/ConfigValidation.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2857": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2857",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "427076ebb6761e80f5ef71bbe6843f21854577c8",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515804618,
            "hunks": 12,
            "message": "STORM-2153: add taskId to disruptor metrics",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index 3af9b2c4d..ecbfb144f 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -236,3 +236,3 @@",
                "                                   (.getStormId worker-context)",
                "-                                  component-id",
                "+                                  (first task-ids) component-id",
                "                                   (.getThisWorkerPort worker-context)",
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/worker.clj b/storm-core/src/clj/org/apache/storm/daemon/worker.clj",
                "index b52da5278..dd1195974 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/worker.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/worker.clj",
                "@@ -213,3 +213,3 @@",
                "                                                   (storm-conf TOPOLOGY-DISRUPTOR-WAIT-TIMEOUT-MILLIS)",
                "-                                                  storm-id worker-id port",
                "+                                                  storm-id (int -1) \"__system\" port",
                "                                                   :batch-size (storm-conf TOPOLOGY-DISRUPTOR-BATCH-SIZE)",
                "@@ -258,3 +258,3 @@",
                "                                                   (storm-conf TOPOLOGY-DISRUPTOR-WAIT-TIMEOUT-MILLIS)",
                "-                                                  storm-id worker-id port",
                "+                                                  storm-id (int -1) \"__system\" port",
                "                                                   :batch-size (storm-conf TOPOLOGY-DISRUPTOR-BATCH-SIZE)",
                "diff --git a/storm-core/src/clj/org/apache/storm/disruptor.clj b/storm-core/src/clj/org/apache/storm/disruptor.clj",
                "index c23c5051d..6bbf0df7f 100644",
                "--- a/storm-core/src/clj/org/apache/storm/disruptor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/disruptor.clj",
                "@@ -30,6 +30,6 @@",
                " (defnk disruptor-queue",
                "-  [^String queue-name buffer-size timeout ^String storm-id ^String component-id ^Integer worker-port :producer-type :multi-threaded :batch-size 100 :batch-timeout 1]",
                "+  [^String queue-name buffer-size timeout ^String storm-id ^Integer task-id  ^String component-id ^Integer worker-port :producer-type :multi-threaded :batch-size 100 :batch-timeout 1]",
                "   (DisruptorQueue. queue-name",
                "                    (PRODUCER-TYPE producer-type) buffer-size",
                "-                   timeout batch-size batch-timeout storm-id component-id worker-port))",
                "+                   timeout batch-size batch-timeout storm-id component-id task-id worker-port))",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "index cfeb711c1..1a5bd456d 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "@@ -48,4 +48,4 @@ public class StormMetricRegistry {",
                "-    public static <T> SimpleGauge<T>  gauge(T initialValue, String name, String topologyId, String componentId, Integer port){",
                "-        String metricName = metricName(name, topologyId, componentId, port);",
                "+    public static <T> SimpleGauge<T>  gauge(T initialValue, String name, String topologyId, String componentId, Integer taskId, Integer port){",
                "+        String metricName = metricName(name, topologyId, componentId, taskId, port);",
                "         if(REGISTRY.getGauges().containsKey(metricName)){",
                "@@ -57,12 +57,12 @@ public class StormMetricRegistry {",
                "-    public static DisruptorMetrics disruptorMetrics(String name, String topologyId, String componentId, Integer port){",
                "+    public static DisruptorMetrics disruptorMetrics(String name, String topologyId, String componentId, Integer taskId, Integer port){",
                "         return new DisruptorMetrics(",
                "-                StormMetricRegistry.gauge(0L, name + \"-capacity\", topologyId, componentId, port),",
                "-                StormMetricRegistry.gauge(0L, name + \"-population\", topologyId, componentId, port),",
                "-                StormMetricRegistry.gauge(0L, name + \"-write-position\", topologyId, componentId, port),",
                "-                StormMetricRegistry.gauge(0L, name + \"-read-position\", topologyId, componentId, port),",
                "-                StormMetricRegistry.gauge(0.0, name + \"-arrival-rate\", topologyId, componentId, port),",
                "-                StormMetricRegistry.gauge(0.0, name + \"-sojourn-time-ms\", topologyId, componentId, port),",
                "-                StormMetricRegistry.gauge(0L, name + \"-overflow\", topologyId, componentId, port),",
                "-                StormMetricRegistry.gauge(0.0F, name + \"-percent-full\", topologyId, componentId, port)",
                "+                StormMetricRegistry.gauge(0L, name + \"-capacity\", topologyId, componentId, taskId, port),",
                "+                StormMetricRegistry.gauge(0L, name + \"-population\", topologyId, componentId, taskId, port),",
                "+                StormMetricRegistry.gauge(0L, name + \"-write-position\", topologyId, componentId, taskId, port),",
                "+                StormMetricRegistry.gauge(0L, name + \"-read-position\", topologyId, componentId, taskId, port),",
                "+                StormMetricRegistry.gauge(0.0, name + \"-arrival-rate\", topologyId, componentId, taskId, port),",
                "+                StormMetricRegistry.gauge(0.0, name + \"-sojourn-time-ms\", topologyId, componentId, taskId, port),",
                "+                StormMetricRegistry.gauge(0L, name + \"-overflow\", topologyId, componentId, taskId, port),",
                "+                StormMetricRegistry.gauge(0.0F, name + \"-percent-full\", topologyId, componentId, taskId, port)",
                "         );",
                "@@ -149,3 +149,3 @@ public class StormMetricRegistry {",
                "-    public static String metricName(String name, String stormId, String componentId, Integer workerPort) {",
                "+    public static String metricName(String name, String stormId, String componentId, Integer taskId, Integer workerPort) {",
                "         StringBuilder sb = new StringBuilder(\"storm.worker.\");",
                "@@ -157,2 +157,4 @@ public class StormMetricRegistry {",
                "         sb.append(\".\");",
                "+        sb.append(taskId);",
                "+        sb.append(\".\");",
                "         sb.append(workerPort);",
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java b/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "index d7497d6fd..afa515867 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "@@ -420,3 +420,5 @@ public class DisruptorQueue implements IStatefulObject {",
                "-    public DisruptorQueue(String queueName, ProducerType type, int size, long readTimeout, int inputBatchSize, long flushInterval, String topologyId, String componentId, int port) {",
                "+    // [^String queue-name buffer-size timeout ^String storm-id ^String component-id ^Integer task-id ^Integer worker-port :producer-type :multi-threaded :batch-size 100 :batch-timeout 1]",
                "+",
                "+    public DisruptorQueue(String queueName, ProducerType type, int size, long readTimeout, int inputBatchSize, long flushInterval, String topologyId, String componentId, Integer taskId, int port) {",
                "         this._queueName = PREFIX + queueName;",
                "@@ -434,3 +436,3 @@ public class DisruptorQueue implements IStatefulObject {",
                "         _metrics = new QueueMetrics();",
                "-        _disruptorMetrics = StormMetricRegistry.disruptorMetrics(_queueName, topologyId, componentId, port);",
                "+        _disruptorMetrics = StormMetricRegistry.disruptorMetrics(_queueName, topologyId, componentId, taskId, port);",
                "         //The batch size can be no larger than half the full queue size."
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "storm-core/src/clj/org/apache/storm/daemon/worker.clj",
                "storm-core/src/clj/org/apache/storm/disruptor.clj",
                "storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6402d436a8700eb743ccd84f7f562c51d8cf9be7",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516658390,
            "hunks": 0,
            "message": "Merge branch 'metrics_v2_docs' of https://github.com/ptgoetz/storm into STORM-2904-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2904": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2904",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b612c09d0deba7a6c0f68ab4fcd2e8af22b192a2",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515786279,
            "hunks": 0,
            "message": "Merge branch 'STORM-2892' of https://github.com/erikdw/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2892": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2892",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "ed005ab5fee7c9650c5c2b6ec2cbfb772799da93",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510615648,
            "hunks": 0,
            "message": "Merge branch 'STORM-2811-1.1.x-merge' into 1.1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2811": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2811",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "80cc88112bf4fab34571ebff03782b759d112288",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517545722,
            "hunks": 0,
            "message": "Merge branch 'STORM-2918-1.x-merge' into 1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2918": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2918",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6bc873cd23ecc4d20758afca322f44f6a3713e8c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510607922,
            "hunks": 1,
            "message": "STORM-2809:  Always create the resources directory so we can check for it",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java",
                "index 68415e1ca..1521a968a 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java",
                "@@ -219,2 +219,6 @@ public class LocallyCachedTopologyBlob extends LocallyCachedBlob {",
                "         LOG.debug(\"EXTRACTING {} from {} and placing it at {}\", dir, jarpath, dest);",
                "+        if (!Files.exists(dest)) {",
                "+            //Create the directory no matter what. This is so we can check if it was downloaded in the future.",
                "+            Files.createDirectories(dest);",
                "+        }",
                "         try (JarFile jarFile = new JarFile(jarpath)) {"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/localizer/LocallyCachedTopologyBlob.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2809": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2809",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "315aa9e633d7e90856f1c2b0cc598159f2398140",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513382541,
            "hunks": 0,
            "message": "Merge branch 'STORM-2826' of https://github.com/srdo/storm into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2826": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2826",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "33f543cf6211f8109d968bb27fecd40ae1f3a9d9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517495754,
            "hunks": 0,
            "message": "Merge branch 'storm2873-1.x-branch' of https://github.com/kishorvpatil/incubator-storm into STORM-2873-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2873": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2873",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "52f2e9f34c275e12c062302b8a5e3c0ed2cbe0bc",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509483738,
            "hunks": 1,
            "message": "[STORM-2794] Translate backtype to org.apache for topology.scheduler.strategy if scheduling older version topologies",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "index 57dcd335e..dc557ffc5 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "@@ -111,3 +111,10 @@ public class ResourceAwareScheduler implements IScheduler {",
                "         try {",
                "-            rasStrategy = ReflectionUtils.newSchedulerStrategyInstance((String) td.getConf().get(Config.TOPOLOGY_SCHEDULER_STRATEGY), conf);",
                "+            String strategy = (String) td.getConf().get(Config.TOPOLOGY_SCHEDULER_STRATEGY);",
                "+            if (strategy.startsWith(\"backtype.storm\")) {",
                "+                // Storm supports to launch workers of older version.",
                "+                // If the config of TOPOLOGY_SCHEDULER_STRATEGY comes from the older version, replace the package name.",
                "+                strategy = strategy.replace(\"backtype.storm\", \"org.apache.storm\");",
                "+                LOG.debug(\"Replace backtype.storm with org.apache.storm for Config.TOPOLOGY_SCHEDULER_STRATEGY\");",
                "+            }",
                "+            rasStrategy = ReflectionUtils.newSchedulerStrategyInstance(strategy, conf);",
                "             rasStrategy.prepare(conf);"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2794": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2794",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b246d37419f89a33cc9895129d42952a82f667f3",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512916117,
            "hunks": 0,
            "message": "Merge branch 'STORM-2850' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2850": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2850",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5e696ffe464d3bcd15dbf29adda2d10902727b33",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514263444,
            "hunks": 0,
            "message": "Merge branch 'STORM-2858' of https://github.com/srdo/storm into STORM-2858-test",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2858": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2858",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "da041895a80d1691d1af95ddc47b9a35999ad917",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508785417,
            "hunks": 3,
            "message": "STORM-2786: Enable tick tuples for ackers",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index 52063fc78..66d18513b 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -349,2 +349,3 @@",
                "   (let [storm-conf (:storm-conf executor-data)",
                "+        comp-id (:component-id executor-data)",
                "         tick-time-secs (storm-conf TOPOLOGY-TICK-TUPLE-FREQ-SECS)",
                "@@ -353,13 +354,13 @@",
                "     (when tick-time-secs",
                "-      (if (or (Utils/isSystemId (:component-id executor-data))",
                "+      (if (or (and (not= \"__acker\" comp-id) (Utils/isSystemId comp-id))",
                "               (and (= false (storm-conf TOPOLOGY-ENABLE-MESSAGE-TIMEOUTS))",
                "                    (= :spout (:type executor-data))))",
                "-        (log-message \"Timeouts disabled for executor \" (:component-id executor-data) \":\" (:executor-id executor-data))",
                "-        (schedule-recurring",
                "-          (:user-timer worker)",
                "-          tick-time-secs",
                "-          tick-time-secs",
                "-          (fn []",
                "-            (let [val [(AddressedTuple. AddressedTuple/BROADCAST_DEST (TupleImpl. context [tick-time-secs] Constants/SYSTEM_TASK_ID Constants/SYSTEM_TICK_STREAM_ID))]]",
                "-              (disruptor/publish receive-queue val))))))))",
                "+        (log-message \"Timeouts disabled for executor \" comp-id \":\" (:executor-id executor-data))",
                "+        (let [val [(AddressedTuple. AddressedTuple/BROADCAST_DEST (TupleImpl. context [tick-time-secs] Constants/SYSTEM_TASK_ID Constants/SYSTEM_TICK_STREAM_ID))]]",
                "+          (schedule-recurring",
                "+            (:user-timer worker)",
                "+            tick-time-secs",
                "+            tick-time-secs",
                "+            (fn []",
                "+                (disruptor/publish receive-queue val))))))))"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2786": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2786",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d884f8a3dda9f9d81064280e61eb9e56d4542e87",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509376940,
            "hunks": 15,
            "message": "STORM-2782 - refactor partial key grouping",
            "diff": [
                "diff --git a/storm-client/src/jvm/org/apache/storm/grouping/PartialKeyGrouping.java b/storm-client/src/jvm/org/apache/storm/grouping/PartialKeyGrouping.java",
                "index e1af16de1..70dfeaa62 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/grouping/PartialKeyGrouping.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/grouping/PartialKeyGrouping.java",
                "@@ -19,2 +19,4 @@ package org.apache.storm.grouping;",
                "+import com.google.common.collect.Maps;",
                "+",
                " import java.io.Serializable;",
                "@@ -24,2 +26,4 @@ import java.util.Arrays;",
                " import java.util.List;",
                "+import java.util.Map;",
                "+import java.util.Random;",
                "@@ -29,11 +33,19 @@ import org.apache.storm.tuple.Fields;",
                "-import com.google.common.hash.HashFunction;",
                "-import com.google.common.hash.Hashing;",
                "+/**",
                "+ * A variation on FieldGrouping. This grouping operates on a partitioning of the incoming",
                "+ * tuples (like a FieldGrouping), but it can send Tuples from a given partition to",
                "+ * multiple downstream tasks.",
                "+ *",
                "+ * Given a total pool of target tasks, this grouping will always send Tuples with a given",
                "+ * key to one member of a subset of those tasks. Each key is assigned a subset of tasks.",
                "+ * Each tuple is then sent to one task from that subset.",
                "+ *",
                "+ * Notes:",
                "+ * - the default TaskSelector ensures each task gets as close to a balanced number of Tuples as possible",
                "+ * - the default AssignmentCreator hashes the key and produces an assignment of two tasks",
                "+ */",
                " public class PartialKeyGrouping implements CustomStreamGrouping, Serializable {",
                "-    private static final long serialVersionUID = -447379837314000353L;",
                "+    private static final long serialVersionUID = -1672360572274911808L;",
                "     private List<Integer> targetTasks;",
                "-    private long[] targetTaskStats;",
                "-    private HashFunction h1 = Hashing.murmur3_128(13);",
                "-    private HashFunction h2 = Hashing.murmur3_128(17);",
                "     private Fields fields = null;",
                "@@ -41,4 +53,7 @@ public class PartialKeyGrouping implements CustomStreamGrouping, Serializable {",
                "+    private AssignmentCreator assignmentCreator;",
                "+    private TargetSelector targetSelector;",
                "+",
                "     public PartialKeyGrouping() {",
                "-        //Empty",
                "+        this(null);",
                "     }",
                "@@ -46,3 +61,13 @@ public class PartialKeyGrouping implements CustomStreamGrouping, Serializable {",
                "     public PartialKeyGrouping(Fields fields) {",
                "+        this(fields, new RandomTwoTaskAssignmentCreator(), new BalancedTargetSelector());",
                "+    }",
                "+",
                "+    public PartialKeyGrouping(Fields fields, AssignmentCreator assignmentCreator) {",
                "+        this(fields, assignmentCreator, new BalancedTargetSelector());",
                "+    }",
                "+    ",
                "+    public PartialKeyGrouping(Fields fields, AssignmentCreator assignmentCreator, TargetSelector targetSelector) {",
                "         this.fields = fields;",
                "+        this.assignmentCreator = assignmentCreator;",
                "+        this.targetSelector = targetSelector;",
                "     }",
                "@@ -52,3 +77,2 @@ public class PartialKeyGrouping implements CustomStreamGrouping, Serializable {",
                "         this.targetTasks = targetTasks;",
                "-        targetTaskStats = new long[this.targetTasks.size()];",
                "         if (this.fields != null) {",
                "@@ -62,45 +86,124 @@ public class PartialKeyGrouping implements CustomStreamGrouping, Serializable {",
                "         if (values.size() > 0) {",
                "-            byte[] raw;",
                "-            if (fields != null) {",
                "-                List<Object> selectedFields = outFields.select(fields, values);",
                "-                ByteBuffer out = ByteBuffer.allocate(selectedFields.size() * 4);",
                "-                for (Object o: selectedFields) {",
                "-                    if (o instanceof List) {",
                "-                        out.putInt(Arrays.deepHashCode(((List)o).toArray()));",
                "-                    } else if (o instanceof Object[]) {",
                "-                        out.putInt(Arrays.deepHashCode((Object[])o));",
                "-                    } else if (o instanceof byte[]) {",
                "-                        out.putInt(Arrays.hashCode((byte[]) o));",
                "-                    } else if (o instanceof short[]) {",
                "-                        out.putInt(Arrays.hashCode((short[]) o));",
                "-                    } else if (o instanceof int[]) {",
                "-                        out.putInt(Arrays.hashCode((int[]) o));",
                "-                    } else if (o instanceof long[]) {",
                "-                        out.putInt(Arrays.hashCode((long[]) o));",
                "-                    } else if (o instanceof char[]) {",
                "-                        out.putInt(Arrays.hashCode((char[]) o));",
                "-                    } else if (o instanceof float[]) {",
                "-                        out.putInt(Arrays.hashCode((float[]) o));",
                "-                    } else if (o instanceof double[]) {",
                "-                        out.putInt(Arrays.hashCode((double[]) o));",
                "-                    } else if (o instanceof boolean[]) {",
                "-                        out.putInt(Arrays.hashCode((boolean[]) o));",
                "-                    } else if (o != null) {",
                "-                        out.putInt(o.hashCode());",
                "-                    } else {",
                "-                      out.putInt(0);",
                "-                    }",
                "+            final byte[] rawKeyBytes = getKeyBytes(values);",
                "+",
                "+            final int[] taskAssignmentForKey = assignmentCreator.createAssignment(this.targetTasks, rawKeyBytes);",
                "+            final int selectedTask = targetSelector.chooseTask(taskAssignmentForKey);",
                "+",
                "+            boltIds.add(selectedTask);",
                "+        }",
                "+        return boltIds;",
                "+    }",
                "+",
                "+",
                "+    /**",
                "+     * Extract the key from the input Tuple.",
                "+     */",
                "+    private byte[] getKeyBytes(List<Object> values) {",
                "+        byte[] raw;",
                "+        if (fields != null) {",
                "+            List<Object> selectedFields = outFields.select(fields, values);",
                "+            ByteBuffer out = ByteBuffer.allocate(selectedFields.size() * 4);",
                "+            for (Object o: selectedFields) {",
                "+                if (o instanceof List) {",
                "+                    out.putInt(Arrays.deepHashCode(((List)o).toArray()));",
                "+                } else if (o instanceof Object[]) {",
                "+                    out.putInt(Arrays.deepHashCode((Object[])o));",
                "+                } else if (o instanceof byte[]) {",
                "+                    out.putInt(Arrays.hashCode((byte[]) o));",
                "+                } else if (o instanceof short[]) {",
                "+                    out.putInt(Arrays.hashCode((short[]) o));",
                "+                } else if (o instanceof int[]) {",
                "+                    out.putInt(Arrays.hashCode((int[]) o));",
                "+                } else if (o instanceof long[]) {",
                "+                    out.putInt(Arrays.hashCode((long[]) o));",
                "+                } else if (o instanceof char[]) {",
                "+                    out.putInt(Arrays.hashCode((char[]) o));",
                "+                } else if (o instanceof float[]) {",
                "+                    out.putInt(Arrays.hashCode((float[]) o));",
                "+                } else if (o instanceof double[]) {",
                "+                    out.putInt(Arrays.hashCode((double[]) o));",
                "+                } else if (o instanceof boolean[]) {",
                "+                    out.putInt(Arrays.hashCode((boolean[]) o));",
                "+                } else if (o != null) {",
                "+                    out.putInt(o.hashCode());",
                "+                } else {",
                "+                    out.putInt(0);",
                "                 }",
                "-                raw = out.array();",
                "-            } else {",
                "-                raw = values.get(0).toString().getBytes(); // assume key is the first field",
                "             }",
                "-            int firstChoice = (int) (Math.abs(h1.hashBytes(raw).asLong()) % this.targetTasks.size());",
                "-            int secondChoice = (int) (Math.abs(h2.hashBytes(raw).asLong()) % this.targetTasks.size());",
                "-            int selected = targetTaskStats[firstChoice] > targetTaskStats[secondChoice] ? secondChoice : firstChoice;",
                "-            boltIds.add(targetTasks.get(selected));",
                "-            targetTaskStats[selected]++;",
                "+            raw = out.array();",
                "+        } else {",
                "+            raw = values.get(0).toString().getBytes(); // assume key is the first field",
                "+        }",
                "+        return raw;",
                "+    }",
                "+",
                "+    /*==================================================",
                "+     * Helper Classes",
                "+     *==================================================*/",
                "+",
                "+    /**",
                "+     * This interface is responsible for choosing a subset of the target tasks to use for a given key.",
                "+     *",
                "+     * NOTE: whatever scheme you use to create the assignment should be deterministic. This may be executed on multiple",
                "+     * Storm Workers, thus each of them needs to come up with the same assignment for a given key.",
                "+     */",
                "+    public interface AssignmentCreator extends Serializable {",
                "+        int[] createAssignment(List<Integer> targetTasks, byte[] key);",
                "+    }",
                "+",
                "+    /**",
                "+     * This interface chooses one element from a task assignment to send a specific Tuple to.",
                "+     */",
                "+    public interface TargetSelector extends Serializable {",
                "+        Integer chooseTask(int[] assignedTasks);",
                "+    }",
                "+",
                "+    /*========== Implementations ==========*/",
                "+",
                "+    /**",
                "+     * This implementation of AssignmentCreator chooses two arbitrary tasks.",
                "+     */",
                "+    public static class RandomTwoTaskAssignmentCreator implements AssignmentCreator {",
                "+        /**",
                "+         * Creates a two task assignment by selecting random tasks.",
                "+         */",
                "+        public int[] createAssignment(List<Integer> tasks, byte[] key) {",
                "+            // It is necessary that this produce a deterministic assignment based on the key, so seed the Random from the key",
                "+            final long seedForRandom = Arrays.hashCode(key);",
                "+            final Random random = new Random(seedForRandom);",
                "+            final int choice1 = random.nextInt(tasks.size());",
                "+            int choice2 = random.nextInt(tasks.size());",
                "+            // ensure that choice1 and choice2 are not the same task",
                "+            choice2 = choice1 == choice2 ? (choice2 + 1) % tasks.size() : choice2;",
                "+            return new int[] {tasks.get(choice1), tasks.get(choice2)};",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * A basic implementation of target selection. This strategy chooses the task within the assignment that has",
                "+     * received the fewest Tuples overall from this instance of the grouping.",
                "+     */",
                "+    public static class BalancedTargetSelector implements TargetSelector {",
                "+        private Map<Integer, Long> targetTaskStats = Maps.newHashMap();",
                "+",
                "+        /**",
                "+         * Chooses one of the incoming tasks and selects the one that has been selected",
                "+         * the fewest times so far.",
                "+         */",
                "+        public Integer chooseTask(int[] assignedTasks) {",
                "+            Integer taskIdWithMinLoad = null;",
                "+            Long minTaskLoad = Long.MAX_VALUE;",
                "+",
                "+            for (Integer currentTaskId : assignedTasks) {",
                "+                final Long currentTaskLoad = targetTaskStats.getOrDefault(currentTaskId, 0L);",
                "+                if (currentTaskLoad < minTaskLoad) {",
                "+                    minTaskLoad = currentTaskLoad;",
                "+                    taskIdWithMinLoad = currentTaskId;",
                "+                }",
                "+            }",
                "+",
                "+            targetTaskStats.put(taskIdWithMinLoad, targetTaskStats.getOrDefault(taskIdWithMinLoad, 0L) + 1);",
                "+            return taskIdWithMinLoad;",
                "         }",
                "-        return boltIds;",
                "     }",
                "-}",
                "+}",
                "\\ No newline at end of file"
            ],
            "changed_files": [
                "storm-client/src/jvm/org/apache/storm/grouping/PartialKeyGrouping.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2782": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2782",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f58d472994297d240af8a478d660c3be8a69bd97",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517264101,
            "hunks": 7,
            "message": "[STORM-2916] separate hdfs-blobstore from storm-hdfs",
            "diff": [
                "diff --git a/external/storm-blobstore-migration/pom.xml b/external/storm-blobstore-migration/pom.xml",
                "index c530eec0e..bd1af9070 100644",
                "--- a/external/storm-blobstore-migration/pom.xml",
                "+++ b/external/storm-blobstore-migration/pom.xml",
                "@@ -49,3 +49,3 @@ limitations under the License.",
                "             <groupId>org.apache.storm</groupId>",
                "-            <artifactId>storm-hdfs</artifactId>",
                "+            <artifactId>storm-hdfs-blobstore</artifactId>",
                "             <version>${project.version}</version>",
                "diff --git a/external/storm-hdfs-blobstore/pom.xml b/external/storm-hdfs-blobstore/pom.xml",
                "new file mode 100644",
                "index 000000000..6383e1145",
                "--- /dev/null",
                "+++ b/external/storm-hdfs-blobstore/pom.xml",
                "@@ -0,0 +1,251 @@",
                "+<?xml version=\"1.0\" encoding=\"UTF-8\"?>",
                "+<!--",
                "+ Licensed to the Apache Software Foundation (ASF) under one or more",
                "+ contributor license agreements.  See the NOTICE file distributed with",
                "+ this work for additional information regarding copyright ownership.",
                "+ The ASF licenses this file to You under the Apache License, Version 2.0",
                "+ (the \"License\"); you may not use this file except in compliance with",
                "+ the License.  You may obtain a copy of the License at",
                "+",
                "+     http://www.apache.org/licenses/LICENSE-2.0",
                "+",
                "+ Unless required by applicable law or agreed to in writing, software",
                "+ distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ See the License for the specific language governing permissions and",
                "+ limitations under the License.",
                "+-->",
                "+<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\">",
                "+    <modelVersion>4.0.0</modelVersion>",
                "+",
                "+    <parent>",
                "+        <artifactId>storm</artifactId>",
                "+        <groupId>org.apache.storm</groupId>",
                "+        <version>2.0.0-SNAPSHOT</version>",
                "+        <relativePath>../../pom.xml</relativePath>",
                "+    </parent>",
                "+",
                "+    <artifactId>storm-hdfs-blobstore</artifactId>",
                "+",
                "+    <developers>",
                "+        <developer>",
                "+            <id>ptgoetz</id>",
                "+            <name>P. Taylor Goetz</name>",
                "+            <email>ptgoetz@gmail.com</email>",
                "+        </developer>",
                "+    </developers>",
                "+",
                "+    <dependencies>",
                "+        <dependency>",
                "+            <groupId>org.apache.storm</groupId>",
                "+            <artifactId>storm-client</artifactId>",
                "+            <version>${project.version}</version>",
                "+            <scope>${provided.scope}</scope>",
                "+            <exclusions>",
                "+                <!--log4j-over-slf4j must be excluded for hadoop-minicluster",
                "+                    see: http://stackoverflow.com/q/20469026/3542091 -->",
                "+                <exclusion>",
                "+                    <groupId>org.slf4j</groupId>",
                "+                    <artifactId>log4j-over-slf4j</artifactId>",
                "+                </exclusion>",
                "+            </exclusions>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.apache.hadoop</groupId>",
                "+            <artifactId>hadoop-hdfs</artifactId>",
                "+            <version>${hadoop.version}</version>",
                "+            <exclusions>",
                "+                <exclusion>",
                "+                    <groupId>junit</groupId>",
                "+                    <artifactId>junit</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.mockito</groupId>",
                "+                    <artifactId>mockito-all</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.mortbay.jetty</groupId>",
                "+                    <artifactId>jetty-util</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.mortbay.jetty</groupId>",
                "+                    <artifactId>jetty</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>javax.servlet</groupId>",
                "+                    <artifactId>servlet-api</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>com.sun.jersey</groupId>",
                "+                    <artifactId>jersey-core</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>com.sun.jersey</groupId>",
                "+                    <artifactId>jersey-server</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>javax.servlet.jsp</groupId>",
                "+                    <artifactId>jsp-api</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.slf4j</groupId>",
                "+                    <artifactId>slf4j-api</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>commons-codec</groupId>",
                "+                    <artifactId>commons-codec</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>log4j</groupId>",
                "+                    <artifactId>log4j</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.slf4j</groupId>",
                "+                    <artifactId>slf4j-log4j12</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.apache.hadoop</groupId>",
                "+                    <artifactId>hadoop-minikdc</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.apache.directory.server</groupId>",
                "+                    <artifactId>apacheds-kerberos-codec</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>log4j</groupId>",
                "+                    <artifactId>log4j</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.apache.httpcomponents</groupId>",
                "+                    <artifactId>httpclient</artifactId>",
                "+                </exclusion>",
                "+            </exclusions>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.apache.hadoop</groupId>",
                "+            <artifactId>hadoop-common</artifactId>",
                "+            <version>${hadoop.version}</version>",
                "+            <exclusions>",
                "+                <exclusion>",
                "+                    <groupId>log4j</groupId>",
                "+                    <artifactId>log4j</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.slf4j</groupId>",
                "+                    <artifactId>slf4j-log4j12</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>com.fasterxml.jackson.core</groupId>",
                "+                    <artifactId>jackson-core</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>javax.servlet</groupId>",
                "+                    <artifactId>servlet-api</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.mortbay.jetty</groupId>",
                "+                    <artifactId>jetty</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>org.mortbay.jetty</groupId>",
                "+                    <artifactId>jetty-util</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>com.sun.jersey</groupId>",
                "+                    <artifactId>jersey-json</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>com.sun.jersey</groupId>",
                "+                    <artifactId>jersey-core</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>com.sun.jersey</groupId>",
                "+                    <artifactId>jersey-server</artifactId>",
                "+                </exclusion>",
                "+                <exclusion>",
                "+                    <groupId>javax.servlet.jsp</groupId>",
                "+                    <artifactId>jsp-api</artifactId>",
                "+                </exclusion>",
                "+            </exclusions>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.apache.hadoop</groupId>",
                "+            <artifactId>hadoop-minicluster</artifactId>",
                "+            <version>${hadoop.version}</version>",
                "+            <exclusions>",
                "+                <exclusion>",
                "+                    <groupId>org.slf4j</groupId>",
                "+                    <artifactId>slf4j-log4j12</artifactId>",
                "+                </exclusion>",
                "+            </exclusions>",
                "+            <scope>test</scope>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.apache.storm</groupId>",
                "+            <artifactId>storm-hdfs</artifactId>",
                "+            <version>${project.version}</version>",
                "+            <exclusions>",
                "+                <exclusion>",
                "+                    <groupId>org.slf4j</groupId>",
                "+                    <artifactId>slf4j-log4j12</artifactId>",
                "+                </exclusion>",
                "+            </exclusions>",
                "+            <type>test-jar</type>",
                "+            <scope>test</scope>",
                "+        </dependency>",
                "+",
                "+    </dependencies>",
                "+    <build>",
                "+        <plugins>",
                "+            <plugin>",
                "+                <groupId>org.apache.maven.plugins</groupId>",
                "+                <artifactId>maven-surefire-plugin</artifactId>",
                "+                <configuration>",
                "+                    <reuseForks>false</reuseForks>",
                "+                    <forkCount>1</forkCount>",
                "+                </configuration>",
                "+            </plugin>",
                "+            <plugin>",
                "+                <groupId>org.apache.maven.plugins</groupId>",
                "+                <artifactId>maven-jar-plugin</artifactId>",
                "+                <version>2.2</version>",
                "+                <executions>",
                "+                    <execution>",
                "+                        <goals>",
                "+                            <goal>test-jar</goal>",
                "+                        </goals>",
                "+                    </execution>",
                "+                </executions>",
                "+            </plugin>",
                "+            <plugin>",
                "+                <artifactId>maven-clean-plugin</artifactId>",
                "+                <version>2.5</version>",
                "+                <executions>",
                "+                    <execution>",
                "+                        <id>cleanup</id>",
                "+                        <phase>clean</phase>",
                "+                        <goals>",
                "+                            <goal>clean</goal>",
                "+                        </goals>",
                "+                        <configuration>",
                "+                            <excludeDefaultDirectories>true</excludeDefaultDirectories>",
                "+                            <filesets>",
                "+                                <fileset>",
                "+                                    <directory>./build/</directory>",
                "+                                </fileset>",
                "+                            </filesets>",
                "+                        </configuration>",
                "+                    </execution>",
                "+                </executions>",
                "+            </plugin>",
                "+            <plugin>",
                "+                <groupId>org.apache.maven.plugins</groupId>",
                "+                <artifactId>maven-checkstyle-plugin</artifactId>",
                "+                <!--Note - the version would be inherited-->",
                "+                <configuration>",
                "+                    <maxAllowedViolations>80</maxAllowedViolations>",
                "+                </configuration>",
                "+            </plugin>",
                "+        </plugins>",
                "+    </build>",
                "+</project>",
                "diff --git a/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStore.java b/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStore.java",
                "new file mode 100644",
                "index 000000000..a4b108ffd",
                "--- /dev/null",
                "+++ b/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStore.java",
                "@@ -0,0 +1,394 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.hdfs.blobstore;",
                "+",
                "+import org.apache.hadoop.conf.Configuration;",
                "+import org.apache.hadoop.fs.Path;",
                "+import org.apache.hadoop.security.UserGroupInformation;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.blobstore.AtomicOutputStream;",
                "+import org.apache.storm.blobstore.BlobStore;",
                "+import org.apache.storm.blobstore.BlobStoreAclHandler;",
                "+import org.apache.storm.blobstore.BlobStoreFile;",
                "+import org.apache.storm.blobstore.InputStreamWithMeta;",
                "+import org.apache.storm.generated.AuthorizationException;",
                "+import org.apache.storm.generated.KeyAlreadyExistsException;",
                "+import org.apache.storm.generated.KeyNotFoundException;",
                "+import org.apache.storm.generated.ReadableBlobMeta;",
                "+import org.apache.storm.generated.SettableBlobMeta;",
                "+import org.apache.storm.nimbus.NimbusInfo;",
                "+import org.apache.storm.utils.Utils;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+import javax.security.auth.Subject;",
                "+import java.io.ByteArrayOutputStream;",
                "+import java.io.FileNotFoundException;",
                "+import java.io.IOException;",
                "+import java.io.InputStream;",
                "+import java.security.AccessController;",
                "+import java.security.PrivilegedAction;",
                "+import java.util.HashMap;",
                "+import java.util.Iterator;",
                "+import java.util.Map;",
                "+",
                "+import static org.apache.storm.blobstore.BlobStoreAclHandler.*;",
                "+",
                "+/**",
                "+ * Provides a HDFS file system backed blob store implementation.",
                "+ * Note that this provides an api for having HDFS be the backing store for the blobstore,",
                "+ * it is not a service/daemon.",
                "+ *",
                "+ * We currently have NIMBUS_ADMINS and SUPERVISOR_ADMINS configuration. NIMBUS_ADMINS are given READ, WRITE and ADMIN",
                "+ * access whereas the SUPERVISOR_ADMINS are given READ access in order to read and download the blobs form the nimbus.",
                "+ *",
                "+ * The ACLs for the blob store are validated against whether the subject is a NIMBUS_ADMIN, SUPERVISOR_ADMIN or USER",
                "+ * who has read, write or admin privileges in order to perform respective operations on the blob.",
                "+ *",
                "+ * For hdfs blob store",
                "+ * 1. The USER interacts with nimbus to upload and access blobs through NimbusBlobStore Client API. Here, unlike",
                "+ * local blob store which stores the blobs locally, the nimbus talks to HDFS to upload the blobs.",
                "+ * 2. The USER sets the ACLs, and the blob access is validated against these ACLs.",
                "+ * 3. The SUPERVISOR interacts with nimbus through HdfsClientBlobStore to download the blobs. Here, unlike local",
                "+ * blob store the supervisor interacts with HDFS directly to download the blobs. The call to HdfsBlobStore is made as a \"null\"",
                "+ * subject. The blobstore gets the hadoop user and validates permissions for the supervisor.",
                "+ */",
                "+public class HdfsBlobStore extends BlobStore {",
                "+    public static final Logger LOG = LoggerFactory.getLogger(HdfsBlobStore.class);",
                "+    private static final String DATA_PREFIX = \"data_\";",
                "+    private static final String META_PREFIX = \"meta_\";",
                "+    private static final HashMap<String, Subject> alreadyLoggedInUsers = new HashMap<>();",
                "+",
                "+    private BlobStoreAclHandler aclHandler;",
                "+    private HdfsBlobStoreImpl hbs;",
                "+    private Subject localSubject;",
                "+    private Map<String, Object> conf;",
                "+",
                "+    /**",
                "+     * Get the subject from Hadoop so we can use it to validate the acls. There is no direct",
                "+     * interface from UserGroupInformation to get the subject, so do a doAs and get the context.",
                "+     * We could probably run everything in the doAs but for now just grab the subject.",
                "+     */",
                "+    private Subject getHadoopUser() {",
                "+        Subject subj;",
                "+        try {",
                "+            subj = UserGroupInformation.getCurrentUser().doAs(",
                "+                    new PrivilegedAction<Subject>() {",
                "+                        @Override",
                "+                        public Subject run() {",
                "+                            return Subject.getSubject(AccessController.getContext());",
                "+                        }",
                "+                    });",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(\"Error creating subject and logging user in!\", e);",
                "+        }",
                "+        return subj;",
                "+    }",
                "+",
                "+    /**",
                "+     * If who is null then we want to use the user hadoop says we are.",
                "+     * Required for the supervisor to call these routines as its not",
                "+     * logged in as anyone.",
                "+     */",
                "+    private Subject checkAndGetSubject(Subject who) {",
                "+        if (who == null) {",
                "+            return localSubject;",
                "+        }",
                "+        return who;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void prepare(Map<String, Object> conf, String overrideBase, NimbusInfo nimbusInfo) {",
                "+        this.conf = conf;",
                "+        prepareInternal(conf, overrideBase, null);",
                "+    }",
                "+",
                "+    /**",
                "+     * Allow a Hadoop Configuration to be passed for testing. If it's null then the hadoop configs",
                "+     * must be in your classpath.",
                "+     */",
                "+    protected void prepareInternal(Map<String, Object> conf, String overrideBase, Configuration hadoopConf) {",
                "+        this.conf = conf;",
                "+        if (overrideBase == null) {",
                "+            overrideBase = (String)conf.get(Config.BLOBSTORE_DIR);",
                "+        }",
                "+        if (overrideBase == null) {",
                "+            throw new RuntimeException(\"You must specify a blobstore directory for HDFS to use!\");",
                "+        }",
                "+        LOG.debug(\"directory is: {}\", overrideBase);",
                "+        try {",
                "+            // if a HDFS keytab/principal have been supplied login, otherwise assume they are",
                "+            // logged in already or running insecure HDFS.",
                "+            String principal = (String) conf.get(Config.BLOBSTORE_HDFS_PRINCIPAL);",
                "+            String keyTab = (String) conf.get(Config.BLOBSTORE_HDFS_KEYTAB);",
                "+",
                "+            if (principal != null && keyTab != null) {",
                "+                String combinedKey = principal + \" from \" + keyTab;",
                "+                synchronized (alreadyLoggedInUsers) {",
                "+                    localSubject = alreadyLoggedInUsers.get(combinedKey);",
                "+                    if (localSubject == null) {",
                "+                        UserGroupInformation.loginUserFromKeytab(principal, keyTab);",
                "+                        localSubject = getHadoopUser();",
                "+                        alreadyLoggedInUsers.put(combinedKey, localSubject);",
                "+                    }",
                "+                }",
                "+            } else {",
                "+                if (principal == null && keyTab != null) {",
                "+                    throw new RuntimeException(\"You must specify an HDFS principal to go with the keytab!\");",
                "+",
                "+                } else {",
                "+                    if (principal != null && keyTab == null) {",
                "+                        throw new RuntimeException(\"You must specify HDFS keytab go with the principal!\");",
                "+                    }",
                "+                }",
                "+                localSubject = getHadoopUser();",
                "+            }",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(\"Error logging in from keytab!\", e);",
                "+        }",
                "+        aclHandler = new BlobStoreAclHandler(conf);",
                "+        Path baseDir = new Path(overrideBase, BASE_BLOBS_DIR_NAME);",
                "+        try {",
                "+            if (hadoopConf != null) {",
                "+                hbs = new HdfsBlobStoreImpl(baseDir, conf, hadoopConf);",
                "+            } else {",
                "+                hbs = new HdfsBlobStoreImpl(baseDir, conf);",
                "+            }",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public AtomicOutputStream createBlob(String key, SettableBlobMeta meta, Subject who)",
                "+            throws AuthorizationException, KeyAlreadyExistsException {",
                "+        if (meta.get_replication_factor() <= 0) {",
                "+            meta.set_replication_factor((int)conf.get(Config.STORM_BLOBSTORE_REPLICATION_FACTOR));",
                "+        }",
                "+        who = checkAndGetSubject(who);",
                "+        validateKey(key);",
                "+        aclHandler.normalizeSettableBlobMeta(key, meta, who, READ | WRITE | ADMIN);",
                "+        BlobStoreAclHandler.validateSettableACLs(key, meta.get_acl());",
                "+        aclHandler.hasPermissions(meta.get_acl(), READ | WRITE | ADMIN, who, key);",
                "+        if (hbs.exists(DATA_PREFIX + key)) {",
                "+            throw new KeyAlreadyExistsException(key);",
                "+        }",
                "+        BlobStoreFileOutputStream mOut = null;",
                "+        try {",
                "+            BlobStoreFile metaFile = hbs.write(META_PREFIX + key, true);",
                "+            metaFile.setMetadata(meta);",
                "+            mOut = new BlobStoreFileOutputStream(metaFile);",
                "+            mOut.write(Utils.thriftSerialize(meta));",
                "+            mOut.close();",
                "+            mOut = null;",
                "+            BlobStoreFile dataFile = hbs.write(DATA_PREFIX + key, true);",
                "+            dataFile.setMetadata(meta);",
                "+            return new BlobStoreFileOutputStream(dataFile);",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(e);",
                "+        } finally {",
                "+            if (mOut != null) {",
                "+                try {",
                "+                    mOut.cancel();",
                "+                } catch (IOException e) {",
                "+                    //Ignored",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public AtomicOutputStream updateBlob(String key, Subject who)",
                "+            throws AuthorizationException, KeyNotFoundException {",
                "+        who = checkAndGetSubject(who);",
                "+        SettableBlobMeta meta = getStoredBlobMeta(key);",
                "+        validateKey(key);",
                "+        aclHandler.hasPermissions(meta.get_acl(), WRITE, who, key);",
                "+        try {",
                "+            BlobStoreFile dataFile = hbs.write(DATA_PREFIX + key, false);",
                "+            dataFile.setMetadata(meta);",
                "+            return new BlobStoreFileOutputStream(dataFile);",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "+    private SettableBlobMeta getStoredBlobMeta(String key) throws KeyNotFoundException {",
                "+        InputStream in = null;",
                "+        try {",
                "+            BlobStoreFile pf = hbs.read(META_PREFIX + key);",
                "+            try {",
                "+                in = pf.getInputStream();",
                "+            } catch (FileNotFoundException fnf) {",
                "+                throw new KeyNotFoundException(key);",
                "+            }",
                "+            ByteArrayOutputStream out = new ByteArrayOutputStream();",
                "+            byte[] buffer = new byte[2048];",
                "+            int len;",
                "+            while ((len = in.read(buffer)) > 0) {",
                "+                out.write(buffer, 0, len);",
                "+            }",
                "+            in.close();",
                "+            in = null;",
                "+            return Utils.thriftDeserialize(SettableBlobMeta.class, out.toByteArray());",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(e);",
                "+        } finally {",
                "+            if (in != null) {",
                "+                try {",
                "+                    in.close();",
                "+                } catch (IOException e) {",
                "+                    //Ignored",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public ReadableBlobMeta getBlobMeta(String key, Subject who)",
                "+            throws AuthorizationException, KeyNotFoundException {",
                "+        who = checkAndGetSubject(who);",
                "+        validateKey(key);",
                "+        SettableBlobMeta meta = getStoredBlobMeta(key);",
                "+        aclHandler.validateUserCanReadMeta(meta.get_acl(), who, key);",
                "+        ReadableBlobMeta rbm = new ReadableBlobMeta();",
                "+        rbm.set_settable(meta);",
                "+        try {",
                "+            BlobStoreFile pf = hbs.read(DATA_PREFIX + key);",
                "+            rbm.set_version(pf.getModTime());",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+        return rbm;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void setBlobMeta(String key, SettableBlobMeta meta, Subject who)",
                "+            throws AuthorizationException, KeyNotFoundException {",
                "+        if (meta.get_replication_factor() <= 0) {",
                "+            meta.set_replication_factor((int)conf.get(Config.STORM_BLOBSTORE_REPLICATION_FACTOR));",
                "+        }",
                "+        who = checkAndGetSubject(who);",
                "+        validateKey(key);",
                "+        aclHandler.normalizeSettableBlobMeta(key,  meta, who, ADMIN);",
                "+        BlobStoreAclHandler.validateSettableACLs(key, meta.get_acl());",
                "+        SettableBlobMeta orig = getStoredBlobMeta(key);",
                "+        aclHandler.hasPermissions(orig.get_acl(), ADMIN, who, key);",
                "+        BlobStoreFileOutputStream mOut = null;",
                "+        writeMetadata(key, meta);",
                "+    }",
                "+",
                "+    @Override",
                "+    public void deleteBlob(String key, Subject who)",
                "+            throws AuthorizationException, KeyNotFoundException {",
                "+        who = checkAndGetSubject(who);",
                "+        validateKey(key);",
                "+        SettableBlobMeta meta = getStoredBlobMeta(key);",
                "+        aclHandler.hasPermissions(meta.get_acl(), WRITE, who, key);",
                "+        try {",
                "+            hbs.deleteKey(DATA_PREFIX + key);",
                "+            hbs.deleteKey(META_PREFIX + key);",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public InputStreamWithMeta getBlob(String key, Subject who)",
                "+            throws AuthorizationException, KeyNotFoundException {",
                "+        who = checkAndGetSubject(who);",
                "+        validateKey(key);",
                "+        SettableBlobMeta meta = getStoredBlobMeta(key);",
                "+        aclHandler.hasPermissions(meta.get_acl(), READ, who, key);",
                "+        try {",
                "+            return new BlobStoreFileInputStream(hbs.read(DATA_PREFIX + key));",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public Iterator<String> listKeys() {",
                "+        try {",
                "+            return new KeyTranslationIterator(hbs.listKeys(), DATA_PREFIX);",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public void shutdown() {",
                "+        //Empty",
                "+    }",
                "+",
                "+    @Override",
                "+    public int getBlobReplication(String key, Subject who) throws AuthorizationException, KeyNotFoundException {",
                "+        who = checkAndGetSubject(who);",
                "+        validateKey(key);",
                "+        SettableBlobMeta meta = getStoredBlobMeta(key);",
                "+        aclHandler.hasAnyPermissions(meta.get_acl(), READ | WRITE | ADMIN, who, key);",
                "+        try {",
                "+            return hbs.getBlobReplication(DATA_PREFIX + key);",
                "+        } catch (IOException exp) {",
                "+            throw new RuntimeException(exp);",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public int updateBlobReplication(String key, int replication, Subject who) throws AuthorizationException, KeyNotFoundException {",
                "+        who = checkAndGetSubject(who);",
                "+        validateKey(key);",
                "+        SettableBlobMeta meta = getStoredBlobMeta(key);",
                "+        meta.set_replication_factor(replication);",
                "+        aclHandler.hasAnyPermissions(meta.get_acl(), WRITE | ADMIN, who, key);",
                "+        try {",
                "+            writeMetadata(key, meta);",
                "+            return hbs.updateBlobReplication(DATA_PREFIX + key, replication);",
                "+        } catch (IOException exp) {",
                "+            throw new RuntimeException(exp);",
                "+        }",
                "+    }",
                "+",
                "+    public void writeMetadata(String key, SettableBlobMeta meta)",
                "+            throws AuthorizationException, KeyNotFoundException {",
                "+        BlobStoreFileOutputStream mOut = null;",
                "+        try {",
                "+            BlobStoreFile hdfsFile = hbs.write(META_PREFIX + key, false);",
                "+            hdfsFile.setMetadata(meta);",
                "+            mOut = new BlobStoreFileOutputStream(hdfsFile);",
                "+            mOut.write(Utils.thriftSerialize(meta));",
                "+            mOut.close();",
                "+            mOut = null;",
                "+        } catch (IOException exp) {",
                "+            throw new RuntimeException(exp);",
                "+        } finally {",
                "+            if (mOut != null) {",
                "+                try {",
                "+                    mOut.cancel();",
                "+                } catch (IOException e) {",
                "+                    //Ignored",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    public void fullCleanup(long age) throws IOException {",
                "+        hbs.fullCleanup(age);",
                "+    }",
                "+}",
                "diff --git a/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreFile.java b/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreFile.java",
                "new file mode 100644",
                "index 000000000..3021e66a0",
                "--- /dev/null",
                "+++ b/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreFile.java",
                "@@ -0,0 +1,196 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+package org.apache.storm.hdfs.blobstore;",
                "+",
                "+import org.apache.hadoop.conf.Configuration;",
                "+import org.apache.hadoop.fs.FileContext;",
                "+import org.apache.hadoop.fs.FileSystem;",
                "+import org.apache.hadoop.fs.Options;",
                "+import org.apache.hadoop.fs.Path;",
                "+import org.apache.hadoop.fs.permission.FsPermission;",
                "+import org.apache.storm.blobstore.BlobStoreFile;",
                "+import org.apache.storm.generated.SettableBlobMeta;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+import java.io.IOException;",
                "+import java.io.InputStream;",
                "+import java.io.OutputStream;",
                "+import java.util.regex.Matcher;",
                "+",
                "+public class HdfsBlobStoreFile extends BlobStoreFile {",
                "+    public static final Logger LOG = LoggerFactory.getLogger(HdfsBlobStoreFile.class);",
                "+",
                "+    private final String _key;",
                "+    private final boolean _isTmp;",
                "+    private final Path _path;",
                "+    private Long _modTime = null;",
                "+    private final boolean _mustBeNew;",
                "+    private final Configuration _hadoopConf;",
                "+    private final FileSystem _fs;",
                "+    private SettableBlobMeta meta;",
                "+",
                "+    // files are world-wide readable and owner writable",
                "+    final public static FsPermission BLOBSTORE_FILE_PERMISSION =",
                "+            FsPermission.createImmutable((short) 0644); // rw-r--r--",
                "+",
                "+    public HdfsBlobStoreFile(Path base, String name, Configuration hconf) {",
                "+        if (BLOBSTORE_DATA_FILE.equals(name)) {",
                "+            _isTmp = false;",
                "+        } else {",
                "+            Matcher m = TMP_NAME_PATTERN.matcher(name);",
                "+            if (!m.matches()) {",
                "+                throw new IllegalArgumentException(\"File name does not match '\"+name+\"' !~ \"+TMP_NAME_PATTERN);",
                "+            }",
                "+            _isTmp = true;",
                "+        }",
                "+        _hadoopConf = hconf;",
                "+        _key = base.getName();",
                "+        _path = new Path(base, name);",
                "+        _mustBeNew = false;",
                "+        try {",
                "+            _fs = _path.getFileSystem(_hadoopConf);",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(\"Error getting filesystem for path: \" + _path, e);",
                "+        }",
                "+    }",
                "+",
                "+    public HdfsBlobStoreFile(Path base, boolean isTmp, boolean mustBeNew, Configuration hconf) {",
                "+        _key = base.getName();",
                "+        _hadoopConf = hconf;",
                "+        _isTmp = isTmp;",
                "+        _mustBeNew = mustBeNew;",
                "+        if (_isTmp) {",
                "+            _path = new Path(base, System.currentTimeMillis()+TMP_EXT);",
                "+        } else {",
                "+            _path = new Path(base, BLOBSTORE_DATA_FILE);",
                "+        }",
                "+        try {",
                "+            _fs = _path.getFileSystem(_hadoopConf);",
                "+        } catch (IOException e) {",
                "+            throw new RuntimeException(\"Error getting filesystem for path: \" + _path, e);",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public void delete() throws IOException {",
                "+        _fs.delete(_path, true);",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean isTmp() {",
                "+        return _isTmp;",
                "+    }",
                "+",
                "+    @Override",
                "+    public String getKey() {",
                "+        return _key;",
                "+    }",
                "+",
                "+    @Override",
                "+    public long getModTime() throws IOException {",
                "+        if (_modTime == null) {",
                "+            FileSystem fs = _path.getFileSystem(_hadoopConf);",
                "+            _modTime = fs.getFileStatus(_path).getModificationTime();",
                "+        }",
                "+        return _modTime;",
                "+    }",
                "+",
                "+    private void checkIsNotTmp() {",
                "+        if (!isTmp()) {",
                "+            throw new IllegalStateException(\"Can only operate on a temporary blobstore file.\");",
                "+        }",
                "+    }",
                "+",
                "+    private void checkIsTmp() {",
                "+        if (isTmp()) {",
                "+            throw new IllegalStateException(\"Cannot operate on a temporary blobstore file.\");",
                "+        }",
                "+    }",
                "+",
                "+    @Override",
                "+    public InputStream getInputStream() throws IOException {",
                "+        checkIsTmp();",
                "+        return _fs.open(_path);",
                "+    }",
                "+",
                "+    @Override",
                "+    public OutputStream getOutputStream() throws IOException {",
                "+        checkIsNotTmp();",
                "+        OutputStream out = null;",
                "+        FsPermission fileperms = new FsPermission(BLOBSTORE_FILE_PERMISSION);",
                "+        try {",
                "+            out = _fs.create(_path, (short)this.getMetadata().get_replication_factor());",
                "+            _fs.setPermission(_path, fileperms);",
                "+            _fs.setReplication(_path, (short)this.getMetadata().get_replication_factor());",
                "+        } catch (IOException e) {",
                "+            //Try to create the parent directory, may not work",
                "+            FsPermission dirperms = new FsPermission(HdfsBlobStoreImpl.BLOBSTORE_DIR_PERMISSION);",
                "+            if (!_fs.mkdirs(_path.getParent(), dirperms)) {",
                "+                LOG.warn(\"error creating parent dir: \" + _path.getParent());",
                "+            }",
                "+            out = _fs.create(_path, (short)this.getMetadata().get_replication_factor());",
                "+            _fs.setPermission(_path, dirperms);",
                "+            _fs.setReplication(_path, (short)this.getMetadata().get_replication_factor());",
                "+        }",
                "+        if (out == null) {",
                "+            throw new IOException(\"Error in creating: \" + _path);",
                "+        }",
                "+        return out;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void commit() throws IOException {",
                "+        checkIsNotTmp();",
                "+        // FileContext supports atomic rename, whereas FileSystem doesn't",
                "+        FileContext fc = FileContext.getFileContext(_hadoopConf);",
                "+        Path dest = new Path(_path.getParent(), BLOBSTORE_DATA_FILE);",
                "+        if (_mustBeNew) {",
                "+            fc.rename(_path, dest);",
                "+        } else {",
                "+            fc.rename(_path, dest, Options.Rename.OVERWRITE);",
                "+        }",
                "+        // Note, we could add support for setting the replication factor",
                "+    }",
                "+",
                "+    @Override",
                "+    public void cancel() throws IOException {",
                "+        checkIsNotTmp();",
                "+        delete();",
                "+    }",
                "+",
                "+    @Override",
                "+    public String toString() {",
                "+        return _path+\":\"+(_isTmp ? \"tmp\": BlobStoreFile.BLOBSTORE_DATA_FILE)+\":\"+_key;",
                "+    }",
                "+",
                "+    @Override",
                "+    public long getFileLength() throws IOException {",
                "+        return _fs.getFileStatus(_path).getLen();",
                "+    }",
                "+",
                "+    @Override",
                "+    public SettableBlobMeta getMetadata() {",
                "+        return meta;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void setMetadata(SettableBlobMeta meta) {",
                "+        this.meta = meta;",
                "+    }",
                "+}",
                "diff --git a/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreImpl.java b/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreImpl.java",
                "new file mode 100644",
                "index 000000000..702a16fb5",
                "--- /dev/null",
                "+++ b/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreImpl.java",
                "@@ -0,0 +1,312 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+package org.apache.storm.hdfs.blobstore;",
                "+",
                "+import org.apache.hadoop.conf.Configuration;",
                "+import org.apache.hadoop.fs.FileStatus;",
                "+import org.apache.hadoop.fs.FileSystem;",
                "+import org.apache.hadoop.fs.Path;",
                "+import org.apache.hadoop.fs.permission.FsPermission;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.blobstore.BlobStoreFile;",
                "+import org.apache.storm.utils.ObjectReader;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+import java.io.FileNotFoundException;",
                "+import java.io.IOException;",
                "+import java.util.ArrayList;",
                "+import java.util.Iterator;",
                "+import java.util.Map;",
                "+import java.util.NoSuchElementException;",
                "+import java.util.Timer;",
                "+import java.util.TimerTask;",
                "+",
                "+/**",
                "+ * HDFS blob store impl.",
                "+ */",
                "+public class HdfsBlobStoreImpl {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(HdfsBlobStoreImpl.class);",
                "+",
                "+    private static final long FULL_CLEANUP_FREQ = 60 * 60 * 1000l;",
                "+    private static final int BUCKETS = 1024;",
                "+    private static final String BLOBSTORE_DATA = \"data\";",
                "+    ",
                "+    private Timer timer;",
                "+",
                "+    public class KeyInHashDirIterator implements Iterator<String> {",
                "+        private int currentBucket = 0;",
                "+        private Iterator<String> it = null;",
                "+        private String next = null;",
                "+",
                "+        public KeyInHashDirIterator() throws IOException {",
                "+            primeNext();",
                "+        }",
                "+",
                "+        private void primeNext() throws IOException {",
                "+            while (it == null && currentBucket < BUCKETS) {",
                "+                String name = String.valueOf(currentBucket);",
                "+                Path dir = new Path(_fullPath, name);",
                "+                try {",
                "+                    it = listKeys(dir);",
                "+                } catch (FileNotFoundException e) {",
                "+                    it = null;",
                "+                }",
                "+                if (it == null || !it.hasNext()) {",
                "+                    it = null;",
                "+                    currentBucket++;",
                "+                } else {",
                "+                    next = it.next();",
                "+                }",
                "+            }",
                "+        }",
                "+",
                "+        @Override",
                "+        public boolean hasNext() {",
                "+            return next != null;",
                "+        }",
                "+",
                "+        @Override",
                "+        public String next() {",
                "+            if (!hasNext()) {",
                "+                throw new NoSuchElementException();",
                "+            }",
                "+            String current = next;",
                "+            next = null;",
                "+            if (it != null) {",
                "+                if (!it.hasNext()) {",
                "+                    it = null;",
                "+                    currentBucket++;",
                "+                    try {",
                "+                        primeNext();",
                "+                    } catch (IOException e) {",
                "+                        throw new RuntimeException(e);",
                "+                    }",
                "+                } else {",
                "+                    next = it.next();",
                "+                }",
                "+            }",
                "+            return current;",
                "+        }",
                "+",
                "+        @Override",
                "+        public void remove() {",
                "+            throw new UnsupportedOperationException(\"Delete Not Supported\");",
                "+        }",
                "+    }",
                "+",
                "+",
                "+    private Path _fullPath;",
                "+    private FileSystem _fs;",
                "+    private Configuration _hadoopConf;",
                "+",
                "+    // blobstore directory is private!",
                "+    final public static FsPermission BLOBSTORE_DIR_PERMISSION =",
                "+            FsPermission.createImmutable((short) 0700); // rwx--------",
                "+",
                "+    public HdfsBlobStoreImpl(Path path, Map<String, Object> conf) throws IOException {",
                "+        this(path, conf, new Configuration());",
                "+    }",
                "+",
                "+    public HdfsBlobStoreImpl(Path path, Map<String, Object> conf,",
                "+                             Configuration hconf) throws IOException {",
                "+        LOG.info(\"Blob store based in {}\", path);",
                "+        _fullPath = path;",
                "+        _hadoopConf = hconf;",
                "+        _fs = path.getFileSystem(_hadoopConf);",
                "+",
                "+        if (!_fs.exists(_fullPath)) {",
                "+            FsPermission perms = new FsPermission(BLOBSTORE_DIR_PERMISSION);",
                "+            boolean success = _fs.mkdirs(_fullPath, perms);",
                "+            if (!success) {",
                "+                throw new IOException(\"Error creating blobstore directory: \" + _fullPath);",
                "+            }",
                "+        }",
                "+",
                "+        Object shouldCleanup = conf.get(Config.BLOBSTORE_CLEANUP_ENABLE);",
                "+        if (ObjectReader.getBoolean(shouldCleanup, false)) {",
                "+            LOG.debug(\"Starting hdfs blobstore cleaner\");",
                "+            TimerTask cleanup = new TimerTask() {",
                "+                @Override",
                "+                public void run() {",
                "+                    try {",
                "+                        fullCleanup(FULL_CLEANUP_FREQ);",
                "+                    } catch (IOException e) {",
                "+                        LOG.error(\"Error trying to cleanup\", e);",
                "+                    }",
                "+                }",
                "+            };",
                "+            timer = new Timer(\"HdfsBlobStore cleanup thread\", true);",
                "+            timer.scheduleAtFixedRate(cleanup, 0, FULL_CLEANUP_FREQ);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * @return all keys that are available for reading.",
                "+     * @throws IOException on any error.",
                "+     */",
                "+    public Iterator<String> listKeys() throws IOException {",
                "+        return new KeyInHashDirIterator();",
                "+    }",
                "+",
                "+    /**",
                "+     * Get an input stream for reading a part.",
                "+     *",
                "+     * @param key the key of the part to read.",
                "+     * @return the where to read the data from.",
                "+     * @throws IOException on any error",
                "+     */",
                "+    public BlobStoreFile read(String key) throws IOException {",
                "+        return new HdfsBlobStoreFile(getKeyDir(key), BLOBSTORE_DATA, _hadoopConf);",
                "+    }",
                "+",
                "+    /**",
                "+     * Get an object tied to writing the data.",
                "+     *",
                "+     * @param key the key of the part to write to.",
                "+     * @param create whether the file needs to be new or not.",
                "+     * @return an object that can be used to both write to, but also commit/cancel the operation.",
                "+     * @throws IOException on any error",
                "+     */",
                "+    public BlobStoreFile write(String key, boolean create) throws IOException {",
                "+        return new HdfsBlobStoreFile(getKeyDir(key), true, create, _hadoopConf);",
                "+    }",
                "+",
                "+    /**",
                "+     * Check if the key exists in the blob store.",
                "+     *",
                "+     * @param key the key to check for",
                "+     * @return true if it exists else false.",
                "+     */",
                "+    public boolean exists(String key) {",
                "+        Path dir = getKeyDir(key);",
                "+        boolean res = false;",
                "+        try {",
                "+            _fs = dir.getFileSystem(_hadoopConf);",
                "+            res = _fs.exists(dir);",
                "+        } catch (IOException e) {",
                "+            LOG.warn(\"Exception checking for exists on: \" + key);",
                "+        }",
                "+        return res;",
                "+    }",
                "+",
                "+    /**",
                "+     * Delete a key from the blob store",
                "+     *",
                "+     * @param key the key to delete",
                "+     * @throws IOException on any error",
                "+     */",
                "+    public void deleteKey(String key) throws IOException {",
                "+        Path keyDir = getKeyDir(key);",
                "+        HdfsBlobStoreFile pf = new HdfsBlobStoreFile(keyDir, BLOBSTORE_DATA,",
                "+                _hadoopConf);",
                "+        pf.delete();",
                "+        delete(keyDir);",
                "+    }",
                "+",
                "+    protected Path getKeyDir(String key) {",
                "+        String hash = String.valueOf(Math.abs((long) key.hashCode()) % BUCKETS);",
                "+        Path hashDir = new Path(_fullPath, hash);",
                "+",
                "+        Path ret = new Path(hashDir, key);",
                "+        LOG.debug(\"{} Looking for {} in {}\", new Object[]{_fullPath, key, hash});",
                "+        return ret;",
                "+    }",
                "+",
                "+    public void fullCleanup(long age) throws IOException {",
                "+        long cleanUpIfBefore = System.currentTimeMillis() - age;",
                "+        Iterator<String> keys = new KeyInHashDirIterator();",
                "+        while (keys.hasNext()) {",
                "+            String key = keys.next();",
                "+            Path keyDir = getKeyDir(key);",
                "+            Iterator<BlobStoreFile> i = listBlobStoreFiles(keyDir);",
                "+            if (!i.hasNext()) {",
                "+                //The dir is empty, so try to delete it, may fail, but that is OK",
                "+                try {",
                "+                    _fs.delete(keyDir, true);",
                "+                } catch (Exception e) {",
                "+                    LOG.warn(\"Could not delete \" + keyDir + \" will try again later\");",
                "+                }",
                "+            }",
                "+            while (i.hasNext()) {",
                "+                BlobStoreFile f = i.next();",
                "+                if (f.isTmp()) {",
                "+                    if (f.getModTime() <= cleanUpIfBefore) {",
                "+                        f.delete();",
                "+                    }",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    protected Iterator<BlobStoreFile> listBlobStoreFiles(Path path) throws IOException {",
                "+        ArrayList<BlobStoreFile> ret = new ArrayList<BlobStoreFile>();",
                "+        FileStatus[] files = _fs.listStatus(new Path[]{path});",
                "+        if (files != null) {",
                "+            for (FileStatus sub : files) {",
                "+                try {",
                "+                    ret.add(new HdfsBlobStoreFile(sub.getPath().getParent(), sub.getPath().getName(),",
                "+                            _hadoopConf));",
                "+                } catch (IllegalArgumentException e) {",
                "+                    //Ignored the file did not match",
                "+                    LOG.warn(\"Found an unexpected file in {} {}\", path, sub.getPath().getName());",
                "+                }",
                "+            }",
                "+        }",
                "+        return ret.iterator();",
                "+    }",
                "+",
                "+    protected Iterator<String> listKeys(Path path) throws IOException {",
                "+        ArrayList<String> ret = new ArrayList<String>();",
                "+        FileStatus[] files = _fs.listStatus(new Path[]{path});",
                "+        if (files != null) {",
                "+            for (FileStatus sub : files) {",
                "+                try {",
                "+                    ret.add(sub.getPath().getName().toString());",
                "+                } catch (IllegalArgumentException e) {",
                "+                    //Ignored the file did not match",
                "+                    LOG.debug(\"Found an unexpected file in {} {}\", path, sub.getPath().getName());",
                "+                }",
                "+            }",
                "+        }",
                "+        return ret.iterator();",
                "+    }",
                "+",
                "+    protected int getBlobReplication(String key) throws IOException {",
                "+        Path path = getKeyDir(key);",
                "+        Path dest = new Path(path, BLOBSTORE_DATA);",
                "+        return _fs.getFileStatus(dest).getReplication();",
                "+    }",
                "+",
                "+    protected int updateBlobReplication(String key, int replication) throws IOException {",
                "+        Path path = getKeyDir(key);",
                "+        Path dest = new Path(path, BLOBSTORE_DATA);",
                "+        _fs.setReplication(dest, (short) replication);",
                "+        return _fs.getFileStatus(dest).getReplication();",
                "+    }",
                "+",
                "+    protected void delete(Path path) throws IOException {",
                "+        _fs.delete(path, true);",
                "+    }",
                "+",
                "+    public void shutdown() {",
                "+        if (timer != null) {",
                "+            timer.cancel();",
                "+        }",
                "+    }",
                "+}",
                "diff --git a/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsClientBlobStore.java b/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsClientBlobStore.java",
                "new file mode 100644",
                "index 000000000..2cb4dc30a",
                "--- /dev/null",
                "+++ b/external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsClientBlobStore.java",
                "@@ -0,0 +1,130 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+package org.apache.storm.hdfs.blobstore;",
                "+",
                "+import org.apache.storm.blobstore.AtomicOutputStream;",
                "+import org.apache.storm.blobstore.ClientBlobStore;",
                "+import org.apache.storm.blobstore.InputStreamWithMeta;",
                "+import org.apache.storm.generated.AuthorizationException;",
                "+import org.apache.storm.generated.KeyAlreadyExistsException;",
                "+import org.apache.storm.generated.KeyNotFoundException;",
                "+import org.apache.storm.generated.ReadableBlobMeta;",
                "+import org.apache.storm.generated.SettableBlobMeta;",
                "+import org.apache.storm.utils.NimbusClient;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+import java.util.Iterator;",
                "+import java.util.Map;",
                "+",
                "+/**",
                "+ *  Client to access the HDFS blobStore. At this point, this is meant to only be used by the",
                "+ *  supervisor.  Don't trust who the client says they are so pass null for all Subjects.",
                "+ *",
                "+ *  The HdfsBlobStore implementation takes care of the null Subjects. It assigns Subjects",
                "+ *  based on what hadoop says who the users are. These users must be configured accordingly",
                "+ *  in the SUPERVISOR_ADMINS for ACL validation and for the supervisors to download the blobs.",
                "+ *  This API is only used by the supervisor in order to talk directly to HDFS.",
                "+ */",
                "+public class HdfsClientBlobStore extends ClientBlobStore {",
                "+    private static final Logger LOG = LoggerFactory.getLogger(HdfsClientBlobStore.class);",
                "+    private HdfsBlobStore _blobStore;",
                "+    private Map _conf;",
                "+    private NimbusClient client;",
                "+",
                "+    @Override",
                "+    public void prepare(Map<String, Object> conf) {",
                "+        this._conf = conf;",
                "+        _blobStore = new HdfsBlobStore();",
                "+        _blobStore.prepare(conf, null, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public AtomicOutputStream createBlobToExtend(String key, SettableBlobMeta meta)",
                "+            throws AuthorizationException, KeyAlreadyExistsException {",
                "+        return _blobStore.createBlob(key, meta, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public AtomicOutputStream updateBlob(String key)",
                "+            throws AuthorizationException, KeyNotFoundException {",
                "+        return _blobStore.updateBlob(key, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public ReadableBlobMeta getBlobMeta(String key)",
                "+            throws AuthorizationException, KeyNotFoundException {",
                "+        return _blobStore.getBlobMeta(key, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public void setBlobMetaToExtend(String key, SettableBlobMeta meta)",
                "+            throws AuthorizationException, KeyNotFoundException {",
                "+        _blobStore.setBlobMeta(key, meta, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public void deleteBlob(String key) throws AuthorizationException, KeyNotFoundException {",
                "+        _blobStore.deleteBlob(key, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public InputStreamWithMeta getBlob(String key)",
                "+            throws AuthorizationException, KeyNotFoundException {",
                "+        return _blobStore.getBlob(key, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public Iterator<String> listKeys() {",
                "+        return _blobStore.listKeys();",
                "+    }",
                "+",
                "+    @Override",
                "+    public int getBlobReplication(String key) throws AuthorizationException, KeyNotFoundException {",
                "+        return _blobStore.getBlobReplication(key, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public int updateBlobReplication(String key, int replication) throws AuthorizationException, KeyNotFoundException {",
                "+        return _blobStore.updateBlobReplication(key, replication, null);",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean setClient(Map<String, Object> conf, NimbusClient client) {",
                "+        this.client = client;",
                "+        return true;",
                "+    }",
                "+",
                "+    @Override",
                "+    public void createStateInZookeeper(String key) {",
                "+        // Do nothing",
                "+    }",
                "+",
                "+    @Override",
                "+    public void shutdown() {",
                "+        close();",
                "+    }",
                "+",
                "+    @Override",
                "+    public void close() {",
                "+        if(client != null) {",
                "+            client.close();",
                "+            client = null;",
                "+        }",
                "+    }",
                "+}",
                "diff --git a/pom.xml b/pom.xml",
                "index ccace428a..dee254e2b 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -353,2 +353,3 @@",
                "         <module>external/storm-hdfs</module>",
                "+        <module>external/storm-hdfs-blobstore</module>",
                "         <module>external/storm-hbase</module>"
            ],
            "changed_files": [
                "external/storm-blobstore-migration/pom.xml",
                "external/storm-hdfs-blobstore/pom.xml",
                "external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStore.java",
                "external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreFile.java",
                "external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsBlobStoreImpl.java",
                "external/storm-hdfs-blobstore/src/main/java/org/apache/storm/hdfs/blobstore/HdfsClientBlobStore.java",
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2916": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2916",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5d2cf6f9d45188caf676eb4dc178d0161d369f91",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517545023,
            "hunks": 0,
            "message": "Merge branch 'STORM-2853' of https://github.com/HeartSaVioR/storm into STORM-2853-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2853": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2853",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "bdcfb5fe23efed0bb594748b1ff91ea3b35fbcda",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515785956,
            "hunks": 0,
            "message": "Merge branch 'STORM-2893' of https://github.com/erikdw/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2893": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2893",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d76370a281b837df3e83f3d49ee253d3ef24d5cf",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513331074,
            "hunks": 0,
            "message": "Merge branch 'STORM-2855-1.x-merge' into 1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2855": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2855",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "73cff66f2b2950c02b9eb52ebc7aacca073c1076",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515079037,
            "hunks": 0,
            "message": "Merge branch 'STORM-2858-1.x' of https://github.com/srdo/storm into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2858": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2858",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "71496cb67ee354d1ee7303094891dd503dca97c6",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517652294,
            "hunks": 9,
            "message": "STORM-2933: Add a storm-perf topology that uses storm-kafka-client",
            "diff": [
                "diff --git a/examples/storm-perf/pom.xml b/examples/storm-perf/pom.xml",
                "index 7dc2579b0..8fad512b6 100644",
                "--- a/examples/storm-perf/pom.xml",
                "+++ b/examples/storm-perf/pom.xml",
                "@@ -110,2 +110,12 @@",
                "         </dependency>",
                "+        <dependency>",
                "+            <groupId>org.apache.storm</groupId>",
                "+            <artifactId>storm-kafka-client</artifactId>",
                "+            <version>${project.version}</version>",
                "+        </dependency>",
                "+        <dependency>",
                "+            <groupId>org.apache.kafka</groupId>",
                "+            <artifactId>kafka-clients</artifactId>",
                "+            <scope>compile</scope>",
                "+        </dependency>",
                "     </dependencies>",
                "diff --git a/examples/storm-perf/src/main/conf/KafkaClientSpoutNullBoltTopo.yml b/examples/storm-perf/src/main/conf/KafkaClientSpoutNullBoltTopo.yml",
                "new file mode 100644",
                "index 000000000..2fb18819e",
                "--- /dev/null",
                "+++ b/examples/storm-perf/src/main/conf/KafkaClientSpoutNullBoltTopo.yml",
                "@@ -0,0 +1,20 @@",
                "+# Licensed to the Apache Software Foundation (ASF) under one",
                "+# or more contributor license agreements.  See the NOTICE file",
                "+# distributed with this work for additional information",
                "+# regarding copyright ownership.  The ASF licenses this file",
                "+# to you under the Apache License, Version 2.0 (the",
                "+# \"License\"); you may not use this file except in compliance",
                "+# with the License.  You may obtain a copy of the License at",
                "+#",
                "+# http://www.apache.org/licenses/LICENSE-2.0",
                "+#",
                "+# Unless required by applicable law or agreed to in writing, software",
                "+# distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+# See the License for the specific language governing permissions and",
                "+# limitations under the License.",
                "+",
                "+bootstrap.servers: \"127.0.0.1:9092\"",
                "+kafka.topic: \"storm-perf-null-bolt-topic\"",
                "+processing.guarantee: \"AT_LEAST_ONCE\"",
                "+offset.commit.period.ms: 30000",
                "\\ No newline at end of file",
                "diff --git a/examples/storm-perf/src/main/java/org/apache/storm/perf/KafkaClientSpoutNullBoltTopo.java b/examples/storm-perf/src/main/java/org/apache/storm/perf/KafkaClientSpoutNullBoltTopo.java",
                "new file mode 100644",
                "index 000000000..4d88702be",
                "--- /dev/null",
                "+++ b/examples/storm-perf/src/main/java/org/apache/storm/perf/KafkaClientSpoutNullBoltTopo.java",
                "@@ -0,0 +1,113 @@",
                "+/*",
                "+ * Copyright 2018 The Apache Software Foundation.",
                "+ *",
                "+ * Licensed under the Apache License, Version 2.0 (the \"License\");",
                "+ * you may not use this file except in compliance with the License.",
                "+ * You may obtain a copy of the License at",
                "+ *",
                "+ *      http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.perf;",
                "+",
                "+import java.util.Map;",
                "+import java.util.Optional;",
                "+import org.apache.storm.Config;",
                "+import org.apache.storm.generated.StormTopology;",
                "+import org.apache.storm.kafka.spout.KafkaSpout;",
                "+import org.apache.storm.kafka.spout.KafkaSpoutConfig;",
                "+import org.apache.storm.kafka.spout.KafkaSpoutConfig.ProcessingGuarantee;",
                "+import org.apache.storm.perf.bolt.DevNullBolt;",
                "+import org.apache.storm.perf.utils.Helper;",
                "+import org.apache.storm.topology.TopologyBuilder;",
                "+import org.apache.storm.utils.Utils;",
                "+",
                "+/**",
                "+ * Benchmark topology for measuring spout read/emit/ack performance. The spout reads and emits tuples. The bolt acks and discards received",
                "+ * tuples.",
                "+ */",
                "+public class KafkaClientSpoutNullBoltTopo {",
                "+",
                "+    // configs - topo parallelism",
                "+    public static final String SPOUT_NUM = \"spout.count\";",
                "+    public static final String BOLT_NUM = \"bolt.count\";",
                "+",
                "+    // configs - kafka spout",
                "+    public static final String BOOTSTRAP_SERVERS = \"bootstrap.servers\";",
                "+    public static final String KAFKA_TOPIC = \"kafka.topic\";",
                "+    public static final String PROCESSING_GUARANTEE = \"processing.guarantee\";",
                "+    public static final String OFFSET_COMMIT_PERIOD_MS = \"offset.commit.period.ms\";",
                "+",
                "+    public static final int DEFAULT_SPOUT_NUM = 1;",
                "+    public static final int DEFAULT_BOLT_NUM = 1;",
                "+",
                "+    // names",
                "+    public static final String TOPOLOGY_NAME = KafkaClientSpoutNullBoltTopo.class.getSimpleName();",
                "+    public static final String SPOUT_ID = \"kafkaSpout\";",
                "+    public static final String BOLT_ID = \"devNullBolt\";",
                "+",
                "+    /**",
                "+     * Create and configure the topology.",
                "+     */",
                "+    public static StormTopology getTopology(Map<String, Object> config) {",
                "+",
                "+        final int spoutNum = Helper.getInt(config, SPOUT_NUM, DEFAULT_SPOUT_NUM);",
                "+        final int boltNum = Helper.getInt(config, BOLT_NUM, DEFAULT_BOLT_NUM);",
                "+        // 1 -  Setup Kafka Spout   --------",
                "+",
                "+        String bootstrapServers = Optional.ofNullable(Helper.getStr(config, BOOTSTRAP_SERVERS)).orElse(\"127.0.0.1:9092\");",
                "+        String kafkaTopic = Optional.ofNullable(Helper.getStr(config, KAFKA_TOPIC)).orElse(\"storm-perf-null-bolt-topic\");",
                "+        ProcessingGuarantee processingGuarantee = ProcessingGuarantee.valueOf(",
                "+            Optional.ofNullable(Helper.getStr(config, PROCESSING_GUARANTEE))",
                "+                .orElse(ProcessingGuarantee.AT_LEAST_ONCE.name()));",
                "+        int offsetCommitPeriodMs = Helper.getInt(config, OFFSET_COMMIT_PERIOD_MS, 30_000);",
                "+",
                "+        KafkaSpoutConfig<String, String> kafkaSpoutConfig = KafkaSpoutConfig.builder(bootstrapServers, kafkaTopic)",
                "+            .setProcessingGuarantee(processingGuarantee)",
                "+            .setOffsetCommitPeriodMs(offsetCommitPeriodMs)",
                "+            .setFirstPollOffsetStrategy(KafkaSpoutConfig.FirstPollOffsetStrategy.EARLIEST)",
                "+            .setTupleTrackingEnforced(true)",
                "+            .build();",
                "+",
                "+        KafkaSpout<String, String> spout = new KafkaSpout<>(kafkaSpoutConfig);",
                "+",
                "+        // 2 -   DevNull Bolt   --------",
                "+        DevNullBolt bolt = new DevNullBolt();",
                "+",
                "+        // 3 - Setup Topology  --------",
                "+        TopologyBuilder builder = new TopologyBuilder();",
                "+        builder.setSpout(SPOUT_ID, spout, spoutNum);",
                "+        builder.setBolt(BOLT_ID, bolt, boltNum)",
                "+            .localOrShuffleGrouping(SPOUT_ID);",
                "+",
                "+        return builder.createTopology();",
                "+    }",
                "+",
                "+    /**",
                "+     * Start the topology.",
                "+     */",
                "+    public static void main(String[] args) throws Exception {",
                "+        int durationSec = -1;",
                "+        Config topoConf = new Config();",
                "+        if (args.length > 0) {",
                "+            durationSec = Integer.parseInt(args[0]);",
                "+        }",
                "+        if (args.length > 1) {",
                "+            topoConf.putAll(Utils.findAndReadConfigFile(args[1]));",
                "+        }",
                "+        if (args.length > 2) {",
                "+            System.err.println(\"args: [runDurationSec]  [optionalConfFile]\");",
                "+            return;",
                "+        }",
                "+",
                "+        //  Submit to Storm cluster",
                "+        Helper.runOnClusterAndPrintMetrics(durationSec, TOPOLOGY_NAME, topoConf, getTopology(topoConf));",
                "+    }",
                "+",
                "+}",
                "diff --git a/examples/storm-perf/src/main/java/org/apache/storm/perf/utils/MetricsSample.java b/examples/storm-perf/src/main/java/org/apache/storm/perf/utils/MetricsSample.java",
                "index 9becb0a39..f1177b617 100755",
                "--- a/examples/storm-perf/src/main/java/org/apache/storm/perf/utils/MetricsSample.java",
                "+++ b/examples/storm-perf/src/main/java/org/apache/storm/perf/utils/MetricsSample.java",
                "@@ -91,4 +91,4 @@ public class MetricsSample {",
                "         for(ExecutorSummary executorSummary : executorSummaries){",
                "-            ExecutorStats execuatorStats = executorSummary.get_stats();",
                "-            if(execuatorStats == null){",
                "+            ExecutorStats executorStats = executorSummary.get_stats();",
                "+            if(executorStats == null){",
                "                 continue;",
                "@@ -96,3 +96,3 @@ public class MetricsSample {",
                "-            ExecutorSpecificStats executorSpecificStats = execuatorStats.get_specific();",
                "+            ExecutorSpecificStats executorSpecificStats = executorStats.get_specific();",
                "             if(executorSpecificStats == null){",
                "@@ -103,3 +103,3 @@ public class MetricsSample {",
                "             // transferred totals",
                "-            Map<String,Map<String,Long>> transferred = execuatorStats.get_transferred();",
                "+            Map<String,Map<String,Long>> transferred = executorStats.get_transferred();",
                "             Map<String, Long> txMap = transferred.get(\":all-time\");",
                "@@ -139,9 +139,11 @@ public class MetricsSample {",
                "                 Map<String, Double> vals = spoutStats.get_complete_ms_avg().get(\":all-time\");",
                "-                for(String key : vals.keySet()){",
                "-                    total += vals.get(key);",
                "+                if (vals != null) {",
                "+                    for (String key : vals.keySet()) {",
                "+                        total += vals.get(key);",
                "+                    }",
                "+                    Double latency = total / vals.size();",
                "+                    spoutLatencySum += latency;",
                "                 }",
                "-                Double latency = total / vals.size();",
                "                 spoutExecCount++;",
                "-                spoutLatencySum += latency;",
                "             }"
            ],
            "changed_files": [
                "examples/storm-perf/pom.xml",
                "examples/storm-perf/src/main/conf/KafkaClientSpoutNullBoltTopo.yml",
                "examples/storm-perf/src/main/java/org/apache/storm/perf/KafkaClientSpoutNullBoltTopo.java",
                "examples/storm-perf/src/main/java/org/apache/storm/perf/utils/MetricsSample.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2933": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2933",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "873028b583e2f8c25d1e9e67f49994d26b457556",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515037330,
            "hunks": 0,
            "message": "Merge branch 'STORM-2840' of https://github.com/govind-menon/storm into STORM-2840-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2840": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2840",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d4c6fe20bd2035b0b64be2580aeabd8715011ba6",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517545094,
            "hunks": 0,
            "message": "Merge branch 'STORM-2853-1.1.x-merge' into 1.1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2853": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2853",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "67f13b530577e2da4993709a62ec3b2aa86b376c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512714792,
            "hunks": 0,
            "message": "Merge branch 'STORM-2796-merge'",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2796": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2796",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "96c461d06274fbeaaccbe713967bdfd0585e3a61",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510581486,
            "hunks": 66,
            "message": "STORM-2805: Clean up confs in TopologyBuilders",
            "diff": [
                "diff --git a/storm-client/pom.xml b/storm-client/pom.xml",
                "index 19354c14f..8a7efc46f 100644",
                "--- a/storm-client/pom.xml",
                "+++ b/storm-client/pom.xml",
                "@@ -263,3 +263,3 @@",
                "                     <excludes>**/generated/**</excludes>",
                "-                    <maxAllowedViolations>10473</maxAllowedViolations>",
                "+                    <maxAllowedViolations>10320</maxAllowedViolations>",
                "                 </configuration>",
                "diff --git a/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "index 9318aa822..9b1495d38 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "@@ -46,5 +46,5 @@ import org.apache.storm.tuple.Fields;",
                " public class BatchSubtopologyBuilder {",
                "-    Map<String, Component> _bolts = new HashMap<String, Component>();",
                "-    Component _masterBolt;",
                "-    String _masterId;",
                "+    Map<String, Component> bolts = new HashMap<>();",
                "+    Component masterBolt;",
                "+    String masterId;",
                "@@ -52,4 +52,4 @@ public class BatchSubtopologyBuilder {",
                "         Integer p = boltParallelism == null ? null : boltParallelism.intValue();",
                "-        _masterBolt = new Component(new BasicBoltExecutor(masterBolt), p);",
                "-        _masterId = masterBoltId;",
                "+        this.masterBolt = new Component(new BasicBoltExecutor(masterBolt), p);",
                "+        masterId = masterBoltId;",
                "     }",
                "@@ -61,3 +61,3 @@ public class BatchSubtopologyBuilder {",
                "     public BoltDeclarer getMasterDeclarer() {",
                "-        return new BoltDeclarerImpl(_masterBolt);",
                "+        return new BoltDeclarerImpl(masterBolt);",
                "     }",
                "@@ -86,3 +86,3 @@ public class BatchSubtopologyBuilder {",
                "         Component component = new Component(bolt, p);",
                "-        _bolts.put(id, component);",
                "+        bolts.put(id, component);",
                "         return new BoltDeclarerImpl(component);",
                "@@ -91,11 +91,11 @@ public class BatchSubtopologyBuilder {",
                "     public void extendTopology(TopologyBuilder builder) {",
                "-        BoltDeclarer declarer = builder.setBolt(_masterId, new CoordinatedBolt(_masterBolt.bolt), _masterBolt.parallelism);",
                "-        for (InputDeclaration decl: _masterBolt.declarations) {",
                "+        BoltDeclarer declarer = builder.setBolt(masterId, new CoordinatedBolt(masterBolt.bolt), masterBolt.parallelism);",
                "+        for (InputDeclaration decl: masterBolt.declarations) {",
                "             decl.declare(declarer);",
                "         }",
                "-        for (Map<String, Object> conf: _masterBolt.componentConfs) {",
                "-            declarer.addConfigurations(conf);",
                "+        if (!masterBolt.componentConf.isEmpty()) {",
                "+            declarer.addConfigurations(masterBolt.componentConf);",
                "         }",
                "-        for (String id: _bolts.keySet()) {",
                "-            Component component = _bolts.get(id);",
                "+        for (String id: bolts.keySet()) {",
                "+            Component component = bolts.get(id);",
                "             Map<String, SourceArgs> coordinatedArgs = new HashMap<String, SourceArgs>();",
                "@@ -103,3 +103,3 @@ public class BatchSubtopologyBuilder {",
                "                 SourceArgs source;",
                "-                if (c.equals(_masterId)) {",
                "+                if (c.equals(masterId)) {",
                "                     source = SourceArgs.single();",
                "@@ -120,4 +120,4 @@ public class BatchSubtopologyBuilder {",
                "             }",
                "-            for (Map<String, Object> conf: component.componentConfs) {",
                "-                input.addConfigurations(conf);",
                "+            if (!component.componentConf.isEmpty()) {",
                "+                input.addConfigurations(component.componentConf);",
                "             }",
                "@@ -133,3 +133,3 @@ public class BatchSubtopologyBuilder {",
                "     private Set<String> componentBoltSubscriptions(Component component) {",
                "-        Set<String> ret = new HashSet<String>();",
                "+        Set<String> ret = new HashSet<>();",
                "         for (InputDeclaration d: component.declarations) {",
                "@@ -144,3 +144,3 @@ public class BatchSubtopologyBuilder {",
                "         public final List<InputDeclaration> declarations = new ArrayList<>();",
                "-        public final List<Map<String, Object>> componentConfs = new ArrayList<>();",
                "+        public final Map<String, Object> componentConf = new HashMap<>();",
                "         public final Set<SharedMemory> sharedMemory = new HashSet<>();",
                "@@ -153,3 +153,3 @@ public class BatchSubtopologyBuilder {",
                "-    private static interface InputDeclaration {",
                "+    private interface InputDeclaration {",
                "         void declare(InputDeclarer declarer);",
                "@@ -159,6 +159,6 @@ public class BatchSubtopologyBuilder {",
                "     private static class BoltDeclarerImpl extends BaseConfigurationDeclarer<BoltDeclarer> implements BoltDeclarer {",
                "-        Component _component;",
                "+        Component component;",
                "         public BoltDeclarerImpl(Component component) {",
                "-            _component = component;",
                "+            this.component = component;",
                "         }",
                "@@ -448,3 +448,3 @@ public class BatchSubtopologyBuilder {",
                "         private void addDeclaration(InputDeclaration declaration) {",
                "-            _component.declarations.add(declaration);",
                "+            component.declarations.add(declaration);",
                "         }",
                "@@ -453,3 +453,5 @@ public class BatchSubtopologyBuilder {",
                "         public BoltDeclarer addConfigurations(Map<String, Object> conf) {",
                "-            _component.componentConfs.add(conf);",
                "+            if (conf != null && !conf.isEmpty()) {",
                "+                component.componentConf.putAll(conf);",
                "+            }",
                "             return this;",
                "@@ -458,12 +460,5 @@ public class BatchSubtopologyBuilder {",
                "         @Override",
                "-        public Map getRASConfiguration() {",
                "-            for (Map<String, Object> conf : _component.componentConfs) {",
                "-                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "-                    return conf;",
                "-                }",
                "-            }",
                "-            Map<String, Object> newConf = new HashMap<>();",
                "-            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "-            _component.componentConfs.add(newConf);",
                "-            return newConf;",
                "+        public Map<String, Object> getRASConfiguration() {",
                "+            //TODO this should not be modifiable",
                "+            return component.componentConf;",
                "         }",
                "@@ -472,3 +467,3 @@ public class BatchSubtopologyBuilder {",
                "         public BoltDeclarer addSharedMemory(SharedMemory request) {",
                "-            _component.sharedMemory.add(request);",
                "+            component.sharedMemory.add(request);",
                "             return this;",
                "diff --git a/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "index 928a5faab..51db42205 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "@@ -55,4 +55,4 @@ import org.apache.storm.tuple.Fields;",
                " public class LinearDRPCTopologyBuilder {    ",
                "-    String _function;",
                "-    List<Component> _components = new ArrayList<Component>();",
                "+    String function;",
                "+    List<Component> components = new ArrayList<>();",
                "@@ -60,3 +60,3 @@ public class LinearDRPCTopologyBuilder {",
                "     public LinearDRPCTopologyBuilder(String function) {",
                "-        _function = function;",
                "+        this.function = function;",
                "     }",
                "@@ -77,3 +77,3 @@ public class LinearDRPCTopologyBuilder {",
                "         Component component = new Component(bolt, parallelism.intValue());",
                "-        _components.add(component);",
                "+        components.add(component);",
                "         return new InputDeclarerImpl(component);",
                "@@ -95,3 +95,3 @@ public class LinearDRPCTopologyBuilder {",
                "     public StormTopology createLocalTopology(ILocalDRPC drpc) {",
                "-        return createTopology(new DRPCSpout(_function, drpc));",
                "+        return createTopology(new DRPCSpout(function, drpc));",
                "     }",
                "@@ -99,3 +99,3 @@ public class LinearDRPCTopologyBuilder {",
                "     public StormTopology createRemoteTopology() {",
                "-        return createTopology(new DRPCSpout(_function));",
                "+        return createTopology(new DRPCSpout(function));",
                "     }",
                "@@ -112,4 +112,4 @@ public class LinearDRPCTopologyBuilder {",
                "         int i = 0;",
                "-        for (; i < _components.size();i++) {",
                "-            Component component = _components.get(i);",
                "+        for (; i < components.size(); i++) {",
                "+            Component component = components.get(i);",
                "@@ -122,3 +122,3 @@ public class LinearDRPCTopologyBuilder {",
                "             IdStreamSpec idSpec = null;",
                "-            if (i == _components.size() - 1 && component.bolt instanceof FinishedCallback) {",
                "+            if (i == components.size() - 1 && component.bolt instanceof FinishedCallback) {",
                "                 idSpec = IdStreamSpec.makeDetectSpec(PREPARE_ID, PrepareRequest.ID_STREAM);",
                "@@ -134,4 +134,4 @@ public class LinearDRPCTopologyBuilder {",
                "-            for (Map<String, Object> conf: component.componentConfs) {",
                "-                declarer.addConfigurations(conf);",
                "+            if (!component.componentConf.isEmpty()) {",
                "+                declarer.addConfigurations(component.componentConf);",
                "             }",
                "@@ -159,3 +159,3 @@ public class LinearDRPCTopologyBuilder {",
                "-        IRichBolt lastBolt = _components.get(_components.size() - 1).bolt;",
                "+        IRichBolt lastBolt = components.get(components.size() - 1).bolt;",
                "         OutputFieldsGetter getter = new OutputFieldsGetter();",
                "@@ -190,3 +190,3 @@ public class LinearDRPCTopologyBuilder {",
                "         public final int parallelism;",
                "-        public final List<Map<String, Object>> componentConfs = new ArrayList<>();",
                "+        public final Map<String, Object> componentConf = new HashMap<>();",
                "         public final List<InputDeclaration> declarations = new ArrayList<>();",
                "@@ -205,6 +205,6 @@ public class LinearDRPCTopologyBuilder {",
                "     private static class InputDeclarerImpl extends BaseConfigurationDeclarer<LinearDRPCInputDeclarer> implements LinearDRPCInputDeclarer {",
                "-        Component _component;",
                "+        Component component;",
                "         public InputDeclarerImpl(Component component) {",
                "-            _component = component;",
                "+            this.component = component;",
                "         }",
                "@@ -398,3 +398,3 @@ public class LinearDRPCTopologyBuilder {",
                "         private void addDeclaration(InputDeclaration declaration) {",
                "-            _component.declarations.add(declaration);",
                "+            component.declarations.add(declaration);",
                "         }",
                "@@ -403,3 +403,5 @@ public class LinearDRPCTopologyBuilder {",
                "         public LinearDRPCInputDeclarer addConfigurations(Map<String, Object> conf) {",
                "-            _component.componentConfs.add(conf);",
                "+            if (conf != null && !conf.isEmpty()) {",
                "+                component.componentConf.putAll(conf);",
                "+            }",
                "             return this;",
                "@@ -420,11 +422,3 @@ public class LinearDRPCTopologyBuilder {",
                "         public Map getRASConfiguration() {",
                "-            for (Map<String, Object> conf : _component.componentConfs) {",
                "-                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "-                    return conf;",
                "-                }",
                "-            }",
                "-            Map<String, Object> newConf = new HashMap<>();",
                "-            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "-            _component.componentConfs.add(newConf);",
                "-            return newConf;",
                "+            return component.componentConf;",
                "         }",
                "@@ -433,3 +427,3 @@ public class LinearDRPCTopologyBuilder {",
                "         public LinearDRPCInputDeclarer addSharedMemory(SharedMemory request) {",
                "-            _component.sharedMemory.add(request);",
                "+            component.sharedMemory.add(request);",
                "             return this;",
                "diff --git a/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "index f2deca879..6f994edf2 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "@@ -60,9 +60,9 @@ import java.util.Set;",
                " public class TransactionalTopologyBuilder {",
                "-    final String _id;",
                "-    final String _spoutId;",
                "-    final ITransactionalSpout _spout;",
                "-    final Map<String, Component> _bolts = new HashMap<String, Component>();",
                "-    final Integer _spoutParallelism;",
                "-    final List<Map<String, Object>> _spoutConfs = new ArrayList<>();",
                "-    final Set<SharedMemory> _spoutSharedMemory = new HashSet<>();",
                "+    final String id;",
                "+    final String spoutId;",
                "+    final ITransactionalSpout spout;",
                "+    final Map<String, Component> bolts = new HashMap<>();",
                "+    final Integer spoutParallelism;",
                "+    final Map<String, Object> spoutConf = new HashMap<>();",
                "+    final Set<SharedMemory> spoutSharedMemory = new HashSet<>();",
                "@@ -71,6 +71,6 @@ public class TransactionalTopologyBuilder {",
                "     public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpout spout, Number spoutParallelism) {",
                "-        _id = id;",
                "-        _spoutId = spoutId;",
                "-        _spout = spout;",
                "-        _spoutParallelism = (spoutParallelism == null) ? null : spoutParallelism.intValue();",
                "+        this.id = id;",
                "+        this.spoutId = spoutId;",
                "+        this.spout = spout;",
                "+        this.spoutParallelism = (spoutParallelism == null) ? null : spoutParallelism.intValue();",
                "     }",
                "@@ -129,3 +129,3 @@ public class TransactionalTopologyBuilder {",
                "         Component component = new Component(bolt, p, committer);",
                "-        _bolts.put(id, component);",
                "+        bolts.put(id, component);",
                "         return new BoltDeclarerImpl(component);",
                "@@ -134,26 +134,26 @@ public class TransactionalTopologyBuilder {",
                "     public TopologyBuilder buildTopologyBuilder() {",
                "-        String coordinator = _spoutId + \"/coordinator\";",
                "+        String coordinator = spoutId + \"/coordinator\";",
                "         TopologyBuilder builder = new TopologyBuilder();",
                "-        SpoutDeclarer declarer = builder.setSpout(coordinator, new TransactionalSpoutCoordinator(_spout));",
                "-        for (SharedMemory request: _spoutSharedMemory) {",
                "+        SpoutDeclarer declarer = builder.setSpout(coordinator, new TransactionalSpoutCoordinator(spout));",
                "+        for (SharedMemory request: spoutSharedMemory) {",
                "             declarer.addSharedMemory(request);",
                "         }",
                "-        for(Map<String, Object> conf: _spoutConfs) {",
                "-            declarer.addConfigurations(conf);",
                "+        if (!spoutConf.isEmpty()) {",
                "+            declarer.addConfigurations(spoutConf);",
                "         }",
                "-        declarer.addConfiguration(Config.TOPOLOGY_TRANSACTIONAL_ID, _id);",
                "+        declarer.addConfiguration(Config.TOPOLOGY_TRANSACTIONAL_ID, id);",
                "         BoltDeclarer emitterDeclarer = ",
                "-                builder.setBolt(_spoutId,",
                "-                        new CoordinatedBolt(new TransactionalSpoutBatchExecutor(_spout),",
                "+                builder.setBolt(spoutId,",
                "+                        new CoordinatedBolt(new TransactionalSpoutBatchExecutor(spout),",
                "                                              null,",
                "                                              null),",
                "-                        _spoutParallelism)",
                "+                    spoutParallelism)",
                "                 .allGrouping(coordinator, TransactionalSpoutCoordinator.TRANSACTION_BATCH_STREAM_ID)",
                "-                .addConfiguration(Config.TOPOLOGY_TRANSACTIONAL_ID, _id);",
                "-        if(_spout instanceof ICommitterTransactionalSpout) {",
                "+                .addConfiguration(Config.TOPOLOGY_TRANSACTIONAL_ID, id);",
                "+        if(spout instanceof ICommitterTransactionalSpout) {",
                "             emitterDeclarer.allGrouping(coordinator, TransactionalSpoutCoordinator.TRANSACTION_COMMIT_STREAM_ID);",
                "         }",
                "-        for(String id: _bolts.keySet()) {",
                "-            Component component = _bolts.get(id);",
                "+        for(String id: bolts.keySet()) {",
                "+            Component component = bolts.get(id);",
                "             Map<String, SourceArgs> coordinatedArgs = new HashMap<String, SourceArgs>();",
                "@@ -175,4 +175,4 @@ public class TransactionalTopologyBuilder {",
                "             }",
                "-            for(Map<String, Object> conf: component.componentConfs) {",
                "-                input.addConfigurations(conf);",
                "+            if (!component.componentConf.isEmpty()) {",
                "+                input.addConfigurations(component.componentConf);",
                "             }",
                "@@ -207,3 +207,3 @@ public class TransactionalTopologyBuilder {",
                "         public final List<InputDeclaration> declarations = new ArrayList<>();",
                "-        public final List<Map<String, Object>> componentConfs = new ArrayList<>();",
                "+        public final Map<String, Object> componentConf = new HashMap<>();",
                "         public final boolean committer;",
                "@@ -226,3 +226,5 @@ public class TransactionalTopologyBuilder {",
                "         public SpoutDeclarer addConfigurations(Map<String, Object> conf) {",
                "-            _spoutConfs.add(conf);",
                "+            if (conf != null && !conf.isEmpty()) {",
                "+                spoutConf.putAll(conf);",
                "+            }",
                "             return this;",
                "@@ -232,11 +234,4 @@ public class TransactionalTopologyBuilder {",
                "         public Map getRASConfiguration() {",
                "-            for(Map<String, Object> conf : _spoutConfs) {",
                "-                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "-                    return conf;",
                "-                }",
                "-            }",
                "-            Map<String, Object> newConf = new HashMap<>();",
                "-            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "-            _spoutConfs.add(newConf);",
                "-            return newConf;",
                "+            //TODO this should be imutable",
                "+            return spoutConf;",
                "         }",
                "@@ -245,3 +240,3 @@ public class TransactionalTopologyBuilder {",
                "         public SpoutDeclarer addSharedMemory(SharedMemory request) {",
                "-            _spoutSharedMemory.add(request);",
                "+            spoutSharedMemory.add(request);",
                "             return this;",
                "@@ -265,6 +260,6 @@ public class TransactionalTopologyBuilder {",
                "     private static class BoltDeclarerImpl extends BaseConfigurationDeclarer<BoltDeclarer> implements BoltDeclarer {",
                "-        Component _component;",
                "+        Component component;",
                "         public BoltDeclarerImpl(Component component) {",
                "-            _component = component;",
                "+            this.component = component;",
                "         }",
                "@@ -554,3 +549,3 @@ public class TransactionalTopologyBuilder {",
                "         private void addDeclaration(InputDeclaration declaration) {",
                "-            _component.declarations.add(declaration);",
                "+            component.declarations.add(declaration);",
                "         }",
                "@@ -559,3 +554,5 @@ public class TransactionalTopologyBuilder {",
                "         public BoltDeclarer addConfigurations(Map<String, Object> conf) {",
                "-            _component.componentConfs.add(conf);",
                "+            if (conf != null && !conf.isEmpty()) {",
                "+                component.componentConf.putAll(conf);",
                "+            }",
                "             return this;",
                "@@ -565,11 +562,4 @@ public class TransactionalTopologyBuilder {",
                "         public Map getRASConfiguration() {",
                "-            for(Map<String, Object> conf : _component.componentConfs) {",
                "-                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "-                    return conf;",
                "-                }",
                "-            }",
                "-            Map<String, Object> newConf = new HashMap<>();",
                "-            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "-            _component.componentConfs.add(newConf);",
                "-            return newConf;",
                "+            //TODO this should be read only",
                "+            return component.componentConf;",
                "         }",
                "@@ -578,3 +568,3 @@ public class TransactionalTopologyBuilder {",
                "         public BoltDeclarer addSharedMemory(SharedMemory request) {",
                "-            _component.sharedMemory.add(request);",
                "+            component.sharedMemory.add(request);",
                "             return this;",
                "diff --git a/storm-client/src/jvm/org/apache/storm/trident/topology/MasterBatchCoordinator.java b/storm-client/src/jvm/org/apache/storm/trident/topology/MasterBatchCoordinator.java",
                "index e54d0e21b..34171122f 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/trident/topology/MasterBatchCoordinator.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/trident/topology/MasterBatchCoordinator.java",
                "@@ -310,3 +310,3 @@ public class MasterBatchCoordinator extends BaseRichSpout {",
                "                 \", _managedSpoutIds=\" + _managedSpoutIds +",
                "-                \", _spouts=\" + _spouts +",
                "+                \", spouts=\" + _spouts +",
                "                 \", _throttler=\" + _throttler +",
                "diff --git a/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java",
                "index be36b4ee5..274e913cb 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java",
                "@@ -53,4 +53,4 @@ import org.apache.storm.trident.topology.TridentBoltExecutor.CoordType;",
                " public class TridentTopologyBuilder {",
                "-    Map<GlobalStreamId, String> _batchIds = new HashMap();",
                "-    Map<String, TransactionalSpoutComponent> _spouts = new HashMap();",
                "+    Map<GlobalStreamId, String> batchIds = new HashMap();",
                "+    Map<String, TransactionalSpoutComponent> spouts = new HashMap();",
                "     Map<String, SpoutComponent> _batchPerTupleSpouts = new HashMap();",
                "@@ -78,3 +78,3 @@ public class TridentTopologyBuilder {",
                "         TransactionalSpoutComponent c = new TransactionalSpoutComponent(spout, streamName, parallelism, txStateId, batchGroup);",
                "-        _spouts.put(id, c);",
                "+        spouts.put(id, c);",
                "         return new SpoutDeclarerImpl(c);",
                "@@ -106,4 +106,4 @@ public class TridentTopologyBuilder {",
                "     Map<GlobalStreamId, String> fleshOutStreamBatchIds(boolean includeCommitStream) {",
                "-        Map<GlobalStreamId, String> ret = new HashMap<>(_batchIds);",
                "-        Set<String> allBatches = new HashSet(_batchIds.values());",
                "+        Map<GlobalStreamId, String> ret = new HashMap<>(batchIds);",
                "+        Set<String> allBatches = new HashSet(batchIds.values());",
                "         for(String b: allBatches) {",
                "@@ -117,4 +117,4 @@ public class TridentTopologyBuilder {",
                "-        for(String id: _spouts.keySet()) {",
                "-            TransactionalSpoutComponent c = _spouts.get(id);",
                "+        for(String id: spouts.keySet()) {",
                "+            TransactionalSpoutComponent c = spouts.get(id);",
                "             if(c.batchGroupId!=null) {",
                "@@ -125,4 +125,4 @@ public class TridentTopologyBuilder {",
                "         //this takes care of setting up coord streams for spouts and bolts",
                "-        for(GlobalStreamId s: _batchIds.keySet()) {",
                "-            String b = _batchIds.get(s);",
                "+        for(GlobalStreamId s: batchIds.keySet()) {",
                "+            String b = batchIds.get(s);",
                "             ret.put(new GlobalStreamId(s.get_componentId(), TridentBoltExecutor.COORD_STREAM(b)), b);",
                "@@ -141,4 +141,4 @@ public class TridentTopologyBuilder {",
                "-        for(String id: _spouts.keySet()) {",
                "-            TransactionalSpoutComponent c = _spouts.get(id);",
                "+        for(String id: spouts.keySet()) {",
                "+            TransactionalSpoutComponent c = spouts.get(id);",
                "             if(c.spout instanceof IRichSpout) {",
                "@@ -275,3 +275,3 @@ public class TridentTopologyBuilder {",
                "         for(Map.Entry<String, String> entry: batchGroups.entrySet()) {",
                "-            _batchIds.put(new GlobalStreamId(component, entry.getKey()), entry.getValue());",
                "+            batchIds.put(new GlobalStreamId(component, entry.getKey()), entry.getValue());",
                "         }",
                "@@ -338,3 +338,3 @@ public class TridentTopologyBuilder {",
                "         for(GlobalStreamId s: getBoltSubscriptionStreams(id)) {",
                "-            String b = _batchIds.get(s);",
                "+            String b = batchIds.get(s);",
                "             if(!ret.containsKey(b)) ret.put(b, new HashSet());"
            ],
            "changed_files": [
                "storm-client/pom.xml",
                "storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/trident/topology/MasterBatchCoordinator.java",
                "storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2805": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2805",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1dcd12de27b813a8c3e42d600028fa97df0a5f15",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512557560,
            "hunks": 43,
            "message": "STORM-2845 Drop standalone mode of Storm SQL * remove all related interfaces/classes on standalone mode * migrate tests to trident mode which are associated to standalone mode * remove comments from tests which Calcite has fixed the issues",
            "diff": [
                "diff --git a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSql.java b/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSql.java",
                "index 3fbc463e3..49e4162c7 100644",
                "--- a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSql.java",
                "+++ b/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSql.java",
                "@@ -21,3 +21,2 @@ import org.apache.storm.StormSubmitter;",
                " import org.apache.storm.generated.SubmitOptions;",
                "-import org.apache.storm.sql.runtime.ChannelHandler;",
                "@@ -34,9 +33,2 @@ import java.util.Map;",
                " public abstract class StormSql {",
                "-  /**",
                "-   * Execute the SQL statements in stand-alone mode. The user can retrieve the result by passing in an instance",
                "-   * of {@see ChannelHandler}.",
                "-   */",
                "-  public abstract void execute(Iterable<String> statements,",
                "-                               ChannelHandler handler) throws Exception;",
                "-",
                "   /**",
                "diff --git a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlContext.java b/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlContext.java",
                "new file mode 100644",
                "index 000000000..bf7ef3e43",
                "--- /dev/null",
                "+++ b/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlContext.java",
                "@@ -0,0 +1,172 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ *",
                "+ */",
                "+",
                "+package org.apache.storm.sql;",
                "+",
                "+import java.lang.reflect.Method;",
                "+import java.util.ArrayList;",
                "+import java.util.Collections;",
                "+import java.util.HashMap;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+",
                "+import org.apache.calcite.adapter.java.JavaTypeFactory;",
                "+import org.apache.calcite.jdbc.CalciteSchema;",
                "+import org.apache.calcite.prepare.CalciteCatalogReader;",
                "+import org.apache.calcite.rel.RelNode;",
                "+import org.apache.calcite.rel.type.RelDataType;",
                "+import org.apache.calcite.rel.type.RelDataTypeSystem;",
                "+import org.apache.calcite.schema.Function;",
                "+import org.apache.calcite.schema.SchemaPlus;",
                "+import org.apache.calcite.schema.Table;",
                "+import org.apache.calcite.schema.impl.AggregateFunctionImpl;",
                "+import org.apache.calcite.schema.impl.ScalarFunctionImpl;",
                "+import org.apache.calcite.sql.SqlExplainLevel;",
                "+import org.apache.calcite.sql.SqlNode;",
                "+import org.apache.calcite.sql.SqlOperatorTable;",
                "+import org.apache.calcite.sql.fun.SqlStdOperatorTable;",
                "+import org.apache.calcite.sql.parser.SqlParseException;",
                "+import org.apache.calcite.sql.util.ChainedSqlOperatorTable;",
                "+import org.apache.calcite.tools.FrameworkConfig;",
                "+import org.apache.calcite.tools.Frameworks;",
                "+import org.apache.calcite.tools.Planner;",
                "+import org.apache.calcite.tools.RelConversionException;",
                "+import org.apache.calcite.tools.ValidationException;",
                "+import org.apache.storm.sql.compiler.CompilerUtil;",
                "+import org.apache.storm.sql.compiler.StormSqlTypeFactoryImpl;",
                "+import org.apache.storm.sql.parser.ColumnConstraint;",
                "+import org.apache.storm.sql.parser.ColumnDefinition;",
                "+import org.apache.storm.sql.parser.SqlCreateFunction;",
                "+import org.apache.storm.sql.parser.SqlCreateTable;",
                "+import org.apache.storm.sql.planner.StormRelUtils;",
                "+import org.apache.storm.sql.planner.trident.QueryPlanner;",
                "+import org.apache.storm.sql.runtime.DataSourcesRegistry;",
                "+import org.apache.storm.sql.runtime.FieldInfo;",
                "+import org.apache.storm.sql.runtime.ISqlTridentDataSource;",
                "+",
                "+public class StormSqlContext {",
                "+    private final JavaTypeFactory typeFactory = new StormSqlTypeFactoryImpl(",
                "+            RelDataTypeSystem.DEFAULT);",
                "+    private final SchemaPlus schema = Frameworks.createRootSchema(true);",
                "+    private boolean hasUdf = false;",
                "+    private Map<String, ISqlTridentDataSource> dataSources = new HashMap<>();",
                "+",
                "+    public void interpretCreateTable(SqlCreateTable n) {",
                "+        CompilerUtil.TableBuilderInfo builder = new CompilerUtil.TableBuilderInfo(typeFactory);",
                "+        List<FieldInfo> fields = new ArrayList<>();",
                "+        for (ColumnDefinition col : n.fieldList()) {",
                "+            builder.field(col.name(), col.type(), col.constraint());",
                "+            RelDataType dataType = col.type().deriveType(typeFactory);",
                "+            Class<?> javaType = (Class<?>)typeFactory.getJavaClass(dataType);",
                "+            ColumnConstraint constraint = col.constraint();",
                "+            boolean isPrimary = constraint != null && constraint instanceof ColumnConstraint.PrimaryKey;",
                "+            fields.add(new FieldInfo(col.name(), javaType, isPrimary));",
                "+        }",
                "+",
                "+        if (n.parallelism() != null) {",
                "+            builder.parallelismHint(n.parallelism());",
                "+        }",
                "+        Table table = builder.build();",
                "+        schema.add(n.tableName(), table);",
                "+",
                "+        ISqlTridentDataSource ds = DataSourcesRegistry.constructTridentDataSource(n.location(), n",
                "+                .inputFormatClass(), n.outputFormatClass(), n.properties(), fields);",
                "+        if (ds == null) {",
                "+            throw new RuntimeException(\"Failed to find data source for \" + n",
                "+                    .tableName() + \" URI: \" + n.location());",
                "+        } else if (dataSources.containsKey(n.tableName())) {",
                "+            throw new RuntimeException(\"Duplicated definition for table \" + n",
                "+                    .tableName());",
                "+        }",
                "+        dataSources.put(n.tableName(), ds);",
                "+    }",
                "+",
                "+    public void interpretCreateFunction(SqlCreateFunction sqlCreateFunction) throws ClassNotFoundException {",
                "+        if(sqlCreateFunction.jarName() != null) {",
                "+            throw new UnsupportedOperationException(\"UDF 'USING JAR' not implemented\");",
                "+        }",
                "+        Method method;",
                "+        Function function;",
                "+        if ((method=findMethod(sqlCreateFunction.className(), \"evaluate\")) != null) {",
                "+            function = ScalarFunctionImpl.create(method);",
                "+        } else if (findMethod(sqlCreateFunction.className(), \"add\") != null) {",
                "+            function = AggregateFunctionImpl.create(Class.forName(sqlCreateFunction.className()));",
                "+        } else {",
                "+            throw new RuntimeException(\"Invalid scalar or aggregate function\");",
                "+        }",
                "+        schema.add(sqlCreateFunction.functionName().toUpperCase(), function);",
                "+        hasUdf = true;",
                "+    }",
                "+",
                "+    public AbstractTridentProcessor compileSql(String query) throws Exception {",
                "+        QueryPlanner planner = new QueryPlanner(schema);",
                "+        return planner.compile(dataSources, query);",
                "+    }",
                "+",
                "+    public String explain(String query) throws SqlParseException, ValidationException, RelConversionException {",
                "+        FrameworkConfig config = buildFrameWorkConfig();",
                "+        Planner planner = Frameworks.getPlanner(config);",
                "+        SqlNode parse = planner.parse(query);",
                "+        SqlNode validate = planner.validate(parse);",
                "+        RelNode tree = planner.convert(validate);",
                "+",
                "+        return StormRelUtils.explain(tree, SqlExplainLevel.ALL_ATTRIBUTES);",
                "+    }",
                "+",
                "+    public FrameworkConfig buildFrameWorkConfig() {",
                "+        if (hasUdf) {",
                "+            List<SqlOperatorTable> sqlOperatorTables = new ArrayList<>();",
                "+            sqlOperatorTables.add(SqlStdOperatorTable.instance());",
                "+            sqlOperatorTables.add(new CalciteCatalogReader(CalciteSchema.from(schema),",
                "+                    false,",
                "+                    Collections.<String>emptyList(), typeFactory));",
                "+            return Frameworks.newConfigBuilder().defaultSchema(schema)",
                "+                    .operatorTable(new ChainedSqlOperatorTable(sqlOperatorTables)).build();",
                "+        } else {",
                "+            return Frameworks.newConfigBuilder().defaultSchema(schema).build();",
                "+        }",
                "+    }",
                "+",
                "+    public JavaTypeFactory getTypeFactory() {",
                "+        return typeFactory;",
                "+    }",
                "+",
                "+    public SchemaPlus getSchema() {",
                "+        return schema;",
                "+    }",
                "+",
                "+    public boolean isHasUdf() {",
                "+        return hasUdf;",
                "+    }",
                "+",
                "+    public Map<String, ISqlTridentDataSource> getDataSources() {",
                "+        return dataSources;",
                "+    }",
                "+",
                "+    private Method findMethod(String clazzName, String methodName) throws ClassNotFoundException {",
                "+        Class<?> clazz = Class.forName(clazzName);",
                "+        for (Method method : clazz.getMethods()) {",
                "+            if (method.getName().equals(methodName)) {",
                "+                return method;",
                "+            }",
                "+        }",
                "+        return null;",
                "+    }",
                "+",
                "+}",
                "diff --git a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlImpl.java b/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlImpl.java",
                "index 1c83ac81e..a77179377 100644",
                "--- a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlImpl.java",
                "+++ b/sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlImpl.java",
                "@@ -19,28 +19,6 @@ package org.apache.storm.sql;",
                "-import org.apache.calcite.adapter.java.JavaTypeFactory;",
                "-import org.apache.calcite.jdbc.CalciteSchema;",
                "-import org.apache.calcite.prepare.CalciteCatalogReader;",
                "-import org.apache.calcite.rel.RelNode;",
                "-import org.apache.calcite.rel.type.RelDataType;",
                "-import org.apache.calcite.rel.type.RelDataTypeSystem;",
                "-import org.apache.calcite.schema.Function;",
                "-import org.apache.calcite.schema.SchemaPlus;",
                "-import org.apache.calcite.schema.Table;",
                "-import org.apache.calcite.schema.impl.AggregateFunctionImpl;",
                "-import org.apache.calcite.schema.impl.ScalarFunctionImpl;",
                "-import org.apache.calcite.sql.SqlExplainLevel;",
                " import org.apache.calcite.sql.SqlNode;",
                "-import org.apache.calcite.sql.SqlOperatorTable;",
                "-import org.apache.calcite.sql.fun.SqlStdOperatorTable;",
                "-import org.apache.calcite.sql.util.ChainedSqlOperatorTable;",
                "-import org.apache.calcite.tools.FrameworkConfig;",
                "-import org.apache.calcite.tools.Frameworks;",
                "-import org.apache.calcite.tools.Planner;",
                " import org.apache.storm.StormSubmitter;",
                " import org.apache.storm.generated.SubmitOptions;",
                "-import org.apache.storm.sql.compiler.StormSqlTypeFactoryImpl;",
                "-import org.apache.storm.sql.compiler.backends.standalone.PlanCompiler;",
                " import org.apache.storm.sql.javac.CompilingClassLoader;",
                "-import org.apache.storm.sql.parser.ColumnConstraint;",
                "-import org.apache.storm.sql.parser.ColumnDefinition;",
                " import org.apache.storm.sql.parser.SqlCreateFunction;",
                "@@ -48,10 +26,2 @@ import org.apache.storm.sql.parser.SqlCreateTable;",
                " import org.apache.storm.sql.parser.StormParser;",
                "-import org.apache.storm.sql.planner.StormRelUtils;",
                "-import org.apache.storm.sql.planner.trident.QueryPlanner;",
                "-import org.apache.storm.sql.runtime.AbstractValuesProcessor;",
                "-import org.apache.storm.sql.runtime.ChannelHandler;",
                "-import org.apache.storm.sql.runtime.DataSource;",
                "-import org.apache.storm.sql.runtime.DataSourcesRegistry;",
                "-import org.apache.storm.sql.runtime.FieldInfo;",
                "-import org.apache.storm.sql.runtime.ISqlTridentDataSource;",
                " import org.apache.storm.trident.TridentTopology;",
                "@@ -62,8 +32,4 @@ import java.io.FileOutputStream;",
                " import java.io.IOException;",
                "-import java.lang.reflect.Method;",
                " import java.nio.file.Files;",
                " import java.nio.file.Path;",
                "-import java.util.ArrayList;",
                "-import java.util.Collections;",
                "-import java.util.HashMap;",
                " import java.util.List;",
                "@@ -75,33 +41,7 @@ import java.util.zip.ZipEntry;",
                "-import static org.apache.storm.sql.compiler.CompilerUtil.TableBuilderInfo;",
                "-",
                " class StormSqlImpl extends StormSql {",
                "-  private final JavaTypeFactory typeFactory = new StormSqlTypeFactoryImpl(",
                "-      RelDataTypeSystem.DEFAULT);",
                "-  private final SchemaPlus schema = Frameworks.createRootSchema(true);",
                "-  private boolean hasUdf = false;",
                "+  private final StormSqlContext sqlContext;",
                "-  @Override",
                "-  public void execute(",
                "-      Iterable<String> statements, ChannelHandler result)",
                "-      throws Exception {",
                "-    Map<String, DataSource> dataSources = new HashMap<>();",
                "-    for (String sql : statements) {",
                "-      StormParser parser = new StormParser(sql);",
                "-      SqlNode node = parser.impl().parseSqlStmtEof();",
                "-      if (node instanceof SqlCreateTable) {",
                "-        handleCreateTable((SqlCreateTable) node, dataSources);",
                "-      } else if (node instanceof SqlCreateFunction) {",
                "-        handleCreateFunction((SqlCreateFunction) node);",
                "-      } else {",
                "-        FrameworkConfig config = buildFrameWorkConfig();",
                "-        Planner planner = Frameworks.getPlanner(config);",
                "-        SqlNode parse = planner.parse(sql);",
                "-        SqlNode validate = planner.validate(parse);",
                "-        RelNode tree = planner.convert(validate);",
                "-        PlanCompiler compiler = new PlanCompiler(typeFactory);",
                "-        AbstractValuesProcessor proc = compiler.compile(tree);",
                "-        proc.initialize(dataSources, result);",
                "-      }",
                "-    }",
                "+  public StormSqlImpl() {",
                "+    sqlContext = new StormSqlContext();",
                "   }",
                "@@ -113,3 +53,2 @@ class StormSqlImpl extends StormSql {",
                "       throws Exception {",
                "-    Map<String, ISqlTridentDataSource> dataSources = new HashMap<>();",
                "     for (String sql : statements) {",
                "@@ -118,8 +57,7 @@ class StormSqlImpl extends StormSql {",
                "       if (node instanceof SqlCreateTable) {",
                "-        handleCreateTableForTrident((SqlCreateTable) node, dataSources);",
                "+        sqlContext.interpretCreateTable((SqlCreateTable) node);",
                "       } else if (node instanceof SqlCreateFunction) {",
                "-        handleCreateFunction((SqlCreateFunction) node);",
                "-      }  else {",
                "-        QueryPlanner planner = new QueryPlanner(schema);",
                "-        AbstractTridentProcessor processor = planner.compile(dataSources, sql);",
                "+        sqlContext.interpretCreateFunction((SqlCreateFunction) node);",
                "+      } else {",
                "+        AbstractTridentProcessor processor = sqlContext.compileSql(sql);",
                "         TridentTopology topo = processor.build();",
                "@@ -147,3 +85,2 @@ class StormSqlImpl extends StormSql {",
                "   public void explain(Iterable<String> statements) throws Exception {",
                "-    Map<String, ISqlTridentDataSource> dataSources = new HashMap<>();",
                "     for (String sql : statements) {",
                "@@ -158,15 +95,9 @@ class StormSqlImpl extends StormSql {",
                "       if (node instanceof SqlCreateTable) {",
                "-        handleCreateTableForTrident((SqlCreateTable) node, dataSources);",
                "+        sqlContext.interpretCreateTable((SqlCreateTable) node);",
                "         System.out.println(\"No plan presented on DDL\");",
                "       } else if (node instanceof SqlCreateFunction) {",
                "-        handleCreateFunction((SqlCreateFunction) node);",
                "+        sqlContext.interpretCreateFunction((SqlCreateFunction) node);",
                "         System.out.println(\"No plan presented on DDL\");",
                "       } else {",
                "-        FrameworkConfig config = buildFrameWorkConfig();",
                "-        Planner planner = Frameworks.getPlanner(config);",
                "-        SqlNode parse = planner.parse(sql);",
                "-        SqlNode validate = planner.validate(parse);",
                "-        RelNode tree = planner.convert(validate);",
                "-",
                "-        String plan = StormRelUtils.explain(tree, SqlExplainLevel.ALL_ATTRIBUTES);",
                "+        String plan = sqlContext.explain(sql);",
                "         System.out.println(\"plan>\");",
                "@@ -198,93 +129,2 @@ class StormSqlImpl extends StormSql {",
                "   }",
                "-",
                "-  private void handleCreateTable(",
                "-      SqlCreateTable n, Map<String, DataSource> dataSources) {",
                "-    List<FieldInfo> fields = updateSchema(n);",
                "-    DataSource ds = DataSourcesRegistry.construct(n.location(), n",
                "-        .inputFormatClass(), n.outputFormatClass(), fields);",
                "-    if (ds == null) {",
                "-      throw new RuntimeException(\"Cannot construct data source for \" + n",
                "-          .tableName());",
                "-    } else if (dataSources.containsKey(n.tableName())) {",
                "-      throw new RuntimeException(\"Duplicated definition for table \" + n",
                "-          .tableName());",
                "-    }",
                "-    dataSources.put(n.tableName(), ds);",
                "-  }",
                "-",
                "-  private void handleCreateFunction(SqlCreateFunction sqlCreateFunction) throws ClassNotFoundException {",
                "-    if(sqlCreateFunction.jarName() != null) {",
                "-      throw new UnsupportedOperationException(\"UDF 'USING JAR' not implemented\");",
                "-    }",
                "-    Method method;",
                "-    Function function;",
                "-    if ((method=findMethod(sqlCreateFunction.className(), \"evaluate\")) != null) {",
                "-      function = ScalarFunctionImpl.create(method);",
                "-    } else if (findMethod(sqlCreateFunction.className(), \"add\") != null) {",
                "-      function = AggregateFunctionImpl.create(Class.forName(sqlCreateFunction.className()));",
                "-    } else {",
                "-      throw new RuntimeException(\"Invalid scalar or aggregate function\");",
                "-    }",
                "-    schema.add(sqlCreateFunction.functionName().toUpperCase(), function);",
                "-    hasUdf = true;",
                "-  }",
                "-",
                "-  private Method findMethod(String clazzName, String methodName) throws ClassNotFoundException {",
                "-    Class<?> clazz = Class.forName(clazzName);",
                "-    for (Method method : clazz.getMethods()) {",
                "-      if (method.getName().equals(methodName)) {",
                "-        return method;",
                "-      }",
                "-    }",
                "-    return null;",
                "-  }",
                "-",
                "-  private void handleCreateTableForTrident(",
                "-      SqlCreateTable n, Map<String, ISqlTridentDataSource> dataSources) {",
                "-    List<FieldInfo> fields = updateSchema(n);",
                "-    ISqlTridentDataSource ds = DataSourcesRegistry.constructTridentDataSource(n.location(), n",
                "-        .inputFormatClass(), n.outputFormatClass(), n.properties(), fields);",
                "-    if (ds == null) {",
                "-      throw new RuntimeException(\"Failed to find data source for \" + n",
                "-          .tableName() + \" URI: \" + n.location());",
                "-    } else if (dataSources.containsKey(n.tableName())) {",
                "-      throw new RuntimeException(\"Duplicated definition for table \" + n",
                "-          .tableName());",
                "-    }",
                "-    dataSources.put(n.tableName(), ds);",
                "-  }",
                "-",
                "-  private List<FieldInfo> updateSchema(SqlCreateTable n) {",
                "-    TableBuilderInfo builder = new TableBuilderInfo(typeFactory);",
                "-    List<FieldInfo> fields = new ArrayList<>();",
                "-    for (ColumnDefinition col : n.fieldList()) {",
                "-      builder.field(col.name(), col.type(), col.constraint());",
                "-      RelDataType dataType = col.type().deriveType(typeFactory);",
                "-      Class<?> javaType = (Class<?>)typeFactory.getJavaClass(dataType);",
                "-      ColumnConstraint constraint = col.constraint();",
                "-      boolean isPrimary = constraint != null && constraint instanceof ColumnConstraint.PrimaryKey;",
                "-      fields.add(new FieldInfo(col.name(), javaType, isPrimary));",
                "-    }",
                "-",
                "-    if (n.parallelism() != null) {",
                "-      builder.parallelismHint(n.parallelism());",
                "-    }",
                "-    Table table = builder.build();",
                "-    schema.add(n.tableName(), table);",
                "-    return fields;",
                "-  }",
                "-",
                "-  private FrameworkConfig buildFrameWorkConfig() {",
                "-    if (hasUdf) {",
                "-      List<SqlOperatorTable> sqlOperatorTables = new ArrayList<>();",
                "-      sqlOperatorTables.add(SqlStdOperatorTable.instance());",
                "-      sqlOperatorTables.add(new CalciteCatalogReader(CalciteSchema.from(schema),",
                "-                                                     false,",
                "-                                                     Collections.<String>emptyList(), typeFactory));",
                "-      return Frameworks.newConfigBuilder().defaultSchema(schema)",
                "-              .operatorTable(new ChainedSqlOperatorTable(sqlOperatorTables)).build();",
                "-    } else {",
                "-      return Frameworks.newConfigBuilder().defaultSchema(schema).build();",
                "-    }",
                "-  }",
                " }",
                "diff --git a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/BuiltinAggregateFunctions.java b/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/BuiltinAggregateFunctions.java",
                "deleted file mode 100644",
                "index 9dc4ba8b0..000000000",
                "--- a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/BuiltinAggregateFunctions.java",
                "+++ /dev/null",
                "@@ -1,238 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-package org.apache.storm.sql.compiler.backends.standalone;",
                "-",
                "-import com.google.common.collect.ImmutableList;",
                "-import org.apache.storm.tuple.Values;",
                "-",
                "-import java.lang.reflect.Type;",
                "-import java.util.HashMap;",
                "-import java.util.List;",
                "-import java.util.Map;",
                "-",
                "-/**",
                "- * Built-in implementations for some of the standard aggregation operations.",
                "- * Aggregations can be implemented as a class with the following methods viz. init, add and result.",
                "- * The class could contain only static methods, only non-static methods or be generic.",
                "- */",
                "-public class BuiltinAggregateFunctions {",
                "-    // binds the type information and the class implementing the aggregation",
                "-    public static class TypeClass {",
                "-        public static class GenericType {",
                "-        }",
                "-",
                "-        public final Type ty;",
                "-        public final Class<?> clazz;",
                "-",
                "-        private TypeClass(Type ty, Class<?> clazz) {",
                "-            this.ty = ty;",
                "-            this.clazz = clazz;",
                "-        }",
                "-",
                "-        static TypeClass of(Type ty, Class<?> clazz) {",
                "-            return new TypeClass(ty, clazz);",
                "-        }",
                "-    }",
                "-",
                "-    static final Map<String, List<TypeClass>> TABLE = new HashMap<>();",
                "-",
                "-    public static class ByteSum {",
                "-        public static Byte init() {",
                "-            return 0;",
                "-        }",
                "-",
                "-        public static Byte add(Byte accumulator, Byte val) {",
                "-            return (byte) (accumulator + val);",
                "-        }",
                "-",
                "-        public static Byte result(Byte accumulator) {",
                "-            return accumulator;",
                "-        }",
                "-    }",
                "-",
                "-    public static class ShortSum {",
                "-        public static Short init() {",
                "-            return 0;",
                "-        }",
                "-",
                "-        public static Short add(Short accumulator, Short val) {",
                "-            return (short) (accumulator + val);",
                "-        }",
                "-",
                "-        public static Short result(Short accumulator) {",
                "-            return accumulator;",
                "-        }",
                "-    }",
                "-",
                "-    public static class IntSum {",
                "-        public static Integer init() {",
                "-            return 0;",
                "-        }",
                "-",
                "-        public static Integer add(Integer accumulator, Integer val) {",
                "-            return accumulator + val;",
                "-        }",
                "-",
                "-        public static Integer result(Integer accumulator) {",
                "-            return accumulator;",
                "-        }",
                "-    }",
                "-",
                "-    public static class LongSum {",
                "-        public static Long init() {",
                "-            return 0L;",
                "-        }",
                "-",
                "-        public static Long add(Long accumulator, Long val) {",
                "-            return accumulator + val;",
                "-        }",
                "-",
                "-        public static Long result(Long accumulator) {",
                "-            return accumulator;",
                "-        }",
                "-    }",
                "-",
                "-    public static class FloatSum {",
                "-        public static Float init() {",
                "-            return 0.0f;",
                "-        }",
                "-",
                "-        public static Float add(Float accumulator, Float val) {",
                "-            return accumulator + val;",
                "-        }",
                "-",
                "-        public static Float result(Float accumulator) {",
                "-            return accumulator;",
                "-        }",
                "-    }",
                "-",
                "-    public static class DoubleSum {",
                "-        public static Double init() {",
                "-            return 0.0;",
                "-        }",
                "-",
                "-        public static Double add(Double accumulator, Double val) {",
                "-            return accumulator + val;",
                "-        }",
                "-",
                "-        public static Double result(Double accumulator) {",
                "-            return accumulator;",
                "-        }",
                "-    }",
                "-",
                "-    public static class Max<T extends Comparable<T>> {",
                "-        public T init() {",
                "-            return null;",
                "-        }",
                "-",
                "-        public T add(T accumulator, T val) {",
                "-            return (accumulator == null || accumulator.compareTo(val) < 0) ? val : accumulator;",
                "-        }",
                "-",
                "-        public T result(T accumulator) {",
                "-            return accumulator;",
                "-        }",
                "-    }",
                "-",
                "-    public static class Min<T extends Comparable<T>> {",
                "-        public T init() {",
                "-            return null;",
                "-        }",
                "-",
                "-        public T add(T accumulator, T val) {",
                "-            return (accumulator == null || accumulator.compareTo(val) > 0) ? val : accumulator;",
                "-        }",
                "-",
                "-        public T result(T accumulator) {",
                "-            return accumulator;",
                "-        }",
                "-    }",
                "-",
                "-    public static class IntAvg {",
                "-        private int count;",
                "-",
                "-        public Integer init() {",
                "-            return 0;",
                "-        }",
                "-",
                "-        public Integer add(Integer accumulator, Integer val) {",
                "-            ++count;",
                "-            return accumulator + val;",
                "-        }",
                "-",
                "-        public Integer result(Integer accumulator) {",
                "-            Integer result = accumulator / count;",
                "-            count = 0;",
                "-            return result;",
                "-        }",
                "-    }",
                "-",
                "-    public static class DoubleAvg {",
                "-        private int count;",
                "-",
                "-        public Double init() {",
                "-            return 0.0;",
                "-        }",
                "-",
                "-        public Double add(Double accumulator, Double val) {",
                "-            ++count;",
                "-            return accumulator + val;",
                "-        }",
                "-",
                "-        public Double result(Double accumulator) {",
                "-            Double result = accumulator / count;",
                "-            count = 0;",
                "-            return result;",
                "-        }",
                "-    }",
                "-",
                "-    public static class Count {",
                "-        public static Long init() {",
                "-            return 0L;",
                "-        }",
                "-",
                "-        public static Long add(Long accumulator, Values vals) {",
                "-            for (Object val : vals) {",
                "-                if (val == null) {",
                "-                    return accumulator;",
                "-                }",
                "-            }",
                "-            return accumulator + 1;",
                "-        }",
                "-",
                "-        public static Long result(Long accumulator) {",
                "-            return accumulator;",
                "-        }",
                "-    }",
                "-",
                "-    static {",
                "-        TABLE.put(\"SUM\", ImmutableList.of(",
                "-                TypeClass.of(float.class, FloatSum.class),",
                "-                TypeClass.of(double.class, DoubleSum.class),",
                "-                TypeClass.of(byte.class, ByteSum.class),",
                "-                TypeClass.of(short.class, ShortSum.class),",
                "-                TypeClass.of(long.class, LongSum.class),",
                "-                TypeClass.of(int.class, IntSum.class)));",
                "-        TABLE.put(\"AVG\", ImmutableList.of(",
                "-                TypeClass.of(double.class, DoubleAvg.class),",
                "-                TypeClass.of(int.class, IntAvg.class)));",
                "-        TABLE.put(\"COUNT\", ImmutableList.of(TypeClass.of(long.class, Count.class)));",
                "-        TABLE.put(\"MAX\", ImmutableList.of(TypeClass.of(TypeClass.GenericType.class, Max.class)));",
                "-        TABLE.put(\"MIN\", ImmutableList.of(TypeClass.of(TypeClass.GenericType.class, Min.class)));",
                "-    }",
                "-}",
                "diff --git a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/PlanCompiler.java b/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/PlanCompiler.java",
                "deleted file mode 100644",
                "index 01546ed63..000000000",
                "--- a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/PlanCompiler.java",
                "+++ /dev/null",
                "@@ -1,139 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- * <p>",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- * <p>",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-package org.apache.storm.sql.compiler.backends.standalone;",
                "-",
                "-import com.google.common.base.Joiner;",
                "-import org.apache.calcite.adapter.java.JavaTypeFactory;",
                "-import org.apache.calcite.rel.RelNode;",
                "-import org.apache.calcite.rel.core.TableScan;",
                "-import org.apache.storm.sql.compiler.CompilerUtil;",
                "-import org.apache.storm.sql.javac.CompilingClassLoader;",
                "-import org.apache.storm.sql.runtime.AbstractValuesProcessor;",
                "-import org.slf4j.Logger;",
                "-import org.slf4j.LoggerFactory;",
                "-",
                "-import java.io.PrintWriter;",
                "-import java.io.StringWriter;",
                "-import java.util.HashSet;",
                "-import java.util.Set;",
                "-",
                "-public class PlanCompiler {",
                "-  private static final Logger LOG = LoggerFactory.getLogger(PlanCompiler.class);",
                "-",
                "-  private static final Joiner NEW_LINE_JOINER = Joiner.on(\"\\n\");",
                "-  private static final String PACKAGE_NAME = \"org.apache.storm.sql.generated\";",
                "-  private static final String PROLOGUE = NEW_LINE_JOINER.join(",
                "-      \"// GENERATED CODE\", \"package \" + PACKAGE_NAME + \";\", \"\",",
                "-      \"import java.util.Iterator;\", \"import java.util.Map;\", \"import java.util.HashMap;\",",
                "-      \"import java.util.List;\", \"import java.util.ArrayList;\",",
                "-      \"import java.util.LinkedHashMap;\",",
                "-      \"import org.apache.storm.tuple.Values;\",",
                "-      \"import org.apache.storm.sql.runtime.AbstractChannelHandler;\",",
                "-      \"import org.apache.storm.sql.runtime.Channels;\",",
                "-      \"import org.apache.storm.sql.runtime.ChannelContext;\",",
                "-      \"import org.apache.storm.sql.runtime.ChannelHandler;\",",
                "-      \"import org.apache.storm.sql.runtime.DataSource;\",",
                "-      \"import org.apache.storm.sql.runtime.AbstractValuesProcessor;\",",
                "-      \"import com.google.common.collect.ArrayListMultimap;\",",
                "-      \"import com.google.common.collect.Multimap;\",",
                "-      \"import org.apache.calcite.interpreter.Context;\",",
                "-      \"import org.apache.calcite.interpreter.StormContext;\",",
                "-      \"import org.apache.calcite.DataContext;\",",
                "-      \"import org.apache.storm.sql.runtime.calcite.StormDataContext;\",",
                "-      \"public final class Processor extends AbstractValuesProcessor {\",",
                "-      \"  public final static DataContext dataContext = new StormDataContext();\",",
                "-      \"\");",
                "-  private static final String INITIALIZER_PROLOGUE = NEW_LINE_JOINER.join(",
                "-      \"  @Override\",",
                "-      \"  public void initialize(Map<String, DataSource> data,\",",
                "-      \"                         ChannelHandler result) {\",",
                "-      \"    ChannelContext r = Channels.chain(Channels.voidContext(), result);\",",
                "-      \"\"",
                "-  );",
                "-",
                "-  private final JavaTypeFactory typeFactory;",
                "-",
                "-  public PlanCompiler(JavaTypeFactory typeFactory) {",
                "-    this.typeFactory = typeFactory;",
                "-  }",
                "-",
                "-  private String generateJavaSource(RelNode root) throws Exception {",
                "-    StringWriter sw = new StringWriter();",
                "-    try (PrintWriter pw = new PrintWriter(sw)) {",
                "-      RelNodeCompiler compiler = new RelNodeCompiler(pw, typeFactory);",
                "-      printPrologue(pw);",
                "-      compiler.traverse(root);",
                "-      printMain(pw, root);",
                "-      printEpilogue(pw);",
                "-    }",
                "-    return sw.toString();",
                "-  }",
                "-",
                "-  private void printMain(PrintWriter pw, RelNode root) {",
                "-    Set<TableScan> tables = new HashSet<>();",
                "-    pw.print(INITIALIZER_PROLOGUE);",
                "-    chainOperators(pw, root, tables);",
                "-    for (TableScan n : tables) {",
                "-      String escaped = CompilerUtil.escapeJavaString(",
                "-          Joiner.on('.').join(n.getTable().getQualifiedName()), true);",
                "-      String r = NEW_LINE_JOINER.join(",
                "-          \"    if (!data.containsKey(%1$s))\",",
                "-          \"      throw new RuntimeException(\\\"Cannot find table \\\" + %1$s);\",",
                "-          \"  data.get(%1$s).open(CTX_%2$d);\",",
                "-          \"\");",
                "-      pw.print(String.format(r, escaped, n.getId()));",
                "-    }",
                "-    pw.print(\"  }\\n\");",
                "-  }",
                "-",
                "-  private void chainOperators(PrintWriter pw, RelNode root, Set<TableScan> tables) {",
                "-    doChainOperators(pw, root, tables, \"r\");",
                "-  }",
                "-",
                "-  private void doChainOperators(PrintWriter pw, RelNode node, Set<TableScan> tables, String parentCtx) {",
                "-    pw.print(",
                "-            String.format(\"    ChannelContext CTX_%d = Channels.chain(%2$s, %3$s);\\n\",",
                "-                          node.getId(), parentCtx, RelNodeCompiler.getStageName(node)));",
                "-    String currentCtx = String.format(\"CTX_%d\", node.getId());",
                "-    if (node instanceof TableScan) {",
                "-      tables.add((TableScan) node);",
                "-    }",
                "-    for (RelNode i : node.getInputs()) {",
                "-      doChainOperators(pw, i, tables, currentCtx);",
                "-    }",
                "-  }",
                "-",
                "-  public AbstractValuesProcessor compile(RelNode plan) throws Exception {",
                "-    String javaCode = generateJavaSource(plan);",
                "-    LOG.debug(\"Compiling... source code {}\", javaCode);",
                "-    ClassLoader cl = new CompilingClassLoader(getClass().getClassLoader(),",
                "-                                              PACKAGE_NAME + \".Processor\",",
                "-                                              javaCode, null);",
                "-    return (AbstractValuesProcessor) cl.loadClass(",
                "-        PACKAGE_NAME + \".Processor\").newInstance();",
                "-  }",
                "-",
                "-  private static void printEpilogue(",
                "-      PrintWriter pw) throws Exception {",
                "-    pw.print(\"}\\n\");",
                "-  }",
                "-",
                "-  private static void printPrologue(PrintWriter pw) {",
                "-    pw.append(PROLOGUE);",
                "-  }",
                "-}",
                "diff --git a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/PostOrderRelNodeVisitor.java b/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/PostOrderRelNodeVisitor.java",
                "deleted file mode 100644",
                "index afed8a9b8..000000000",
                "--- a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/PostOrderRelNodeVisitor.java",
                "+++ /dev/null",
                "@@ -1,132 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- * <p>",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- * <p>",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-package org.apache.storm.sql.compiler.backends.standalone;",
                "-",
                "-import org.apache.calcite.rel.RelNode;",
                "-import org.apache.calcite.rel.core.*;",
                "-import org.apache.calcite.rel.stream.Delta;",
                "-",
                "-import java.util.ArrayList;",
                "-import java.util.List;",
                "-",
                "-public abstract class PostOrderRelNodeVisitor<T> {",
                "-  public final T traverse(RelNode n) throws Exception {",
                "-    List<T> inputStreams = new ArrayList<>();",
                "-    for (RelNode input : n.getInputs()) {",
                "-      inputStreams.add(traverse(input));",
                "-    }",
                "-",
                "-    if (n instanceof Aggregate) {",
                "-      return visitAggregate((Aggregate) n, inputStreams);",
                "-    } else if (n instanceof Calc) {",
                "-      return visitCalc((Calc) n, inputStreams);",
                "-    } else if (n instanceof Collect) {",
                "-      return visitCollect((Collect) n, inputStreams);",
                "-    } else if (n instanceof Correlate) {",
                "-      return visitCorrelate((Correlate) n, inputStreams);",
                "-    } else if (n instanceof Delta) {",
                "-      return visitDelta((Delta) n, inputStreams);",
                "-    } else if (n instanceof Exchange) {",
                "-      return visitExchange((Exchange) n, inputStreams);",
                "-    } else if (n instanceof Project) {",
                "-      return visitProject((Project) n, inputStreams);",
                "-    } else if (n instanceof Filter) {",
                "-      return visitFilter((Filter) n, inputStreams);",
                "-    } else if (n instanceof Sample) {",
                "-      return visitSample((Sample) n, inputStreams);",
                "-    } else if (n instanceof Sort) {",
                "-      return visitSort((Sort) n, inputStreams);",
                "-    } else if (n instanceof TableModify) {",
                "-      return visitTableModify((TableModify) n, inputStreams);",
                "-    } else if (n instanceof TableScan) {",
                "-      return visitTableScan((TableScan) n, inputStreams);",
                "-    } else if (n instanceof Uncollect) {",
                "-      return visitUncollect((Uncollect) n, inputStreams);",
                "-    } else if (n instanceof Window) {",
                "-      return visitWindow((Window) n, inputStreams);",
                "-    } else if (n instanceof Join) {",
                "-      return visitJoin((Join) n, inputStreams);",
                "-    } else {",
                "-      return defaultValue(n, inputStreams);",
                "-    }",
                "-  }",
                "-",
                "-  public T visitAggregate(Aggregate aggregate, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(aggregate, inputStreams);",
                "-  }",
                "-",
                "-  public T visitCalc(Calc calc, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(calc, inputStreams);",
                "-  }",
                "-",
                "-  public T visitCollect(Collect collect, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(collect, inputStreams);",
                "-  }",
                "-",
                "-  public T visitCorrelate(Correlate correlate, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(correlate, inputStreams);",
                "-  }",
                "-",
                "-  public T visitDelta(Delta delta, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(delta, inputStreams);",
                "-  }",
                "-",
                "-  public T visitExchange(Exchange exchange, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(exchange, inputStreams);",
                "-  }",
                "-",
                "-  public T visitProject(Project project, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(project, inputStreams);",
                "-  }",
                "-",
                "-  public T visitFilter(Filter filter, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(filter, inputStreams);",
                "-  }",
                "-",
                "-  public T visitSample(Sample sample, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(sample, inputStreams);",
                "-  }",
                "-",
                "-  public T visitSort(Sort sort, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(sort, inputStreams);",
                "-  }",
                "-",
                "-  public T visitTableModify(TableModify modify, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(modify, inputStreams);",
                "-  }",
                "-",
                "-  public T visitTableScan(TableScan scan, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(scan, inputStreams);",
                "-  }",
                "-",
                "-  public T visitUncollect(Uncollect uncollect, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(uncollect, inputStreams);",
                "-  }",
                "-",
                "-  public T visitWindow(Window window, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(window, inputStreams);",
                "-  }",
                "-",
                "-  public T visitJoin(Join join, List<T> inputStreams) throws Exception {",
                "-    return defaultValue(join, inputStreams);",
                "-  }",
                "-",
                "-  public T defaultValue(RelNode n, List<T> inputStreams) {",
                "-    return null;",
                "-  }",
                "-}",
                "diff --git a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/RelNodeCompiler.java b/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/RelNodeCompiler.java",
                "deleted file mode 100644",
                "index 97995c7c3..000000000",
                "--- a/sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/RelNodeCompiler.java",
                "+++ /dev/null",
                "@@ -1,484 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- * <p>",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- * <p>",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-package org.apache.storm.sql.compiler.backends.standalone;",
                "-",
                "-import com.google.common.base.Joiner;",
                "-import org.apache.calcite.adapter.java.JavaTypeFactory;",
                "-import org.apache.calcite.plan.RelOptUtil;",
                "-import org.apache.calcite.rel.RelNode;",
                "-import org.apache.calcite.rel.core.Aggregate;",
                "-import org.apache.calcite.rel.core.AggregateCall;",
                "-import org.apache.calcite.rel.core.Filter;",
                "-import org.apache.calcite.rel.core.Join;",
                "-import org.apache.calcite.rel.core.Project;",
                "-import org.apache.calcite.rel.core.TableScan;",
                "-import org.apache.calcite.rel.logical.LogicalJoin;",
                "-import org.apache.calcite.rel.stream.Delta;",
                "-import org.apache.calcite.rel.type.RelDataType;",
                "-import org.apache.calcite.rex.RexBuilder;",
                "-import org.apache.calcite.rex.RexNode;",
                "-import org.apache.calcite.schema.AggregateFunction;",
                "-import org.apache.calcite.schema.impl.AggregateFunctionImpl;",
                "-import org.apache.calcite.sql.SqlAggFunction;",
                "-import org.apache.calcite.sql.validate.SqlUserDefinedAggFunction;",
                "-import org.apache.storm.sql.compiler.RexNodeToJavaCodeCompiler;",
                "-",
                "-import java.io.PrintWriter;",
                "-import java.io.StringWriter;",
                "-import java.lang.reflect.Method;",
                "-import java.lang.reflect.Modifier;",
                "-import java.lang.reflect.Type;",
                "-import java.util.ArrayList;",
                "-import java.util.HashMap;",
                "-import java.util.List;",
                "-import java.util.Map;",
                "-",
                "-/**",
                "- * Compile RelNodes into individual functions.",
                "- */",
                "-class RelNodeCompiler extends PostOrderRelNodeVisitor<Void> {",
                "-  public static Joiner NEW_LINE_JOINER = Joiner.on('\\n');",
                "-",
                "-  private final PrintWriter pw;",
                "-  private final JavaTypeFactory typeFactory;",
                "-  private final RexNodeToJavaCodeCompiler rexCompiler;",
                "-",
                "-  private static final String STAGE_PROLOGUE = NEW_LINE_JOINER.join(",
                "-    \"  private static final ChannelHandler %1$s = \",",
                "-    \"    new AbstractChannelHandler() {\",",
                "-    \"    @Override\",",
                "-    \"    public void dataReceived(ChannelContext ctx, Values _data) {\",",
                "-    \"\"",
                "-  );",
                "-",
                "-  private static final String AGGREGATE_STAGE_PROLOGUE = NEW_LINE_JOINER.join(",
                "-          \"  private static final ChannelHandler %1$s = \",",
                "-          \"    new AbstractChannelHandler() {\",",
                "-          \"    private final Values EMPTY_VALUES = new Values();\",",
                "-          \"    private final Map<List<Object>, Map<String, Object>> state = new LinkedHashMap<>();\",",
                "-          \"    private final int[] groupIndices = new int[] {%2$s};\",",
                "-          \"    private List<Object> getGroupValues(Values _data) {\",",
                "-          \"      List<Object> res = new ArrayList<>();\",",
                "-          \"      for (int i: groupIndices) {\",",
                "-          \"        res.add(_data.get(i));\",",
                "-          \"      }\",",
                "-          \"      return res;\",",
                "-          \"    }\",",
                "-          \"\",",
                "-          \"    @Override\",",
                "-          \"    public void flush(ChannelContext ctx) {\",",
                "-          \"      emitAggregateResults(ctx);\",",
                "-          \"      super.flush(ctx);\",",
                "-          \"      state.clear();\",",
                "-          \"    }\",",
                "-          \"\",",
                "-          \"    private void emitAggregateResults(ChannelContext ctx) {\",",
                "-          \"        for (Map.Entry<List<Object>, Map<String, Object>> entry: state.entrySet()) {\",",
                "-          \"          List<Object> groupValues = entry.getKey();\",",
                "-          \"          Map<String, Object> accumulators = entry.getValue();\",",
                "-          \"          %3$s\",",
                "-          \"        }\",",
                "-          \"    }\",",
                "-          \"\",",
                "-          \"    @Override\",",
                "-          \"    public void dataReceived(ChannelContext ctx, Values _data) {\",",
                "-          \"\"",
                "-  );",
                "-",
                "-  private static final String JOIN_STAGE_PROLOGUE = NEW_LINE_JOINER.join(",
                "-          \"  private static final ChannelHandler %1$s = \",",
                "-          \"    new AbstractChannelHandler() {\",",
                "-          \"      Object left = %2$s;\",",
                "-          \"      Object right = %3$s;\",",
                "-          \"      Object source = null;\",",
                "-          \"      List<Values> leftRows = new ArrayList<>();\",",
                "-          \"      List<Values> rightRows = new ArrayList<>();\",",
                "-          \"      boolean leftDone = false;\",",
                "-          \"      boolean rightDone = false;\",",
                "-          \"      int[] ordinals = new int[] {%4$s, %5$s};\",",
                "-          \"\",",
                "-          \"      Multimap<Object, Values> getJoinTable(List<Values> rows, int joinIndex) {\",",
                "-          \"         Multimap<Object, Values> m = ArrayListMultimap.create();\",",
                "-          \"         for(Values v: rows) {\",",
                "-          \"           m.put(v.get(joinIndex), v);\",",
                "-          \"         }\",",
                "-          \"         return m;\",",
                "-          \"      }\",",
                "-          \"\",",
                "-          \"      List<Values> join(Multimap<Object, Values> tab, List<Values> rows, int rowIdx, boolean rev) {\",",
                "-          \"         List<Values> res = new ArrayList<>();\",",
                "-          \"         for (Values row: rows) {\",",
                "-          \"           for (Values mapValue: tab.get(row.get(rowIdx))) {\",",
                "-          \"             if (mapValue != null) {\",",
                "-          \"               Values joinedRow = new Values();\",",
                "-          \"               if(rev) {\",",
                "-          \"                 joinedRow.addAll(row);\",",
                "-          \"                 joinedRow.addAll(mapValue);\",",
                "-          \"               } else {\",",
                "-          \"                 joinedRow.addAll(mapValue);\",",
                "-          \"                 joinedRow.addAll(row);\",",
                "-          \"               }\",",
                "-          \"               res.add(joinedRow);\",",
                "-          \"             }\",",
                "-          \"           }\",",
                "-          \"         }\",",
                "-          \"         return res;\",",
                "-          \"      }\",",
                "-          \"\",",
                "-          \"    @Override\",",
                "-          \"    public void setSource(ChannelContext ctx, Object source) {\",",
                "-          \"      this.source = source;\",",
                "-          \"    }\",",
                "-          \"\",",
                "-          \"    @Override\",",
                "-          \"    public void flush(ChannelContext ctx) {\",",
                "-          \"        if (source == left) {\",",
                "-          \"            leftDone = true;\",",
                "-          \"        } else if (source == right) {\",",
                "-          \"            rightDone = true;\",",
                "-          \"        }\",",
                "-          \"        if (leftDone && rightDone) {\",",
                "-          \"          if (leftRows.size() <= rightRows.size()) {\",",
                "-          \"            for(Values res: join(getJoinTable(leftRows, ordinals[0]), rightRows, ordinals[1], false)) {\",",
                "-          \"              ctx.emit(res);\",",
                "-          \"            }\",",
                "-          \"          } else {\",",
                "-          \"            for(Values res: join(getJoinTable(rightRows, ordinals[1]), leftRows, ordinals[0], true)) {\",",
                "-          \"              ctx.emit(res);\",",
                "-          \"            }\",",
                "-          \"          }\",",
                "-          \"          leftDone = rightDone = false;\",",
                "-          \"          leftRows.clear();\",",
                "-          \"          rightRows.clear();\",",
                "-          \"          super.flush(ctx);\",",
                "-          \"        }\",",
                "-          \"    }\",",
                "-          \"\",",
                "-          \"    @Override\",",
                "-          \"    public void dataReceived(ChannelContext ctx, Values _data) {\",",
                "-          \"\"",
                "-  );",
                "-",
                "-  private static final String STAGE_PASSTHROUGH = NEW_LINE_JOINER.join(",
                "-      \"  private static final ChannelHandler %1$s = AbstractChannelHandler.PASS_THROUGH;\",",
                "-      \"\");",
                "-",
                "-  private static final String STAGE_ENUMERABLE_TABLE_SCAN = NEW_LINE_JOINER.join(",
                "-          \"  private static final ChannelHandler %1$s = new AbstractChannelHandler() {\",",
                "-          \"    @Override\",",
                "-          \"    public void flush(ChannelContext ctx) {\",",
                "-          \"      ctx.setSource(this);\",",
                "-          \"      super.flush(ctx);\",",
                "-          \"    }\",",
                "-          \"\",",
                "-          \"    @Override\",",
                "-          \"    public void dataReceived(ChannelContext ctx, Values _data) {\",",
                "-          \"      ctx.setSource(this);\",",
                "-          \"      ctx.emit(_data);\",",
                "-          \"    }\",",
                "-          \"  };\",",
                "-          \"\");",
                "-",
                "-  private int nameCount;",
                "-  private Map<AggregateCall, String> aggregateCallVarNames = new HashMap<>();",
                "-",
                "-  RelNodeCompiler(PrintWriter pw, JavaTypeFactory typeFactory) {",
                "-    this.pw = pw;",
                "-    this.typeFactory = typeFactory;",
                "-    this.rexCompiler = new RexNodeToJavaCodeCompiler(new RexBuilder(typeFactory));",
                "-  }",
                "-",
                "-  @Override",
                "-  public Void visitDelta(Delta delta, List<Void> inputStreams) throws Exception {",
                "-    pw.print(String.format(STAGE_PASSTHROUGH, getStageName(delta)));",
                "-    return null;",
                "-  }",
                "-",
                "-  @Override",
                "-  public Void visitFilter(Filter filter, List<Void> inputStreams) throws Exception {",
                "-    beginStage(filter);",
                "-",
                "-    List<RexNode> childExps = filter.getChildExps();",
                "-    RelDataType inputRowType = filter.getInput(0).getRowType();",
                "-",
                "-    pw.print(\"Context context = new StormContext(Processor.dataContext);\\n\");",
                "-    pw.print(\"context.values = _data.toArray();\\n\");",
                "-    pw.print(\"Object[] outputValues = new Object[1];\\n\");",
                "-",
                "-    pw.write(rexCompiler.compileToBlock(childExps, inputRowType).toString());",
                "-",
                "-    String r = \"((Boolean) outputValues[0])\";",
                "-    if (filter.getCondition().getType().isNullable()) {",
                "-      pw.print(String.format(\"    if (%s != null && %s) { ctx.emit(_data); }\\n\", r, r));",
                "-    } else {",
                "-      pw.print(String.format(\"    if (%s) { ctx.emit(_data); }\\n\", r, r));",
                "-    }",
                "-    endStage();",
                "-    return null;",
                "-  }",
                "-",
                "-  @Override",
                "-  public Void visitProject(Project project, List<Void> inputStreams) throws Exception {",
                "-    beginStage(project);",
                "-",
                "-    List<RexNode> childExps = project.getChildExps();",
                "-    RelDataType inputRowType = project.getInput(0).getRowType();",
                "-    int outputCount = project.getRowType().getFieldCount();",
                "-",
                "-    pw.print(\"Context context = new StormContext(Processor.dataContext);\\n\");",
                "-    pw.print(\"context.values = _data.toArray();\\n\");",
                "-    pw.print(String.format(\"Object[] outputValues = new Object[%d];\\n\", outputCount));",
                "-",
                "-    pw.write(rexCompiler.compileToBlock(childExps, inputRowType).toString());",
                "-",
                "-    pw.print(\"    ctx.emit(new Values(outputValues));\\n\");",
                "-    endStage();",
                "-    return null;",
                "-  }",
                "-",
                "-  @Override",
                "-  public Void defaultValue(RelNode n, List<Void> inputStreams) {",
                "-    throw new UnsupportedOperationException();",
                "-  }",
                "-",
                "-  @Override",
                "-  public Void visitTableScan(TableScan scan, List<Void> inputStreams) throws Exception {",
                "-    pw.print(String.format(STAGE_ENUMERABLE_TABLE_SCAN, getStageName(scan)));",
                "-    return null;",
                "-  }",
                "-",
                "-  @Override",
                "-  public Void visitAggregate(Aggregate aggregate, List<Void> inputStreams) throws Exception {",
                "-    beginAggregateStage(aggregate);",
                "-    pw.println(\"        if (_data != null) {\");",
                "-    pw.println(\"        List<Object> curGroupValues = getGroupValues(_data);\");",
                "-    pw.println(\"        if (!state.containsKey(curGroupValues)) {\");",
                "-    pw.println(\"          state.put(curGroupValues, new HashMap<String, Object>());\");",
                "-    pw.println(\"        }\");",
                "-    pw.println(\"        Map<String, Object> accumulators = state.get(curGroupValues);\");",
                "-    for (AggregateCall call : aggregate.getAggCallList()) {",
                "-      aggregate(call);",
                "-    }",
                "-    pw.println(\"        }\");",
                "-    endStage();",
                "-    return null;",
                "-  }",
                "-",
                "-  @Override",
                "-  public Void visitJoin(Join join, List<Void> inputStreams) {",
                "-    beginJoinStage(join);",
                "-    pw.println(\"        if (source == left) {\");",
                "-    pw.println(\"            leftRows.add(_data);\");",
                "-    pw.println(\"        } else if (source == right) {\");",
                "-    pw.println(\"            rightRows.add(_data);\");",
                "-    pw.println(\"        }\");",
                "-    endStage();",
                "-    return null;",
                "-  }",
                "-",
                "-  private String groupValueEmitStr(String var, int n) {",
                "-    int count = 0;",
                "-    StringBuilder sb = new StringBuilder();",
                "-    for (int i = 0; i < n; i++) {",
                "-      if (++count > 1) {",
                "-        sb.append(\", \");",
                "-      }",
                "-      sb.append(var).append(\".\").append(\"get(\").append(i).append(\")\");",
                "-    }",
                "-    return sb.toString();",
                "-  }",
                "-",
                "-  private String emitAggregateStmts(Aggregate aggregate) {",
                "-    List<String> res = new ArrayList<>();",
                "-    StringWriter sw = new StringWriter();",
                "-    for (AggregateCall call : aggregate.getAggCallList()) {",
                "-      res.add(aggregateResult(call, new PrintWriter(sw)));",
                "-    }",
                "-    return NEW_LINE_JOINER.join(sw.toString(),",
                "-                                String.format(\"          ctx.emit(new Values(%s, %s));\",",
                "-                                              groupValueEmitStr(\"groupValues\", aggregate.getGroupSet().cardinality()),",
                "-                                              Joiner.on(\", \").join(res)));",
                "-  }",
                "-",
                "-  private String aggregateResult(AggregateCall call, PrintWriter pw) {",
                "-    SqlAggFunction aggFunction = call.getAggregation();",
                "-    String aggregationName = call.getAggregation().getName();",
                "-    Type ty = typeFactory.getJavaClass(call.getType());",
                "-    String result;",
                "-    if (aggFunction instanceof SqlUserDefinedAggFunction) {",
                "-      AggregateFunction aggregateFunction = ((SqlUserDefinedAggFunction) aggFunction).function;",
                "-      result = doAggregateResult((AggregateFunctionImpl) aggregateFunction, reserveAggVarName(call), ty, pw);",
                "-    } else {",
                "-      List<BuiltinAggregateFunctions.TypeClass> typeClasses = BuiltinAggregateFunctions.TABLE.get(aggregationName);",
                "-      if (typeClasses == null) {",
                "-        throw new UnsupportedOperationException(aggregationName + \" Not implemented\");",
                "-      }",
                "-      result = doAggregateResult(AggregateFunctionImpl.create(findMatchingClass(aggregationName, typeClasses, ty)),",
                "-                                 reserveAggVarName(call), ty, pw);",
                "-    }",
                "-    return result;",
                "-  }",
                "-",
                "-  private String doAggregateResult(AggregateFunctionImpl aggFn, String varName, Type ty, PrintWriter pw) {",
                "-    String resultName = varName + \"_result\";",
                "-    Class<?> accumulatorType = aggFn.accumulatorType;",
                "-    Class<?> resultType = aggFn.resultType;",
                "-    List<String> args = new ArrayList<>();",
                "-    if (!aggFn.isStatic) {",
                "-      String aggObjName = String.format(\"%s_obj\", varName);",
                "-      String aggObjClassName = aggFn.initMethod.getDeclaringClass().getCanonicalName();",
                "-      pw.println(\"          @SuppressWarnings(\\\"unchecked\\\")\");",
                "-      pw.println(String.format(\"          final %1$s %2$s = (%1$s) accumulators.get(\\\"%2$s\\\");\", aggObjClassName,",
                "-              aggObjName));",
                "-      args.add(aggObjName);",
                "-    }",
                "-    args.add(String.format(\"(%s)accumulators.get(\\\"%s\\\")\", accumulatorType.getCanonicalName(), varName));",
                "-    pw.println(String.format(\"          final %s %s = %s;\", resultType.getCanonicalName(),",
                "-                             resultName, printMethodCall(aggFn.resultMethod, args)));",
                "-",
                "-    return resultName;",
                "-  }",
                "-",
                "-  private void aggregate(AggregateCall call) {",
                "-    SqlAggFunction aggFunction = call.getAggregation();",
                "-    String aggregationName = call.getAggregation().getName();",
                "-    Type ty = typeFactory.getJavaClass(call.getType());",
                "-    if (call.getArgList().size() != 1) {",
                "-      if (aggregationName.equals(\"COUNT\")) {",
                "-        if (call.getArgList().size() != 0) {",
                "-          throw new UnsupportedOperationException(\"Count with nullable fields\");",
                "-        }",
                "-      }",
                "-    }",
                "-    if (aggFunction instanceof SqlUserDefinedAggFunction) {",
                "-      AggregateFunction aggregateFunction = ((SqlUserDefinedAggFunction) aggFunction).function;",
                "-      doAggregate((AggregateFunctionImpl) aggregateFunction, reserveAggVarName(call), ty, call.getArgList());",
                "-    } else {",
                "-      List<BuiltinAggregateFunctions.TypeClass> typeClasses = BuiltinAggregateFunctions.TABLE.get(aggregationName);",
                "-      if (typeClasses == null) {",
                "-        throw new UnsupportedOperationException(aggregationName + \" Not implemented\");",
                "-      }",
                "-      doAggregate(AggregateFunctionImpl.create(findMatchingClass(aggregationName, typeClasses, ty)),",
                "-                  reserveAggVarName(call), ty, call.getArgList());",
                "-    }",
                "-  }",
                "-",
                "-  private Class<?> findMatchingClass(String aggregationName, List<BuiltinAggregateFunctions.TypeClass> typeClasses, Type ty) {",
                "-    for (BuiltinAggregateFunctions.TypeClass typeClass : typeClasses) {",
                "-      if (typeClass.ty.equals(BuiltinAggregateFunctions.TypeClass.GenericType.class) || typeClass.ty.equals(ty)) {",
                "-        return typeClass.clazz;",
                "-      }",
                "-    }",
                "-    throw new UnsupportedOperationException(aggregationName + \" Not implemeted for type '\" + ty + \"'\");",
                "-  }",
                "-",
                "-  private void doAggregate(AggregateFunctionImpl aggFn, String varName, Type ty, List<Integer> argList) {",
                "-    List<String> args = new ArrayList<>();",
                "-    Class<?> accumulatorType = aggFn.accumulatorType;",
                "-    if (!aggFn.isStatic) {",
                "-      String aggObjName = String.format(\"%s_obj\", varName);",
                "-      String aggObjClassName = aggFn.initMethod.getDeclaringClass().getCanonicalName();",
                "-      pw.println(String.format(\"          if (!accumulators.containsKey(\\\"%s\\\")) { \", aggObjName));",
                "-      pw.println(String.format(\"            accumulators.put(\\\"%s\\\", new %s());\", aggObjName, aggObjClassName));",
                "-      pw.println(\"          }\");",
                "-      pw.println(\"          @SuppressWarnings(\\\"unchecked\\\")\");",
                "-      pw.println(String.format(\"          final %1$s %2$s = (%1$s) accumulators.get(\\\"%2$s\\\");\", aggObjClassName,",
                "-              aggObjName));",
                "-      args.add(aggObjName);",
                "-    }",
                "-    args.add(String.format(\"%1$s == null ? %2$s : (%3$s) %1$s\",",
                "-                           \"accumulators.get(\\\"\" + varName + \"\\\")\",",
                "-                           printMethodCall(aggFn.initMethod, args),",
                "-                           accumulatorType.getCanonicalName()));",
                "-    if (argList.isEmpty()) {",
                "-      args.add(\"EMPTY_VALUES\");",
                "-    } else {",
                "-      for (int i = 0; i < aggFn.valueTypes.size(); i++) {",
                "-        args.add(String.format(\"(%s) %s\", aggFn.valueTypes.get(i).getCanonicalName(), \"_data.get(\" + argList.get(i) + \")\"));",
                "-      }",
                "-    }",
                "-    pw.print(String.format(\"          accumulators.put(\\\"%s\\\", %s);\\n\",",
                "-                           varName,",
                "-                           printMethodCall(aggFn.addMethod, args)));",
                "-  }",
                "-",
                "-  private String reserveAggVarName(AggregateCall call) {",
                "-    String varName;",
                "-    if ((varName = aggregateCallVarNames.get(call)) == null) {",
                "-      varName = call.getAggregation().getName() + ++nameCount;",
                "-      aggregateCallVarNames.put(call, varName);",
                "-    }",
                "-    return varName;",
                "-  }",
                "-",
                "-  private void beginStage(RelNode n) {",
                "-    pw.print(String.format(STAGE_PROLOGUE, getStageName(n)));",
                "-  }",
                "-",
                "-  private void beginAggregateStage(Aggregate n) {",
                "-    pw.print(String.format(AGGREGATE_STAGE_PROLOGUE, getStageName(n), getGroupByIndices(n), emitAggregateStmts(n)));",
                "-  }",
                "-",
                "-  private void beginJoinStage(Join join) {",
                "-    int[] ordinals = new int[2];",
                "-    if (!RelOptUtil.analyzeSimpleEquiJoin((LogicalJoin) join, ordinals)) {",
                "-      throw new UnsupportedOperationException(\"Only simple equi joins are supported\");",
                "-    }",
                "-",
                "-    pw.print(String.format(JOIN_STAGE_PROLOGUE, getStageName(join),",
                "-                           getStageName(join.getLeft()),",
                "-                           getStageName(join.getRight()),",
                "-                           ordinals[0],",
                "-                           ordinals[1]));",
                "-  }",
                "-",
                "-  private void endStage() {",
                "-    pw.print(\"  }\\n  };\\n\");",
                "-  }",
                "-",
                "-  static String getStageName(RelNode n) {",
                "-    return n.getClass().getSimpleName().toUpperCase() + \"_\" + n.getId();",
                "-  }",
                "-",
                "-  private String getGroupByIndices(Aggregate n) {",
                "-    StringBuilder res = new StringBuilder();",
                "-    int count = 0;",
                "-    for (int i : n.getGroupSet()) {",
                "-      if (++count > 1) {",
                "-        res.append(\", \");",
                "-      }",
                "-      res.append(i);",
                "-    }",
                "-    return res.toString();",
                "-  }",
                "-",
                "-  public static String printMethodCall(Method method, List<String> args) {",
                "-    return printMethodCall(method.getDeclaringClass(), method.getName(),",
                "-            Modifier.isStatic(method.getModifiers()), args);",
                "-  }",
                "-",
                "-  private static String printMethodCall(Class<?> clazz, String method, boolean isStatic, List<String> args) {",
                "-    if (isStatic) {",
                "-      return String.format(\"%s.%s(%s)\", clazz.getCanonicalName(), method, Joiner.on(',').join(args));",
                "-    } else {",
                "-      return String.format(\"%s.%s(%s)\", args.get(0), method,",
                "-              Joiner.on(',').join(args.subList(1, args.size())));",
                "-    }",
                "-  }",
                "-",
                "-}",
                "diff --git a/sql/storm-sql-external/storm-sql-hdfs/src/jvm/org/apache/storm/sql/hdfs/HdfsDataSourcesProvider.java b/sql/storm-sql-external/storm-sql-hdfs/src/jvm/org/apache/storm/sql/hdfs/HdfsDataSourcesProvider.java",
                "index f918c2971..70ede5bc3 100644",
                "--- a/sql/storm-sql-external/storm-sql-hdfs/src/jvm/org/apache/storm/sql/hdfs/HdfsDataSourcesProvider.java",
                "+++ b/sql/storm-sql-external/storm-sql-hdfs/src/jvm/org/apache/storm/sql/hdfs/HdfsDataSourcesProvider.java",
                "@@ -35,3 +35,2 @@ import org.apache.storm.hdfs.trident.rotation.FileSizeRotationPolicy;",
                " import org.apache.storm.hdfs.trident.rotation.TimedRotationPolicy;",
                "-import org.apache.storm.sql.runtime.DataSource;",
                " import org.apache.storm.sql.runtime.DataSourcesProvider;",
                "@@ -122,8 +121,2 @@ public class HdfsDataSourcesProvider implements DataSourcesProvider {",
                "-    @Override",
                "-    public DataSource construct(URI uri, String inputFormatClass, String outputFormatClass,",
                "-                                List<FieldInfo> fields) {",
                "-        throw new UnsupportedOperationException();",
                "-    }",
                "-",
                "     @Override",
                "diff --git a/sql/storm-sql-external/storm-sql-kafka/src/jvm/org/apache/storm/sql/kafka/KafkaDataSourcesProvider.java b/sql/storm-sql-external/storm-sql-kafka/src/jvm/org/apache/storm/sql/kafka/KafkaDataSourcesProvider.java",
                "index b0e653087..81b94b171 100644",
                "--- a/sql/storm-sql-external/storm-sql-kafka/src/jvm/org/apache/storm/sql/kafka/KafkaDataSourcesProvider.java",
                "+++ b/sql/storm-sql-external/storm-sql-kafka/src/jvm/org/apache/storm/sql/kafka/KafkaDataSourcesProvider.java",
                "@@ -39,3 +39,2 @@ import org.apache.storm.spout.Scheme;",
                " import org.apache.storm.spout.SchemeAsMultiScheme;",
                "-import org.apache.storm.sql.runtime.DataSource;",
                " import org.apache.storm.sql.runtime.DataSourcesProvider;",
                "@@ -125,8 +124,2 @@ public class KafkaDataSourcesProvider implements DataSourcesProvider {",
                "-    @Override",
                "-    public DataSource construct(URI uri, String inputFormatClass, String outputFormatClass,",
                "-                                List<FieldInfo> fields) {",
                "-        throw new UnsupportedOperationException();",
                "-    }",
                "-",
                "     @Override",
                "diff --git a/sql/storm-sql-external/storm-sql-mongodb/src/jvm/org/apache/storm/sql/mongodb/MongoDataSourcesProvider.java b/sql/storm-sql-external/storm-sql-mongodb/src/jvm/org/apache/storm/sql/mongodb/MongoDataSourcesProvider.java",
                "index f3682b873..e82f9b648 100644",
                "--- a/sql/storm-sql-external/storm-sql-mongodb/src/jvm/org/apache/storm/sql/mongodb/MongoDataSourcesProvider.java",
                "+++ b/sql/storm-sql-external/storm-sql-mongodb/src/jvm/org/apache/storm/sql/mongodb/MongoDataSourcesProvider.java",
                "@@ -30,3 +30,2 @@ import org.apache.storm.mongodb.trident.state.MongoStateFactory;",
                " import org.apache.storm.mongodb.trident.state.MongoStateUpdater;",
                "-import org.apache.storm.sql.runtime.DataSource;",
                " import org.apache.storm.sql.runtime.DataSourcesProvider;",
                "@@ -113,8 +112,2 @@ public class MongoDataSourcesProvider implements DataSourcesProvider {",
                "-    @Override",
                "-    public DataSource construct(URI uri, String inputFormatClass, String outputFormatClass,",
                "-                                List<FieldInfo> fields) {",
                "-        throw new UnsupportedOperationException();",
                "-    }",
                "-",
                "     @Override",
                "diff --git a/sql/storm-sql-external/storm-sql-redis/src/jvm/org/apache/storm/sql/redis/RedisDataSourcesProvider.java b/sql/storm-sql-external/storm-sql-redis/src/jvm/org/apache/storm/sql/redis/RedisDataSourcesProvider.java",
                "index b020a1589..8dbdd7407 100644",
                "--- a/sql/storm-sql-external/storm-sql-redis/src/jvm/org/apache/storm/sql/redis/RedisDataSourcesProvider.java",
                "+++ b/sql/storm-sql-external/storm-sql-redis/src/jvm/org/apache/storm/sql/redis/RedisDataSourcesProvider.java",
                "@@ -37,3 +37,2 @@ import org.apache.storm.redis.trident.state.RedisState;",
                " import org.apache.storm.redis.trident.state.RedisStateUpdater;",
                "-import org.apache.storm.sql.runtime.DataSource;",
                " import org.apache.storm.sql.runtime.DataSourcesProvider;",
                "@@ -149,7 +148,2 @@ public class RedisDataSourcesProvider implements DataSourcesProvider {",
                "-    @Override",
                "-    public DataSource construct(URI uri, String inputFormatClass, String outputFormatClass, List<FieldInfo> fields) {",
                "-        throw new UnsupportedOperationException();",
                "-    }",
                "-",
                "     @Override",
                "diff --git a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractChannelHandler.java b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractChannelHandler.java",
                "deleted file mode 100644",
                "index effdf5526..000000000",
                "--- a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractChannelHandler.java",
                "+++ /dev/null",
                "@@ -1,53 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-",
                "-package org.apache.storm.sql.runtime;",
                "-",
                "-import org.apache.storm.tuple.Values;",
                "-",
                "-public abstract class AbstractChannelHandler implements ChannelHandler {",
                "-    @Override",
                "-    public abstract void dataReceived(ChannelContext ctx, Values data);",
                "-",
                "-    @Override",
                "-    public void channelInactive(ChannelContext ctx) {",
                "-",
                "-    }",
                "-",
                "-    @Override",
                "-    public void exceptionCaught(Throwable cause) {",
                "-",
                "-    }",
                "-",
                "-    @Override",
                "-    public void flush(ChannelContext ctx) {",
                "-        ctx.flush();",
                "-    }",
                "-",
                "-    @Override",
                "-    public void setSource(ChannelContext ctx, Object source) {",
                "-",
                "-    }",
                "-",
                "-    public static final AbstractChannelHandler PASS_THROUGH = new AbstractChannelHandler() {",
                "-        @Override",
                "-        public void dataReceived(ChannelContext ctx, Values data) {",
                "-            ctx.emit(data);",
                "-        }",
                "-    };",
                "-}",
                "diff --git a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractValuesProcessor.java b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractValuesProcessor.java",
                "deleted file mode 100644",
                "index 144932509..000000000",
                "--- a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractValuesProcessor.java",
                "+++ /dev/null",
                "@@ -1,45 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-",
                "-package org.apache.storm.sql.runtime;",
                "-",
                "-import java.util.Map;",
                "-",
                "-import org.apache.storm.tuple.Values;",
                "-",
                "-/**",
                "- * Subclass of AbstractTupleProcessor provides a series of tuple. It",
                "- * takes a series of iterators of {@link Values} and produces a stream of",
                "- * tuple.",
                "- * <p/>",
                "- * The subclass implements the {@see next()} method to provide",
                "- * the output of the stream. It can choose to return null in {@see next()} to",
                "- * indicate that this particular iteration is a no-op. SQL processors depend",
                "- * on this semantic to implement filtering and nullable records.",
                "- */",
                "-public abstract class AbstractValuesProcessor {",
                "-",
                "-    /**",
                "-     * Initialize the data sources.",
                "-     *",
                "-     * @param data a map from the table name to the iterators of the values.",
                "-     *",
                "-     */",
                "-    public abstract void initialize(Map<String, DataSource> data, ChannelHandler",
                "-            result);",
                "-}",
                "diff --git a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelContext.java b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelContext.java",
                "deleted file mode 100644",
                "index 9d6f6626a..000000000",
                "--- a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelContext.java",
                "+++ /dev/null",
                "@@ -1,34 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-",
                "-package org.apache.storm.sql.runtime;",
                "-",
                "-import org.apache.storm.tuple.Values;",
                "-",
                "-public interface ChannelContext {",
                "-    /**",
                "-     * Emit data to the next stage of the data pipeline.",
                "-     */",
                "-    void emit(Values data);",
                "-",
                "-    void fireChannelInactive();",
                "-",
                "-    void flush();",
                "-",
                "-    void setSource(Object source);",
                "-}",
                "diff --git a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelHandler.java b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelHandler.java",
                "deleted file mode 100644",
                "index 3009df477..000000000",
                "--- a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelHandler.java",
                "+++ /dev/null",
                "@@ -1,42 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-",
                "-package org.apache.storm.sql.runtime;",
                "-",
                "-import org.apache.storm.tuple.Values;",
                "-",
                "-/**",
                "- * DataListener provides an event-driven interface for the user to process",
                "- * series of events.",
                "- */",
                "-public interface ChannelHandler {",
                "-    void dataReceived(ChannelContext ctx, Values data);",
                "-",
                "-    /**",
                "-     * The producer of the data has indicated that the channel is no longer",
                "-     * active.",
                "-     * @param ctx ChannelContext",
                "-     */",
                "-    void channelInactive(ChannelContext ctx);",
                "-",
                "-    void exceptionCaught(Throwable cause);",
                "-",
                "-    void flush(ChannelContext ctx);",
                "-",
                "-    void setSource(ChannelContext ctx, Object source);",
                "-}",
                "diff --git a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/Channels.java b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/Channels.java",
                "deleted file mode 100644",
                "index d389c3829..000000000",
                "--- a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/Channels.java",
                "+++ /dev/null",
                "@@ -1,110 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-",
                "-package org.apache.storm.sql.runtime;",
                "-",
                "-import org.apache.storm.tuple.Values;",
                "-",
                "-public class Channels {",
                "-    private static final ChannelContext VOID_CTX = new ChannelContext() {",
                "-        @Override",
                "-        public void emit(Values data) {}",
                "-",
                "-        @Override",
                "-        public void fireChannelInactive() {}",
                "-",
                "-        @Override",
                "-        public void flush() {",
                "-",
                "-        }",
                "-",
                "-        @Override",
                "-        public void setSource(java.lang.Object source) {",
                "-",
                "-        }",
                "-    };",
                "-",
                "-    private static class ChannelContextAdapter implements ChannelContext {",
                "-        private final ChannelHandler handler;",
                "-        private final ChannelContext next;",
                "-",
                "-        public ChannelContextAdapter(",
                "-                ChannelContext next, ChannelHandler handler) {",
                "-            this.handler = handler;",
                "-            this.next = next;",
                "-        }",
                "-",
                "-        @Override",
                "-        public void emit(Values data) {",
                "-            handler.dataReceived(next, data);",
                "-        }",
                "-",
                "-        @Override",
                "-        public void fireChannelInactive() {",
                "-            handler.channelInactive(next);",
                "-        }",
                "-",
                "-        @Override",
                "-        public void flush() {",
                "-            handler.flush(next);",
                "-        }",
                "-",
                "-        @Override",
                "-        public void setSource(java.lang.Object source) {",
                "-            handler.setSource(next, source);",
                "-            next.setSource(source); // propagate through the chain",
                "-        }",
                "-    }",
                "-",
                "-    private static class ForwardingChannelContext implements ChannelContext {",
                "-        private final ChannelContext next;",
                "-",
                "-        public ForwardingChannelContext(ChannelContext next) {",
                "-            this.next = next;",
                "-        }",
                "-",
                "-        @Override",
                "-        public void emit(Values data) {",
                "-            next.emit(data);",
                "-        }",
                "-",
                "-        @Override",
                "-        public void fireChannelInactive() {",
                "-            next.fireChannelInactive();",
                "-        }",
                "-",
                "-        @Override",
                "-        public void flush() {",
                "-            next.flush();",
                "-        }",
                "-",
                "-        @Override",
                "-        public void setSource(Object source) {",
                "-            next.setSource(source);",
                "-        }",
                "-    }",
                "-",
                "-    public static ChannelContext chain(",
                "-            ChannelContext next, ChannelHandler handler) {",
                "-        return new ChannelContextAdapter(next, handler);",
                "-    }",
                "-",
                "-    public static ChannelContext voidContext() {",
                "-        return VOID_CTX;",
                "-    }",
                "-}",
                "diff --git a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSource.java b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSource.java",
                "deleted file mode 100644",
                "index b90b4f5ce..000000000",
                "--- a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSource.java",
                "+++ /dev/null",
                "@@ -1,28 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-",
                "-package org.apache.storm.sql.runtime;",
                "-",
                "-/**",
                "- * A DataSource ingests data in StormSQL. It provides a series of tuple to",
                "- * the downstream {@link ChannelHandler}.",
                "- *",
                "- */",
                "-public interface DataSource {",
                "-    void open(ChannelContext ctx);",
                "-}",
                "diff --git a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesProvider.java b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesProvider.java",
                "index baec6cd7f..d89e81527 100644",
                "--- a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesProvider.java",
                "+++ b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesProvider.java",
                "@@ -31,16 +31,2 @@ public interface DataSourcesProvider {",
                "-    /**",
                "-     * Construct a new data source.",
                "-     * @param uri The URI that specifies the data source. The format of the URI",
                "-     *            is fully customizable.",
                "-     * @param inputFormatClass the name of the class that deserializes data.",
                "-     *                         It is null when unspecified.",
                "-     * @param outputFormatClass the name of the class that serializes data. It",
                "-     *                          is null when unspecified.",
                "-     * @param fields The name of the fields and the schema of the table.",
                "-     */",
                "-    DataSource construct(",
                "-            URI uri, String inputFormatClass, String outputFormatClass,",
                "-            List<FieldInfo> fields);",
                "-",
                "     ISqlTridentDataSource constructTrident(",
                "diff --git a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesRegistry.java b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesRegistry.java",
                "index 9d8368a3c..7383235cd 100644",
                "--- a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesRegistry.java",
                "+++ b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesRegistry.java",
                "@@ -48,21 +48,2 @@ public class DataSourcesRegistry {",
                "-    /**",
                "-     * Construct a data source.",
                "-     * @param uri data source uri",
                "-     * @param inputFormatClass input format class",
                "-     * @param outputFormatClass output format class",
                "-     * @param fields fields info list",
                "-     * @return DataSource object",
                "-     */",
                "-    public static DataSource construct(",
                "-            URI uri, String inputFormatClass, String outputFormatClass,",
                "-            List<FieldInfo> fields) {",
                "-        DataSourcesProvider provider = providers.get(uri.getScheme());",
                "-        if (provider == null) {",
                "-            return null;",
                "-        }",
                "-",
                "-        return provider.construct(uri, inputFormatClass, outputFormatClass, fields);",
                "-    }",
                "-",
                "     /**",
                "diff --git a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/StormSqlFunctions.java b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/StormSqlFunctions.java",
                "deleted file mode 100644",
                "index 095e6ba8d..000000000",
                "--- a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/StormSqlFunctions.java",
                "+++ /dev/null",
                "@@ -1,48 +0,0 @@",
                "-/**",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- * http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-",
                "-package org.apache.storm.sql.runtime;",
                "-",
                "-public class StormSqlFunctions {",
                "-",
                "-    /**",
                "-     * Whether the object equals the other one.",
                "-     * @param b0 one object",
                "-     * @param b1 the other object",
                "-     * @return true if the object equals the other one",
                "-     */",
                "-    public static Boolean eq(Object b0, Object b1) {",
                "-        if (b0 == null || b1 == null) {",
                "-            return null;",
                "-        }",
                "-        return b0.equals(b1);",
                "-    }",
                "-",
                "-    /**",
                "-     * Whether the object dose not equals the other one.",
                "-     * @param b0 one object",
                "-     * @param b1 the other object",
                "-     * @return true if the object dose not equals the other one",
                "-     */",
                "-    public static Boolean ne(Object b0, Object b1) {",
                "-        if (b0 == null || b1 == null) {",
                "-            return null;",
                "-        }",
                "-        return !b0.equals(b1);",
                "-    }",
                "-}",
                "diff --git a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/datasource/socket/SocketDataSourcesProvider.java b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/datasource/socket/SocketDataSourcesProvider.java",
                "index fe4d024ad..10e470d1c 100644",
                "--- a/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/datasource/socket/SocketDataSourcesProvider.java",
                "+++ b/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/datasource/socket/SocketDataSourcesProvider.java",
                "@@ -25,3 +25,2 @@ import java.util.Properties;",
                " import org.apache.storm.spout.Scheme;",
                "-import org.apache.storm.sql.runtime.DataSource;",
                " import org.apache.storm.sql.runtime.DataSourcesProvider;",
                "@@ -80,7 +79,2 @@ public class SocketDataSourcesProvider implements DataSourcesProvider {",
                "-    @Override",
                "-    public DataSource construct(URI uri, String inputFormatClass, String outputFormatClass, List<FieldInfo> fields) {",
                "-        throw new UnsupportedOperationException();",
                "-    }",
                "-",
                "     @Override"
            ],
            "changed_files": [
                "sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSql.java",
                "sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlContext.java",
                "sql/storm-sql-core/src/jvm/org/apache/storm/sql/StormSqlImpl.java",
                "sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/BuiltinAggregateFunctions.java",
                "sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/PlanCompiler.java",
                "sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/PostOrderRelNodeVisitor.java",
                "sql/storm-sql-core/src/jvm/org/apache/storm/sql/compiler/backends/standalone/RelNodeCompiler.java",
                "sql/storm-sql-external/storm-sql-hdfs/src/jvm/org/apache/storm/sql/hdfs/HdfsDataSourcesProvider.java",
                "sql/storm-sql-external/storm-sql-kafka/src/jvm/org/apache/storm/sql/kafka/KafkaDataSourcesProvider.java",
                "sql/storm-sql-external/storm-sql-mongodb/src/jvm/org/apache/storm/sql/mongodb/MongoDataSourcesProvider.java",
                "sql/storm-sql-external/storm-sql-redis/src/jvm/org/apache/storm/sql/redis/RedisDataSourcesProvider.java",
                "sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractChannelHandler.java",
                "sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractValuesProcessor.java",
                "sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelContext.java",
                "sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelHandler.java",
                "sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/Channels.java",
                "sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSource.java",
                "sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesProvider.java",
                "sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesRegistry.java",
                "sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/StormSqlFunctions.java",
                "sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/datasource/socket/SocketDataSourcesProvider.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2845": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2845",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f998e267c9aa78609088b7d58e072f49e8befb3f",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514284405,
            "hunks": 0,
            "message": "Merge branch 'Apache_master_STORM-2844_ISEEarliest' of https://github.com/hmcl/storm-apache into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2844": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2844",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "96864e8356eba99bed8c61c0a6d96bceda83cab4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509253684,
            "hunks": 0,
            "message": "Merge branch 'STORM-2791-1.x' of https://github.com/danielsd/storm into STORM-2791-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2791": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2791",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8af4fcac3267b8468c7a4cb02afbe9b7fd083d4b",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515011018,
            "hunks": 1,
            "message": "STORM-2153: set disruptor reporter threads to daemon",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java b/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "index 6ea3683a6..d7497d6fd 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                "@@ -68,3 +68,4 @@ public class DisruptorQueue implements IStatefulObject {",
                "     private static final FlusherPool FLUSHER = new FlusherPool();",
                "-    private static final ScheduledThreadPoolExecutor METRICS_REPORTER_EXECUTOR = new ScheduledThreadPoolExecutor(1);",
                "+    private static final ScheduledThreadPoolExecutor METRICS_REPORTER_EXECUTOR = new ScheduledThreadPoolExecutor(1,",
                "+            new ThreadFactoryBuilder().setDaemon(true).setNameFormat(PREFIX + \"metrics-reporter\").build());"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "080d9ddcee34c0a3bfe3a3c6797552192e05d7c8",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508302798,
            "hunks": 0,
            "message": "Merge branch 'STORM-2779-1.1.x' of https://github.com/HeartSaVioR/storm into STORM-2779-1.1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2779": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2779",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "41d6cdcc99133168d1ecf400381e3dd1cf48f28e",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514513588,
            "hunks": 0,
            "message": "Merge branch 'STORM-2869' of https://github.com/srdo/storm into STORM-2869-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2869": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2869",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "c893c8dd82dc3fcc5f1c9fce469d62c429e6e84c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513417292,
            "hunks": 0,
            "message": "Merge branch 'STORM-2847' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2847": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2847",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "67c65e774f20d3091caed6da82d155887e966772",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510846495,
            "hunks": 0,
            "message": "Merge branch 'STORM-2815-1.1.x-merge' into 1.1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2815": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2815",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b8f76afceeb33afbe904f529d410cd650fbd6824",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513638135,
            "hunks": 0,
            "message": "Merge branch 'STORM-2854-1.x' into 1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2854": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2854",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "01bd4f821c940e979c360a4667c27c0477fde9a7",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507592499,
            "hunks": 27,
            "message": "[STORM-2686] Add locality awareness to LoadAwareShuffleGrouping",
            "diff": [
                "diff --git a/conf/defaults.yaml b/conf/defaults.yaml",
                "index 103b04f21..ad3405408 100644",
                "--- a/conf/defaults.yaml",
                "+++ b/conf/defaults.yaml",
                "@@ -263,2 +263,4 @@ topology.disable.loadaware.messaging: false",
                " topology.state.checkpoint.interval.ms: 1000",
                "+topology.localityaware.higher.bound.percent: 0.8",
                "+topology.localityaware.lower.bound.percent: 0.2",
                "diff --git a/storm-client/src/jvm/org/apache/storm/Config.java b/storm-client/src/jvm/org/apache/storm/Config.java",
                "index e296e8ffb..6be0c2167 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/Config.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/Config.java",
                "@@ -66,2 +66,20 @@ public class Config extends HashMap<String, Object> {",
                "+    /**",
                "+     * This signifies the load congestion among target tasks in scope. Currently it's only used in LoadAwareShuffleGrouping.",
                "+     * When the average load is higher than the higher bound, the executor should choose target tasks in a higher scope,",
                "+     * The scopes and their orders are: EVERYTHING > RACK_LOCAL > HOST_LOCAL > WORKER_LOCAL",
                "+     */",
                "+    @isPositiveNumber",
                "+    @NotNull",
                "+    public static final String TOPOLOGY_LOCALITYAWARE_HIGHER_BOUND_PERCENT = \"topology.localityaware.higher.bound.percent\";",
                "+",
                "+    /**",
                "+     * This signifies the load congestion among target tasks in scope. Currently it's only used in LoadAwareShuffleGrouping.",
                "+     * When the average load is lower than the lower bound, the executor should choose target tasks in a lower scope.",
                "+     * The scopes and their orders are: EVERYTHING > RACK_LOCAL > HOST_LOCAL > WORKER_LOCAL",
                "+     */",
                "+    @isPositiveNumber",
                "+    @NotNull",
                "+    public static final String TOPOLOGY_LOCALITYAWARE_LOWER_BOUND_PERCENT = \"topology.localityaware.lower.bound.percent\";",
                "+",
                "     /**",
                "diff --git a/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java b/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java",
                "index 825de4b3a..ec2ff59b6 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java",
                "@@ -587,3 +587,3 @@ public class WorkerState {",
                "                 defaultSharedResources,",
                "-                userSharedResources);",
                "+                userSharedResources, cachedTaskToNodePort, assignmentId);",
                "         } catch (IOException e) {",
                "diff --git a/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java b/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java",
                "index f5b63ec5e..3fd75e53b 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java",
                "@@ -21,5 +21,9 @@ package org.apache.storm.grouping;",
                " import com.google.common.annotations.VisibleForTesting;",
                "+import com.google.common.collect.Sets;",
                "+",
                " import java.io.Serializable;",
                "+import java.util.ArrayList;",
                " import java.util.Arrays;",
                " import java.util.HashMap;",
                "+import java.util.HashSet;",
                " import java.util.List;",
                "@@ -27,5 +31,15 @@ import java.util.Map;",
                " import java.util.Random;",
                "+import java.util.Set;",
                " import java.util.concurrent.atomic.AtomicInteger;",
                "+import java.util.concurrent.atomic.AtomicReference;",
                "+",
                "+import org.apache.storm.Config;",
                " import org.apache.storm.generated.GlobalStreamId;",
                "+import org.apache.storm.generated.NodeInfo;",
                "+import org.apache.storm.networktopography.DNSToSwitchMapping;",
                " import org.apache.storm.task.WorkerTopologyContext;",
                "+import org.apache.storm.utils.ObjectReader;",
                "+import org.apache.storm.utils.ReflectionUtils;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "@@ -42,4 +56,9 @@ public class LoadAwareShuffleGrouping implements LoadAwareCustomStreamGrouping,",
                "         }",
                "+",
                "+        void resetWeight() {",
                "+            weight = MAX_WEIGHT;",
                "+        }",
                "     }",
                "+    private static final Logger LOG = LoggerFactory.getLogger(LoadAwareShuffleGrouping.class);",
                "     private final Map<Integer, IndexAndWeights> orig = new HashMap<>();",
                "@@ -52,2 +71,11 @@ public class LoadAwareShuffleGrouping implements LoadAwareCustomStreamGrouping,",
                "     private AtomicInteger current;",
                "+    private Scope currentScope;",
                "+    private NodeInfo sourceNodeInfo;",
                "+    private List<Integer> targetTasks;",
                "+    private AtomicReference<Map<Integer, NodeInfo>> taskToNodePort;",
                "+    private Map<String, Object> conf;",
                "+    private DNSToSwitchMapping dnsToSwitchMapping;",
                "+    private Map<Scope, List<Integer>> localityGroup;",
                "+    private double higherBound;",
                "+    private double lowerBound;",
                "@@ -56,2 +84,11 @@ public class LoadAwareShuffleGrouping implements LoadAwareCustomStreamGrouping,",
                "         random = new Random();",
                "+        sourceNodeInfo = new NodeInfo(context.getThisWorkerHost(), Sets.newHashSet((long) context.getThisWorkerPort()));",
                "+        taskToNodePort = context.getTaskToNodePort();",
                "+        this.targetTasks = targetTasks;",
                "+        conf = context.getConf();",
                "+        dnsToSwitchMapping = ReflectionUtils.newInstance((String) conf.get(Config.STORM_NETWORK_TOPOGRAPHY_PLUGIN));",
                "+        localityGroup = new HashMap<>();",
                "+        currentScope = Scope.WORKER_LOCAL;",
                "+        higherBound = ObjectReader.getDouble(conf.get(Config.TOPOLOGY_LOCALITYAWARE_HIGHER_BOUND_PERCENT));",
                "+        lowerBound = ObjectReader.getDouble(conf.get(Config.TOPOLOGY_LOCALITYAWARE_LOWER_BOUND_PERCENT));",
                "@@ -95,8 +132,77 @@ public class LoadAwareShuffleGrouping implements LoadAwareCustomStreamGrouping,",
                "+    private void refreshLocalityGroup() {",
                "+        Map<Integer, NodeInfo> cachedTaskToNodePort = taskToNodePort.get();",
                "+        Map<String, String> hostToRack = getHostToRackMapping(cachedTaskToNodePort);",
                "+",
                "+        localityGroup.values().stream().forEach(v -> v.clear());",
                "+",
                "+        for (int target: targetTasks) {",
                "+            Scope scope = calculateScope(cachedTaskToNodePort, hostToRack, target);",
                "+            if (!localityGroup.containsKey(scope)) {",
                "+                localityGroup.put(scope, new ArrayList<>());",
                "+            }",
                "+            localityGroup.get(scope).add(target);",
                "+        }",
                "+    }",
                "+",
                "+    private List<Integer> getTargetsInScope(Scope scope) {",
                "+        List<Integer> rets = new ArrayList<>();",
                "+        List<Integer> targetInScope = localityGroup.get(scope);",
                "+        if (null != targetInScope) {",
                "+            rets.addAll(targetInScope);",
                "+        }",
                "+        Scope downgradeScope = Scope.downgrade(scope);",
                "+        if (downgradeScope != scope) {",
                "+            rets.addAll(getTargetsInScope(downgradeScope));",
                "+        }",
                "+        return rets;",
                "+    }",
                "+",
                "+    private Scope transition(LoadMapping load) {",
                "+        List<Integer> targetInScope = getTargetsInScope(currentScope);",
                "+        if (targetInScope.isEmpty()) {",
                "+            Scope upScope = Scope.upgrade(currentScope);",
                "+            if (upScope == currentScope) {",
                "+                throw new RuntimeException(\"This executor has no target tasks.\");",
                "+            }",
                "+            currentScope = upScope;",
                "+            return transition(load);",
                "+        }",
                "+",
                "+        if (null == load) {",
                "+            return currentScope;",
                "+        }",
                "+",
                "+        double avg = targetInScope.stream().mapToDouble((key) -> load.get(key)).average().getAsDouble();",
                "+        Scope nextScope;",
                "+        if (avg < lowerBound) {",
                "+            nextScope = Scope.downgrade(currentScope);",
                "+            if (getTargetsInScope(nextScope).isEmpty()) {",
                "+                nextScope = currentScope;",
                "+            }",
                "+        } else if (avg > higherBound) {",
                "+            nextScope = Scope.upgrade(currentScope);",
                "+        } else {",
                "+            nextScope = currentScope;",
                "+        }",
                "+",
                "+        return nextScope;",
                "+    }",
                "+",
                "     private synchronized void updateRing(LoadMapping load) {",
                "+        refreshLocalityGroup();",
                "+        Scope prevScope = currentScope;",
                "+        currentScope = transition(load);",
                "+        if (currentScope != prevScope) {",
                "+            //reset all the weights",
                "+            orig.values().stream().forEach(o -> o.resetWeight());",
                "+        }",
                "+",
                "+        List<Integer> targetsInScope = getTargetsInScope(currentScope);",
                "+",
                "         //We will adjust weights based off of the minimum load",
                "-        double min = load == null ? 0 : orig.keySet().stream().mapToDouble((key) -> load.get(key)).min().getAsDouble();",
                "-        for (Map.Entry<Integer, IndexAndWeights> target: orig.entrySet()) {",
                "-            IndexAndWeights val = target.getValue();",
                "-            double l = load == null ? 0.0 : load.get(target.getKey());",
                "+        double min = load == null ? 0 : targetsInScope.stream().mapToDouble((key) -> load.get(key)).min().getAsDouble();",
                "+        for (int target: targetsInScope) {",
                "+            IndexAndWeights val = orig.get(target);",
                "+            double l = load == null ? 0.0 : load.get(target);",
                "             if (l <= min + (0.05)) {",
                "@@ -111,3 +217,3 @@ public class LoadAwareShuffleGrouping implements LoadAwareCustomStreamGrouping,",
                "         //Now we need to build the array",
                "-        long weightSum = orig.values().stream().mapToLong((w) -> w.weight).sum();",
                "+        long weightSum = targetsInScope.stream().mapToLong((target) -> orig.get(target).weight).sum();",
                "         //Now we can calculate a percentage",
                "@@ -116,3 +222,4 @@ public class LoadAwareShuffleGrouping implements LoadAwareCustomStreamGrouping,",
                "         if (weightSum > 0) {",
                "-            for (IndexAndWeights indexAndWeights : orig.values()) {",
                "+            for (int target: targetsInScope) {",
                "+                IndexAndWeights indexAndWeights = orig.get(target);",
                "                 int count = (int) ((indexAndWeights.weight / (double) weightSum) * CAPACITY);",
                "@@ -158,2 +265,61 @@ public class LoadAwareShuffleGrouping implements LoadAwareCustomStreamGrouping,",
                "     }",
                "+",
                "+",
                "+    private Scope calculateScope(Map<Integer, NodeInfo> taskToNodePort, Map<String, String> hostToRack, int target) {",
                "+        NodeInfo targetNodeInfo = taskToNodePort.get(target);",
                "+",
                "+        if (targetNodeInfo == null) {",
                "+            return Scope.EVERYTHING;",
                "+        }",
                "+",
                "+        String sourceRack = hostToRack.get(sourceNodeInfo.get_node());",
                "+        String targetRack = hostToRack.get(targetNodeInfo.get_node());",
                "+",
                "+        if(sourceRack != null && targetRack != null && sourceRack.equals(targetRack)) {",
                "+            if(sourceNodeInfo.get_node().equals(targetNodeInfo.get_node())) {",
                "+                if(sourceNodeInfo.get_port().equals(targetNodeInfo.get_port())) {",
                "+                    return Scope.WORKER_LOCAL;",
                "+                }",
                "+                return Scope.HOST_LOCAL;",
                "+            }",
                "+            return Scope.RACK_LOCAL;",
                "+        } else {",
                "+            return Scope.EVERYTHING;",
                "+        }",
                "+    }",
                "+",
                "+    private Map<String, String> getHostToRackMapping(Map<Integer, NodeInfo> taskToNodePort) {",
                "+        Set<String> hosts = new HashSet();",
                "+        for (int task: targetTasks) {",
                "+            hosts.add(taskToNodePort.get(task).get_node());",
                "+        }",
                "+        hosts.add(sourceNodeInfo.get_node());",
                "+        return dnsToSwitchMapping.resolve(new ArrayList<>(hosts));",
                "+    }",
                "+",
                "+    enum Scope {",
                "+        WORKER_LOCAL, HOST_LOCAL, RACK_LOCAL, EVERYTHING;",
                "+",
                "+        public static Scope downgrade(Scope current) {",
                "+            switch (current) {",
                "+                case EVERYTHING: return RACK_LOCAL;",
                "+                case RACK_LOCAL: return HOST_LOCAL;",
                "+                case HOST_LOCAL:",
                "+                case WORKER_LOCAL:",
                "+                default:",
                "+                    return WORKER_LOCAL;",
                "+            }",
                "+        }",
                "+",
                "+        public static Scope upgrade(Scope current) {",
                "+            switch (current) {",
                "+                case WORKER_LOCAL: return HOST_LOCAL;",
                "+                case HOST_LOCAL: return RACK_LOCAL;",
                "+                case RACK_LOCAL:",
                "+                case EVERYTHING:",
                "+                default:",
                "+                    return EVERYTHING;",
                "+            }",
                "+        }",
                "+    }",
                " }",
                "\\ No newline at end of file",
                "diff --git a/storm-client/src/jvm/org/apache/storm/task/GeneralTopologyContext.java b/storm-client/src/jvm/org/apache/storm/task/GeneralTopologyContext.java",
                "index 6614f94c0..deae7cfb0 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/task/GeneralTopologyContext.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/task/GeneralTopologyContext.java",
                "@@ -201,2 +201,6 @@ public class GeneralTopologyContext implements JSONAware {",
                "     }",
                "+",
                "+    public Map<String, Object> getConf() {",
                "+        return _topoConf;",
                "+    }",
                " }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/task/WorkerTopologyContext.java b/storm-client/src/jvm/org/apache/storm/task/WorkerTopologyContext.java",
                "index 2817b6548..8c63eb623 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/task/WorkerTopologyContext.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/task/WorkerTopologyContext.java",
                "@@ -19,2 +19,3 @@ package org.apache.storm.task;",
                "+import org.apache.storm.generated.NodeInfo;",
                " import org.apache.storm.generated.StormTopology;",
                "@@ -26,2 +27,3 @@ import java.util.Map;",
                " import java.util.concurrent.ExecutorService;",
                "+import java.util.concurrent.atomic.AtomicReference;",
                "@@ -36,2 +38,4 @@ public class WorkerTopologyContext extends GeneralTopologyContext {",
                "     Map<String, Object> _defaultResources;",
                "+    private AtomicReference<Map<Integer, NodeInfo>> taskToNodePort;",
                "+    private String assignmentId;",
                "@@ -49,3 +53,5 @@ public class WorkerTopologyContext extends GeneralTopologyContext {",
                "             Map<String, Object> defaultResources,",
                "-            Map<String, Object> userResources",
                "+            Map<String, Object> userResources,",
                "+            AtomicReference<Map<Integer, NodeInfo>> taskToNodePort,",
                "+            String assignmentId",
                "             ) {",
                "@@ -66,2 +72,22 @@ public class WorkerTopologyContext extends GeneralTopologyContext {",
                "         _workerTasks = workerTasks;",
                "+        this.taskToNodePort = taskToNodePort;",
                "+        this.assignmentId = assignmentId;",
                "+",
                "+    }",
                "+",
                "+    public WorkerTopologyContext(",
                "+            StormTopology topology,",
                "+            Map<String, Object> topoConf,",
                "+            Map<Integer, String> taskToComponent,",
                "+            Map<String, List<Integer>> componentToSortedTasks,",
                "+            Map<String, Map<String, Fields>> componentToStreamToFields,",
                "+            String stormId,",
                "+            String codeDir,",
                "+            String pidDir,",
                "+            Integer workerPort,",
                "+            List<Integer> workerTasks,",
                "+            Map<String, Object> defaultResources,",
                "+            Map<String, Object> userResources) {",
                "+        this(topology, topoConf, taskToComponent, componentToSortedTasks, componentToStreamToFields, stormId,",
                "+                codeDir, pidDir, workerPort, workerTasks, defaultResources, userResources, null, null);",
                "     }",
                "@@ -80,2 +106,14 @@ public class WorkerTopologyContext extends GeneralTopologyContext {",
                "+    public String getThisWorkerHost() {",
                "+        return assignmentId;",
                "+    }",
                "+",
                "+    /**",
                "+     * Get a map from task Id to NodePort",
                "+     * @return a map from task To NodePort",
                "+     */",
                "+    public AtomicReference<Map<Integer, NodeInfo>> getTaskToNodePort() {",
                "+        return taskToNodePort;",
                "+    }",
                "+",
                "     /**"
            ],
            "changed_files": [
                "conf/defaults.yaml",
                "storm-client/src/jvm/org/apache/storm/Config.java",
                "storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java",
                "storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java",
                "storm-client/src/jvm/org/apache/storm/task/GeneralTopologyContext.java",
                "storm-client/src/jvm/org/apache/storm/task/WorkerTopologyContext.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2686": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2686",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5d0889b848991613f1c2618afa6330ecbb76a3c9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509324461,
            "hunks": 0,
            "message": "Merge branch '1.x-branch_STORM-2787_KSInitFlag' of https://github.com/hmcl/storm-apache into STORM-2787-on-top-of-STORM-2781-merge-1.x",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2787": "",
                "STORM-2781": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2787, STORM-2781",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "8abdec8b53fe454d068cd592f1568021de960dff",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510807408,
            "hunks": 0,
            "message": "Merge branch 'STORM-2814-merge'",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2814": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2814",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7239bbda655df48d263ccbfa298de871e1c07358",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516655059,
            "hunks": 2,
            "message": "STORM-2904: Document Metrics V2",
            "diff": [
                "diff --git a/conf/storm.yaml.example b/conf/storm.yaml.example",
                "index 911ad07ba..290483fea 100644",
                "--- a/conf/storm.yaml.example",
                "+++ b/conf/storm.yaml.example",
                "@@ -81,2 +81,25 @@",
                " #     arguments:",
                "-#       endpoint: \"event-logger.mycompany.org\"",
                "\\ No newline at end of file",
                "+#       endpoint: \"event-logger.mycompany.org\"",
                "+",
                "+# Metrics v2 configuration (optional)",
                "+#storm.metrics.reporters:",
                "+#  # Graphite Reporter",
                "+#  - class: \"org.apache.storm.metrics2.reporters.GraphiteStormReporter\"",
                "+#    daemons:",
                "+#        - \"supervisor\"",
                "+#        - \"nimbus\"",
                "+#        - \"worker\"",
                "+#    report.period: 60",
                "+#    report.period.units: \"SECONDS\"",
                "+#    graphite.host: \"localhost\"",
                "+#    graphite.port: 2003",
                "+#",
                "+#  # Console Reporter",
                "+#  - class: \"org.apache.storm.metrics2.reporters.ConsoleStormReporter\"",
                "+#    daemons:",
                "+#        - \"worker\"",
                "+#    report.period: 10",
                "+#    report.period.units: \"SECONDS\"",
                "+#    filter:",
                "+#        class: \"org.apache.storm.metrics2.filters.RegexFilter\"",
                "+#        expression: \".*my_component.*emitted.*\""
            ],
            "changed_files": [
                "conf/storm.yaml.example"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2904": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2904",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "445f439391f211ca064df818f752f3ee49e06f47",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510261571,
            "hunks": 0,
            "message": "Merge branch 'YSTORM-2727' of https://github.com/govind-menon/storm into STORM-2725-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "YSTORM-2727": "",
                "STORM-2725": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: YSTORM-2727, STORM-2725",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "c2b36738903daa072a201dc25c7c38e26622a3d9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515139609,
            "hunks": 0,
            "message": "Merge branch 'STORM-2881-1.x' of https://github.com/arunmahadevan/storm into STORM-2881-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2881": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2881",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "2eef71c8becadfcea1d173ac14bd5a82f8067d5d",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514248922,
            "hunks": 0,
            "message": "Merge branch 'STORM-2690__1.x-branch' of https://github.com/erikdw/storm into STORM-2690-1.x",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2690": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2690",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "366c3dcd305292e5122f8625177d6b9489c683f5",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515371930,
            "hunks": 0,
            "message": "Merge branch 'STORM-2879-1.0.x' into 1.0.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2879": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2879",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "3ce7ada65fe38beb6ecc6c5cffd773a2aaeaa3c8",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516309692,
            "hunks": 0,
            "message": "Merge branch 'STORM-2900' of https://github.com/satishd/storm into STORM-2900",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2900": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2900",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "107a9c9ab34aebfabad646e8d30b51e94e3ae060",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511802575,
            "hunks": 3,
            "message": "STORM-2833: Use the original host name when closing.",
            "diff": [
                "diff --git a/storm-client/src/jvm/org/apache/storm/messaging/netty/Client.java b/storm-client/src/jvm/org/apache/storm/messaging/netty/Client.java",
                "index dd07d602f..fcaf4e521 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/messaging/netty/Client.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/messaging/netty/Client.java",
                "@@ -84,2 +84,6 @@ public class Client extends ConnectionWithStatus implements IStatefulObject, ISa",
                "     protected final String dstAddressPrefixedName;",
                "+    //The actual name of the host we are trying to connect to so that",
                "+    // when we remove ourselves from the connection cache there is no concern that",
                "+    // the resolved host name is different.",
                "+    private final String dstHost;",
                "     private volatile Map<Integer, Double> serverLoad = null;",
                "@@ -159,2 +163,3 @@ public class Client extends ConnectionWithStatus implements IStatefulObject, ISa",
                "         bootstrap = createClientBootstrap(factory, bufferSize, topoConf);",
                "+        dstHost = host;",
                "         dstAddress = new InetSocketAddress(host, port);",
                "@@ -428,3 +433,3 @@ public class Client extends ConnectionWithStatus implements IStatefulObject, ISa",
                "             LOG.info(\"closing Netty Client {}\", dstAddressPrefixedName);",
                "-            context.removeClient(dstAddress.getHostName(),dstAddress.getPort());",
                "+            context.removeClient(dstHost, dstAddress.getPort());",
                "             // Set closing to true to prevent any further reconnection attempts."
            ],
            "changed_files": [
                "storm-client/src/jvm/org/apache/storm/messaging/netty/Client.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2833": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2833",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "af6b7f30241db17584184c359ac24dbd555f8c59",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513180051,
            "hunks": 3,
            "message": "STORM-2856: Make Storm build work on post-2017Q4 Travis Trusty image",
            "diff": [
                "diff --git a/.travis.yml b/.travis.yml",
                "index 89df2f89e..e54bdd412 100644",
                "--- a/.travis.yml",
                "+++ b/.travis.yml",
                "@@ -24,3 +24,2 @@ dist: trusty",
                " sudo: required",
                "-group: deprecated-2017Q4",
                "@@ -30,5 +29,6 @@ jdk:",
                " before_install:",
                "-  - rvm use 2.1.5 --install",
                "-  - nvm install 0.12.2",
                "-  - nvm use 0.12.2",
                "+  - rvm reload",
                "+  - rvm use 2.4.2 --install",
                "+  - nvm install 8.9.3",
                "+  - nvm use 8.9.3",
                " install: /bin/bash ./dev-tools/travis/travis-install.sh `pwd`",
                "@@ -40,2 +40,2 @@ cache:",
                "     - \"$HOME/.rvm\"",
                "-    - \"$NVM_DIR\"",
                "+    - \"$NVM_DIR\"",
                "\\ No newline at end of file"
            ],
            "changed_files": [
                ".travis.yml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2856": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2856",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5fad1ad30785e3f25ce094ad06e39d29a4a914e7",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513868716,
            "hunks": 2,
            "message": "STORM-2863: Reset NormalizedResources for tests",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "index 8ed1a5728..f332da5ff 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java",
                "@@ -22,2 +22,3 @@ import static org.apache.storm.Constants.*;",
                "+import com.google.common.annotations.VisibleForTesting;",
                " import java.util.Arrays;",
                "@@ -81,2 +82,13 @@ public abstract class NormalizedResources {",
                "+    /**",
                "+     * This is for testing only.  It allows a test to reset the mapping of resource names in the array.",
                "+     * We reset the mapping because some algorithms sadly have different behavior if a resource exists",
                "+     * or not.",
                "+     */",
                "+    @VisibleForTesting",
                "+    public static void resetResourceNames() {",
                "+        resourceNames.clear();",
                "+        counter.set(0);",
                "+    }",
                "+",
                "     public NormalizedResources(NormalizedResources other) {"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/NormalizedResources.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2863": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2863",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "975174bee0c757f5c1a465ffea4abf0c3b0b1dca",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515820675,
            "hunks": 0,
            "message": "Merge branch 'STORM-2877' of https://github.com/srishtyagrawal/storm into STORM-2877-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2877": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2877",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "eafc003794310f6e69c09dc7fd23f9cc5d5aaeff",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515704159,
            "hunks": 1,
            "message": "STORM-2153: Add missing license header",
            "diff": [
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java b/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java",
                "index 5bb01d216..550b17688 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java",
                "@@ -1 +1,18 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                " package org.apache.storm.metrics2;"
            ],
            "changed_files": [
                "storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "278cf205672ebc74a8f6e3973c0080f74e3012c7",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507673190,
            "hunks": 5,
            "message": "STORM-2771: Don't double the tests in normal mode",
            "diff": [
                "diff --git a/pom.xml b/pom.xml",
                "index 85d575fd3..d61f02165 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -311,2 +311,3 @@",
                "         <java.unit.test.include>**/Test*.java, **/*Test.java, **/*TestCase.java</java.unit.test.include>    <!--maven surefire plugin default test list-->",
                "+        <java.integration.test.include>no.tests</java.integration.test.include>",
                "         <!-- by default the clojure test set are all clojure tests that are not integration tests. This property is overridden in the profiles -->",
                "@@ -602,3 +603,2 @@",
                "                 <java.integration.test.include>**/*.java</java.integration.test.include>",
                "-                <java.integration.test.group>org.apache.storm.testing.IntegrationTest</java.integration.test.group>",
                "                 <clojure.test.set>*.*</clojure.test.set>",
                "@@ -612,3 +612,2 @@",
                "                 <java.integration.test.include>**/*.java</java.integration.test.include>",
                "-                <java.integration.test.group>org.apache.storm.testing.IntegrationTest</java.integration.test.group>",
                "                 <!--Clojure-->",
                "@@ -1111,2 +1110,3 @@",
                "                     <configuration>",
                "+                        <redirectTestOutputToFile>true</redirectTestOutputToFile>",
                "                         <includes>",
                "@@ -1114,3 +1114,3 @@",
                "                         </includes>",
                "-                        <groups>${java.integration.test.group}</groups>  <!--set in integration-test the profile-->",
                "+                        <groups>org.apache.storm.testing.IntegrationTest</groups>",
                "                         <argLine>-Xmx1536m</argLine>"
            ],
            "changed_files": [
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2771": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2771",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "34faca7162d335ab0dcb88a23e991a3bdeacda45",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514249502,
            "hunks": 0,
            "message": "Merge branch 'STORM-2843' of https://github.com/vesense/storm into STORM-2843-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2843": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2843",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "4b28a60a3748b3346ff18f0fca28a20ec8f40463",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515820948,
            "hunks": 0,
            "message": "Merge branch 'kerberos-solr-bolt-master1' of https://github.com/omkreddy/storm into STORM-2860-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2860": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2860",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1c2286bc2c776fe14869111070131f1407d698fe",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513659613,
            "hunks": 0,
            "message": "Merge branch 'STORM-2857' of https://github.com/Ethanlm/storm into STORM-2857-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2857": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2857",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "a6c4965900f54edf3b45cea67e19da125f90c547",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514321800,
            "hunks": 0,
            "message": "Merge branch 'update_multilang_modules' of https://github.com/bitcloud/storm into STORM-2868-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2868": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2868",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "ca25384c5424fbd71fedfae969c808734625c657",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517545067,
            "hunks": 0,
            "message": "Merge branch 'STORM-2853-1.x' of https://github.com/HeartSaVioR/storm into STORM-2853-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2853": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2853",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "c3b56624356224743f5e22a186807a3875ddd83e",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509830698,
            "hunks": 5,
            "message": "STORM-2800: Use Maven JAXB api artifact instead of assuming it is available from the JDK",
            "diff": [
                "diff --git a/external/storm-autocreds/pom.xml b/external/storm-autocreds/pom.xml",
                "index 40dd97dff..0e4a1cf69 100644",
                "--- a/external/storm-autocreds/pom.xml",
                "+++ b/external/storm-autocreds/pom.xml",
                "@@ -126,2 +126,6 @@",
                "         </dependency>",
                "+        <dependency>",
                "+            <groupId>javax.xml.bind</groupId>",
                "+            <artifactId>jaxb-api</artifactId>",
                "+        </dependency>",
                "     </dependencies>",
                "diff --git a/external/storm-pmml/pom.xml b/external/storm-pmml/pom.xml",
                "index 4af2058a5..346e7a8a9 100644",
                "--- a/external/storm-pmml/pom.xml",
                "+++ b/external/storm-pmml/pom.xml",
                "@@ -73,2 +73,7 @@",
                "         </dependency>",
                "+        <!-- JAXB api -->",
                "+        <dependency>",
                "+            <groupId>javax.xml.bind</groupId>",
                "+            <artifactId>jaxb-api</artifactId>",
                "+        </dependency>",
                "     </dependencies>",
                "diff --git a/pom.xml b/pom.xml",
                "index 0b740c754..ce40ef3e6 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -325,2 +325,3 @@",
                "         <caffeine.version>2.3.5</caffeine.version>",
                "+        <jaxb-version>2.3.0</jaxb-version>",
                "@@ -1052,2 +1053,7 @@",
                "             </dependency>",
                "+            <dependency>",
                "+                <groupId>javax.xml.bind</groupId>",
                "+                <artifactId>jaxb-api</artifactId>",
                "+                <version>${jaxb-version}</version>",
                "+            </dependency>",
                "         </dependencies>",
                "diff --git a/storm-client/pom.xml b/storm-client/pom.xml",
                "index d0cbe0e05..19354c14f 100644",
                "--- a/storm-client/pom.xml",
                "+++ b/storm-client/pom.xml",
                "@@ -117,2 +117,8 @@",
                "         </dependency>",
                "+        ",
                "+        <!-- JAXB -->",
                "+        <dependency>",
                "+            <groupId>javax.xml.bind</groupId>",
                "+            <artifactId>jaxb-api</artifactId>",
                "+        </dependency>"
            ],
            "changed_files": [
                "external/storm-autocreds/pom.xml",
                "external/storm-pmml/pom.xml",
                "pom.xml",
                "storm-client/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2800": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2800",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "818ed16f2281a6ee0ff65768a36fc8b8354b3ef5",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510812800,
            "hunks": 0,
            "message": "Merge branch 'STORM-2546-1.x' of https://github.com/srdo/storm into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2546": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2546",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f942cf4f1a73b447d8687f83410199c5575ed174",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514248965,
            "hunks": 0,
            "message": "Merge branch 'STORM-2690__1.0.x-branch' of https://github.com/erikdw/storm into STORM-2690-1.0.x",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2690": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2690",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "0fa8209e455915d357c070d9b5dbdc4f64c26d93",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511909716,
            "hunks": 0,
            "message": "Merge branch 'STORM-2833-1.x' of https://github.com/revans2/incubator-storm into STORM-2833-1.x",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2833": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2833",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "577b8c924106d6eae0641114a4ad11eb51700ffd",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515820826,
            "hunks": 0,
            "message": "Merge branch 'STORM-2899' of https://github.com/srdo/storm into STORM-2899-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2899": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2899",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d0c5457c0efa4cf5838fdd9626804ff05e541ed0",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1508783357,
            "hunks": 0,
            "message": "Merge branch 'STORM-2706' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2706": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2706",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "9063524f31bb6822ee1b94dc13a2c1393f7d0732",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510522641,
            "hunks": 0,
            "message": "Merge branch 'STORM-2810' of https://github.com/srdo/storm into STORM-2810-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2810": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2810",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "ef73328df9f3f70cdf0d492dca3cfb9fea6b342e",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510422821,
            "hunks": 0,
            "message": "Merge branch 'STORM-2549-1.x' of https://github.com/srdo/storm into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2549": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2549",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7dd06ef3bd9f90979a6e344427ab6861caa45337",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513413456,
            "hunks": 0,
            "message": "Merge branch 'STORM-2851-1.x' of https://github.com/srdo/storm into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2851": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2851",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "11bdbb0637a7c62f072775c01816cbfab441c1c4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507590696,
            "hunks": 0,
            "message": "Merge branch 'STORM-2666-1.1.x' of https://github.com/srdo/storm into STORM-2666-1.1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2666": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2666",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f37a6bd99d10f65a43becadcd7f7615715e5dc0b",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517087564,
            "hunks": 0,
            "message": "Merge branch 'STORM-2912' of https://github.com/HeartSaVioR/storm into STORM-2192-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2912": "",
                "STORM-2192": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2912, STORM-2192",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5d95e7139493270643e35ffb1ba59628c8402e57",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509546157,
            "hunks": 2,
            "message": "STORM-2795: Fix race",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java b/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java",
                "index 4f01c304f..1ad582beb 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java",
                "@@ -31,2 +31,3 @@ import java.io.PrintWriter;",
                " import java.nio.file.DirectoryStream;",
                "+import java.nio.file.FileAlreadyExistsException;",
                " import java.nio.file.Files;",
                "@@ -261,3 +262,8 @@ public class LocalizedResource extends LocallyCachedBlob {",
                "             if (!Files.exists(parent)) {",
                "-                Files.createDirectory(parent);",
                "+                //There is a race here that we can still lose",
                "+                try {",
                "+                    Files.createDirectory(parent);",
                "+                } catch (FileAlreadyExistsException e) {",
                "+                    //Ignored",
                "+                }",
                "             }"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/localizer/LocalizedResource.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2795": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2795",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "3ee5543f06978765ee1d66bce0fc6530f68076d9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510730478,
            "hunks": 0,
            "message": "Merge branch 'STORM-2546' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2546": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2546",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6d45875dac89be8d3c35f84194a7e1b95e364343",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510642248,
            "hunks": 0,
            "message": "Merge branch 'STORM-2810-1.x' of https://github.com/srdo/storm into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2810": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2810",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "442efb94f82a7d2a30d2a973774d627307b62d63",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1514249654,
            "hunks": 0,
            "message": "Merge branch 'STORM-2843-1.x' of https://github.com/vesense/storm into STORM-2843-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2843": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2843",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d3c00ee7705b2d7b1bba4afd1146fb4c258a471d",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515703177,
            "hunks": 13,
            "message": "STORM-2153: eliminate string concatenation when looking up metrics",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index e8d23e5e4..3dd7289e6 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -37,3 +37,3 @@",
                "   (:import [org.apache.storm.cluster ClusterStateContext DaemonType])",
                "-  (:import [org.apache.storm.metrics2 StormMetricRegistry])",
                "+  (:import [org.apache.storm.metrics2 StormMetricRegistry TaskMetrics])",
                "   (:import [com.codahale.metrics Meter Counter])",
                "@@ -267,2 +267,3 @@",
                "      :task->component (:task->component worker)",
                "+     :task-metrics (TaskMetrics/taskMetricsMap (first task-ids) (last task-ids) worker-context component-id)",
                "      :stream->component->grouper (outbound-components worker-context component-id storm-conf)",
                "@@ -444,3 +445,3 @@",
                "     (when time-delta",
                "-      (stats/spout-failed-tuple! (:stats executor-data) (StormMetricRegistry/counter \"failed\" (:worker-context executor-data) (:component-id executor-data) task-id (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-failed-tuple! (:stats executor-data)  (.getFailed ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "@@ -453,3 +454,3 @@",
                "     (when time-delta",
                "-      (stats/spout-acked-tuple! (:stats executor-data) (StormMetricRegistry/counter \"acked\" (:worker-context executor-data) (:component-id executor-data) task-id (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "+      (stats/spout-acked-tuple! (:stats executor-data) (.getAcked ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) (:stream tuple-info)) (:stream tuple-info) time-delta))))",
                "@@ -822,3 +823,3 @@",
                "                              (stats/bolt-acked-tuple! executor-stats",
                "-                                                      (StormMetricRegistry/counter \"acked\" worker-context  (:component-id executor-data) task-id (.getSourceStreamId tuple))",
                "+                                                      (.getAcked ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) (.getSourceStreamId tuple))",
                "                                                       (.getSourceComponent tuple)",
                "@@ -838,3 +839,3 @@",
                "                              (stats/bolt-failed-tuple! executor-stats",
                "-                                                       (StormMetricRegistry/counter \"failed\" worker-context (:component-id executor-data) task-id (.getSourceStreamId tuple))",
                "+                                                       (.getFailed ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) (.getSourceStreamId tuple))",
                "                                                        (.getSourceComponent tuple)",
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/task.clj b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "index 9e18331b7..26ce76c55 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "@@ -25,3 +25,3 @@",
                "   (:import [org.apache.storm.task TopologyContext ShellBolt WorkerTopologyContext])",
                "-  (:import [org.apache.storm.metrics2 StormMetricRegistry])",
                "+  (:import [org.apache.storm.metrics2 StormMetricRegistry TaskMetrics])",
                "   (:import [org.apache.storm.utils Utils])",
                "@@ -29,3 +29,3 @@",
                "   (:import [org.apache.storm.spout ShellSpout])",
                "-  (:import [java.util Collection List ArrayList])",
                "+  (:import [java.util Collection List ArrayList Map])",
                "   (:import [com.codahale.metrics Meter Counter])",
                "@@ -145,5 +145,5 @@",
                "             (when (emit-sampler)",
                "-              (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id task-id stream) stream)",
                "+              (stats/emitted-tuple! executor-stats (.getEmitted ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) stream) stream)",
                "               (if out-task-id",
                "-                (stats/transferred-tuples! executor-stats (StormMetricRegistry/counter \"transferred\" worker-context component-id task-id stream) stream 1)))",
                "+                (stats/transferred-tuples! executor-stats (.getTransferred ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) stream) stream 1)))",
                "             (if out-task-id [out-task-id])",
                "@@ -165,4 +165,4 @@",
                "              (when (emit-sampler)",
                "-               (stats/emitted-tuple! executor-stats (StormMetricRegistry/counter \"emitted\" worker-context component-id task-id stream) stream)",
                "-               (stats/transferred-tuples! executor-stats (StormMetricRegistry/counter \"transferred\" worker-context component-id task-id stream) stream (count out-tasks)))",
                "+               (stats/emitted-tuple! executor-stats (.getEmitted (.get ^Map (:task-metrics executor-data) task-id) stream) stream)",
                "+               (stats/transferred-tuples! executor-stats (.getTransferred ^TaskMetrics (.get ^Map (:task-metrics executor-data) task-id) stream) stream (count out-tasks)))",
                "              out-tasks)))",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "index e1305f990..e0023fd7e 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "@@ -80,2 +80,7 @@ public class StormMetricRegistry {",
                "+    public static Counter counter(String name, String topologyId, String componentId, Integer taskId, Integer workerPort, String streamId){",
                "+        String metricName = metricName(name, topologyId, componentId, streamId,taskId, workerPort);",
                "+        return REGISTRY.counter(metricName);",
                "+    }",
                "+",
                "     public static void start(Map<String, Object> stormConfig, DaemonType type){",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java b/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java",
                "new file mode 100644",
                "index 000000000..5bb01d216",
                "--- /dev/null",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java",
                "@@ -0,0 +1,72 @@",
                "+package org.apache.storm.metrics2;",
                "+",
                "+import com.codahale.metrics.Counter;",
                "+import org.apache.storm.task.WorkerTopologyContext;",
                "+",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+import java.util.concurrent.ConcurrentHashMap;",
                "+import java.util.concurrent.ConcurrentMap;",
                "+",
                "+public class TaskMetrics {",
                "+    ConcurrentMap<String, Counter> ackedByStream = new ConcurrentHashMap<>();",
                "+    ConcurrentMap<String, Counter> failedByStream = new ConcurrentHashMap<>();",
                "+    ConcurrentMap<String, Counter> emittedByStream = new ConcurrentHashMap<>();",
                "+    ConcurrentMap<String, Counter> transferredByStream = new ConcurrentHashMap<>();",
                "+",
                "+    private String topologyId;",
                "+    private String componentId;",
                "+    private Integer taskId;",
                "+    private Integer workerPort;",
                "+",
                "+    public TaskMetrics(WorkerTopologyContext context, String componentId, Integer taskid){",
                "+        this.topologyId = context.getStormId();",
                "+        this.componentId = componentId;",
                "+        this.taskId = taskid;",
                "+        this.workerPort = context.getThisWorkerPort();",
                "+    }",
                "+",
                "+    public Counter getAcked(String streamId) {",
                "+        Counter c = this.ackedByStream.get(streamId);",
                "+        if (c == null) {",
                "+            c = StormMetricRegistry.counter(\"acked\", this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "+            this.ackedByStream.put(streamId, c);",
                "+        }",
                "+        return c;",
                "+    }",
                "+",
                "+    public Counter getFailed(String streamId) {",
                "+        Counter c = this.ackedByStream.get(streamId);",
                "+        if (c == null) {",
                "+            c = StormMetricRegistry.counter(\"failed\", this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "+            this.failedByStream.put(streamId, c);",
                "+        }",
                "+        return c;",
                "+    }",
                "+",
                "+    public Counter getEmitted(String streamId) {",
                "+        Counter c = this.emittedByStream.get(streamId);",
                "+        if (c == null) {",
                "+            c = StormMetricRegistry.counter(\"emitted\", this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "+            this.emittedByStream.put(streamId, c);",
                "+        }",
                "+        return c;",
                "+    }",
                "+",
                "+    public Counter getTransferred(String streamId) {",
                "+        Counter c = this.transferredByStream.get(streamId);",
                "+        if (c == null) {",
                "+            c = StormMetricRegistry.counter(\"transferred\", this.topologyId, this.componentId, this.taskId, this.workerPort, streamId);",
                "+            this.transferredByStream.put(streamId, c);",
                "+        }",
                "+        return c;",
                "+    }",
                "+",
                "+    public static Map<Integer, TaskMetrics> taskMetricsMap(Integer startTaskId, Integer endTaskId, WorkerTopologyContext context, String componentId){",
                "+        Map<Integer, TaskMetrics> retval = new HashMap<>();",
                "+        for (int i = startTaskId; i < endTaskId + 1; i++) {",
                "+            retval.put(i, new TaskMetrics(context, componentId, i));",
                "+        }",
                "+        return retval;",
                "+    }",
                "+}"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "storm-core/src/jvm/org/apache/storm/metrics2/TaskMetrics.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2153": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2153",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "0b4741c5a9d0b1e87ed9becf1c8fffdbe084ec32",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510788142,
            "hunks": 0,
            "message": "Merge branch 'STORM-2815-merge'",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2815": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2815",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1e957a64af1a3ea2855bbb29e28f94b87ae91932",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510776930,
            "hunks": 3,
            "message": "[STORM-2820] fix validateTopologyWorkerMaxHeapSizeConfigs function never picks up the value set by nimbus problem",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index f878cf052..fc427a8e8 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -2548,6 +2548,6 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "     static void validateTopologyWorkerMaxHeapSizeConfigs(",
                "-        Map<String, Object> stormConf, StormTopology topology) {",
                "+        Map<String, Object> stormConf, StormTopology topology, double defaultWorkerMaxHeapSizeMB) {",
                "         double largestMemReq = getMaxExecutorMemoryUsageForTopo(topology, stormConf);",
                "         double topologyWorkerMaxHeapSize =",
                "-            ObjectReader.getDouble(stormConf.get(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB), 768.0);",
                "+            ObjectReader.getDouble(stormConf.get(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB), defaultWorkerMaxHeapSizeMB);",
                "         if (topologyWorkerMaxHeapSize < largestMemReq) {",
                "@@ -2628,3 +2628,4 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "             }",
                "-            validateTopologyWorkerMaxHeapSizeConfigs(topoConf, topology);",
                "+            validateTopologyWorkerMaxHeapSizeConfigs(topoConf, topology,",
                "+                    ObjectReader.getDouble(conf.get(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB)));",
                "             Utils.validateTopologyBlobStoreMap(topoConf, blobStore);"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2820": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2820",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "dcfbf274b0b6d75051423eb81440c85b4b27beb3",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510871896,
            "hunks": 75,
            "message": "STORM-2805: Addressed review comments and fixed some bugs",
            "diff": [
                "diff --git a/storm-client/pom.xml b/storm-client/pom.xml",
                "index 8a7efc46f..6097b359e 100644",
                "--- a/storm-client/pom.xml",
                "+++ b/storm-client/pom.xml",
                "@@ -263,3 +263,3 @@",
                "                     <excludes>**/generated/**</excludes>",
                "-                    <maxAllowedViolations>10320</maxAllowedViolations>",
                "+                    <maxAllowedViolations>10298</maxAllowedViolations>",
                "                 </configuration>",
                "diff --git a/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "index 9b1495d38..d8e523159 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "@@ -95,5 +95,3 @@ public class BatchSubtopologyBuilder {",
                "         }",
                "-        if (!masterBolt.componentConf.isEmpty()) {",
                "-            declarer.addConfigurations(masterBolt.componentConf);",
                "-        }",
                "+        declarer.addConfigurations(masterBolt.componentConf);",
                "         for (String id: bolts.keySet()) {",
                "@@ -453,3 +451,3 @@ public class BatchSubtopologyBuilder {",
                "         public BoltDeclarer addConfigurations(Map<String, Object> conf) {",
                "-            if (conf != null && !conf.isEmpty()) {",
                "+            if (conf != null) {",
                "                 component.componentConf.putAll(conf);",
                "@@ -460,5 +458,9 @@ public class BatchSubtopologyBuilder {",
                "         @Override",
                "-        public Map<String, Object> getRASConfiguration() {",
                "-            //TODO this should not be modifiable",
                "-            return component.componentConf;",
                "+        public BoltDeclarer addResources(Map<String, Double> resources) {",
                "+            if (resources != null) {",
                "+                Map<String, Double> currentResources = (Map<String, Double>) component.componentConf.computeIfAbsent(",
                "+                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "+                currentResources.putAll(resources);",
                "+            }",
                "+            return this;",
                "         }",
                "@@ -474,7 +476,6 @@ public class BatchSubtopologyBuilder {",
                "         public BoltDeclarer addResource(String resourceName, Number resourceValue) {",
                "-            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) component.componentConf.computeIfAbsent(",
                "+                Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "             resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "-",
                "-            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "             return this;",
                "diff --git a/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "index 51db42205..6844ad05f 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "@@ -403,3 +403,3 @@ public class LinearDRPCTopologyBuilder {",
                "         public LinearDRPCInputDeclarer addConfigurations(Map<String, Object> conf) {",
                "-            if (conf != null && !conf.isEmpty()) {",
                "+            if (conf != null) {",
                "                 component.componentConf.putAll(conf);",
                "@@ -409,2 +409,12 @@ public class LinearDRPCTopologyBuilder {",
                "+        @Override",
                "+        public LinearDRPCInputDeclarer addResources(Map<String, Double> resources) {",
                "+            if (resources != null) {",
                "+                Map<String, Double> currentResources = (Map<String, Double>) component.componentConf.computeIfAbsent(",
                "+                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "+                currentResources.putAll(resources);",
                "+            }",
                "+            return this;",
                "+        }",
                "+",
                "         @SuppressWarnings(\"unchecked\")",
                "@@ -412,7 +422,6 @@ public class LinearDRPCTopologyBuilder {",
                "         public LinearDRPCInputDeclarer addResource(String resourceName, Number resourceValue) {",
                "-            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) component.componentConf.computeIfAbsent(",
                "+                Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "             resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "-",
                "-            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "             return this;",
                "@@ -420,7 +429,2 @@ public class LinearDRPCTopologyBuilder {",
                "-        @Override",
                "-        public Map getRASConfiguration() {",
                "-            return component.componentConf;",
                "-        }",
                "-",
                "         @Override",
                "diff --git a/storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java b/storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java",
                "index eac23bbb6..6515c4689 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java",
                "@@ -88,11 +88,2 @@ public abstract class BaseConfigurationDeclarer<T extends ComponentConfiguration",
                "     }",
                "-",
                "-    @SuppressWarnings(\"unchecked\")",
                "-    @Override",
                "-    public T addResources(Map<String, Double> resources) {",
                "-        if (resources != null) {",
                "-            return addConfiguration(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resources);",
                "-        }",
                "-        return (T) this;",
                "-    }",
                " }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java b/storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java",
                "index 80980298b..ce3839205 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java",
                "@@ -30,9 +30,2 @@ public interface ComponentConfigurationDeclarer<T extends ComponentConfiguration",
                "-    /**",
                "-     * return the configuration.",
                "-     *",
                "-     * @return the current configuration.",
                "-     */",
                "-    Map<String, Object> getRASConfiguration();",
                "-",
                "     /**",
                "diff --git a/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "index 0ebdae6ac..f2e09b81d 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "@@ -52,4 +52,2 @@ import java.nio.ByteBuffer;",
                " import java.util.ArrayList;",
                "-import java.util.Collection;",
                "-import java.util.Collections;",
                " import java.util.HashMap;",
                "@@ -119,3 +117,3 @@ public class TopologyBuilder {",
                "     private final Map<String, IRichSpout> _spouts = new HashMap<>();",
                "-    private final Map<String, ComponentCommon> _commons = new HashMap<>();",
                "+    private final Map<String, ComponentCommon> commons = new HashMap<>();",
                "     private final Map<String, Set<String>> _componentToSharedMemory = new HashMap<>();",
                "@@ -545,3 +543,3 @@ public class TopologyBuilder {",
                "     private ComponentCommon getComponentCommon(String id, IComponent component) {",
                "-        ComponentCommon ret = new ComponentCommon(_commons.get(id));",
                "+        ComponentCommon ret = new ComponentCommon(commons.get(id));",
                "         OutputFieldsGetter getter = new OutputFieldsGetter();",
                "@@ -564,3 +562,3 @@ public class TopologyBuilder {",
                "         if(conf!=null) common.set_json_conf(JSONValue.toJSONString(conf));",
                "-        _commons.put(id, common);",
                "+        commons.put(id, common);",
                "     }",
                "@@ -568,6 +566,6 @@ public class TopologyBuilder {",
                "     protected class ConfigGetter<T extends ComponentConfigurationDeclarer> extends BaseConfigurationDeclarer<T> {",
                "-        String _id;",
                "+        String id;",
                "         public ConfigGetter(String id) {",
                "-            _id = id;",
                "+            this.id = id;",
                "         }",
                "@@ -577,7 +575,11 @@ public class TopologyBuilder {",
                "         public T addConfigurations(Map<String, Object> conf) {",
                "-            if(conf!=null && conf.containsKey(Config.TOPOLOGY_KRYO_REGISTER)) {",
                "-                throw new IllegalArgumentException(\"Cannot set serializations for a component using fluent API\");",
                "+            if (conf != null) {",
                "+                if (conf.containsKey(Config.TOPOLOGY_KRYO_REGISTER)) {",
                "+                    throw new IllegalArgumentException(\"Cannot set serializations for a component using fluent API\");",
                "+                }",
                "+                if (!conf.isEmpty()) {",
                "+                    String currConf = commons.get(id).get_json_conf();",
                "+                    commons.get(id).set_json_conf(mergeIntoJson(parseJson(currConf), conf));",
                "+                }",
                "             }",
                "-            String currConf = _commons.get(_id).get_json_conf();",
                "-            _commons.get(_id).set_json_conf(mergeIntoJson(parseJson(currConf), conf));",
                "             return (T) this;",
                "@@ -585,11 +587,12 @@ public class TopologyBuilder {",
                "-        @SuppressWarnings(\"unchecked\")",
                "         @Override",
                "-        public T addResource(String resourceName, Number resourceValue) {",
                "-            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().getOrDefault(",
                "-                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "-",
                "-            resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "-",
                "-            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "+        public T addResources(Map<String, Double> resources) {",
                "+            if (resources != null && !resources.isEmpty()) {",
                "+                String currConf = commons.get(id).get_json_conf();",
                "+                Map<String, Object> conf = parseJson(currConf);",
                "+                Map<String, Double> currentResources =",
                "+                    (Map<String, Double>) conf.computeIfAbsent(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "+                currentResources.putAll(resources);",
                "+                commons.get(id).set_json_conf(JSONValue.toJSONString(currConf));",
                "+            }",
                "             return (T) this;",
                "@@ -599,4 +602,10 @@ public class TopologyBuilder {",
                "         @Override",
                "-        public Map getRASConfiguration() {",
                "-            return parseJson(_commons.get(_id).get_json_conf());",
                "+        public T addResource(String resourceName, Number resourceValue) {",
                "+            Map<String, Object> componentConf = parseJson(commons.get(id).get_json_conf());",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) componentConf.computeIfAbsent(",
                "+                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "+",
                "+            resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "+",
                "+            return addConfiguration(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "         }",
                "@@ -611,3 +620,3 @@ public class TopologyBuilder {",
                "             _sharedMemory.put(request.get_name(), request);",
                "-            Set<String> mems = _componentToSharedMemory.computeIfAbsent(_id, (k) -> new HashSet<>());",
                "+            Set<String> mems = _componentToSharedMemory.computeIfAbsent(id, (k) -> new HashSet<>());",
                "             mems.add(request.get_name());",
                "@@ -688,3 +697,3 @@ public class TopologyBuilder {",
                "         private BoltDeclarer grouping(String componentId, String streamId, Grouping grouping) {",
                "-            _commons.get(_boltId).put_to_inputs(new GlobalStreamId(componentId, streamId), grouping);",
                "+            commons.get(_boltId).put_to_inputs(new GlobalStreamId(componentId, streamId), grouping);",
                "             return this;",
                "@@ -719,4 +728,4 @@ public class TopologyBuilder {",
                "     private static String mergeIntoJson(Map into, Map newMap) {",
                "-        Map res = new HashMap(into);",
                "-        if(newMap!=null) res.putAll(newMap);",
                "+        Map res = new HashMap<>(into);",
                "+        res.putAll(newMap);",
                "         return JSONValue.toJSONString(res);",
                "diff --git a/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "index 6f994edf2..466bd4ca9 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "@@ -226,3 +226,3 @@ public class TransactionalTopologyBuilder {",
                "         public SpoutDeclarer addConfigurations(Map<String, Object> conf) {",
                "-            if (conf != null && !conf.isEmpty()) {",
                "+            if (conf != null) {",
                "                 spoutConf.putAll(conf);",
                "@@ -233,5 +233,9 @@ public class TransactionalTopologyBuilder {",
                "         @Override",
                "-        public Map getRASConfiguration() {",
                "-            //TODO this should be imutable",
                "-            return spoutConf;",
                "+        public SpoutDeclarerImpl addResources(Map<String, Double> resources) {",
                "+            if (resources != null) {",
                "+                Map<String, Double> currentResources = (Map<String, Double>) spoutConf.computeIfAbsent(",
                "+                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "+                currentResources.putAll(resources);",
                "+            }",
                "+            return this;",
                "         }",
                "@@ -247,10 +251,6 @@ public class TransactionalTopologyBuilder {",
                "         public SpoutDeclarer addResource(String resourceName, Number resourceValue) {",
                "-            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) spoutConf.computeIfAbsent(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP,",
                "+                (k) -> new HashMap<>());",
                "-            if (resourcesMap == null) {",
                "-                resourcesMap = new HashMap<>();",
                "-            }",
                "             resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "-",
                "-            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "             return this;",
                "@@ -554,3 +554,3 @@ public class TransactionalTopologyBuilder {",
                "         public BoltDeclarer addConfigurations(Map<String, Object> conf) {",
                "-            if (conf != null && !conf.isEmpty()) {",
                "+            if (conf != null) {",
                "                 component.componentConf.putAll(conf);",
                "@@ -561,7 +561,10 @@ public class TransactionalTopologyBuilder {",
                "         @Override",
                "-        public Map getRASConfiguration() {",
                "-            //TODO this should be read only",
                "-            return component.componentConf;",
                "+        public BoltDeclarer addResources(Map<String, Double> resources) {",
                "+            if (resources != null) {",
                "+                Map<String, Double> currentResources = (Map<String, Double>) component.componentConf.computeIfAbsent(",
                "+                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "+                currentResources.putAll(resources);",
                "+            }",
                "+            return this;",
                "         }",
                "-",
                "         @Override",
                "@@ -575,7 +578,6 @@ public class TransactionalTopologyBuilder {",
                "         public BoltDeclarer addResource(String resourceName, Number resourceValue) {",
                "-            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) component.componentConf.computeIfAbsent(",
                "+                Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "             resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "-",
                "-            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "             return this;",
                "diff --git a/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java b/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java",
                "index 274e913cb..39c52f9b6 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java",
                "@@ -17,2 +17,3 @@",
                "  */",
                "+",
                " package org.apache.storm.trident.topology;",
                "@@ -53,6 +54,6 @@ import org.apache.storm.trident.topology.TridentBoltExecutor.CoordType;",
                " public class TridentTopologyBuilder {",
                "-    Map<GlobalStreamId, String> batchIds = new HashMap();",
                "-    Map<String, TransactionalSpoutComponent> spouts = new HashMap();",
                "-    Map<String, SpoutComponent> _batchPerTupleSpouts = new HashMap();",
                "-    Map<String, Component> _bolts = new HashMap();",
                "+    Map<GlobalStreamId, String> batchIds = new HashMap<>();",
                "+    Map<String, TransactionalSpoutComponent> spouts = new HashMap<>();",
                "+    Map<String, SpoutComponent> _batchPerTupleSpouts = new HashMap<>();",
                "+    Map<String, Component> bolts = new HashMap<>();",
                "@@ -60,3 +61,3 @@ public class TridentTopologyBuilder {",
                "     public SpoutDeclarer setBatchPerTupleSpout(String id, String streamName, IRichSpout spout, Integer parallelism, String batchGroup) {",
                "-        Map<String, String> batchGroups = new HashMap();",
                "+        Map<String, String> batchGroups = new HashMap<>();",
                "         batchGroups.put(streamName, batchGroup);",
                "@@ -73,3 +74,3 @@ public class TridentTopologyBuilder {",
                "     public SpoutDeclarer setSpout(String id, String streamName, String txStateId, ITridentSpout spout, Integer parallelism, String batchGroup) {",
                "-        Map<String, String> batchGroups = new HashMap();",
                "+        Map<String, String> batchGroups = new HashMap<>();",
                "         batchGroups.put(streamName, batchGroup);",
                "@@ -86,3 +87,3 @@ public class TridentTopologyBuilder {",
                "         Component c = new Component(bolt, parallelism, committerBatches);",
                "-        _bolts.put(id, c);",
                "+        bolts.put(id, c);",
                "         return new BoltDeclarerImpl(c);",
                "@@ -107,3 +108,3 @@ public class TridentTopologyBuilder {",
                "         Map<GlobalStreamId, String> ret = new HashMap<>(batchIds);",
                "-        Set<String> allBatches = new HashSet(batchIds.values());",
                "+        Set<String> allBatches = new HashSet<>(batchIds.values());",
                "         for(String b: allBatches) {",
                "@@ -167,7 +168,5 @@ public class TridentTopologyBuilder {",
                "                 }",
                "-                for(Map<String, Object> m: c.componentConfs) {",
                "-                    scd.addConfigurations(m);",
                "-                }",
                "+                scd.addConfigurations(c.componentConf);",
                "-                Map<String, TridentBoltExecutor.CoordSpec> specs = new HashMap();",
                "+                Map<String, TridentBoltExecutor.CoordSpec> specs = new HashMap<>();",
                "                 specs.put(c.batchGroupId, new CoordSpec());",
                "@@ -184,8 +183,6 @@ public class TridentTopologyBuilder {",
                "                 bd.allGrouping(masterCoordinator(batchGroup), MasterBatchCoordinator.SUCCESS_STREAM_ID);",
                "-                if(c.spout instanceof ICommitterTridentSpout) {",
                "+                if (c.spout instanceof ICommitterTridentSpout) {",
                "                     bd.allGrouping(masterCoordinator(batchGroup), MasterBatchCoordinator.COMMIT_STREAM_ID);",
                "                 }",
                "-                for(Map<String, Object> m: c.componentConfs) {",
                "-                    bd.addConfigurations(m);",
                "-                }",
                "+                bd.addConfigurations(c.componentConf);",
                "             }",
                "@@ -196,6 +193,4 @@ public class TridentTopologyBuilder {",
                "             SpoutDeclarer d = builder.setSpout(id, new RichSpoutBatchTriggerer((IRichSpout) c.spout, c.streamName, c.batchGroupId), c.parallelism);",
                "-            ",
                "-            for(Map<String, Object> conf: c.componentConfs) {",
                "-                d.addConfigurations(conf);",
                "-            }",
                "+",
                "+            d.addConfigurations(c.componentConf);",
                "         }",
                "@@ -224,4 +219,4 @@ public class TridentTopologyBuilder {",
                "-        for(String id: _bolts.keySet()) {",
                "-            Component c = _bolts.get(id);",
                "+        for(String id: bolts.keySet()) {",
                "+            Component c = bolts.get(id);",
                "@@ -250,5 +245,3 @@ public class TridentTopologyBuilder {",
                "             }",
                "-            for(Map<String, Object> conf: c.componentConfs) {",
                "-                d.addConfigurations(conf);",
                "-            }",
                "+            d.addConfigurations(c.componentConf);",
                "@@ -283,3 +276,3 @@ public class TridentTopologyBuilder {",
                "         public final Integer parallelism;",
                "-        public final List<Map<String, Object>> componentConfs = new ArrayList<>();",
                "+        public final Map<String, Object> componentConf = new HashMap<>();",
                "         final String batchGroupId;",
                "@@ -319,3 +312,3 @@ public class TridentTopologyBuilder {",
                "         public final List<InputDeclaration> declarations = new ArrayList<>();",
                "-        public final List<Map<String, Object>> componentConfs = new ArrayList<>();",
                "+        public final Map<String, Object> componentConf = new HashMap<>();",
                "         public final Set<String> committerBatches;",
                "@@ -336,6 +329,6 @@ public class TridentTopologyBuilder {",
                "     Map<String, Set<String>> getBoltBatchToComponentSubscriptions(String id) {",
                "-        Map<String, Set<String>> ret = new HashMap();",
                "+        Map<String, Set<String>> ret = new HashMap<>();",
                "         for(GlobalStreamId s: getBoltSubscriptionStreams(id)) {",
                "             String b = batchIds.get(s);",
                "-            if(!ret.containsKey(b)) ret.put(b, new HashSet());",
                "+            if(!ret.containsKey(b)) ret.put(b, new HashSet<>());",
                "             ret.get(b).add(s.get_componentId());",
                "@@ -346,4 +339,4 @@ public class TridentTopologyBuilder {",
                "     List<GlobalStreamId> getBoltSubscriptionStreams(String id) {",
                "-        List<GlobalStreamId> ret = new ArrayList();",
                "-        Component c = _bolts.get(id);",
                "+        List<GlobalStreamId> ret = new ArrayList<>();",
                "+        Component c = bolts.get(id);",
                "         for(InputDeclaration d: c.declarations) {",
                "@@ -361,6 +354,6 @@ public class TridentTopologyBuilder {",
                "     private static class SpoutDeclarerImpl extends BaseConfigurationDeclarer<SpoutDeclarer> implements SpoutDeclarer {",
                "-        SpoutComponent _component;",
                "+        SpoutComponent component;",
                "         public SpoutDeclarerImpl(SpoutComponent component) {",
                "-            _component = component;",
                "+            this.component = component;",
                "         }",
                "@@ -369,3 +362,15 @@ public class TridentTopologyBuilder {",
                "         public SpoutDeclarer addConfigurations(Map<String, Object> conf) {",
                "-            _component.componentConfs.add(conf);",
                "+            if (conf != null) {",
                "+                component.componentConf.putAll(conf);",
                "+            }",
                "+            return this;",
                "+        }",
                "+",
                "+        @Override",
                "+        public SpoutDeclarer addResources(Map<String, Double> resources) {",
                "+            if (resources != null) {",
                "+                Map<String, Double> currentResources = (Map<String, Double>) component.componentConf.computeIfAbsent(",
                "+                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "+                currentResources.putAll(resources);",
                "+            }",
                "             return this;",
                "@@ -376,7 +381,6 @@ public class TridentTopologyBuilder {",
                "         public SpoutDeclarer addResource(String resourceName, Number resourceValue) {",
                "-            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) component.componentConf.computeIfAbsent(",
                "+                Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "             resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "-",
                "-            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "             return this;",
                "@@ -384,18 +388,5 @@ public class TridentTopologyBuilder {",
                "-        @Override",
                "-        public Map getRASConfiguration() {",
                "-            for(Map<String, Object> conf : _component.componentConfs) {",
                "-                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "-                    return conf;",
                "-                }",
                "-            }",
                "-            Map<String, Object> newConf = new HashMap<>();",
                "-            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "-            _component.componentConfs.add(newConf);",
                "-            return newConf;",
                "-        }",
                "-",
                "         @Override",
                "         public SpoutDeclarer addSharedMemory(SharedMemory request) {",
                "-            _component.sharedMemory.add(request);",
                "+            component.sharedMemory.add(request);",
                "             return this;",
                "@@ -405,6 +396,6 @@ public class TridentTopologyBuilder {",
                "     private static class BoltDeclarerImpl extends BaseConfigurationDeclarer<BoltDeclarer> implements BoltDeclarer {",
                "-        Component _component;",
                "+        Component component;",
                "         public BoltDeclarerImpl(Component component) {",
                "-            _component = component;",
                "+            this.component = component;",
                "         }",
                "@@ -779,3 +770,3 @@ public class TridentTopologyBuilder {",
                "         private void addDeclaration(InputDeclaration declaration) {",
                "-            _component.declarations.add(declaration);",
                "+            component.declarations.add(declaration);",
                "         }",
                "@@ -784,3 +775,5 @@ public class TridentTopologyBuilder {",
                "         public BoltDeclarer addConfigurations(Map<String, Object> conf) {",
                "-            _component.componentConfs.add(conf);",
                "+            if (conf != null) {",
                "+                component.componentConf.putAll(conf);",
                "+            }",
                "             return this;",
                "@@ -789,12 +782,9 @@ public class TridentTopologyBuilder {",
                "         @Override",
                "-        public Map getRASConfiguration() {",
                "-            for(Map<String, Object> conf : _component.componentConfs) {",
                "-                if (conf.containsKey(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP)) {",
                "-                    return conf;",
                "-                }",
                "+        public BoltDeclarer addResources(Map<String, Double> resources) {",
                "+            if (resources != null) {",
                "+                Map<String, Double> currentResources = (Map<String, Double>) component.componentConf.computeIfAbsent(",
                "+                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "+                currentResources.putAll(resources);",
                "             }",
                "-            Map<String, Object> newConf = new HashMap<>();",
                "-            newConf.put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap());",
                "-            _component.componentConfs.add(newConf);",
                "-            return newConf;",
                "+            return this;",
                "         }",
                "@@ -803,3 +793,3 @@ public class TridentTopologyBuilder {",
                "         public BoltDeclarer addSharedMemory(SharedMemory request) {",
                "-            _component.sharedMemory.add(request);",
                "+            component.sharedMemory.add(request);",
                "             return this;",
                "@@ -810,7 +800,6 @@ public class TridentTopologyBuilder {",
                "         public BoltDeclarer addResource(String resourceName, Number resourceValue) {",
                "-            Map<String, Double> resourcesMap = (Map<String, Double>) getRASConfiguration().get(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP);",
                "+            Map<String, Double> resourcesMap = (Map<String, Double>) component.componentConf.computeIfAbsent(",
                "+                Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());",
                "             resourcesMap.put(resourceName, resourceValue.doubleValue());",
                "-",
                "-            getRASConfiguration().put(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, resourcesMap);",
                "             return this;",
                "diff --git a/storm-server/pom.xml b/storm-server/pom.xml",
                "index 9a1b4c0c0..18fec7f6b 100644",
                "--- a/storm-server/pom.xml",
                "+++ b/storm-server/pom.xml",
                "@@ -132,3 +132,3 @@",
                "                 <configuration>",
                "-                    <maxAllowedViolations>2699</maxAllowedViolations>",
                "+                    <maxAllowedViolations>2655</maxAllowedViolations>",
                "                 </configuration>",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java b/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "index 69c875678..ba1c61882 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "@@ -502,4 +502,3 @@ public class TopologyDetails {",
                "                 Map<String, Double>) this.topologyConf.getOrDefault(",
                "-                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<>()",
                "-        );",
                "+                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<>());",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "index c13dce68e..9b0ee1532 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                "@@ -28,3 +28,2 @@ import java.util.Set;",
                " import org.apache.storm.Config;",
                "-import org.apache.storm.Constants;",
                " import org.apache.storm.generated.Bolt;",
                "@@ -33,3 +32,2 @@ import org.apache.storm.generated.SpoutSpec;",
                " import org.apache.storm.generated.StormTopology;",
                "-import org.apache.storm.utils.ConfigUtils;",
                " import org.apache.storm.utils.ObjectReader;",
                "@@ -161,3 +159,3 @@ public class ResourceUtils {",
                "                 (Map<String, Double>) topologyConf.getOrDefault(",
                "-                        Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, Collections.emptyMap());",
                "+                        Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, new HashMap<>());",
                "@@ -234,4 +232,4 @@ public class ResourceUtils {",
                "                     Map<String, Number> rawResourcesMap =",
                "-                            (Map<String, Number>) jsonObject.getOrDefault(",
                "-                                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, Collections.emptyMap());",
                "+                            (Map<String, Number>) jsonObject.computeIfAbsent(",
                "+                                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());"
            ],
            "changed_files": [
                "storm-client/pom.xml",
                "storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java",
                "storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java",
                "storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java",
                "storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java",
                "storm-server/pom.xml",
                "storm-server/src/main/java/org/apache/storm/scheduler/TopologyDetails.java",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2805": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2805",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "67dc3611a228d44ef13c511bb447f9cdfaeb82c4",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517499023,
            "hunks": 0,
            "message": "Merge branch 'STORM-2877-1.0.x' of https://github.com/srishtyagrawal/storm into STORM-2877-1.0.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2877": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2877",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "792ac9761dcc0ce2462eb4b2c02467e87e0103fc",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511896496,
            "hunks": 6,
            "message": "[STORM-2834] getOwnerResourceSummaries not working properly because scheduler is wrapped as BlacklistScheduler",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index ea3c9c4a3..b495cfaef 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -473,2 +473,8 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "+    private static IScheduler wrapAsBlacklistScheduler(Map<String, Object> conf, IScheduler scheduler) {",
                "+        BlacklistScheduler blacklistWrappedScheduler = new BlacklistScheduler(scheduler);",
                "+        blacklistWrappedScheduler.prepare(conf);",
                "+        return blacklistWrappedScheduler;",
                "+    }",
                "+",
                "     private static IScheduler makeScheduler(Map<String, Object> conf, INimbus inimbus) {",
                "@@ -485,5 +491,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "         }",
                "-        BlacklistScheduler blacklistWrappedScheduler = new BlacklistScheduler(scheduler);",
                "-        blacklistWrappedScheduler.prepare(conf);",
                "-        return blacklistWrappedScheduler;",
                "+        return scheduler;",
                "     }",
                "@@ -1043,2 +1047,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "     private final IScheduler scheduler;",
                "+    private final IScheduler underlyingScheduler;",
                "     private final ILeaderElector leaderElector;",
                "@@ -1114,3 +1119,4 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "         });",
                "-        this.scheduler = makeScheduler(conf, inimbus);",
                "+        this.underlyingScheduler = makeScheduler(conf, inimbus);",
                "+        this.scheduler = wrapAsBlacklistScheduler(conf, underlyingScheduler);",
                "         if (leaderElector == null) {",
                "@@ -4100,3 +4106,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "                 if (clusterSchedulerConfig.containsKey(theOwner)) {",
                "-                    if (scheduler instanceof ResourceAwareScheduler) {",
                "+                    if (underlyingScheduler instanceof ResourceAwareScheduler) {",
                "                         Map<String, Object> schedulerConfig = (Map) clusterSchedulerConfig.get(theOwner);",
                "@@ -4110,3 +4116,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "                         }",
                "-                    } else if (scheduler instanceof  MultitenantScheduler) {",
                "+                    } else if (underlyingScheduler instanceof  MultitenantScheduler) {",
                "                         ownerResourceSummary.set_isolated_node_guarantee((int) clusterSchedulerConfig.getOrDefault(theOwner, 0));"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2834": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2834",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "1811351fdf199d5ddfe8037a445715239b2311fd",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511106153,
            "hunks": 17,
            "message": "STORM-2826: Set key/value deserializer fields when using the convenience builder methods in KafkaSpoutConfig",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "index d89b674ec..6d4bd44cb 100644",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java",
                "@@ -282,3 +282,6 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "             SerializableDeserializer<V> valDes, Class<? extends Deserializer<V>> valDesClazz, Subscription subscription) {",
                "-            kafkaProps = new HashMap<>();",
                "+",
                "+            this(keyDes, keyDesClazz, valDes, valDesClazz, subscription,",
                "+                    new DefaultRecordTranslator<K, V>(), new HashMap<String, Object>());",
                "+",
                "             if (bootstrapServers == null || bootstrapServers.isEmpty()) {",
                "@@ -287,27 +290,17 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "             kafkaProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);",
                "-            this.keyDes = keyDes;",
                "-            this.keyDesClazz = keyDesClazz;",
                "-            this.valueDes = valDes;",
                "-            this.valueDesClazz = valDesClazz;",
                "-            this.subscription = subscription;",
                "-            this.translator = new DefaultRecordTranslator<>();",
                "-            ",
                "-            if (keyDesClazz != null) {",
                "-                this.kafkaProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDesClazz);",
                "-            }",
                "-            if (keyDes != null) {",
                "-                this.kafkaProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDes.getClass());",
                "-            }",
                "-            if (valueDesClazz != null) {",
                "-                this.kafkaProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDesClazz);",
                "-            }",
                "-            if (valueDes != null) {",
                "-                this.kafkaProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDes.getClass());",
                "-            }",
                "+",
                "+            setNonNullSerDeKafkaProp(keyDes, keyDesClazz, valueDes, valueDesClazz);",
                "         }",
                "-        private Builder(Builder<?, ?> builder, SerializableDeserializer<K> keyDes, Class<? extends Deserializer<K>> keyDesClazz,",
                "-            SerializableDeserializer<V> valueDes, Class<? extends Deserializer<V>> valueDesClazz) {",
                "-            this.kafkaProps = new HashMap<>(builder.kafkaProps);",
                "-            this.subscription = builder.subscription;",
                "+        /**",
                "+         * This constructor will always be called by one of the methods {@code setKey} or {@code setVal}, which implies",
                "+         * that only one of its SerDe parameters will be non null, for which the corresponding Kafka property will be set",
                "+         */",
                "+        @SuppressWarnings(\"unchecked\")",
                "+        private Builder(final Builder<?, ?> builder, SerializableDeserializer<K> keyDes, Class<? extends Deserializer<K>> keyDesClazz,",
                "+                        SerializableDeserializer<V> valueDes, Class<? extends Deserializer<V>> valueDesClazz) {",
                "+",
                "+            this(keyDes, keyDesClazz, valueDes, valueDesClazz, builder.subscription,",
                "+                    (RecordTranslator<K, V>) builder.translator, new HashMap<>(builder.kafkaProps));",
                "+",
                "             this.pollTimeoutMs = builder.pollTimeoutMs;",
                "@@ -316,26 +309,33 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "             this.maxUncommittedOffsets = builder.maxUncommittedOffsets;",
                "-            //this could result in a lot of class case exceptions at runtime,",
                "-            // but because some translators will work no matter what the generics",
                "-            // are I thought it best not to force someone to reset the translator",
                "-            // when they change the key/value types.",
                "-            this.translator = (RecordTranslator<K, V>) builder.translator;",
                "             this.retryService = builder.retryService;",
                "-            ",
                "+",
                "+            setNonNullSerDeKafkaProp(keyDes, keyDesClazz, valueDes, valueDesClazz);",
                "+        }",
                "+",
                "+        private Builder(SerializableDeserializer<K> keyDes, Class<? extends Deserializer<K>> keyDesClazz,",
                "+               SerializableDeserializer<V> valueDes, Class<? extends Deserializer<V>> valueDesClazz,",
                "+               Subscription subscription, RecordTranslator<K, V> translator, Map<String, Object> kafkaProps) {",
                "+            this.keyDes = keyDes;",
                "+            this.keyDesClazz = keyDesClazz;",
                "+            this.valueDes = valueDes;",
                "+            this.valueDesClazz = valueDesClazz;",
                "+            this.subscription = subscription;",
                "+            this.translator = translator;",
                "+            this.kafkaProps = kafkaProps;",
                "+        }",
                "+",
                "+        private void setNonNullSerDeKafkaProp(SerializableDeserializer<K> keyDes, Class<? extends Deserializer<K>> keyDesClazz,",
                "+                SerializableDeserializer<V> valueDes, Class<? extends Deserializer<V>> valueDesClazz) {",
                "             if (keyDesClazz != null) {",
                "-                this.kafkaProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDesClazz);",
                "+                kafkaProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDesClazz);",
                "             }",
                "             if (keyDes != null) {",
                "-                this.kafkaProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDes.getClass());",
                "+                kafkaProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, keyDes.getClass());",
                "             }",
                "             if (valueDesClazz != null) {",
                "-                this.kafkaProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDesClazz);",
                "+                kafkaProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDesClazz);",
                "             }",
                "             if (valueDes != null) {",
                "-                this.kafkaProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDes.getClass());",
                "+                kafkaProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, valueDes.getClass());",
                "             }",
                "-            ",
                "-            this.keyDes = keyDes;",
                "-            this.keyDesClazz = keyDesClazz;",
                "-            this.valueDes = valueDes;",
                "-            this.valueDesClazz = valueDesClazz;",
                "         }",
                "@@ -350,3 +350,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         public <NK> Builder<NK, V> setKey(SerializableDeserializer<NK> keyDeserializer) {",
                "-            return new Builder<>(this, keyDeserializer, null, valueDes, valueDesClazz);",
                "+            return new Builder<>(this, keyDeserializer, null, null, null);",
                "         }",
                "@@ -361,3 +361,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         public <NK> Builder<NK, V> setKey(Class<? extends Deserializer<NK>> clazz) {",
                "-            return new Builder<>(this, null, clazz, valueDes, valueDesClazz);",
                "+            return new Builder<>(this, null, clazz, null, null);",
                "         }",
                "@@ -372,3 +372,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         public <NV> Builder<K, NV> setValue(SerializableDeserializer<NV> valueDeserializer) {",
                "-            return new Builder<>(this, keyDes, keyDesClazz, valueDeserializer, null);",
                "+            return new Builder<>(this, null, null, valueDeserializer, null);",
                "         }",
                "@@ -383,3 +383,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "         public <NV> Builder<K, NV> setValue(Class<? extends Deserializer<NV>> clazz) {",
                "-            return new Builder<>(this, keyDes, keyDesClazz, null, clazz);",
                "+            return new Builder<>(this, null, null, null, clazz);",
                "         }",
                "@@ -682,3 +682,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "     public static Builder<String, String> builder(String bootstrapServers, String... topics) {",
                "-        return setStringDeserializers(new Builder<String, String>(bootstrapServers, topics));",
                "+        return setStringDeserializers(new Builder<String, String>(bootstrapServers, StringDeserializer.class, StringDeserializer.class, topics));",
                "     }",
                "@@ -693,3 +693,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "     public static Builder<String, String> builder(String bootstrapServers, Collection<String> topics) {",
                "-        return setStringDeserializers(new Builder<String, String>(bootstrapServers, topics));",
                "+        return setStringDeserializers(new Builder<String, String>(bootstrapServers, StringDeserializer.class, StringDeserializer.class, topics));",
                "     }",
                "@@ -704,3 +704,3 @@ public class KafkaSpoutConfig<K, V> implements Serializable {",
                "     public static Builder<String, String> builder(String bootstrapServers, Pattern topics) {",
                "-        return setStringDeserializers(new Builder<String, String>(bootstrapServers, topics));",
                "+        return setStringDeserializers(new Builder<String, String>(bootstrapServers, StringDeserializer.class, StringDeserializer.class, topics));",
                "     }"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpoutConfig.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2826": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2826",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "70d8ba9d5832c1893351dcfd0e285d287f221794",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507511717,
            "hunks": 0,
            "message": "Merge branch 'STORM-2716' of https://github.com/srdo/storm into STORM-2716-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2716": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2716",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "9c8930036c4829bcf47947cb03a276d32c1a317a",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517545758,
            "hunks": 0,
            "message": "Merge branch 'STORM-2918-1.0.x-merge' into 1.0.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {
                "STORM-2918": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: STORM-2918",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "0b3930dc4506b0613e30d9d736becfac77b9a41b",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507558529,
            "hunks": 1,
            "message": "Remove unwanted comment",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index d2529c4af..5ab0ea161 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -1847,3 +1847,2 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "                 LOG.info(\"Fragmentation after scheduling is: {} MB, {} PCore CPUs\", fragmentedMemory(), fragmentedCpu());",
                "-                //map.forEach((x, y) -> System.out.println(x + \": \" + y));",
                "                 nodeIdToResources.get().forEach((id, node) ->"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "e9a9f507eaeb1022615066a98b7822e829f58e0a",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511772604,
            "hunks": 12,
            "message": "WIP: replace Meter to Counter",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index c6f206ef5..94bd7af90 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -38,3 +38,3 @@",
                "   (:import [org.apache.storm.metrics2 StormMetricRegistry])",
                "-  (:import [com.codahale.metrics Meter])",
                "+  (:import [com.codahale.metrics Meter Counter])",
                "   (:import [org.apache.storm.grouping LoadAwareCustomStreamGrouping LoadAwareShuffleGrouping LoadMapping ShuffleGrouping])",
                "@@ -282,4 +282,4 @@",
                "      :sampler (mk-stats-sampler storm-conf)",
                "-     :failed-meter (StormMetricRegistry/meter \"failed\" worker-context component-id)",
                "-     :acked-meter (StormMetricRegistry/meter \"acked\" worker-context component-id)",
                "+     :failed-meter (StormMetricRegistry/counter \"failed\" worker-context component-id)",
                "+     :acked-meter (StormMetricRegistry/counter \"acked\" worker-context component-id)",
                "      :spout-throttling-metrics (if (= executor-type :spout)",
                "@@ -444,3 +444,3 @@",
                "       (log-message \"SPOUT Failing \" id \": \" tuple-info \" REASON: \" reason \" MSG-ID: \" msg-id))",
                "-    (.mark failed-meter)",
                "+    (.inc ^Counter failed-meter)",
                "     (.fail spout msg-id)",
                "@@ -455,3 +455,3 @@",
                "     (when debug? (log-message \"SPOUT Acking message \" id \" \" msg-id))",
                "-    (.mark acked-meter)",
                "+    (.inc ^Counter acked-meter)",
                "     (.ack spout msg-id)",
                "@@ -825,3 +825,3 @@",
                "                              (log-message \"BOLT ack TASK: \" task-id \" TIME: \" delta \" TUPLE: \" tuple))",
                "-                           (.mark  ^Meter (:acked-meter (:executor-data task-data)))",
                "+                           (.inc  ^Counter (:acked-meter (:executor-data task-data)))",
                "                            (task/apply-hooks user-context .boltAck (BoltAckInfo. tuple task-id delta))",
                "@@ -841,3 +841,3 @@",
                "                              (log-message \"BOLT fail TASK: \" task-id \" TIME: \" delta \" TUPLE: \" tuple))",
                "-                           (.mark  ^Meter (:failed-meter (:executor-data task-data)))",
                "+                           (.inc  ^Counter (:failed-meter (:executor-data task-data)))",
                "                            (task/apply-hooks user-context .boltFail (BoltFailInfo. tuple task-id delta))",
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/task.clj b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "index 2e4df75df..7162f7f58 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "@@ -30,3 +30,3 @@",
                "   (:import [java.util Collection List ArrayList])",
                "-  (:import [com.codahale.metrics Meter])",
                "+  (:import [com.codahale.metrics Meter Counter])",
                "   (:require [org.apache.storm",
                "@@ -133,6 +133,6 @@",
                "         debug? (= true (storm-conf TOPOLOGY-DEBUG))",
                "-        ^Meter emitted-meter (StormMetricRegistry/meter \"emitted\" worker-context component-id)]",
                "+        ^Counter emitted-meter (StormMetricRegistry/counter \"emitted\" worker-context component-id)]",
                "     (fn ([^Integer out-task-id ^String stream ^List values]",
                "-          (.mark emitted-meter)",
                "+          (.inc ^Counter emitted-meter)",
                "           (when debug?",
                "@@ -153,3 +153,3 @@",
                "         ([^String stream ^List values]",
                "-           (.mark emitted-meter)",
                "+           (.inc ^Counter emitted-meter)",
                "            (when debug?",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "index 60d419196..912d88837 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "@@ -19,2 +19,3 @@ package org.apache.storm.metrics2;",
                "+import com.codahale.metrics.Counter;",
                " import com.codahale.metrics.Meter;",
                "@@ -74,2 +75,7 @@ public class StormMetricRegistry {",
                "+    public static Counter counter(String name, WorkerTopologyContext context, String componentId){",
                "+        String metricName = metricName(name, context.getStormId(), componentId, context.getThisWorkerPort());",
                "+        return REGISTRY.counter(metricName);",
                "+    }",
                "+",
                "     public static void start(Map<String, Object> stormConfig, DaemonType type){"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "85dbacdd058ee8b3246ff6982a4079713923b66e",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511837505,
            "hunks": 12,
            "message": "WIP apply sampling to new metrics",
            "diff": [
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/executor.clj b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "index 94bd7af90..720bfa76d 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "@@ -444,3 +444,2 @@",
                "       (log-message \"SPOUT Failing \" id \": \" tuple-info \" REASON: \" reason \" MSG-ID: \" msg-id))",
                "-    (.inc ^Counter failed-meter)",
                "     (.fail spout msg-id)",
                "@@ -448,2 +447,3 @@",
                "     (when time-delta",
                "+      (.inc ^Counter failed-meter)",
                "       (stats/spout-failed-tuple! (:stats executor-data) (:stream tuple-info) time-delta))))",
                "@@ -455,3 +455,2 @@",
                "     (when debug? (log-message \"SPOUT Acking message \" id \" \" msg-id))",
                "-    (.inc ^Counter acked-meter)",
                "     (.ack spout msg-id)",
                "@@ -459,2 +458,3 @@",
                "     (when time-delta",
                "+      (.inc ^Counter acked-meter)",
                "       (stats/spout-acked-tuple! (:stats executor-data) (:stream tuple-info) time-delta))))",
                "@@ -825,5 +825,5 @@",
                "                              (log-message \"BOLT ack TASK: \" task-id \" TIME: \" delta \" TUPLE: \" tuple))",
                "-                           (.inc  ^Counter (:acked-meter (:executor-data task-data)))",
                "                            (task/apply-hooks user-context .boltAck (BoltAckInfo. tuple task-id delta))",
                "                            (when (<= 0 delta)",
                "+                             (.inc ^Counter (:acked-meter (:executor-data task-data)))",
                "                              (stats/bolt-acked-tuple! executor-stats",
                "@@ -841,5 +841,5 @@",
                "                              (log-message \"BOLT fail TASK: \" task-id \" TIME: \" delta \" TUPLE: \" tuple))",
                "-                           (.inc  ^Counter (:failed-meter (:executor-data task-data)))",
                "                            (task/apply-hooks user-context .boltFail (BoltFailInfo. tuple task-id delta))",
                "                            (when (<= 0 delta)",
                "+                             (.inc  ^Counter (:failed-meter (:executor-data task-data)))",
                "                              (stats/bolt-failed-tuple! executor-stats",
                "diff --git a/storm-core/src/clj/org/apache/storm/daemon/task.clj b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "index 7162f7f58..c43d20d97 100644",
                "--- a/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "+++ b/storm-core/src/clj/org/apache/storm/daemon/task.clj",
                "@@ -136,3 +136,2 @@",
                "     (fn ([^Integer out-task-id ^String stream ^List values]",
                "-          (.inc ^Counter emitted-meter)",
                "           (when debug?",
                "@@ -147,2 +146,3 @@",
                "             (when (emit-sampler)",
                "+              (.inc ^Counter emitted-meter)",
                "               (stats/emitted-tuple! executor-stats stream)",
                "@@ -153,3 +153,2 @@",
                "         ([^String stream ^List values]",
                "-           (.inc ^Counter emitted-meter)",
                "            (when debug?",
                "@@ -168,2 +167,3 @@",
                "              (when (emit-sampler)",
                "+               (.inc ^Counter emitted-meter)",
                "                (stats/emitted-tuple! executor-stats stream)"
            ],
            "changed_files": [
                "storm-core/src/clj/org/apache/storm/daemon/executor.clj",
                "storm-core/src/clj/org/apache/storm/daemon/task.clj"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "61a944fd15e564482a5c55655daad9736948d961",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511207985,
            "hunks": 0,
            "message": "Merge branch 'use-bool-instead-of-string' of https://github.com/srdo/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "a9647d8bffd2df8c7e3dcc31d8ce720babf4d0f8",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512764093,
            "hunks": 8,
            "message": "address review comments",
            "diff": [
                "diff --git a/conf/defaults.yaml b/conf/defaults.yaml",
                "index e51b50c0b..435a7e8a5 100644",
                "--- a/conf/defaults.yaml",
                "+++ b/conf/defaults.yaml",
                "@@ -316,3 +316,2 @@ storm.cluster.metrics.consumer.publish.interval.secs: 60",
                " #    report.period.units: \"SECONDS\"",
                "-#",
                " #    filter:",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/DisruptorMetrics.java b/storm-core/src/jvm/org/apache/storm/metrics2/DisruptorMetrics.java",
                "index 22eb6c515..42f698c0e 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/DisruptorMetrics.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/DisruptorMetrics.java",
                "@@ -22,10 +22,10 @@ import org.apache.storm.utils.DisruptorQueue;",
                " public class DisruptorMetrics {",
                "-    private SimpleGauge<Long> capacity;",
                "-    private SimpleGauge<Long> population;",
                "-    private SimpleGauge<Long> writePosition;",
                "-    private SimpleGauge<Long> readPosition;",
                "-    private SimpleGauge<Double> arrivalRate;",
                "-    private SimpleGauge<Double> sojournTime;",
                "-    private SimpleGauge<Long> overflow;",
                "-    private SimpleGauge<Float> pctFull;",
                "+    private final SimpleGauge<Long> capacity;",
                "+    private final SimpleGauge<Long> population;",
                "+    private final SimpleGauge<Long> writePosition;",
                "+    private final SimpleGauge<Long> readPosition;",
                "+    private final SimpleGauge<Double> arrivalRate;",
                "+    private final SimpleGauge<Double> sojournTime;",
                "+    private final SimpleGauge<Long> overflow;",
                "+    private final SimpleGauge<Float> pctFull;",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "index 912d88837..200ddcf4b 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "@@ -85,3 +85,3 @@ public class StormMetricRegistry {",
                "         } catch (UnknownHostException e) {",
                "-             LOG.warn(\"Unable to determine hostname while starting the metrics system. Hostname ill be reported\" +",
                "+             LOG.warn(\"Unable to determine hostname while starting the metrics system. Hostname will be reported\" +",
                "                      \" as 'localhost'.\");",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/filters/StormMetricsFilter.java b/storm-core/src/jvm/org/apache/storm/metrics2/filters/StormMetricsFilter.java",
                "index 57f72552c..8e6b95f51 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/filters/StormMetricsFilter.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/filters/StormMetricsFilter.java",
                "@@ -27,3 +27,3 @@ public interface StormMetricsFilter extends MetricFilter {",
                "      * Called after the filter is instantiated.",
                "-     * @param config an arbitrary configuration map pulled from the yaml configuration.",
                "+     * @param config A map of the properties from the 'filter' section of the reporter configuration.",
                "      */",
                "diff --git a/storm-core/src/jvm/org/apache/storm/metrics2/reporters/GangliaStormReporter.java b/storm-core/src/jvm/org/apache/storm/metrics2/reporters/GangliaStormReporter.java",
                "index 09af2e1f3..497cb04ec 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/metrics2/reporters/GangliaStormReporter.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/metrics2/reporters/GangliaStormReporter.java",
                "@@ -86,5 +86,2 @@ public class GangliaStormReporter extends ScheduledStormReporter {",
                "-        // Not exposed:",
                "-        // * withClock(Clock)",
                "-",
                "         String group = getMetricsTargetUDPGroup(reporterConf);",
                "diff --git a/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java b/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "index 8bcea8471..89aecee49 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java",
                "@@ -520,5 +520,6 @@ public class ConfigValidation {",
                "                                     ((String) string).equals(\"worker\"))) {",
                "-                        return;",
                "+                        continue;",
                "                     }",
                "-                    throw new IllegalArgumentException(\"Field daemons must contain at least one of \\\"nimbus\\\", \\\"supervisor\\\", or \\\"worker\\\"\");",
                "+                    throw new IllegalArgumentException(\"Field 'daemons' must contain at least one of the following:\" +",
                "+                            \" \\\"nimbus\\\", \\\"supervisor\\\", or \\\"worker\\\"\");",
                "                 }",
                "@@ -528,3 +529,3 @@ public class ConfigValidation {",
                "                 Map filterMap = (Map)((Map)o).get(\"filter\");",
                "-                SimpleTypeValidator.validateField(\"filter\", String.class, filterMap.get(\"class\"));",
                "+                SimpleTypeValidator.validateField(\"class\", String.class, filterMap.get(\"class\"));",
                "             }"
            ],
            "changed_files": [
                "conf/defaults.yaml",
                "storm-core/src/jvm/org/apache/storm/metrics2/DisruptorMetrics.java",
                "storm-core/src/jvm/org/apache/storm/metrics2/StormMetricRegistry.java",
                "storm-core/src/jvm/org/apache/storm/metrics2/filters/StormMetricsFilter.java",
                "storm-core/src/jvm/org/apache/storm/metrics2/reporters/GangliaStormReporter.java",
                "storm-core/src/jvm/org/apache/storm/validation/ConfigValidation.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "3b5fa38588d85a65592d93a1c738bce0855022de",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509740696,
            "hunks": 9,
            "message": "Addressed some review comments",
            "diff": [
                "diff --git a/storm-client/pom.xml b/storm-client/pom.xml",
                "index c4accca33..d0cbe0e05 100644",
                "--- a/storm-client/pom.xml",
                "+++ b/storm-client/pom.xml",
                "@@ -195,6 +195,2 @@",
                "         </dependency>",
                "-        <dependency>",
                "-            <groupId>org.apache.curator</groupId>",
                "-            <artifactId>curator-client</artifactId>",
                "-        </dependency>",
                "     </dependencies>",
                "diff --git a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "index fa7763e24..57dcd335e 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java",
                "@@ -74,3 +74,5 @@ public class ResourceAwareScheduler implements IScheduler {",
                "         List<TopologyDetails> orderedTopologies = new ArrayList<>(schedulingPriorityStrategy.getOrderedTopologies(cluster, userMap));",
                "-        LOG.info(\"Ordered list of topologies is: {}\", orderedTopologies.stream().map((t) -> t.getId()).collect(Collectors.toList()));",
                "+        if (LOG.isDebugEnabled()) {",
                "+            LOG.debug(\"Ordered list of topologies is: {}\", orderedTopologies.stream().map((t) -> t.getId()).collect(Collectors.toList()));",
                "+        }",
                "         for (TopologyDetails td : orderedTopologies) {",
                "@@ -132,3 +134,6 @@ public class ResourceAwareScheduler implements IScheduler {",
                "                 LOG.debug(\"scheduling result: {}\", result);",
                "-                if (result != null) {",
                "+                if (result == null) {",
                "+                    markFailedTopology(topologySubmitter, cluster, td, \"Internal scheduler error\");",
                "+                    return;",
                "+                } else {",
                "                     if (result.isSuccess()) {",
                "@@ -139,49 +144,52 @@ public class ResourceAwareScheduler implements IScheduler {",
                "                     } else if (result.getStatus() == SchedulingStatus.FAIL_NOT_ENOUGH_RESOURCES) {",
                "-                        LOG.info(\"Not enough resources to schedule {}\", td.getName());",
                "+                        LOG.debug(\"Not enough resources to schedule {}\", td.getName());",
                "                         List<TopologyDetails> reversedList = ImmutableList.copyOf(orderedTopologies).reverse();",
                "-                        try {",
                "-                            boolean evictedSomething = false;",
                "-                            LOG.debug(\"attempting to make space for topo {} from user {}\", td.getName(), td.getTopologySubmitter());",
                "-                            int tdIndex = reversedList.indexOf(td);",
                "-                            double cpuNeeded = td.getTotalRequestedCpu();",
                "-                            double memoryNeeded = td.getTotalRequestedMemOffHeap() + td.getTotalRequestedMemOnHeap();",
                "-                            SchedulerAssignment assignment = cluster.getAssignmentById(td.getId());",
                "-                            if (assignment != null) {",
                "-                                cpuNeeded -= getCpuUsed(assignment);",
                "-                                memoryNeeded -= getMemoryUsed(assignment);",
                "-                            }",
                "-                            cluster.getTopologyResourcesMap();",
                "-                            for (int index = 0; index < tdIndex; index++) {",
                "-                                TopologyDetails topologyEvict = reversedList.get(index);",
                "-                                SchedulerAssignment evictAssignemnt = workingState.getAssignmentById(topologyEvict.getId());",
                "-                                if (evictAssignemnt != null && !evictAssignemnt.getSlots().isEmpty()) {",
                "-                                    Collection<WorkerSlot> workersToEvict = workingState.getUsedSlotsByTopologyId(topologyEvict.getId());",
                "+                        boolean evictedSomething = false;",
                "+                        LOG.debug(\"attempting to make space for topo {} from user {}\", td.getName(), td.getTopologySubmitter());",
                "+                        int tdIndex = reversedList.indexOf(td);",
                "+                        double cpuNeeded = td.getTotalRequestedCpu();",
                "+                        double memoryNeeded = td.getTotalRequestedMemOffHeap() + td.getTotalRequestedMemOnHeap();",
                "+                        SchedulerAssignment assignment = cluster.getAssignmentById(td.getId());",
                "+                        if (assignment != null) {",
                "+                            cpuNeeded -= getCpuUsed(assignment);",
                "+                            memoryNeeded -= getMemoryUsed(assignment);",
                "+                        }",
                "+                        for (int index = 0; index < tdIndex; index++) {",
                "+                            TopologyDetails topologyEvict = reversedList.get(index);",
                "+                            SchedulerAssignment evictAssignemnt = workingState.getAssignmentById(topologyEvict.getId());",
                "+                            if (evictAssignemnt != null && !evictAssignemnt.getSlots().isEmpty()) {",
                "+                                Collection<WorkerSlot> workersToEvict = workingState.getUsedSlotsByTopologyId(topologyEvict.getId());",
                "-                                    LOG.debug(\"Evicting Topology {} with workers: {} from user {}\", topologyEvict.getName(), workersToEvict,",
                "-                                        topologyEvict.getTopologySubmitter());",
                "-                                    cpuNeeded -= getCpuUsed(evictAssignemnt);",
                "-                                    memoryNeeded -= getMemoryUsed(evictAssignemnt);",
                "-                                    evictedSomething = true;",
                "-                                    nodes.freeSlots(workersToEvict);",
                "-                                    if (cpuNeeded <= 0 && memoryNeeded <= 0) {",
                "-                                        //We evicted enough topologies to have a hope of scheduling, so try it now, and don't evict more",
                "-                                        // than is needed",
                "-                                        break;",
                "-                                    }",
                "+                                LOG.debug(\"Evicting Topology {} with workers: {} from user {}\", topologyEvict.getName(), workersToEvict,",
                "+                                    topologyEvict.getTopologySubmitter());",
                "+                                cpuNeeded -= getCpuUsed(evictAssignemnt);",
                "+                                memoryNeeded -= getMemoryUsed(evictAssignemnt);",
                "+                                evictedSomething = true;",
                "+                                nodes.freeSlots(workersToEvict);",
                "+                                if (cpuNeeded <= 0 && memoryNeeded <= 0) {",
                "+                                    //We evicted enough topologies to have a hope of scheduling, so try it now, and don't evict more",
                "+                                    // than is needed",
                "+                                    break;",
                "                                 }",
                "                             }",
                "+                        }",
                "-                            if (!evictedSomething) {",
                "-                                markFailedTopology(topologySubmitter, cluster, td,",
                "-                                    \"Not enough resources to schedule - \" + result.getErrorMessage());",
                "-                                return;",
                "+                        if (!evictedSomething) {",
                "+                            StringBuilder message = new StringBuilder();",
                "+                            message.append(\"Not enough resources to schedule \");",
                "+                            if (memoryNeeded > 0 || cpuNeeded > 0) {",
                "+                                if (memoryNeeded > 0) {",
                "+                                    message.append(memoryNeeded).append(\" MB \");",
                "+                                }",
                "+                                if (cpuNeeded > 0) {",
                "+                                    message.append(cpuNeeded).append(\"% CPU \");",
                "+                                }",
                "+                                message.append(\"needed even after evicting lower priority topologies. \");",
                "                             }",
                "-                        } catch (Exception ex) {",
                "-                            LOG.error(\"Exception thrown when running eviction to schedule topology {}.\"",
                "-                                    + \" No evictions will be done! Error: {}\",",
                "-                                td.getName(), ex.getClass().getName(), ex);",
                "+                            message.append(result.getErrorMessage());",
                "+                            markFailedTopology(topologySubmitter, cluster, td, message.toString());",
                "+                            return;",
                "                         }",
                "                         //Only place we fall though to do the loop over again...",
                "-                        continue;",
                "-                    } else {",
                "+                    } else { //Any other failure result",
                "                         //The assumption is that the strategy set the status...",
                "@@ -190,5 +198,2 @@ public class ResourceAwareScheduler implements IScheduler {",
                "                     }",
                "-                } else {",
                "-                    markFailedTopology(topologySubmitter, cluster, td, \"Internal scheduler error\");",
                "-                    return;",
                "                 }"
            ],
            "changed_files": [
                "storm-client/pom.xml",
                "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceAwareScheduler.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "65be2709c5fbd686211d5ec25e46835c56c9733d",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513963820,
            "hunks": 0,
            "message": "Merge branch 'master' of https://github.com/OuYangLiang/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "1a857637e6ab04241501ace192ffbf06487c750a",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507574136,
            "hunks": 0,
            "message": "Merge branch 'partitionAggregate-docs' of https://github.com/F30/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "e7d9881c1876494b179e06c7c3ee64c606590343",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509055175,
            "hunks": 20,
            "message": "Add nimbus admins groups",
            "diff": [
                "diff --git a/storm-client/src/jvm/org/apache/storm/Config.java b/storm-client/src/jvm/org/apache/storm/Config.java",
                "index 6be0c2167..8dd47cb7d 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/Config.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/Config.java",
                "@@ -1047,2 +1047,10 @@ public class Config extends HashMap<String, Object> {",
                "+    /**",
                "+     * Initialization parameters for the group mapping service plugin.",
                "+     * Provides a way for a @link{STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN}",
                "+     * implementation to access optional settings.",
                "+     */",
                "+    @isType(type=Map.class)",
                "+    public static final String STORM_GROUP_MAPPING_SERVICE_PARAMS = \"storm.group.mapping.service.params\";",
                "+",
                "     /**",
                "@@ -1418,2 +1426,8 @@ public class Config extends HashMap<String, Object> {",
                "+    /**",
                "+     * A list of groups that are cluster admins and can run any command.",
                "+     */",
                "+    @isStringList",
                "+    public static final String NIMBUS_ADMINS_GROUPS = \"nimbus.admins.groups\";",
                "+",
                "     /**",
                "diff --git a/storm-client/src/jvm/org/apache/storm/blobstore/BlobStoreAclHandler.java b/storm-client/src/jvm/org/apache/storm/blobstore/BlobStoreAclHandler.java",
                "index 296d64f32..f65541ea1 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/blobstore/BlobStoreAclHandler.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/blobstore/BlobStoreAclHandler.java",
                "@@ -25,2 +25,3 @@ import org.apache.storm.generated.SettableBlobMeta;",
                " import org.apache.storm.security.auth.AuthUtils;",
                "+import org.apache.storm.security.auth.IGroupMappingServiceProvider;",
                " import org.apache.storm.security.auth.IPrincipalToLocal;",
                "@@ -32,2 +33,3 @@ import org.slf4j.LoggerFactory;",
                " import javax.security.auth.Subject;",
                "+import java.io.IOException;",
                " import java.security.Principal;",
                "@@ -47,2 +49,3 @@ public class BlobStoreAclHandler {",
                "     private final IPrincipalToLocal _ptol;",
                "+    private final IGroupMappingServiceProvider _groupMappingProvider;",
                "@@ -56,2 +59,3 @@ public class BlobStoreAclHandler {",
                "     private Set<String> _admins;",
                "+    private Set<String> _adminsGroups;",
                "     private boolean doAclValidation;",
                "@@ -60,4 +64,10 @@ public class BlobStoreAclHandler {",
                "         _ptol = AuthUtils.GetPrincipalToLocalPlugin(conf);",
                "+        if (conf.get(Config.STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN) != null) {",
                "+            _groupMappingProvider = AuthUtils.GetGroupMappingServiceProviderPlugin(conf);",
                "+        } else {",
                "+            _groupMappingProvider = null;",
                "+        }",
                "         _supervisors = new HashSet<String>();",
                "         _admins = new HashSet<String>();",
                "+        _adminsGroups = new HashSet<>();",
                "         if (conf.containsKey(Config.NIMBUS_SUPERVISOR_USERS)) {",
                "@@ -68,2 +78,5 @@ public class BlobStoreAclHandler {",
                "         }",
                "+        if (conf.containsKey(Config.NIMBUS_ADMINS_GROUPS)) {",
                "+            _adminsGroups.addAll((List<String>)conf.get(Config.NIMBUS_ADMINS_GROUPS));",
                "+        }",
                "         if (conf.containsKey(Config.STORM_BLOBSTORE_ACL_VALIDATION_ENABLED)) {",
                "@@ -189,2 +202,17 @@ public class BlobStoreAclHandler {",
                "             }",
                "+            if (_adminsGroups.size() > 0 && _groupMappingProvider != null) {",
                "+                Set<String> userGroups = null;",
                "+                try {",
                "+                    userGroups = _groupMappingProvider.getGroups(u);",
                "+                } catch (IOException e) {",
                "+                    LOG.warn(\"Error while trying to fetch user groups\", e);",
                "+                }",
                "+                if (userGroups != null) {",
                "+                    for (String tgroup : userGroups) {",
                "+                        if (_adminsGroups.contains(tgroup)) {",
                "+                            return true;",
                "+                        }",
                "+                    }",
                "+                }",
                "+            }",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/security/auth/FixedGroupsMapping.java b/storm-client/src/jvm/org/apache/storm/security/auth/FixedGroupsMapping.java",
                "new file mode 100644",
                "index 000000000..4956b396b",
                "--- /dev/null",
                "+++ b/storm-client/src/jvm/org/apache/storm/security/auth/FixedGroupsMapping.java",
                "@@ -0,0 +1,68 @@",
                "+/**",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ * <p>",
                "+ * http://www.apache.org/licenses/LICENSE-2.0",
                "+ * <p>",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.security.auth;",
                "+",
                "+import java.io.IOException;",
                "+import java.util.Set;",
                "+import java.util.HashSet;",
                "+import java.util.HashMap;",
                "+import java.util.Map;",
                "+",
                "+import org.apache.storm.Config;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+",
                "+public class FixedGroupsMapping implements IGroupMappingServiceProvider {",
                "+",
                "+  public static Logger LOG = LoggerFactory.getLogger(FixedGroupsMapping.class);",
                "+  public static final String STORM_FIXED_GROUP_MAPPING = \"storm.fixed.group.mapping\";",
                "+  public Map<String, Set<String>> cachedGroups = new HashMap<String, Set<String>>();",
                "+",
                "+  /**",
                "+   * Invoked once immediately after construction",
                "+   * @param storm_conf Storm configuration",
                "+   */",
                "+  @Override",
                "+  public void prepare(Map storm_conf) {",
                "+    Map<?, ?> params = (Map<?, ?>) storm_conf.get(Config.STORM_GROUP_MAPPING_SERVICE_PARAMS);",
                "+    Map<String, Set<String>> mapping = (Map<String, Set<String>>) params.get(STORM_FIXED_GROUP_MAPPING);",
                "+    if (mapping != null) {",
                "+      cachedGroups.putAll(mapping);",
                "+    } else {",
                "+      LOG.warn(\"There is no initial group mapping\");",
                "+    }",
                "+  }",
                "+",
                "+  /**",
                "+   * Returns list of groups for a user",
                "+   *",
                "+   * @param user get groups for this user",
                "+   * @return list of groups for a given user",
                "+   */",
                "+  @Override",
                "+  public Set<String> getGroups(String user) throws IOException {",
                "+    if (cachedGroups.containsKey(user)) {",
                "+      return cachedGroups.get(user);",
                "+    }",
                "+",
                "+    // I don't have anything",
                "+    return new HashSet<String>();",
                "+  }",
                "+}",
                "diff --git a/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java b/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java",
                "index b8d02d14c..a39dd6d94 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java",
                "@@ -82,2 +82,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "     protected Set<String> _admins;",
                "+    protected Set<String> _adminsGroups;",
                "     protected Set<String> _supervisors;",
                "@@ -94,2 +95,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "         _admins = new HashSet<>();",
                "+        _adminsGroups = new HashSet<>();",
                "         _supervisors = new HashSet<>();",
                "@@ -101,2 +103,7 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "         }",
                "+",
                "+        if (conf.containsKey(Config.NIMBUS_ADMINS_GROUPS)) {",
                "+            _adminsGroups.addAll((Collection<String>)conf.get(Config.NIMBUS_ADMINS_GROUPS));",
                "+        }",
                "+",
                "         if (conf.containsKey(Config.NIMBUS_SUPERVISOR_USERS)) {",
                "@@ -104,2 +111,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "         }",
                "+",
                "         if (conf.containsKey(Config.NIMBUS_USERS)) {",
                "@@ -137,3 +145,3 @@ public class SimpleACLAuthorizer implements IAuthorizer {",
                "-        if (_admins.contains(principal) || _admins.contains(user)) {",
                "+        if (_admins.contains(principal) || _admins.contains(user) || checkUserGroupAllowed(userGroups, _adminsGroups)) {",
                "             return true;",
                "diff --git a/storm-server/src/main/java/org/apache/storm/DaemonConfig.java b/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "index edc81f9f3..79b44ef07 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "@@ -160,10 +160,2 @@ public class DaemonConfig implements Validated {",
                "-    /**",
                "-     * Initialization parameters for the group mapping service plugin.",
                "-     * Provides a way for a @link{STORM_GROUP_MAPPING_SERVICE_PROVIDER_PLUGIN}",
                "-     * implementation to access optional settings.",
                "-     */",
                "-    @isType(type = Map.class)",
                "-    public static final String STORM_GROUP_MAPPING_SERVICE_PARAMS = \"storm.group.mapping.service.params\";",
                "-",
                "     /**",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index 3b914baa8..f51d169a3 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -3925,2 +3925,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "             List<String> adminUsers = (List<String>) conf.getOrDefault(Config.NIMBUS_ADMINS, Collections.emptyList());",
                "+            List<String> adminGroups = (List<String>) conf.getOrDefault(Config.NIMBUS_ADMINS_GROUPS, Collections.emptyList());",
                "             IStormClusterState state = stormClusterState;",
                "@@ -3936,2 +3937,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "                         isUserPartOf(user, groups) ||",
                "+                        isUserPartOf(user, adminGroups) ||",
                "                         topoLogUsers.contains(user)) {",
                "diff --git a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/ResourceAuthorizer.java b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/ResourceAuthorizer.java",
                "index 3d6a7d9e2..e24b9886b 100644",
                "--- a/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/ResourceAuthorizer.java",
                "+++ b/storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/ResourceAuthorizer.java",
                "@@ -90,2 +90,3 @@ public class ResourceAuthorizer {",
                "             logsGroups.addAll(ObjectReader.getStrings(stormConf.get(DaemonConfig.LOGS_GROUPS)));",
                "+            logsGroups.addAll(ObjectReader.getStrings(stormConf.get(Config.NIMBUS_ADMINS_GROUPS)));",
                "             logsGroups.addAll(whitelist.getGroupWhitelist());"
            ],
            "changed_files": [
                "storm-client/src/jvm/org/apache/storm/Config.java",
                "storm-client/src/jvm/org/apache/storm/blobstore/BlobStoreAclHandler.java",
                "storm-client/src/jvm/org/apache/storm/security/auth/FixedGroupsMapping.java",
                "storm-client/src/jvm/org/apache/storm/security/auth/authorizer/SimpleACLAuthorizer.java",
                "storm-server/src/main/java/org/apache/storm/DaemonConfig.java",
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "storm-webapp/src/main/java/org/apache/storm/daemon/logviewer/utils/ResourceAuthorizer.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "b5ae9c3426e69eba11446d055649307daecb05c7",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1513801717,
            "hunks": 0,
            "message": "Merge branch '1.x-branch' into metrics_v2",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "cf91236b385065209cf50d86d2d3071a402e5cd8",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507603866,
            "hunks": 3,
            "message": "In the DRPCSpout class, when the fetch from the DRPC server fails, the log should return to get the DRPC request failed instead of getting the DRPC result failed",
            "diff": [
                "diff --git a/storm-client/src/jvm/org/apache/storm/drpc/DRPCSpout.java b/storm-client/src/jvm/org/apache/storm/drpc/DRPCSpout.java",
                "index 8605c0563..00ae469b6 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/drpc/DRPCSpout.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/drpc/DRPCSpout.java",
                "@@ -215,8 +215,8 @@ public class DRPCSpout extends BaseRichSpout {",
                "                     reconnectAsync(client);",
                "-                    LOG.error(\"Not authorized to fetch DRPC result from DRPC server\", aze);",
                "+                    LOG.error(\"Not authorized to fetch DRPC request from DRPC server\", aze);",
                "                 } catch (TException e) {",
                "                     reconnectAsync(client);",
                "-                    LOG.error(\"Failed to fetch DRPC result from DRPC server\", e);",
                "+                    LOG.error(\"Failed to fetch DRPC request from DRPC server\", e);",
                "                 } catch (Exception e) {",
                "-                    LOG.error(\"Failed to fetch DRPC result from DRPC server\", e);",
                "+                    LOG.error(\"Failed to fetch DRPC request from DRPC server\", e);",
                "                 }"
            ],
            "changed_files": [
                "storm-client/src/jvm/org/apache/storm/drpc/DRPCSpout.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "c4404cab69966fbf98c4f1e6d4bc41945e51cfcb",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516638972,
            "hunks": 0,
            "message": "Merge branch '1.x-branch' into metrics_v2",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "00c247742781d4c6407d8cd87ae70820739714f9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512639593,
            "hunks": 0,
            "message": "Merge branch 'patch-1' of https://github.com/Ethanlm/storm",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "99bcf68449df63a83e124afaadaa61f61da15d22",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1511203944,
            "hunks": 0,
            "message": "Merge branch '1.x-branch' into metrics_v2",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "dd977e8d384c49a7b38cdf30d1ce9b7f09055dcd",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1512764108,
            "hunks": 0,
            "message": "Merge branch 'metrics_v2' of https://git-wip-us.apache.org/repos/asf/storm into metrics_v2",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "ca022899b3c433318f4e427e873d1ae583bee13b",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507964900,
            "hunks": 1,
            "message": "MINOR: Fix spelling error in comment",
            "diff": [
                "diff --git a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "index 56f670128..075c4dd63 100755",
                "--- a/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "+++ b/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java",
                "@@ -99,3 +99,3 @@ public class OffsetManager {",
                "                         the next logical point in the topic. Next logical offset should be the",
                "-                        first element after nextCommitOffsset in the ascending ordered emitted set.",
                "+                        first element after nextCommitOffset in the ascending ordered emitted set.",
                "                      */"
            ],
            "changed_files": [
                "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "326afbe6288046a89254a089e9384930ca76713c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516631397,
            "hunks": 1,
            "message": "fix BlobSynchronizer zk-client non set",
            "diff": [
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index afb1c2815..5a0f62272 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -2285,2 +2285,3 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "                 sync.setZookeeperKeySet(zkKeys);",
                "+                sync.setZkClient(zkClient);",
                "                 sync.syncBlobs();"
            ],
            "changed_files": [
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "cb331d4547e94694e2e1a5bcc4add2b227524fb9",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507991032,
            "hunks": 0,
            "message": "Merge branch 'fix-kafka-partition-metric-names' of https://github.com/kevinconaway/storm into asfgit-1.1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "f10fefc7f8dde7ff6886f7e09627d65b78b2ceb7",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516577977,
            "hunks": 0,
            "message": "Merge branch 'AUTO-CREDS-CLEANUP' of https://github.com/omkreddy/storm into STORm-2903-1.x-merge",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "174f4862f175b57a1888e239800a6fdd43159089",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1517513546,
            "hunks": 0,
            "message": "Merge branch '1.0.x-branch' of https://git-wip-us.apache.org/repos/asf/storm into 1.0.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.0.6",
                "v1.0.7"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "69e596ac60ac152c854c238e98783c57e432d86c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515002531,
            "hunks": 0,
            "message": "Merge branch 'kafka-metrics-apache-master' of https://github.com/omkreddy/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "37f9c548f75fe4e43fc496e8b59472edaa123cda",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1510339597,
            "hunks": 1,
            "message": "Fix pom dependency indent The indent in this dependency is incorrect.",
            "diff": [
                "diff --git a/pom.xml b/pom.xml",
                "index f53f8286b..aa4cf8d17 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -893,8 +893,8 @@",
                "             </dependency>",
                "-                <dependency>",
                "-                    <groupId>com.google.auto.service</groupId>",
                "-                    <artifactId>auto-service</artifactId>",
                "-                    <version>${auto-service.version}</version>",
                "-                    <optional>true</optional>",
                "-                </dependency>",
                "+            <dependency>",
                "+                <groupId>com.google.auto.service</groupId>",
                "+                <artifactId>auto-service</artifactId>",
                "+                <version>${auto-service.version}</version>",
                "+                <optional>true</optional>",
                "+            </dependency>",
                "             <dependency>"
            ],
            "changed_files": [
                "pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "94dab8d9f59ce4416fcb24076c8ae518690a229e",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1507964013,
            "hunks": 0,
            "message": "Merge branch 'fix-kafka-partition-metric-names-master' of https://github.com/kevinconaway/storm into asfgit-master",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "0c3aa3d801eefaec4c063adedb213d81505de78c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516723393,
            "hunks": 55,
            "message": "[maven-release-plugin] rollback the release of v1.1.2",
            "diff": [
                "diff --git a/examples/storm-elasticsearch-examples/pom.xml b/examples/storm-elasticsearch-examples/pom.xml",
                "index f76291f1f..348637aed 100644",
                "--- a/examples/storm-elasticsearch-examples/pom.xml",
                "+++ b/examples/storm-elasticsearch-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hbase-examples/pom.xml b/examples/storm-hbase-examples/pom.xml",
                "index 28670b3f1..3a11a9c6c 100644",
                "--- a/examples/storm-hbase-examples/pom.xml",
                "+++ b/examples/storm-hbase-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hdfs-examples/pom.xml b/examples/storm-hdfs-examples/pom.xml",
                "index f8dca7fcd..8bab8fcf2 100644",
                "--- a/examples/storm-hdfs-examples/pom.xml",
                "+++ b/examples/storm-hdfs-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-hive-examples/pom.xml b/examples/storm-hive-examples/pom.xml",
                "index 5052343ef..50bf51214 100644",
                "--- a/examples/storm-hive-examples/pom.xml",
                "+++ b/examples/storm-hive-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jdbc-examples/pom.xml b/examples/storm-jdbc-examples/pom.xml",
                "index 43c15d77b..f599de795 100644",
                "--- a/examples/storm-jdbc-examples/pom.xml",
                "+++ b/examples/storm-jdbc-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-jms-examples/pom.xml b/examples/storm-jms-examples/pom.xml",
                "index 37598b3c2..44452502c 100644",
                "--- a/examples/storm-jms-examples/pom.xml",
                "+++ b/examples/storm-jms-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-client-examples/pom.xml b/examples/storm-kafka-client-examples/pom.xml",
                "index 3b8abfae2..6c40475a5 100644",
                "--- a/examples/storm-kafka-client-examples/pom.xml",
                "+++ b/examples/storm-kafka-client-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-kafka-examples/pom.xml b/examples/storm-kafka-examples/pom.xml",
                "index 1dd9f45f5..658e8f760 100644",
                "--- a/examples/storm-kafka-examples/pom.xml",
                "+++ b/examples/storm-kafka-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mongodb-examples/pom.xml b/examples/storm-mongodb-examples/pom.xml",
                "index b09f3c41a..f4ad7f55e 100644",
                "--- a/examples/storm-mongodb-examples/pom.xml",
                "+++ b/examples/storm-mongodb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-mqtt-examples/pom.xml b/examples/storm-mqtt-examples/pom.xml",
                "index ff6bf71f9..d32cb992f 100644",
                "--- a/examples/storm-mqtt-examples/pom.xml",
                "+++ b/examples/storm-mqtt-examples/pom.xml",
                "@@ -28,3 +28,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.3-SNAPSHOT</version>",
                "+    <version>1.1.2-SNAPSHOT</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-opentsdb-examples/pom.xml b/examples/storm-opentsdb-examples/pom.xml",
                "index ad6c41d1a..63cde9167 100644",
                "--- a/examples/storm-opentsdb-examples/pom.xml",
                "+++ b/examples/storm-opentsdb-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-perf/pom.xml b/examples/storm-perf/pom.xml",
                "index 3780e7646..6203793c0 100644",
                "--- a/examples/storm-perf/pom.xml",
                "+++ b/examples/storm-perf/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-pmml-examples/pom.xml b/examples/storm-pmml-examples/pom.xml",
                "index e15001ab0..c13fa7a1b 100644",
                "--- a/examples/storm-pmml-examples/pom.xml",
                "+++ b/examples/storm-pmml-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-redis-examples/pom.xml b/examples/storm-redis-examples/pom.xml",
                "index 0af5a2041..28d51aaf5 100644",
                "--- a/examples/storm-redis-examples/pom.xml",
                "+++ b/examples/storm-redis-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-solr-examples/pom.xml b/examples/storm-solr-examples/pom.xml",
                "index 6d7877513..e830e7cb7 100644",
                "--- a/examples/storm-solr-examples/pom.xml",
                "+++ b/examples/storm-solr-examples/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/examples/storm-starter/pom.xml b/examples/storm-starter/pom.xml",
                "index 50bc34c8d..9d029ff83 100644",
                "--- a/examples/storm-starter/pom.xml",
                "+++ b/examples/storm-starter/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.1.3-SNAPSHOT</version>",
                "+      <version>1.1.2-SNAPSHOT</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-core/pom.xml b/external/flux/flux-core/pom.xml",
                "index d69d9b297..adde30d02 100644",
                "--- a/external/flux/flux-core/pom.xml",
                "+++ b/external/flux/flux-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/flux-wrappers/pom.xml b/external/flux/flux-wrappers/pom.xml",
                "index 5d4d73a10..61f258b9a 100644",
                "--- a/external/flux/flux-wrappers/pom.xml",
                "+++ b/external/flux/flux-wrappers/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <artifactId>flux</artifactId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../pom.xml</relativePath>",
                "diff --git a/external/flux/pom.xml b/external/flux/pom.xml",
                "index 31b2a24bb..cbc33657c 100644",
                "--- a/external/flux/pom.xml",
                "+++ b/external/flux/pom.xml",
                "@@ -28,3 +28,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/pom.xml b/external/sql/pom.xml",
                "index 8adac96ce..aedc08ab8 100644",
                "--- a/external/sql/pom.xml",
                "+++ b/external/sql/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-core/pom.xml b/external/sql/storm-sql-core/pom.xml",
                "index 375dafc07..a7980084a 100644",
                "--- a/external/sql/storm-sql-core/pom.xml",
                "+++ b/external/sql/storm-sql-core/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "index 55c378fb1..a03e25925 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "index 64b58fa19..7aad20378 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "index 9a3d10b45..4b6153fc5 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-external/storm-sql-redis/pom.xml b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "index cc78b6363..5889e6a9a 100644",
                "--- a/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "+++ b/external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../../pom.xml</relativePath>",
                "diff --git a/external/sql/storm-sql-runtime/pom.xml b/external/sql/storm-sql-runtime/pom.xml",
                "index db7f852a7..a506e92b7 100644",
                "--- a/external/sql/storm-sql-runtime/pom.xml",
                "+++ b/external/sql/storm-sql-runtime/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../../pom.xml</relativePath>",
                "diff --git a/external/storm-cassandra/pom.xml b/external/storm-cassandra/pom.xml",
                "index a123e6c32..580000f4f 100644",
                "--- a/external/storm-cassandra/pom.xml",
                "+++ b/external/storm-cassandra/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-druid/pom.xml b/external/storm-druid/pom.xml",
                "index 24f71445a..02748eec7 100644",
                "--- a/external/storm-druid/pom.xml",
                "+++ b/external/storm-druid/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-elasticsearch/pom.xml b/external/storm-elasticsearch/pom.xml",
                "index cfa49dd2f..380ce4e5e 100644",
                "--- a/external/storm-elasticsearch/pom.xml",
                "+++ b/external/storm-elasticsearch/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-eventhubs/pom.xml b/external/storm-eventhubs/pom.xml",
                "index 6597833dd..9ce5e2de6 100755",
                "--- a/external/storm-eventhubs/pom.xml",
                "+++ b/external/storm-eventhubs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hbase/pom.xml b/external/storm-hbase/pom.xml",
                "index 6eefa95c5..da3a9b51b 100644",
                "--- a/external/storm-hbase/pom.xml",
                "+++ b/external/storm-hbase/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hdfs/pom.xml b/external/storm-hdfs/pom.xml",
                "index 54899dc5c..321569909 100644",
                "--- a/external/storm-hdfs/pom.xml",
                "+++ b/external/storm-hdfs/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-hive/pom.xml b/external/storm-hive/pom.xml",
                "index a7800adda..706e8f843 100644",
                "--- a/external/storm-hive/pom.xml",
                "+++ b/external/storm-hive/pom.xml",
                "@@ -23,3 +23,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.3-SNAPSHOT</version>",
                "+    <version>1.1.2-SNAPSHOT</version>",
                "     <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jdbc/pom.xml b/external/storm-jdbc/pom.xml",
                "index e7113b878..ea5d735c2 100644",
                "--- a/external/storm-jdbc/pom.xml",
                "+++ b/external/storm-jdbc/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-jms/pom.xml b/external/storm-jms/pom.xml",
                "index 6e1871873..7c31b37ff 100644",
                "--- a/external/storm-jms/pom.xml",
                "+++ b/external/storm-jms/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-client/pom.xml b/external/storm-kafka-client/pom.xml",
                "index f29f2ba70..304e703ef 100644",
                "--- a/external/storm-kafka-client/pom.xml",
                "+++ b/external/storm-kafka-client/pom.xml",
                "@@ -24,3 +24,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka-monitor/pom.xml b/external/storm-kafka-monitor/pom.xml",
                "index 3af00251e..475e0732d 100644",
                "--- a/external/storm-kafka-monitor/pom.xml",
                "+++ b/external/storm-kafka-monitor/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kafka/pom.xml b/external/storm-kafka/pom.xml",
                "index 30fff9058..28ec77ced 100644",
                "--- a/external/storm-kafka/pom.xml",
                "+++ b/external/storm-kafka/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-kinesis/pom.xml b/external/storm-kinesis/pom.xml",
                "index 2ef7659af..013a6efc3 100644",
                "--- a/external/storm-kinesis/pom.xml",
                "+++ b/external/storm-kinesis/pom.xml",
                "@@ -19,3 +19,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-metrics/pom.xml b/external/storm-metrics/pom.xml",
                "index 77ded6aa9..85139343e 100644",
                "--- a/external/storm-metrics/pom.xml",
                "+++ b/external/storm-metrics/pom.xml",
                "@@ -22,3 +22,3 @@",
                "       <groupId>org.apache.storm</groupId>",
                "-      <version>1.1.3-SNAPSHOT</version>",
                "+      <version>1.1.2-SNAPSHOT</version>",
                "       <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mongodb/pom.xml b/external/storm-mongodb/pom.xml",
                "index 8147ddba5..5ad9958eb 100644",
                "--- a/external/storm-mongodb/pom.xml",
                "+++ b/external/storm-mongodb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-mqtt/pom.xml b/external/storm-mqtt/pom.xml",
                "index c02aaf60b..9a3056c58 100644",
                "--- a/external/storm-mqtt/pom.xml",
                "+++ b/external/storm-mqtt/pom.xml",
                "@@ -27,3 +27,3 @@",
                "         <artifactId>storm</artifactId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-opentsdb/pom.xml b/external/storm-opentsdb/pom.xml",
                "index 7cd005467..0be768f76 100644",
                "--- a/external/storm-opentsdb/pom.xml",
                "+++ b/external/storm-opentsdb/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-pmml/pom.xml b/external/storm-pmml/pom.xml",
                "index 54d6b3f68..73f76f54e 100644",
                "--- a/external/storm-pmml/pom.xml",
                "+++ b/external/storm-pmml/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-redis/pom.xml b/external/storm-redis/pom.xml",
                "index 210558789..33418eb9d 100644",
                "--- a/external/storm-redis/pom.xml",
                "+++ b/external/storm-redis/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-solr/pom.xml b/external/storm-solr/pom.xml",
                "index d5dd89fb2..c5303f467 100644",
                "--- a/external/storm-solr/pom.xml",
                "+++ b/external/storm-solr/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/external/storm-submit-tools/pom.xml b/external/storm-submit-tools/pom.xml",
                "index 460316c5a..340061c30 100644",
                "--- a/external/storm-submit-tools/pom.xml",
                "+++ b/external/storm-submit-tools/pom.xml",
                "@@ -21,3 +21,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/pom.xml b/pom.xml",
                "index b7aecbdb0..d7fd43b2e 100644",
                "--- a/pom.xml",
                "+++ b/pom.xml",
                "@@ -29,3 +29,3 @@",
                "     <artifactId>storm</artifactId>",
                "-    <version>1.1.3-SNAPSHOT</version>",
                "+    <version>1.1.2-SNAPSHOT</version>",
                "     <packaging>pom</packaging>",
                "diff --git a/storm-buildtools/maven-shade-clojure-transformer/pom.xml b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "index c2011e7ab..9585b9d35 100644",
                "--- a/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "+++ b/storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-buildtools/storm-maven-plugins/pom.xml b/storm-buildtools/storm-maven-plugins/pom.xml",
                "index a826f0b42..759e35bfd 100644",
                "--- a/storm-buildtools/storm-maven-plugins/pom.xml",
                "+++ b/storm-buildtools/storm-maven-plugins/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-core/pom.xml b/storm-core/pom.xml",
                "index 69883d080..61db89b77 100644",
                "--- a/storm-core/pom.xml",
                "+++ b/storm-core/pom.xml",
                "@@ -22,3 +22,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>..</relativePath>",
                "diff --git a/storm-multilang/javascript/pom.xml b/storm-multilang/javascript/pom.xml",
                "index 4ca916889..58f9d5334 100644",
                "--- a/storm-multilang/javascript/pom.xml",
                "+++ b/storm-multilang/javascript/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/python/pom.xml b/storm-multilang/python/pom.xml",
                "index 54bdeb193..194bc22bd 100644",
                "--- a/storm-multilang/python/pom.xml",
                "+++ b/storm-multilang/python/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-multilang/ruby/pom.xml b/storm-multilang/ruby/pom.xml",
                "index 955df5ad2..d366ecc41 100644",
                "--- a/storm-multilang/ruby/pom.xml",
                "+++ b/storm-multilang/ruby/pom.xml",
                "@@ -23,3 +23,3 @@",
                "         <groupId>org.apache.storm</groupId>",
                "-        <version>1.1.3-SNAPSHOT</version>",
                "+        <version>1.1.2-SNAPSHOT</version>",
                "         <relativePath>../../pom.xml</relativePath>",
                "diff --git a/storm-rename-hack/pom.xml b/storm-rename-hack/pom.xml",
                "index a194b984b..b27e4e897 100644",
                "--- a/storm-rename-hack/pom.xml",
                "+++ b/storm-rename-hack/pom.xml",
                "@@ -22,3 +22,3 @@",
                "     <groupId>org.apache.storm</groupId>",
                "-    <version>1.1.3-SNAPSHOT</version>",
                "+    <version>1.1.2-SNAPSHOT</version>",
                "     <relativePath>../pom.xml</relativePath>"
            ],
            "changed_files": [
                "examples/storm-elasticsearch-examples/pom.xml",
                "examples/storm-hbase-examples/pom.xml",
                "examples/storm-hdfs-examples/pom.xml",
                "examples/storm-hive-examples/pom.xml",
                "examples/storm-jdbc-examples/pom.xml",
                "examples/storm-jms-examples/pom.xml",
                "examples/storm-kafka-client-examples/pom.xml",
                "examples/storm-kafka-examples/pom.xml",
                "examples/storm-mongodb-examples/pom.xml",
                "examples/storm-mqtt-examples/pom.xml",
                "examples/storm-opentsdb-examples/pom.xml",
                "examples/storm-perf/pom.xml",
                "examples/storm-pmml-examples/pom.xml",
                "examples/storm-redis-examples/pom.xml",
                "examples/storm-solr-examples/pom.xml",
                "examples/storm-starter/pom.xml",
                "external/flux/flux-core/pom.xml",
                "external/flux/flux-wrappers/pom.xml",
                "external/flux/pom.xml",
                "external/sql/pom.xml",
                "external/sql/storm-sql-core/pom.xml",
                "external/sql/storm-sql-external/storm-sql-hdfs/pom.xml",
                "external/sql/storm-sql-external/storm-sql-kafka/pom.xml",
                "external/sql/storm-sql-external/storm-sql-mongodb/pom.xml",
                "external/sql/storm-sql-external/storm-sql-redis/pom.xml",
                "external/sql/storm-sql-runtime/pom.xml",
                "external/storm-cassandra/pom.xml",
                "external/storm-druid/pom.xml",
                "external/storm-elasticsearch/pom.xml",
                "external/storm-eventhubs/pom.xml",
                "external/storm-hbase/pom.xml",
                "external/storm-hdfs/pom.xml",
                "external/storm-hive/pom.xml",
                "external/storm-jdbc/pom.xml",
                "external/storm-jms/pom.xml",
                "external/storm-kafka-client/pom.xml",
                "external/storm-kafka-monitor/pom.xml",
                "external/storm-kafka/pom.xml",
                "external/storm-kinesis/pom.xml",
                "external/storm-metrics/pom.xml",
                "external/storm-mongodb/pom.xml",
                "external/storm-mqtt/pom.xml",
                "external/storm-opentsdb/pom.xml",
                "external/storm-pmml/pom.xml",
                "external/storm-redis/pom.xml",
                "external/storm-solr/pom.xml",
                "external/storm-submit-tools/pom.xml",
                "pom.xml",
                "storm-buildtools/maven-shade-clojure-transformer/pom.xml",
                "storm-buildtools/storm-maven-plugins/pom.xml",
                "storm-core/pom.xml",
                "storm-multilang/javascript/pom.xml",
                "storm-multilang/python/pom.xml",
                "storm-multilang/ruby/pom.xml",
                "storm-rename-hack/pom.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "544aa6450dc9ce0ea3e06e66638266cc90b154d0",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1516729649,
            "hunks": 1,
            "message": "remove CHANGELOG.md reference from binary distribution",
            "diff": [
                "diff --git a/storm-dist/binary/src/main/assembly/binary.xml b/storm-dist/binary/src/main/assembly/binary.xml",
                "index 636bf45fa..bd4cd7603 100644",
                "--- a/storm-dist/binary/src/main/assembly/binary.xml",
                "+++ b/storm-dist/binary/src/main/assembly/binary.xml",
                "@@ -380,7 +380,2 @@",
                "         </file>",
                "-        <file>",
                "-            <source>${project.basedir}/../../CHANGELOG.md</source>",
                "-            <outputDirectory>/</outputDirectory>",
                "-        </file>",
                "-",
                "         <file>"
            ],
            "changed_files": [
                "storm-dist/binary/src/main/assembly/binary.xml"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.1.2",
                "v1.1.3",
                "v1.1.4"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "75e4a027d874266b302b84c5507ca148c5be7e52",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1509987712,
            "hunks": 44,
            "message": "STORM:2803: Fix leaking threads from Nimbus/TimeCacheMap, slightly refactor Time to use more final fields, replaced uses of deprecated classes/methods and added a few tests.",
            "diff": [
                "diff --git a/.travis.yml b/.travis.yml",
                "index 5e3dab2c6..128b5e045 100644",
                "--- a/.travis.yml",
                "+++ b/.travis.yml",
                "@@ -35,3 +35,2 @@ script:",
                "   - /bin/bash ./dev-tools/travis/travis-script.sh `pwd` $MODULES",
                "-sudo: true",
                " cache:",
                "diff --git a/storm-client/src/jvm/org/apache/storm/security/auth/ShellBasedGroupsMapping.java b/storm-client/src/jvm/org/apache/storm/security/auth/ShellBasedGroupsMapping.java",
                "index 3ee575422..d1a7596dd 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/security/auth/ShellBasedGroupsMapping.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/security/auth/ShellBasedGroupsMapping.java",
                "@@ -20,2 +20,3 @@ package org.apache.storm.security.auth;",
                "+import com.google.common.annotations.VisibleForTesting;",
                " import java.io.IOException;",
                "@@ -24,8 +25,11 @@ import java.util.HashSet;",
                " import java.util.Map;",
                "-import java.util.StringTokenizer;",
                "+import java.util.concurrent.TimeUnit;",
                " import org.apache.storm.Config;",
                " import org.apache.storm.utils.ObjectReader;",
                "+import org.apache.storm.utils.RotatingMap;",
                "+import org.apache.storm.utils.ShellCommandRunner;",
                "+import org.apache.storm.utils.ShellCommandRunnerImpl;",
                " import org.apache.storm.utils.ShellUtils;",
                "-import org.apache.storm.utils.TimeCacheMap;",
                " import org.apache.storm.utils.ShellUtils.ExitCodeException;",
                "+import org.apache.storm.utils.Time;",
                " import org.slf4j.Logger;",
                "@@ -38,3 +42,17 @@ public class ShellBasedGroupsMapping implements",
                "     public static final Logger LOG = LoggerFactory.getLogger(ShellBasedGroupsMapping.class);",
                "-    public TimeCacheMap<String, Set<String>> cachedGroups;",
                "+    public RotatingMap<String, Set<String>> cachedGroups;",
                "+    ",
                "+    private final ShellCommandRunner shellCommandRunner;",
                "+  ",
                "+    private long timeoutMs;",
                "+    private volatile long lastRotationMs;",
                "+",
                "+    public ShellBasedGroupsMapping() {",
                "+        this(new ShellCommandRunnerImpl());",
                "+    }",
                "+    ",
                "+    @VisibleForTesting",
                "+    ShellBasedGroupsMapping(ShellCommandRunner shellCommandRunner) {",
                "+        this.shellCommandRunner = shellCommandRunner;",
                "+    }",
                "@@ -46,6 +64,7 @@ public class ShellBasedGroupsMapping implements",
                "     public void prepare(Map<String, Object> topoConf) {",
                "-        int timeout = ObjectReader.getInt(topoConf.get(Config.STORM_GROUP_MAPPING_SERVICE_CACHE_DURATION_SECS));",
                "-        cachedGroups = new TimeCacheMap<>(timeout);",
                "+        timeoutMs = TimeUnit.SECONDS.toMillis(ObjectReader.getInt(topoConf.get(Config.STORM_GROUP_MAPPING_SERVICE_CACHE_DURATION_SECS)));",
                "+        lastRotationMs = Time.currentTimeMillis();",
                "+        cachedGroups = new RotatingMap<>(2);",
                "     }",
                "-",
                "+    ",
                "     /**",
                "@@ -59,2 +78,3 @@ public class ShellBasedGroupsMapping implements",
                "         synchronized(this) {",
                "+            rotateIfNeeded();",
                "             if (cachedGroups.containsKey(user)) {",
                "@@ -71,2 +91,15 @@ public class ShellBasedGroupsMapping implements",
                "     }",
                "+   ",
                "+    private void rotateIfNeeded() {",
                "+        long nowMs = Time.currentTimeMillis();",
                "+        if (nowMs >= lastRotationMs + timeoutMs) {",
                "+            //Rotate once per timeout period that has passed since last time this was called.",
                "+            //This is necessary since this method may be called at arbitrary intervals.",
                "+            int rotationsToDo = (int)((nowMs - lastRotationMs) / timeoutMs);",
                "+            for (int i = 0; i < rotationsToDo; i++) {",
                "+                cachedGroups.rotate();",
                "+            }",
                "+            lastRotationMs = nowMs;",
                "+        }",
                "+    }",
                "@@ -79,9 +112,9 @@ public class ShellBasedGroupsMapping implements",
                "      */",
                "-    private static Set<String> getUnixGroups(final String user) throws IOException {",
                "+    private Set<String> getUnixGroups(final String user) throws IOException {",
                "         String result;",
                "         try {",
                "-            result = ShellUtils.execCommand(ShellUtils.getGroupsForUserCommand(user));",
                "+            result = shellCommandRunner.execCommand(ShellUtils.getGroupsForUserCommand(user));",
                "         } catch (ExitCodeException e) {",
                "             // if we didn't get the group - just return empty list;",
                "-            LOG.debug(\"unable to get groups for user \" + user + \".ShellUtils command failed with exit code \"+ e.getExitCode());",
                "+            LOG.debug(\"Unable to get groups for user \" + user + \". ShellUtils command failed with exit code \"+ e.getExitCode());",
                "             return new HashSet<>();",
                "@@ -89,7 +122,5 @@ public class ShellBasedGroupsMapping implements",
                "-        StringTokenizer tokenizer =",
                "-            new StringTokenizer(result, ShellUtils.TOKEN_SEPARATOR_REGEX);",
                "         Set<String> groups = new HashSet<>();",
                "-        while (tokenizer.hasMoreTokens()) {",
                "-            groups.add(tokenizer.nextToken());",
                "+        for (String group : result.split(shellCommandRunner.getTokenSeparatorRegex())) {",
                "+            groups.add(group);",
                "         }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/RotatingMap.java b/storm-client/src/jvm/org/apache/storm/utils/RotatingMap.java",
                "index dfd6bdfbd..89dedd473 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/utils/RotatingMap.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/RotatingMap.java",
                "@@ -69,3 +69,3 @@ public class RotatingMap<K, V> {",
                "         this(numBuckets, null);",
                "-    }   ",
                "+    }",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/ShellCommandRunner.java b/storm-client/src/jvm/org/apache/storm/utils/ShellCommandRunner.java",
                "new file mode 100644",
                "index 000000000..971856968",
                "--- /dev/null",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/ShellCommandRunner.java",
                "@@ -0,0 +1,63 @@",
                "+/*",
                "+ * Copyright 2017 The Apache Software Foundation.",
                "+ *",
                "+ * Licensed under the Apache License, Version 2.0 (the \"License\");",
                "+ * you may not use this file except in compliance with the License.",
                "+ * You may obtain a copy of the License at",
                "+ *",
                "+ *      http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.utils;",
                "+",
                "+import java.io.IOException;",
                "+import java.util.Map;",
                "+",
                "+/**",
                "+ * Contains convenience functions for running shell commands for cases that are too simple to need a full {@link ShellUtils} implementation.",
                "+ */",
                "+public interface ShellCommandRunner {",
                "+    ",
                "+    /**",
                "+     * Method to execute a shell command.",
                "+     * Covers most of the simple cases without requiring the user to implement",
                "+     * the {@link ShellUtils} interface.",
                "+     * @param cmd shell command to execute.",
                "+     * @return the output of the executed command.",
                "+     */",
                "+    String execCommand(String ... cmd) throws IOException;",
                "+",
                "+    /**",
                "+     * Method to execute a shell command.",
                "+     * Covers most of the simple cases without requiring the user to implement",
                "+     * the {@link ShellUtils} interface.",
                "+     * @param env the map of environment key=value",
                "+     * @param cmd shell command to execute.",
                "+     * @param timeout time in milliseconds after which script should be marked timeout",
                "+     * @return the output of the executed command.",
                "+     */",
                "+",
                "+    String execCommand(Map<String, String> env, String[] cmd,",
                "+                                     long timeout) throws IOException;",
                "+",
                "+    /**",
                "+     * Method to execute a shell command.",
                "+     * Covers most of the simple cases without requiring the user to implement",
                "+     * the {@link ShellUtils} interface.",
                "+     * @param env the map of environment key=value",
                "+     * @param cmd shell command to execute.",
                "+     * @return the output of the executed command.",
                "+     */",
                "+    String execCommand(Map<String,String> env, String ... cmd)",
                "+        throws IOException;",
                "+    ",
                "+    /** Token separator regex used to parse Shell tool outputs */",
                "+    String getTokenSeparatorRegex();",
                "+",
                "+}",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/ShellCommandRunnerImpl.java b/storm-client/src/jvm/org/apache/storm/utils/ShellCommandRunnerImpl.java",
                "new file mode 100644",
                "index 000000000..fd9d46667",
                "--- /dev/null",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/ShellCommandRunnerImpl.java",
                "@@ -0,0 +1,46 @@",
                "+/*",
                "+ * Copyright 2017 The Apache Software Foundation.",
                "+ *",
                "+ * Licensed under the Apache License, Version 2.0 (the \"License\");",
                "+ * you may not use this file except in compliance with the License.",
                "+ * You may obtain a copy of the License at",
                "+ *",
                "+ *      http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.storm.utils;",
                "+",
                "+import java.io.IOException;",
                "+import java.util.Map;",
                "+",
                "+public class ShellCommandRunnerImpl implements ShellCommandRunner {",
                "+",
                "+    @Override",
                "+    public String execCommand(String... cmd) throws IOException {",
                "+        return execCommand(null, cmd, 0L);",
                "+    }",
                "+",
                "+    @Override",
                "+    public String execCommand(Map<String, String> env, String[] cmd, long timeout) throws IOException {",
                "+        ShellUtils.ShellCommandExecutor exec = new ShellUtils.ShellCommandExecutor(cmd, null, env,",
                "+                                                             timeout);",
                "+        exec.execute();",
                "+        return exec.getOutput();",
                "+    }",
                "+",
                "+    @Override",
                "+    public String execCommand(Map<String, String> env, String... cmd) throws IOException {",
                "+        return execCommand(env, cmd, 0L);",
                "+    }",
                "+",
                "+    @Override",
                "+    public String getTokenSeparatorRegex() {",
                "+        return ShellUtils.TOKEN_SEPARATOR_REGEX;",
                "+    }",
                "+}",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/ShellUtils.java b/storm-client/src/jvm/org/apache/storm/utils/ShellUtils.java",
                "index ef869b055..a77278769 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/utils/ShellUtils.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/ShellUtils.java",
                "@@ -137,4 +137,6 @@ abstract public class ShellUtils {",
                "     public static String[] getGroupsCommand() {",
                "-        return (WINDOWS)? new String[]{\"cmd\", \"/c\", \"groups\"}",
                "-        : new String[]{\"bash\", \"-c\", \"groups\"};",
                "+        if (WINDOWS) {",
                "+            throw new UnsupportedOperationException(\"Getting user groups is not supported on Windows\");",
                "+        }",
                "+        return new String[]{\"bash\", \"-c\", \"groups\"};",
                "     }",
                "@@ -148,2 +150,5 @@ abstract public class ShellUtils {",
                "     public static String[] getGroupsForUserCommand(final String user) {",
                "+        if (WINDOWS) {",
                "+            throw new UnsupportedOperationException(\"Getting user groups is not supported on Windows\");",
                "+        }",
                "         //'groups username' command return is non-consistent across different unixes",
                "@@ -433,45 +438,2 @@ abstract public class ShellUtils {",
                "-",
                "-    /**",
                "-     * Static method to execute a shell command.",
                "-     * Covers most of the simple cases without requiring the user to implement",
                "-     * the <code>Shell</code> interface.",
                "-     * @param cmd shell command to execute.",
                "-     * @return the output of the executed command.",
                "-     */",
                "-    public static String execCommand(String ... cmd) throws IOException {",
                "-        return execCommand(null, cmd, 0L);",
                "-    }",
                "-",
                "-    /**",
                "-     * Static method to execute a shell command.",
                "-     * Covers most of the simple cases without requiring the user to implement",
                "-     * the <code>Shell</code> interface.",
                "-     * @param env the map of environment key=value",
                "-     * @param cmd shell command to execute.",
                "-     * @param timeout time in milliseconds after which script should be marked timeout",
                "-     * @return the output of the executed command.o",
                "-     */",
                "-",
                "-    public static String execCommand(Map<String, String> env, String[] cmd,",
                "-                                     long timeout) throws IOException {",
                "-        ShellCommandExecutor exec = new ShellCommandExecutor(cmd, null, env,",
                "-                                                             timeout);",
                "-        exec.execute();",
                "-        return exec.getOutput();",
                "-    }",
                "-",
                "-    /**",
                "-     * Static method to execute a shell command.",
                "-     * Covers most of the simple cases without requiring the user to implement",
                "-     * the <code>Shell</code> interface.",
                "-     * @param env the map of environment key=value",
                "-     * @param cmd shell command to execute.",
                "-     * @return the output of the executed command.",
                "-     */",
                "-    public static String execCommand(Map<String,String> env, String ... cmd)",
                "-        throws IOException {",
                "-        return execCommand(env, cmd, 0L);",
                "-    }",
                "-",
                "     /**",
                "diff --git a/storm-client/src/jvm/org/apache/storm/utils/Time.java b/storm-client/src/jvm/org/apache/storm/utils/Time.java",
                "index 0401829ad..142c43245 100644",
                "--- a/storm-client/src/jvm/org/apache/storm/utils/Time.java",
                "+++ b/storm-client/src/jvm/org/apache/storm/utils/Time.java",
                "@@ -33,7 +33,7 @@ public class Time {",
                "     private static final Logger LOG = LoggerFactory.getLogger(Time.class);",
                "-    private static AtomicBoolean simulating = new AtomicBoolean(false);",
                "-    private static AtomicLong autoAdvanceNanosOnSleep = new AtomicLong(0);",
                "-    private static volatile Map<Thread, AtomicLong> threadSleepTimesNanos;",
                "-    private static final Object sleepTimesLock = new Object();",
                "-    private static AtomicLong simulatedCurrTimeNanos;",
                "+    private static final AtomicBoolean SIMULATING = new AtomicBoolean(false);",
                "+    private static final AtomicLong AUTO_ADVANCE_NANOS_ON_SLEEP = new AtomicLong(0);",
                "+    private static final Map<Thread, AtomicLong> THREAD_SLEEP_TIMES_NANOS = new ConcurrentHashMap<>();",
                "+    private static final Object SLEEP_TIMES_LOCK = new Object();",
                "+    private static final AtomicLong SIMULATED_CURR_TIME_NANOS = new AtomicLong(0);",
                "@@ -46,8 +46,10 @@ public class Time {",
                "         public SimulatedTime(Number advanceTimeMs) {",
                "-            synchronized(Time.sleepTimesLock) {",
                "-                Time.simulating.set(true);",
                "-                Time.simulatedCurrTimeNanos = new AtomicLong(0);",
                "-                Time.threadSleepTimesNanos = new ConcurrentHashMap<>();",
                "+            synchronized(Time.SLEEP_TIMES_LOCK) {",
                "+                Time.SIMULATING.set(true);",
                "+                Time.SIMULATED_CURR_TIME_NANOS.set(0);",
                "+                Time.THREAD_SLEEP_TIMES_NANOS.clear();",
                "                 if (advanceTimeMs != null) {",
                "-                    Time.autoAdvanceNanosOnSleep.set(millisToNanos(advanceTimeMs.longValue()));",
                "+                    Time.AUTO_ADVANCE_NANOS_ON_SLEEP.set(millisToNanos(advanceTimeMs.longValue()));",
                "+                } else {",
                "+                    Time.AUTO_ADVANCE_NANOS_ON_SLEEP.set(0);",
                "                 }",
                "@@ -59,6 +61,4 @@ public class Time {",
                "         public void close() {",
                "-            synchronized(Time.sleepTimesLock) {",
                "-                Time.simulating.set(false);    ",
                "-                Time.autoAdvanceNanosOnSleep.set(0);",
                "-                Time.threadSleepTimesNanos = null;",
                "+            synchronized(Time.SLEEP_TIMES_LOCK) {",
                "+                Time.SIMULATING.set(false);    ",
                "                 LOG.warn(\"AutoCloseable Simulated Time Ending...\");",
                "@@ -70,6 +70,7 @@ public class Time {",
                "     public static void startSimulating() {",
                "-        synchronized(Time.sleepTimesLock) {",
                "-            Time.simulating.set(true);",
                "-            Time.simulatedCurrTimeNanos = new AtomicLong(0);",
                "-            Time.threadSleepTimesNanos = new ConcurrentHashMap<>();",
                "+        synchronized(Time.SLEEP_TIMES_LOCK) {",
                "+            Time.SIMULATING.set(true);",
                "+            Time.SIMULATED_CURR_TIME_NANOS.set(0);",
                "+            Time.THREAD_SLEEP_TIMES_NANOS.clear();",
                "+            Time.AUTO_ADVANCE_NANOS_ON_SLEEP.set(0);",
                "             LOG.warn(\"Simulated Time Starting...\");",
                "@@ -80,6 +81,4 @@ public class Time {",
                "     public static void stopSimulating() {",
                "-        synchronized(Time.sleepTimesLock) {",
                "-            Time.simulating.set(false);    ",
                "-            Time.autoAdvanceNanosOnSleep.set(0);",
                "-            Time.threadSleepTimesNanos = null;",
                "+        synchronized(Time.SLEEP_TIMES_LOCK) {",
                "+            Time.SIMULATING.set(false);    ",
                "             LOG.warn(\"Simulated Time Ending...\");",
                "@@ -89,3 +88,3 @@ public class Time {",
                "     public static boolean isSimulating() {",
                "-        return simulating.get();",
                "+        return SIMULATING.get();",
                "     }",
                "@@ -93,3 +92,3 @@ public class Time {",
                "     public static void sleepUntil(long targetTimeMs) throws InterruptedException {",
                "-        if(simulating.get()) {",
                "+        if(SIMULATING.get()) {",
                "             simulatedSleepUntilNanos(millisToNanos(targetTimeMs));",
                "@@ -104,3 +103,3 @@ public class Time {",
                "     public static void sleepUntilNanos(long targetTimeNanos) throws InterruptedException {",
                "-        if(simulating.get()) {",
                "+        if(SIMULATING.get()) {",
                "             simulatedSleepUntilNanos(targetTimeNanos);",
                "@@ -118,4 +117,4 @@ public class Time {",
                "         try {",
                "-            synchronized (sleepTimesLock) {",
                "-                if (threadSleepTimesNanos == null) {",
                "+            synchronized (SLEEP_TIMES_LOCK) {",
                "+                if (THREAD_SLEEP_TIMES_NANOS == null) {",
                "                     LOG.debug(\"{} is still sleeping after simulated time disabled.\", Thread.currentThread(), new RuntimeException(\"STACK TRACE\"));",
                "@@ -123,7 +122,7 @@ public class Time {",
                "                 }",
                "-                threadSleepTimesNanos.put(Thread.currentThread(), new AtomicLong(targetTimeNanos));",
                "+                THREAD_SLEEP_TIMES_NANOS.put(Thread.currentThread(), new AtomicLong(targetTimeNanos));",
                "             }",
                "-            while (simulatedCurrTimeNanos.get() < targetTimeNanos) {",
                "-                synchronized (sleepTimesLock) {",
                "-                    if (threadSleepTimesNanos == null) {",
                "+            while (SIMULATED_CURR_TIME_NANOS.get() < targetTimeNanos) {",
                "+                synchronized (SLEEP_TIMES_LOCK) {",
                "+                    if (THREAD_SLEEP_TIMES_NANOS == null) {",
                "                         LOG.debug(\"{} is still sleeping after simulated time disabled.\", Thread.currentThread(), new RuntimeException(\"STACK TRACE\"));",
                "@@ -132,3 +131,3 @@ public class Time {",
                "                 }",
                "-                long autoAdvance = autoAdvanceNanosOnSleep.get();",
                "+                long autoAdvance = AUTO_ADVANCE_NANOS_ON_SLEEP.get();",
                "                 if (autoAdvance > 0) {",
                "@@ -139,5 +138,5 @@ public class Time {",
                "         } finally {",
                "-            synchronized (sleepTimesLock) {",
                "-                if (simulating.get() && threadSleepTimesNanos != null) {",
                "-                    threadSleepTimesNanos.remove(Thread.currentThread());",
                "+            synchronized (SLEEP_TIMES_LOCK) {",
                "+                if (SIMULATING.get() && THREAD_SLEEP_TIMES_NANOS != null) {",
                "+                    THREAD_SLEEP_TIMES_NANOS.remove(Thread.currentThread());",
                "                 }",
                "@@ -162,4 +161,4 @@ public class Time {",
                "     public static long nanoTime() {",
                "-        if (simulating.get()) {",
                "-            return simulatedCurrTimeNanos.get();",
                "+        if (SIMULATING.get()) {",
                "+            return SIMULATED_CURR_TIME_NANOS.get();",
                "         } else {",
                "@@ -170,4 +169,4 @@ public class Time {",
                "     public static long currentTimeMillis() {",
                "-        if(simulating.get()) {",
                "-            return nanosToMillis(simulatedCurrTimeNanos.get());",
                "+        if(SIMULATING.get()) {",
                "+            return nanosToMillis(SIMULATED_CURR_TIME_NANOS.get());",
                "         } else {",
                "@@ -210,3 +209,3 @@ public class Time {",
                "     public static void advanceTimeNanos(long nanos) {",
                "-        if (!simulating.get()) {",
                "+        if (!SIMULATING.get()) {",
                "             throw new IllegalStateException(\"Cannot simulate time unless in simulation mode\");",
                "@@ -216,3 +215,3 @@ public class Time {",
                "         }",
                "-        long newTime = simulatedCurrTimeNanos.addAndGet(nanos);",
                "+        long newTime = SIMULATED_CURR_TIME_NANOS.addAndGet(nanos);",
                "         LOG.debug(\"Advanced simulated time to {}\", newTime);",
                "@@ -225,3 +224,3 @@ public class Time {",
                "     public static boolean isThreadWaiting(Thread t) {",
                "-        if(!simulating.get()) {",
                "+        if(!SIMULATING.get()) {",
                "             throw new IllegalStateException(\"Must be in simulation mode\");",
                "@@ -229,4 +228,4 @@ public class Time {",
                "         AtomicLong time;",
                "-        synchronized(sleepTimesLock) {",
                "-            time = threadSleepTimesNanos.get(t);",
                "+        synchronized(SLEEP_TIMES_LOCK) {",
                "+            time = THREAD_SLEEP_TIMES_NANOS.get(t);",
                "         }",
                "diff --git a/storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java b/storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java",
                "index 07573c9de..057726161 100644",
                "--- a/storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java",
                "+++ b/storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java",
                "@@ -164,3 +164,3 @@ public class TopologySpoutLag {",
                "             if (!commands.contains(null)) {",
                "-                String resultFromMonitor = ShellUtils.execCommand(commands.toArray(new String[0]));",
                "+                String resultFromMonitor = new ShellCommandRunnerImpl().execCommand(commands.toArray(new String[0]));",
                "diff --git a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "index d9020e2ad..a9bb43e79 100644",
                "--- a/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "+++ b/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                "@@ -4134,2 +4134,5 @@ public class Nimbus implements Iface, Shutdownable, DaemonCommon {",
                "             uploaders.cleanup();",
                "+            blobDownloaders.cleanup();",
                "+            blobUploaders.cleanup();",
                "+            blobListers.cleanup();",
                "             blobStore.shutdown();"
            ],
            "changed_files": [
                ".travis.yml",
                "storm-client/src/jvm/org/apache/storm/security/auth/ShellBasedGroupsMapping.java",
                "storm-client/src/jvm/org/apache/storm/utils/RotatingMap.java",
                "storm-client/src/jvm/org/apache/storm/utils/ShellCommandRunner.java",
                "storm-client/src/jvm/org/apache/storm/utils/ShellCommandRunnerImpl.java",
                "storm-client/src/jvm/org/apache/storm/utils/ShellUtils.java",
                "storm-client/src/jvm/org/apache/storm/utils/Time.java",
                "storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java",
                "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v2.0.0",
                "v2.1.0",
                "v2.1.1",
                "v2.2.0",
                "v2.2.1",
                "v2.2.2",
                "v2.3.0",
                "v2.3.1",
                "v2.4.0",
                "v2.5.0",
                "v2.6.0",
                "v2.6.1",
                "v2.6.2"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "bade834add1f8c8b6c9b97d1b61325cd6813a66c",
            "repository": "https://github.com/apache/storm",
            "timestamp": 1515002714,
            "hunks": 0,
            "message": "Merge branch 'kafka-metrics-apache-1.x' of https://github.com/omkreddy/storm into asfgit-1.x-branch",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "v1.2.0",
                "v1.2.1",
                "v1.2.2",
                "v1.2.3",
                "v1.2.4"
            ],
            "matched_rules": []
        }
    ]
}
