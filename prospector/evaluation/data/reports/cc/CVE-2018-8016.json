{
    "advisory_record": {
        "cve_id": "CVE-2018-8016",
        "description": "The default configuration in Apache Cassandra 3.8 through 3.11.1 binds an unauthenticated JMX/RMI interface to all network interfaces, which allows remote attackers to execute arbitrary Java code via an RMI request. This issue is a regression of CVE-2015-0225. The regression was introduced in https://issues.apache.org/jira/browse/CASSANDRA-12109. The fix for the regression is implemented in https://issues.apache.org/jira/browse/CASSANDRA-14173. This fix is contained in the 3.11.2 release of Apache Cassandra.",
        "reserved_timestamp": 1520553600,
        "published_timestamp": 1529884800,
        "updated_timestamp": 1530201421,
        "repository_url": null,
        "references": {
            "": 59,
            "https://access.redhat.com/support/": 5,
            "https://access.redhat.com/downloads/": 4,
            "https://docs.atlassian.com/jira/jcore-docs-0820/": 4,
            "http://www.atlassian.com/software/jira": 4,
            "https://access.redhat.com/errata/": 3,
            "https://access.redhat.com/labs/": 3,
            "https://lists.apache.org/thread.html/bafb9060bbdf958a1c15ba66c68531116fba4a83858a2796254da066%40%3Cuser.cassandra.apache.org%3E": 2,
            "https://access.redhat.com/security/cve/CVE-2015-0225": 2,
            "http://seclists.org/oss-sec/2018/q2/234": 2,
            "https://issues.apache.org/jira/browse/CASSANDRA-12109": 2,
            "https://issues.apache.org/jira/browse/CASSANDRA-14173": 2,
            "https://access.redhat.com/management/": 2,
            "https://access.redhat.com/": 2,
            "https://access.redhat.com/products/red-hat-enterprise-linux/": 2,
            "https://access.redhat.com/products/red-hat-openshift-container-platform": 2,
            "https://access.redhat.com/products/red-hat-ansible-automation-platform/": 2,
            "https://access.redhat.com/products/": 2,
            "https://access.redhat.com/documentation": 2,
            "https://access.redhat.com/product-life-cycles/": 2,
            "https://access.redhat.com/security": 2,
            "https://access.redhat.com/security/security-updates/#/security-advisories": 2,
            "https://access.redhat.com/security/security-updates/#/cve": 2,
            "https://access.redhat.com/support/contact/": 2,
            "https://status.redhat.com": 2,
            "https://issues.apache.org/jira/secure/MyJiraHome.jspa": 2,
            "https://selfserve.apache.org/jira-account.html": 2,
            "https://issues.apache.org/jira/browse/CASSANDRA-10091": 2,
            "https://www.atlassian.com/software/jira": 2,
            "http://www.atlassian.com/": 2,
            "https://access.redhat.com/downloads/content/package-browser": 1,
            "https://catalog.redhat.com/software/containers/explore/": 1,
            "https://access.redhat.com/articles/1202803": 1,
            "https://access.redhat.com/search/?q=*&p=1&rows=10&documentKind=Solution": 1,
            "https://access.redhat.com/search/?q=*&p=1&rows=10&documentKind=Article": 1,
            "https://access.redhat.com/documentation/en/red_hat_enterprise_linux": 1,
            "https://access.redhat.com/documentation/en/openshift_container_platform": 1,
            "https://access.redhat.com/documentation/en/red_hat_ansible_automation_platform": 1,
            "https://access.redhat.com/documentation/": 1,
            "https://access.redhat.com/security/": 1,
            "https://access.redhat.com/security/vulnerabilities": 1,
            "https://access.redhat.com/security/data": 1,
            "https://access.redhat.com/security/security-updates/#/security-labs": 1,
            "https://access.redhat.com/security/updates/backporting/": 1,
            "https://access.redhat.com/support/cases/": 1,
            "https://access.redhat.com/support/cases/#/troubleshoot": 1,
            "https://access.redhat.com/community": 1,
            "https://access.redhat.com/community/": 1,
            "https://access.redhat.com/discussions/": 1,
            "https://access.redhat.com/announcements/": 1,
            "https://access.redhat.com/accelerators/": 1,
            "https://access.redhat.com/jbossnetwork/restricted/listSoftware.html": 1,
            "https://cloud.redhat.com/insights": 1,
            "https://access.redhat.com/changeLanguage?language=en": 1,
            "https://access.redhat.com/changeLanguage?language=fr": 1,
            "https://access.redhat.com/changeLanguage?language=ko": 1,
            "https://access.redhat.com/changeLanguage?language=ja": 1,
            "https://access.redhat.com/changeLanguage?language=zh_CN": 1,
            "https://access.redhat.com/products/red-hat-satellite/": 1,
            "https://access.redhat.com/products/red-hat-subscription-management/": 1,
            "https://access.redhat.com/products/red-hat-insights/": 1,
            "https://access.redhat.com/products/red-hat-openstack-platform/": 1,
            "https://access.redhat.com/products/red-hat-openshift-container-platform/": 1,
            "https://access.redhat.com/products/red-hat-openshift-ai/": 1,
            "https://access.redhat.com/products/openshift-dedicated-red-hat/": 1,
            "https://access.redhat.com/products/red-hat-advanced-cluster-security-for-kubernetes/": 1,
            "https://access.redhat.com/products/red-hat-advanced-cluster-management-for-kubernetes/": 1,
            "https://access.redhat.com/products/red-hat-quay/": 1,
            "https://access.redhat.com/products/red-hat-openshift-dev-spaces": 1,
            "https://access.redhat.com/products/red-hat-openshift-service-aws": 1,
            "https://access.redhat.com/products/red-hat-storage/": 1,
            "https://access.redhat.com/products/red-hat-hyperconverged-infrastructure/": 1,
            "https://access.redhat.com/products/red-hat-ceph-storage/": 1,
            "https://access.redhat.com/products/red-hat-openshift-data-foundation": 1,
            "https://access.redhat.com/products/red-hat-runtimes/": 1,
            "https://access.redhat.com/products/red-hat-jboss-enterprise-application-platform/": 1,
            "https://access.redhat.com/products/red-hat-data-grid/": 1,
            "https://access.redhat.com/products/red-hat-jboss-web-server/": 1,
            "https://access.redhat.com/products/red-hat-build-of-keycloak/": 1,
            "https://access.redhat.com/products/spring-boot/": 1,
            "https://access.redhat.com/products/nodejs/": 1,
            "https://access.redhat.com/products/quarkus/": 1,
            "https://access.redhat.com/products/red-hat-application-foundations/": 1,
            "https://access.redhat.com/products/red-hat-fuse/": 1,
            "https://access.redhat.com/products/red-hat-amq/": 1,
            "https://access.redhat.com/products/red-hat-3scale/": 1,
            "https://redhat.com/en": 1,
            "https://twitter.com/RedHat": 1,
            "https://access.redhat.com/management": 1,
            "https://access.redhat.com/support": 1,
            "https://access.redhat.com/support/customer-service": 1,
            "https://access.redhat.com/articles/33844": 1,
            "https://access.redhat.com/help/login_assistance": 1,
            "https://www.redhat.com/en/trust": 1,
            "https://www.redhat.com/en/about/browser-support": 1,
            "https://www.redhat.com/en/about/digital-accessibility": 1,
            "https://access.redhat.com/recognition/": 1,
            "https://access.redhat.com/help/colophon/": 1,
            "https://www.redhat.com/": 1,
            "http://developers.redhat.com/": 1,
            "https://connect.redhat.com/": 1,
            "https://cloud.redhat.com/": 1,
            "https://access.redhat.com/subscription-value": 1,
            "https://www.redhat.com/about/": 1,
            "http://jobs.redhat.com": 1,
            "https://redhat.com/en/about/company": 1,
            "https://redhat.com/en/jobs": 1,
            "https://redhat.com/en/events": 1,
            "https://redhat.com/en/about/office-locations": 1,
            "https://redhat.com/en/contact": 1,
            "https://redhat.com/en/blog": 1,
            "https://redhat.com/en/about/our-culture/diversity-equity-inclusion": 1,
            "https://coolstuff.redhat.com/": 1,
            "https://www.redhat.com/en/summit": 1,
            "https://redhat.com/en/about/privacy-policy": 1,
            "https://redhat.com/en/about/terms-use": 1,
            "https://redhat.com/en/about/all-policies-guidelines": 1,
            "https://redhat.com/en/about/digital-accessibility": 1,
            "commit::28ee665b3c0c9238b61a871064f024d54cddcc79": 1,
            "https://issues.apache.org/jira/browse/CASSANDRA-2967": 1,
            "https://issues.apache.org/jira/browse/CASSANDRA-9608": 1
        },
        "affected_products": [
            "Cassandra",
            "RMI",
            "Java",
            "Apache",
            "Apache Cassandra",
            "JMX"
        ],
        "versions": {
            "status": "affected",
            "version": "Apache Cassandra 3.8 to 3.11.1"
        },
        "files": [
            "CASSANDRA-12109",
            "CVE-2015-0225",
            "CASSANDRA-14173",
            "RMI"
        ],
        "keywords": [
            "java",
            "implement",
            "code",
            "cassandra",
            "network",
            "apache",
            "attacker",
            "bind",
            "regression",
            "default",
            "request",
            "release",
            "contain",
            "introduce",
            "issue",
            "configuration",
            "interface",
            "execute",
            "allow"
        ],
        "files_extension": [],
        "has_fixing_commit": true
    },
    "commits": [
        {
            "commit_id": "28ee665b3c0c9238b61a871064f024d54cddcc79",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516988558,
            "hunks": 14,
            "message": "Remove dependencies on JVM internals for JMX support Patch by Sam Tunnicliffe; reviewed by Jason Brown for CASSANDRA-14173",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/utils/JMXServerUtils.java b/src/java/org/apache/cassandra/utils/JMXServerUtils.java",
                "index e78ed01746..056bd6c50e 100644",
                "--- a/src/java/org/apache/cassandra/utils/JMXServerUtils.java",
                "+++ b/src/java/org/apache/cassandra/utils/JMXServerUtils.java",
                "@@ -26,6 +26,6 @@ import java.net.Inet6Address;",
                " import java.net.InetAddress;",
                "-import java.rmi.*;",
                "+import java.rmi.registry.LocateRegistry;",
                "+import java.rmi.registry.Registry;",
                " import java.rmi.server.RMIClientSocketFactory;",
                " import java.rmi.server.RMIServerSocketFactory;",
                "-import java.rmi.server.UnicastRemoteObject;",
                " import java.util.Arrays;",
                "@@ -36,2 +36,3 @@ import javax.management.remote.*;",
                " import javax.management.remote.rmi.RMIConnectorServer;",
                "+import javax.management.remote.rmi.RMIJRMPServerImpl;",
                " import javax.rmi.ssl.SslRMIClientSocketFactory;",
                "@@ -45,7 +46,4 @@ import org.slf4j.LoggerFactory;",
                "-import com.sun.jmx.remote.internal.RMIExporter;",
                " import com.sun.jmx.remote.security.JMXPluggableAuthenticator;",
                " import org.apache.cassandra.auth.jmx.AuthenticationProxy;",
                "-import sun.rmi.registry.RegistryImpl;",
                "-import sun.rmi.server.UnicastServerRef2;",
                "@@ -55,4 +53,2 @@ public class JMXServerUtils",
                "-    private static java.rmi.registry.Registry registry;",
                "-",
                "     /**",
                "@@ -61,2 +57,3 @@ public class JMXServerUtils",
                "      */",
                "+    @SuppressWarnings(\"resource\")",
                "     public static JMXConnectorServer createJMXServer(int port, boolean local)",
                "@@ -76,2 +73,6 @@ public class JMXServerUtils",
                "+        // configure the RMI registry to use the socket factories we just created",
                "+        Registry registry = LocateRegistry.createRegistry(port,",
                "+                                                          (RMIClientSocketFactory) env.get(RMIConnectorServer.RMI_CLIENT_SOCKET_FACTORY_ATTRIBUTE),",
                "+                                                          (RMIServerSocketFactory) env.get(RMIConnectorServer.RMI_SERVER_SOCKET_FACTORY_ATTRIBUTE));",
                "@@ -88,12 +89,29 @@ public class JMXServerUtils",
                "-        // Make sure we use our custom exporter so a full GC doesn't get scheduled every",
                "-        // sun.rmi.dgc.server.gcInterval millis (default is 3600000ms/1 hour)",
                "-        env.put(RMIExporter.EXPORTER_ATTRIBUTE, new Exporter());",
                "-",
                "-",
                "+        // Mark the JMX server as a permanently exported object. This allows the JVM to exit with the",
                "+        // server running and also exempts it from the distributed GC scheduler which otherwise would",
                "+        // potentially attempt a full GC every `sun.rmi.dgc.server.gcInterval` millis (default is 3600000ms)",
                "+        // For more background see:",
                "+        //   - CASSANDRA-2967",
                "+        //   - https://www.jclarity.com/2015/01/27/rmi-system-gc-unplugged/",
                "+        //   - https://bugs.openjdk.java.net/browse/JDK-6760712",
                "+        env.put(\"jmx.remote.x.daemon\", \"true\");",
                "+",
                "+        // Set the port used to create subsequent connections to exported objects over RMI. This simplifies",
                "+        // configuration in firewalled environments, but it can't be used in conjuction with SSL sockets.",
                "+        // See: CASSANDRA-7087",
                "         int rmiPort = Integer.getInteger(\"com.sun.management.jmxremote.rmi.port\", 0);",
                "-        JMXConnectorServer jmxServer =",
                "-            JMXConnectorServerFactory.newJMXConnectorServer(new JMXServiceURL(\"rmi\", null, rmiPort),",
                "-                                                            env,",
                "-                                                            ManagementFactory.getPlatformMBeanServer());",
                "+",
                "+        // We create the underlying RMIJRMPServerImpl so that we can manually bind it to the registry,",
                "+        // rather then specifying a binding address in the JMXServiceURL and letting it be done automatically",
                "+        // when the server is started. The reason for this is that if the registry is configured with SSL",
                "+        // sockets, the JMXConnectorServer acts as its client during the binding which means it needs to",
                "+        // have a truststore configured which contains the registry's certificate. Manually binding removes",
                "+        // this problem.",
                "+        // See CASSANDRA-12109.",
                "+        RMIJRMPServerImpl server = new RMIJRMPServerImpl(rmiPort,",
                "+                                                         (RMIClientSocketFactory) env.get(RMIConnectorServer.RMI_CLIENT_SOCKET_FACTORY_ATTRIBUTE),",
                "+                                                         (RMIServerSocketFactory) env.get(RMIConnectorServer.RMI_SERVER_SOCKET_FACTORY_ATTRIBUTE),",
                "+                                                         env);",
                "+        JMXServiceURL serviceURL = new JMXServiceURL(\"rmi\", null, rmiPort);",
                "+        RMIConnectorServer jmxServer = new RMIConnectorServer(serviceURL, env, server, ManagementFactory.getPlatformMBeanServer());",
                "@@ -102,8 +120,5 @@ public class JMXServerUtils",
                "             jmxServer.setMBeanServerForwarder(authzProxy);",
                "-",
                "         jmxServer.start();",
                "-        // use a custom Registry to avoid having to interact with it internally using the remoting interface",
                "-        configureRMIRegistry(port, env);",
                "-",
                "+        registry.rebind(\"jmxrmi\", server);",
                "         logJmxServiceUrl(serverAddress, port);",
                "@@ -112,20 +127,2 @@ public class JMXServerUtils",
                "-    private static void configureRMIRegistry(int port, Map<String, Object> env) throws RemoteException",
                "-    {",
                "-        Exporter exporter = (Exporter)env.get(RMIExporter.EXPORTER_ATTRIBUTE);",
                "-        // If ssl is enabled, make sure it's also in place for the RMI registry",
                "-        // by using the SSL socket factories already created and stashed in env",
                "-        if (Boolean.getBoolean(\"com.sun.management.jmxremote.ssl\"))",
                "-        {",
                "-            registry = new Registry(port,",
                "-                                   (RMIClientSocketFactory)env.get(RMIConnectorServer.RMI_CLIENT_SOCKET_FACTORY_ATTRIBUTE),",
                "-                                   (RMIServerSocketFactory)env.get(RMIConnectorServer.RMI_SERVER_SOCKET_FACTORY_ATTRIBUTE),",
                "-                                   exporter.connectorServer);",
                "-        }",
                "-        else",
                "-        {",
                "-            registry = new Registry(port, exporter.connectorServer);",
                "-        }",
                "-    }",
                "-",
                "     private static Map<String, Object> configureJmxAuthentication()",
                "@@ -277,113 +274,2 @@ public class JMXServerUtils",
                "     }",
                "-",
                "-    /**",
                "-     * In the RMI subsystem, the ObjectTable instance holds references to remote",
                "-     * objects for distributed garbage collection purposes. When objects are",
                "-     * added to the ObjectTable (exported), a flag is passed to * indicate the",
                "-     * \"permanence\" of that object. Exporting as permanent has two effects; the",
                "-     * object is not eligible for distributed garbage collection, and its",
                "-     * existence will not prevent the JVM from exiting after termination of all",
                "-     * non-daemon threads terminate. Neither of these is bad for our case, as we",
                "-     * attach the server exactly once (i.e. at startup, not subsequently using",
                "-     * the Attach API) and don't disconnect it before shutdown. The primary",
                "-     * benefit we gain is that it doesn't trigger the scheduled full GC that",
                "-     * is otherwise incurred by programatically configuring the management server.",
                "-     *",
                "-     * To that end, we use this private implementation of RMIExporter to register",
                "-     * our JMXConnectorServer as a permanent object by adding it to the map of",
                "-     * environment variables under the key RMIExporter.EXPORTER_ATTRIBUTE",
                "-     * (com.sun.jmx.remote.rmi.exporter) prior to calling server.start()",
                "-     *",
                "-     * See also:",
                "-     *  * CASSANDRA-2967 for background",
                "-     *  * https://www.jclarity.com/2015/01/27/rmi-system-gc-unplugged/ for more detail",
                "-     *  * https://bugs.openjdk.java.net/browse/JDK-6760712 for info on setting the exporter",
                "-     *  * sun.management.remote.ConnectorBootstrap to trace how the inbuilt management agent",
                "-     *    sets up the JMXConnectorServer",
                "-     */",
                "-    private static class Exporter implements RMIExporter",
                "-    {",
                "-        // the first object to be exported by this instance is *always* the JMXConnectorServer",
                "-        // instance created by createJMXServer. Keep a handle to it, as it needs to be supplied",
                "-        // to our custom Registry too.",
                "-        private Remote connectorServer;",
                "-",
                "-        public Remote exportObject(Remote obj, int port, RMIClientSocketFactory csf, RMIServerSocketFactory ssf)",
                "-        throws RemoteException",
                "-        {",
                "-            Remote remote = new UnicastServerRef2(port, csf, ssf).exportObject(obj, null, true);",
                "-            // Keep a reference to the first object exported, the JMXConnectorServer",
                "-            if (connectorServer == null)",
                "-                connectorServer = remote;",
                "-",
                "-            return remote;",
                "-        }",
                "-",
                "-        public boolean unexportObject(Remote obj, boolean force) throws NoSuchObjectException",
                "-        {",
                "-            return UnicastRemoteObject.unexportObject(obj, force);",
                "-        }",
                "-    }",
                "-",
                "-    /**",
                "-     * Using this class avoids the necessity to interact with the registry via its",
                "-     * remoting interface. This is necessary because when SSL is enabled for the registry,",
                "-     * that remote interaction is treated just the same as one from an external client.",
                "-     * That is problematic when binding the JMXConnectorServer to the Registry as it requires",
                "-     * the client, which in this case is our own internal code, to connect like any other SSL",
                "-     * client, meaning we need a truststore containing our own certificate.",
                "-     * This bypasses the binding API completely, which emulates the behaviour of",
                "-     * ConnectorBootstrap when the subsystem is initialized by the JVM Agent directly.",
                "-     *",
                "-     * See CASSANDRA-12109.",
                "-     */",
                "-    private static class Registry extends RegistryImpl",
                "-    {",
                "-        private final static String KEY = \"jmxrmi\";",
                "-        private final Remote connectorServer;",
                "-",
                "-        private Registry(int port, Remote connectorServer) throws RemoteException",
                "-        {",
                "-            super(port);",
                "-            this.connectorServer = connectorServer;",
                "-        }",
                "-",
                "-        private Registry(int port,",
                "-                         RMIClientSocketFactory csf,",
                "-                         RMIServerSocketFactory ssf,",
                "-                         Remote connectorServer) throws RemoteException",
                "-        {",
                "-            super(port, csf, ssf);",
                "-            this.connectorServer = connectorServer;",
                "-        }",
                "-",
                "-        public Remote lookup(String name) throws RemoteException, NotBoundException",
                "-        {",
                "-            if (name.equals(KEY))",
                "-                return connectorServer;",
                "-",
                "-            throw new NotBoundException(String.format(\"Only the JMX Connector Server named %s \" +",
                "-                                                      \"is bound in this registry\", KEY));",
                "-        }",
                "-",
                "-        public void bind(String name, Remote obj) throws RemoteException, AlreadyBoundException",
                "-        {",
                "-            throw new UnsupportedOperationException(\"Unsupported\");",
                "-        }",
                "-",
                "-        public void unbind(String name) throws RemoteException, NotBoundException",
                "-        {",
                "-            throw new UnsupportedOperationException(\"Unsupported\");",
                "-        }",
                "-",
                "-        public void rebind(String name, Remote obj) throws RemoteException",
                "-        {",
                "-            throw new UnsupportedOperationException(\"Unsupported\");",
                "-        }",
                "-",
                "-        public String[] list() throws RemoteException",
                "-        {",
                "-            return new String[] {KEY};",
                "-        }",
                "-    }",
                " }"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/utils/JMXServerUtils.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14173": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "XREF_BUG",
                    "message": "The commit and the advisory (including referenced pages) mention the same bug tracking ticket: CASSANDRA-14173",
                    "relevance": 32
                },
                {
                    "id": "COMMIT_IN_REFERENCE",
                    "message": "This commit is mentioned 1 times in the references.",
                    "relevance": 64
                },
                {
                    "id": "RELEVANT_WORDS_IN_MESSAGE",
                    "message": "The commit message contains some relevant words: CASSANDRA-14173",
                    "relevance": 8
                },
                {
                    "id": "CHANGES_RELEVANT_CODE",
                    "message": "The commit modifies code containing relevant filename or methods: CASSANDRA-12109, RMI",
                    "relevance": 8
                },
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14173",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "9d649d69a56a91fcb06a3582b22606f0fe361f49",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518116498,
            "hunks": 20,
            "message": "Count deleted rows scanned during reads for tracing and tombstone thresholds. If a row is read but is not live anymore (which happens with row level tombstones) it is not counted anywhere in the metrics nor reported in tracing. Row tombstones themselves are not reported anywhere. The consequence is that some delete heavy workloads will show no tombstone read but endure severe performance issues. This commit counts deleted rows as standard tombstone cells. Patch by Alexander Dejanovski; Reviewed by Jon Haddad for CASSANDRA-8527",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/db/ReadCommand.java b/src/java/org/apache/cassandra/db/ReadCommand.java",
                "index ab8779ef69..c8b256ae04 100644",
                "--- a/src/java/org/apache/cassandra/db/ReadCommand.java",
                "+++ b/src/java/org/apache/cassandra/db/ReadCommand.java",
                "@@ -487,2 +487,10 @@ public abstract class ReadCommand extends MonitorableImpl implements ReadQuery",
                "+            /**",
                "+             * Count the number of live rows returned by the read command and the number of tombstones.",
                "+             *",
                "+             * Tombstones come in two forms on rows :",
                "+             * - cells that aren't live anymore (either expired through TTL or deleted) : 1 tombstone per cell",
                "+             * - Rows that aren't live and have no cell (DELETEs performed on the primary key) : 1 tombstone per row ",
                "+             * We avoid counting rows as tombstones if they contain nothing but expired cells.",
                "+             */",
                "             @Override",
                "@@ -490,5 +498,3 @@ public abstract class ReadCommand extends MonitorableImpl implements ReadQuery",
                "             {",
                "-                if (row.hasLiveData(ReadCommand.this.nowInSec(), enforceStrictLiveness))",
                "-                    ++liveRows;",
                "-",
                "+                boolean hasTombstones = false;",
                "                 for (Cell cell : row.cells())",
                "@@ -496,4 +502,18 @@ public abstract class ReadCommand extends MonitorableImpl implements ReadQuery",
                "                     if (!cell.isLive(ReadCommand.this.nowInSec()))",
                "+                    {",
                "                         countTombstone(row.clustering());",
                "+                        hasTombstones = true; // allows to avoid counting an extra tombstone if the whole row expired",
                "+                    }",
                "                 }",
                "+",
                "+                if (row.hasLiveData(ReadCommand.this.nowInSec(), enforceStrictLiveness))",
                "+                    ++liveRows;",
                "+                else if (!row.primaryKeyLivenessInfo().isLive(ReadCommand.this.nowInSec())",
                "+                        && row.hasDeletion(ReadCommand.this.nowInSec())",
                "+                        && !hasTombstones)",
                "+                {",
                "+                    // We're counting primary key deletions only here.",
                "+                    countTombstone(row.clustering());",
                "+                }",
                "+",
                "                 return row;",
                "@@ -530,3 +550,5 @@ public abstract class ReadCommand extends MonitorableImpl implements ReadQuery",
                "                 {",
                "-                    String msg = String.format(\"Read %d live rows and %d tombstone cells for query %1.512s (see tombstone_warn_threshold)\", liveRows, tombstones, ReadCommand.this.toCQLString());",
                "+                    String msg = String.format(",
                "+                            \"Read %d live rows and %d tombstone cells for query %1.512s (see tombstone_warn_threshold)\",",
                "+                            liveRows, tombstones, ReadCommand.this.toCQLString());",
                "                     ClientWarn.instance.warn(msg);",
                "@@ -535,3 +557,5 @@ public abstract class ReadCommand extends MonitorableImpl implements ReadQuery",
                "-                Tracing.trace(\"Read {} live and {} tombstone cells{}\", liveRows, tombstones, (warnTombstones ? \" (see tombstone_warn_threshold)\" : \"\"));",
                "+                Tracing.trace(\"Read {} live rows and {} tombstone cells{}\",",
                "+                        liveRows, tombstones,",
                "+                        (warnTombstones ? \" (see tombstone_warn_threshold)\" : \"\"));",
                "             }",
                "diff --git a/src/java/org/apache/cassandra/metrics/TableMetrics.java b/src/java/org/apache/cassandra/metrics/TableMetrics.java",
                "index e78bb665c6..620ef72344 100644",
                "--- a/src/java/org/apache/cassandra/metrics/TableMetrics.java",
                "+++ b/src/java/org/apache/cassandra/metrics/TableMetrics.java",
                "@@ -124,3 +124,3 @@ public class TableMetrics",
                "     public final TableHistogram tombstoneScannedHistogram;",
                "-    /** Live cells scanned in queries on this CF */",
                "+    /** Live rows scanned in queries on this CF */",
                "     public final TableHistogram liveScannedHistogram;",
                "diff --git a/test/unit/org/apache/cassandra/db/ReadCommandTest.java b/test/unit/org/apache/cassandra/db/ReadCommandTest.java",
                "index 92642974c5..960539c9f0 100644",
                "--- a/test/unit/org/apache/cassandra/db/ReadCommandTest.java",
                "+++ b/test/unit/org/apache/cassandra/db/ReadCommandTest.java",
                "@@ -20,2 +20,3 @@ package org.apache.cassandra.db;",
                "+import java.io.IOException;",
                " import java.nio.ByteBuffer;",
                "@@ -212,5 +213,135 @@ public class ReadCommandTest",
                "         String[] expectedRows = new String[] { \"aa\", \"ff\", \"ee\", \"cc\", \"dd\", \"cc\", \"bb\"};",
                "+        int nowInSeconds = FBUtilities.nowInSeconds();",
                "-        List<ByteBuffer> buffers = new ArrayList<>(groups.length);",
                "+        List<UnfilteredPartitionIterator> iterators = writeAndThenReadPartitions(cfs, groups, nowInSeconds);",
                "+        UnfilteredPartitionIterators.MergeListener listener =",
                "+            new UnfilteredPartitionIterators.MergeListener()",
                "+            {",
                "+                public UnfilteredRowIterators.MergeListener getRowMergeListener(DecoratedKey partitionKey, List<UnfilteredRowIterator> versions)",
                "+                {",
                "+                    return null;",
                "+                }",
                "+",
                "+                public void close()",
                "+                {",
                "+",
                "+                }",
                "+            };",
                "+",
                "+        try (PartitionIterator partitionIterator = UnfilteredPartitionIterators.filter(UnfilteredPartitionIterators.merge(iterators, nowInSeconds, listener), nowInSeconds))",
                "+        {",
                "+",
                "+            int i = 0;",
                "+            int numPartitions = 0;",
                "+            while (partitionIterator.hasNext())",
                "+            {",
                "+                numPartitions++;",
                "+                try(RowIterator rowIterator = partitionIterator.next())",
                "+                {",
                "+                    while (rowIterator.hasNext())",
                "+                    {",
                "+                        Row row = rowIterator.next();",
                "+                        assertEquals(\"col=\" + expectedRows[i++], row.clustering().toString(cfs.metadata));",
                "+                        //System.out.print(row.toString(cfs.metadata, true));",
                "+                    }",
                "+                }",
                "+            }",
                "+",
                "+            assertEquals(5, numPartitions);",
                "+            assertEquals(expectedRows.length, i);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * This test will create several partitions with several rows each. Then, it will perform up to 5 row deletions on",
                "+     * some partitions. We check that when reading the partitions, the maximum number of tombstones reported in the",
                "+     * metrics is indeed equal to 5.",
                "+     */",
                "+    @Test",
                "+    public void testCountDeletedRows() throws Exception",
                "+    {",
                "+        ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(CF3);",
                "+",
                "+        String[][][] groups = new String[][][] {",
                "+                new String[][] {",
                "+                        new String[] { \"1\", \"key1\", \"aa\", \"a\" }, // \"1\" indicates to create the data, \"-1\" to delete the",
                "+                                                                 // row",
                "+                        new String[] { \"1\", \"key2\", \"bb\", \"b\" },",
                "+                        new String[] { \"1\", \"key3\", \"cc\", \"c\" }",
                "+                },",
                "+                new String[][] {",
                "+                        new String[] { \"1\", \"key3\", \"dd\", \"d\" },",
                "+                        new String[] { \"1\", \"key2\", \"ee\", \"e\" },",
                "+                        new String[] { \"1\", \"key1\", \"ff\", \"f\" }",
                "+                },",
                "+                new String[][] {",
                "+                        new String[] { \"1\", \"key6\", \"aa\", \"a\" },",
                "+                        new String[] { \"1\", \"key5\", \"bb\", \"b\" },",
                "+                        new String[] { \"1\", \"key4\", \"cc\", \"c\" }",
                "+                },",
                "+                new String[][] {",
                "+                        new String[] { \"1\", \"key2\", \"aa\", \"a\" },",
                "+                        new String[] { \"1\", \"key2\", \"cc\", \"c\" },",
                "+                        new String[] { \"1\", \"key2\", \"dd\", \"d\" }",
                "+                },",
                "+                new String[][] {",
                "+                        new String[] { \"-1\", \"key6\", \"aa\", \"a\" },",
                "+                        new String[] { \"-1\", \"key2\", \"bb\", \"b\" },",
                "+                        new String[] { \"-1\", \"key2\", \"ee\", \"e\" },",
                "+                        new String[] { \"-1\", \"key2\", \"aa\", \"a\" },",
                "+                        new String[] { \"-1\", \"key2\", \"cc\", \"c\" },",
                "+                        new String[] { \"-1\", \"key2\", \"dd\", \"d\" }",
                "+                }",
                "+        };",
                "         int nowInSeconds = FBUtilities.nowInSeconds();",
                "+",
                "+        writeAndThenReadPartitions(cfs, groups, nowInSeconds);",
                "+",
                "+        assertEquals(5, cfs.metric.tombstoneScannedHistogram.cf.getSnapshot().getMax());",
                "+    }",
                "+",
                "+    /**",
                "+     * This test will create several partitions with several rows each and no deletions. We check that when reading the",
                "+     * partitions, the maximum number of tombstones reported in the metrics is equal to 1, which is apparently the",
                "+     * default max value for histograms in the metrics lib (equivalent to having no element reported).",
                "+     */",
                "+    @Test",
                "+    public void testCountWithNoDeletedRow() throws Exception",
                "+    {",
                "+        ColumnFamilyStore cfs = Keyspace.open(KEYSPACE).getColumnFamilyStore(CF3);",
                "+",
                "+        String[][][] groups = new String[][][] {",
                "+                new String[][] {",
                "+                        new String[] { \"1\", \"key1\", \"aa\", \"a\" }, // \"1\" indicates to create the data, \"-1\" to delete the",
                "+                                                                 // row",
                "+                        new String[] { \"1\", \"key2\", \"bb\", \"b\" },",
                "+                        new String[] { \"1\", \"key3\", \"cc\", \"c\" }",
                "+                },",
                "+                new String[][] {",
                "+                        new String[] { \"1\", \"key3\", \"dd\", \"d\" },",
                "+                        new String[] { \"1\", \"key2\", \"ee\", \"e\" },",
                "+                        new String[] { \"1\", \"key1\", \"ff\", \"f\" }",
                "+                },",
                "+                new String[][] {",
                "+                        new String[] { \"1\", \"key6\", \"aa\", \"a\" },",
                "+                        new String[] { \"1\", \"key5\", \"bb\", \"b\" },",
                "+                        new String[] { \"1\", \"key4\", \"cc\", \"c\" }",
                "+                }",
                "+        };",
                "+",
                "+        int nowInSeconds = FBUtilities.nowInSeconds();",
                "+",
                "+        writeAndThenReadPartitions(cfs, groups, nowInSeconds);",
                "+",
                "+        assertEquals(1, cfs.metric.tombstoneScannedHistogram.cf.getSnapshot().getMax());",
                "+    }",
                "+",
                "+    /**",
                "+     * Writes rows to the column family store using the groups as input and then reads them. Returns the iterators from",
                "+     * the read.",
                "+     */",
                "+    private List<UnfilteredPartitionIterator> writeAndThenReadPartitions(ColumnFamilyStore cfs, String[][][] groups,",
                "+            int nowInSeconds) throws IOException",
                "+    {",
                "+        List<ByteBuffer> buffers = new ArrayList<>(groups.length);",
                "         ColumnFilter columnFilter = ColumnFilter.allColumnsBuilder(cfs.metadata).build();",
                "@@ -218,3 +349,4 @@ public class ReadCommandTest",
                "         Slice slice = Slice.make(ClusteringBound.BOTTOM, ClusteringBound.TOP);",
                "-        ClusteringIndexSliceFilter sliceFilter = new ClusteringIndexSliceFilter(Slices.with(cfs.metadata.comparator, slice), false);",
                "+        ClusteringIndexSliceFilter sliceFilter = new ClusteringIndexSliceFilter(",
                "+                Slices.with(cfs.metadata.comparator, slice), false);",
                "@@ -231,6 +363,6 @@ public class ReadCommandTest",
                "                     new RowUpdateBuilder(cfs.metadata, 0, ByteBufferUtil.bytes(data[1]))",
                "-                    .clustering(data[2])",
                "-                    .add(data[3], ByteBufferUtil.bytes(\"blah\"))",
                "-                    .build()",
                "-                    .apply();",
                "+                            .clustering(data[2])",
                "+                            .add(data[3], ByteBufferUtil.bytes(\"blah\"))",
                "+                            .build()",
                "+                            .apply();",
                "                 }",
                "@@ -238,5 +370,7 @@ public class ReadCommandTest",
                "                 {",
                "-                    RowUpdateBuilder.deleteRow(cfs.metadata, FBUtilities.timestampMicros(), ByteBufferUtil.bytes(data[1]), data[2]).apply();",
                "+                    RowUpdateBuilder.deleteRow(cfs.metadata, FBUtilities.timestampMicros(),",
                "+                            ByteBufferUtil.bytes(data[1]), data[2]).apply();",
                "                 }",
                "-                commands.add(SinglePartitionReadCommand.create(cfs.metadata, nowInSeconds, columnFilter, rowFilter, DataLimits.NONE, Util.dk(data[1]), sliceFilter));",
                "+                commands.add(SinglePartitionReadCommand.create(cfs.metadata, nowInSeconds, columnFilter, rowFilter,",
                "+                        DataLimits.NONE, Util.dk(data[1]), sliceFilter));",
                "             }",
                "@@ -248,9 +382,9 @@ public class ReadCommandTest",
                "             try (ReadExecutionController executionController = query.executionController();",
                "-                 UnfilteredPartitionIterator iter = query.executeLocally(executionController);",
                "-                 DataOutputBuffer buffer = new DataOutputBuffer())",
                "+                    UnfilteredPartitionIterator iter = query.executeLocally(executionController);",
                "+                    DataOutputBuffer buffer = new DataOutputBuffer())",
                "             {",
                "                 UnfilteredPartitionIterators.serializerForIntraNode().serialize(iter,",
                "-                                                                                columnFilter,",
                "-                                                                                buffer,",
                "-                                                                                MessagingService.current_version);",
                "+                        columnFilter,",
                "+                        buffer,",
                "+                        MessagingService.current_version);",
                "                 buffers.add(buffer.buffer());",
                "@@ -267,6 +401,6 @@ public class ReadCommandTest",
                "                 iterators.add(UnfilteredPartitionIterators.serializerForIntraNode().deserialize(in,",
                "-                                                                                                MessagingService.current_version,",
                "-                                                                                                cfs.metadata,",
                "-                                                                                                columnFilter,",
                "-                                                                                                SerializationHelper.Flag.LOCAL));",
                "+                        MessagingService.current_version,",
                "+                        cfs.metadata,",
                "+                        columnFilter,",
                "+                        SerializationHelper.Flag.LOCAL));",
                "             }",
                "@@ -274,39 +408,5 @@ public class ReadCommandTest",
                "-        UnfilteredPartitionIterators.MergeListener listener =",
                "-            new UnfilteredPartitionIterators.MergeListener()",
                "-            {",
                "-                public UnfilteredRowIterators.MergeListener getRowMergeListener(DecoratedKey partitionKey, List<UnfilteredRowIterator> versions)",
                "-                {",
                "-                    return null;",
                "-                }",
                "-",
                "-                public void close()",
                "-                {",
                "-",
                "-                }",
                "-            };",
                "-",
                "-        try (PartitionIterator partitionIterator = UnfilteredPartitionIterators.filter(UnfilteredPartitionIterators.merge(iterators, nowInSeconds, listener), nowInSeconds))",
                "-        {",
                "-",
                "-            int i = 0;",
                "-            int numPartitions = 0;",
                "-            while (partitionIterator.hasNext())",
                "-            {",
                "-                numPartitions++;",
                "-                try(RowIterator rowIterator = partitionIterator.next())",
                "-                {",
                "-                    while (rowIterator.hasNext())",
                "-                    {",
                "-                        Row row = rowIterator.next();",
                "-                        assertEquals(\"col=\" + expectedRows[i++], row.clustering().toString(cfs.metadata));",
                "-                        //System.out.print(row.toString(cfs.metadata, true));",
                "-                    }",
                "-                }",
                "-            }",
                "-",
                "-            assertEquals(5, numPartitions);",
                "-            assertEquals(expectedRows.length, i);",
                "-        }",
                "+        return iterators;",
                "     }",
                "+",
                " }"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/db/ReadCommand.java",
                "src/java/org/apache/cassandra/metrics/TableMetrics.java",
                "test/unit/org/apache/cassandra/db/ReadCommandTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-8527": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [
                [
                    "no-tag",
                    "b2d20d4bb10e73935a97d6fbd848e4cb649c105c"
                ]
            ],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: issue",
                    "relevance": 4
                },
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "COMMIT_HAS_TWINS",
                    "message": "This commit has one or more twins.",
                    "relevance": 2
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-8527",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7533ff089ab913262e7a540cc778d7f67ce17ce1",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1515891376,
            "hunks": 43,
            "message": "Remove unused on-heap Bloomfilter implementation patch by Jay Zhuang; reviewed by jasobrown for CASSANDRA-14152",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java b/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java",
                "index 5ab396472f..6722e1bfc4 100644",
                "--- a/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java",
                "+++ b/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java",
                "@@ -743,3 +743,3 @@ public abstract class SSTableReader extends SSTable implements SelfRefCounted<SS",
                "         {",
                "-            bf = FilterFactory.deserialize(stream, true);",
                "+            bf = FilterFactory.deserialize(stream);",
                "         }",
                "@@ -829,3 +829,3 @@ public abstract class SSTableReader extends SSTable implements SelfRefCounted<SS",
                "             if (recreateBloomFilter)",
                "-                bf = FilterFactory.getFilter(estimatedKeys, metadata().params.bloomFilterFpChance, true);",
                "+                bf = FilterFactory.getFilter(estimatedKeys, metadata().params.bloomFilterFpChance);",
                "diff --git a/src/java/org/apache/cassandra/io/sstable/format/big/BigTableWriter.java b/src/java/org/apache/cassandra/io/sstable/format/big/BigTableWriter.java",
                "index 4ae43311ed..04c7bbfd14 100644",
                "--- a/src/java/org/apache/cassandra/io/sstable/format/big/BigTableWriter.java",
                "+++ b/src/java/org/apache/cassandra/io/sstable/format/big/BigTableWriter.java",
                "@@ -441,3 +441,3 @@ public class BigTableWriter extends SSTableWriter",
                "             summary = new IndexSummaryBuilder(keyCount, metadata().params.minIndexInterval, Downsampling.BASE_SAMPLING_LEVEL);",
                "-            bf = FilterFactory.getFilter(keyCount, metadata().params.bloomFilterFpChance, true);",
                "+            bf = FilterFactory.getFilter(keyCount, metadata().params.bloomFilterFpChance);",
                "             // register listeners to be alerted when the data files are flushed",
                "diff --git a/src/java/org/apache/cassandra/utils/BloomFilterSerializer.java b/src/java/org/apache/cassandra/utils/BloomFilterSerializer.java",
                "index 17ab1232f0..9e9d15ae92 100644",
                "--- a/src/java/org/apache/cassandra/utils/BloomFilterSerializer.java",
                "+++ b/src/java/org/apache/cassandra/utils/BloomFilterSerializer.java",
                "@@ -26,3 +26,2 @@ import org.apache.cassandra.utils.obs.IBitSet;",
                " import org.apache.cassandra.utils.obs.OffHeapBitSet;",
                "-import org.apache.cassandra.utils.obs.OpenBitSet;",
                "@@ -40,12 +39,7 @@ final class BloomFilterSerializer",
                "-    public static BloomFilter deserialize(DataInput in) throws IOException",
                "-    {",
                "-        return deserialize(in, false);",
                "-    }",
                "-",
                "     @SuppressWarnings(\"resource\")",
                "-    public static BloomFilter deserialize(DataInput in, boolean offheap) throws IOException",
                "+    public static BloomFilter deserialize(DataInput in) throws IOException",
                "     {",
                "         int hashes = in.readInt();",
                "-        IBitSet bs = offheap ? OffHeapBitSet.deserialize(in) : OpenBitSet.deserialize(in);",
                "+        IBitSet bs = OffHeapBitSet.deserialize(in);",
                "diff --git a/src/java/org/apache/cassandra/utils/FilterFactory.java b/src/java/org/apache/cassandra/utils/FilterFactory.java",
                "index f79f720681..947945228c 100644",
                "--- a/src/java/org/apache/cassandra/utils/FilterFactory.java",
                "+++ b/src/java/org/apache/cassandra/utils/FilterFactory.java",
                "@@ -28,3 +28,2 @@ import org.apache.cassandra.utils.obs.IBitSet;",
                " import org.apache.cassandra.utils.obs.OffHeapBitSet;",
                "-import org.apache.cassandra.utils.obs.OpenBitSet;",
                "@@ -42,5 +41,5 @@ public class FilterFactory",
                "-    public static IFilter deserialize(DataInput input, boolean offheap) throws IOException",
                "+    public static IFilter deserialize(DataInput input) throws IOException",
                "     {",
                "-        return BloomFilterSerializer.deserialize(input, offheap);",
                "+        return BloomFilterSerializer.deserialize(input);",
                "     }",
                "@@ -51,3 +50,3 @@ public class FilterFactory",
                "      */",
                "-    public static IFilter getFilter(long numElements, int targetBucketsPerElem, boolean offheap)",
                "+    public static IFilter getFilter(long numElements, int targetBucketsPerElem)",
                "     {",
                "@@ -60,3 +59,3 @@ public class FilterFactory",
                "         BloomCalculations.BloomSpecification spec = BloomCalculations.computeBloomSpec(bucketsPerElement);",
                "-        return createFilter(spec.K, numElements, spec.bucketsPerElement, offheap);",
                "+        return createFilter(spec.K, numElements, spec.bucketsPerElement);",
                "     }",
                "@@ -70,3 +69,3 @@ public class FilterFactory",
                "      */",
                "-    public static IFilter getFilter(long numElements, double maxFalsePosProbability, boolean offheap)",
                "+    public static IFilter getFilter(long numElements, double maxFalsePosProbability)",
                "     {",
                "@@ -77,3 +76,3 @@ public class FilterFactory",
                "         BloomCalculations.BloomSpecification spec = BloomCalculations.computeBloomSpec(bucketsPerElement, maxFalsePosProbability);",
                "-        return createFilter(spec.K, numElements, spec.bucketsPerElement, offheap);",
                "+        return createFilter(spec.K, numElements, spec.bucketsPerElement);",
                "     }",
                "@@ -81,6 +80,6 @@ public class FilterFactory",
                "     @SuppressWarnings(\"resource\")",
                "-    private static IFilter createFilter(int hash, long numElements, int bucketsPer, boolean offheap)",
                "+    private static IFilter createFilter(int hash, long numElements, int bucketsPer)",
                "     {",
                "         long numBits = (numElements * bucketsPer) + BITSET_EXCESS;",
                "-        IBitSet bitset = offheap ? new OffHeapBitSet(numBits) : new OpenBitSet(numBits);",
                "+        IBitSet bitset = new OffHeapBitSet(numBits);",
                "         return new BloomFilter(hash, bitset);",
                "diff --git a/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java b/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java",
                "index 8593a11dd1..d2c15ca93d 100644",
                "--- a/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java",
                "+++ b/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java",
                "@@ -37,4 +37,4 @@ public class OffHeapBitSet implements IBitSet",
                "     {",
                "-        // OpenBitSet.bits2words calculation is there for backward compatibility.",
                "-        long wordCount = OpenBitSet.bits2words(numBits);",
                "+        /** returns the number of 64 bit words it would take to hold numBits */",
                "+        long wordCount = (((numBits - 1) >>> 6) + 1);",
                "         if (wordCount > Integer.MAX_VALUE)",
                "diff --git a/src/java/org/apache/cassandra/utils/obs/OpenBitSet.java b/src/java/org/apache/cassandra/utils/obs/OpenBitSet.java",
                "deleted file mode 100644",
                "index a21729aeda..0000000000",
                "--- a/src/java/org/apache/cassandra/utils/obs/OpenBitSet.java",
                "+++ /dev/null",
                "@@ -1,494 +0,0 @@",
                "-/*",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- *     http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-package org.apache.cassandra.utils.obs;",
                "-",
                "-import java.util.Arrays;",
                "-import java.io.DataInput;",
                "-import java.io.DataOutput;",
                "-import java.io.IOException;",
                "-",
                "-import org.apache.cassandra.db.TypeSizes;",
                "-import org.apache.cassandra.utils.concurrent.Ref;",
                "-",
                "-/**",
                "- * <p>",
                "- * An \"open\" BitSet implementation that allows direct access to the arrays of words",
                "- * storing the bits.  Derived from Lucene's OpenBitSet, but with a paged backing array",
                "- * (see bits delaration, below).",
                "- * </p>",
                "- * <p>",
                "- * Unlike java.util.bitset, the fact that bits are packed into an array of longs",
                "- * is part of the interface.  This allows efficient implementation of other algorithms",
                "- * by someone other than the author.  It also allows one to efficiently implement",
                "- * alternate serialization or interchange formats.",
                "- * </p>",
                "- * <p>",
                "- * <code>OpenBitSet</code> is faster than <code>java.util.BitSet</code> in most operations",
                "- * and *much* faster at calculating cardinality of sets and results of set operations.",
                "- * It can also handle sets of larger cardinality (up to 64 * 2**32-1)",
                "- * </p>",
                "- * <p>",
                "- * The goals of <code>OpenBitSet</code> are the fastest implementation possible, and",
                "- * maximum code reuse.  Extra safety and encapsulation",
                "- * may always be built on top, but if that's built in, the cost can never be removed (and",
                "- * hence people re-implement their own version in order to get better performance).",
                "- * If you want a \"safe\", totally encapsulated (and slower and limited) BitSet",
                "- * class, use <code>java.util.BitSet</code>.",
                "- * </p>",
                "- */",
                "-",
                "-public class OpenBitSet implements IBitSet",
                "-{",
                "-  /**",
                "-   * We break the bitset up into multiple arrays to avoid promotion failure caused by attempting to allocate",
                "-   * large, contiguous arrays (CASSANDRA-2466).  All sub-arrays but the last are uniformly PAGE_SIZE words;",
                "-   * to avoid waste in small bloom filters (of which Cassandra has many: one per row) the last sub-array",
                "-   * is sized to exactly the remaining number of words required to achieve the desired set size (CASSANDRA-3618).",
                "-   */",
                "-  private final long[][] bits;",
                "-  private int wlen; // number of words (elements) used in the array",
                "-  private final int pageCount;",
                "-  private static final int PAGE_SIZE = 4096;",
                "-",
                "-  /**",
                "-   * Constructs an OpenBitSet large enough to hold numBits.",
                "-   * @param numBits",
                "-   */",
                "-  public OpenBitSet(long numBits)",
                "-  {",
                "-      wlen = (int) bits2words(numBits);",
                "-      int lastPageSize = wlen % PAGE_SIZE;",
                "-      int fullPageCount = wlen / PAGE_SIZE;",
                "-      pageCount = fullPageCount + (lastPageSize == 0 ? 0 : 1);",
                "-",
                "-      bits = new long[pageCount][];",
                "-",
                "-      for (int i = 0; i < fullPageCount; ++i)",
                "-          bits[i] = new long[PAGE_SIZE];",
                "-",
                "-      if (lastPageSize != 0)",
                "-          bits[bits.length - 1] = new long[lastPageSize];",
                "-  }",
                "-",
                "-  public OpenBitSet()",
                "-  {",
                "-    this(64);",
                "-  }",
                "-",
                "-  /**",
                "-   * @return the pageSize",
                "-   */",
                "-  public int getPageSize()",
                "-  {",
                "-      return PAGE_SIZE;",
                "-  }",
                "-",
                "-  public int getPageCount()",
                "-  {",
                "-      return pageCount;",
                "-  }",
                "-",
                "-  public long[] getPage(int pageIdx)",
                "-  {",
                "-      return bits[pageIdx];",
                "-  }",
                "-",
                "-  /** Returns the current capacity in bits (1 greater than the index of the last bit) */",
                "-  public long capacity() { return ((long)wlen) << 6; }",
                "-",
                "-  @Override",
                "-  public long offHeapSize()",
                "-  {",
                "-      return 0;",
                "-  }",
                "-",
                "-    public void addTo(Ref.IdentityCollection identities)",
                "-    {",
                "-    }",
                "-",
                "-    /**",
                "-  * Returns the current capacity of this set.  Included for",
                "-  * compatibility.  This is *not* equal to {@link #cardinality}",
                "-  */",
                "-  public long size()",
                "-  {",
                "-      return capacity();",
                "-  }",
                "-",
                "-  // @Override -- not until Java 1.6",
                "-  public long length()",
                "-  {",
                "-    return capacity();",
                "-  }",
                "-",
                "-  /** Returns true if there are no set bits */",
                "-  public boolean isEmpty() { return cardinality()==0; }",
                "-",
                "-",
                "-  /** Expert: gets the number of longs in the array that are in use */",
                "-  public int getNumWords() { return wlen; }",
                "-",
                "-",
                "-  /**",
                "-   * Returns true or false for the specified bit index.",
                "-   * The index should be less than the OpenBitSet size",
                "-   */",
                "-  public boolean get(int index)",
                "-  {",
                "-    int i = index >> 6;               // div 64",
                "-    // signed shift will keep a negative index and force an",
                "-    // array-index-out-of-bounds-exception, removing the need for an explicit check.",
                "-    int bit = index & 0x3f;           // mod 64",
                "-    long bitmask = 1L << bit;",
                "-    // TODO perfectionist one can implement this using bit operations",
                "-    return (bits[i / PAGE_SIZE][i % PAGE_SIZE ] & bitmask) != 0;",
                "-  }",
                "-",
                "-  /**",
                "-   * Returns true or false for the specified bit index.",
                "-   * The index should be less than the OpenBitSet size.",
                "-   */",
                "-  public boolean get(long index)",
                "-  {",
                "-    int i = (int)(index >> 6);               // div 64",
                "-    int bit = (int)index & 0x3f;           // mod 64",
                "-    long bitmask = 1L << bit;",
                "-    // TODO perfectionist one can implement this using bit operations",
                "-    return (bits[i / PAGE_SIZE][i % PAGE_SIZE ] & bitmask) != 0;",
                "-  }",
                "-",
                "-  /**",
                "-   * Sets the bit at the specified index.",
                "-   * The index should be less than the OpenBitSet size.",
                "-   */",
                "-  public void set(long index)",
                "-  {",
                "-    int wordNum = (int)(index >> 6);",
                "-    int bit = (int)index & 0x3f;",
                "-    long bitmask = 1L << bit;",
                "-    bits[ wordNum / PAGE_SIZE ][ wordNum % PAGE_SIZE ] |= bitmask;",
                "-  }",
                "-",
                "-  /**",
                "-   * Sets the bit at the specified index.",
                "-   * The index should be less than the OpenBitSet size.",
                "-   */",
                "-  public void set(int index)",
                "-  {",
                "-    int wordNum = index >> 6;      // div 64",
                "-    int bit = index & 0x3f;     // mod 64",
                "-    long bitmask = 1L << bit;",
                "-    bits[ wordNum / PAGE_SIZE ][ wordNum % PAGE_SIZE ] |= bitmask;",
                "-  }",
                "-",
                "-  /**",
                "-   * clears a bit.",
                "-   * The index should be less than the OpenBitSet size.",
                "-   */",
                "-  public void clear(int index)",
                "-  {",
                "-    int wordNum = index >> 6;",
                "-    int bit = index & 0x03f;",
                "-    long bitmask = 1L << bit;",
                "-    bits[wordNum / PAGE_SIZE][wordNum % PAGE_SIZE] &= ~bitmask;",
                "-    // hmmm, it takes one more instruction to clear than it does to set... any",
                "-    // way to work around this?  If there were only 63 bits per word, we could",
                "-    // use a right shift of 10111111...111 in binary to position the 0 in the",
                "-    // correct place (using sign extension).",
                "-    // Could also use Long.rotateRight() or rotateLeft() *if* they were converted",
                "-    // by the JVM into a native instruction.",
                "-    // bits[word] &= Long.rotateLeft(0xfffffffe,bit);",
                "-  }",
                "-",
                "-  /**",
                "-   * clears a bit.",
                "-   * The index should be less than the OpenBitSet size.",
                "-   */",
                "-  public void clear(long index)",
                "-  {",
                "-    int wordNum = (int)(index >> 6); // div 64",
                "-    int bit = (int)index & 0x3f;     // mod 64",
                "-    long bitmask = 1L << bit;",
                "-    bits[wordNum / PAGE_SIZE][wordNum % PAGE_SIZE] &= ~bitmask;",
                "-  }",
                "-",
                "-  /**",
                "-   * Clears a range of bits.  Clearing past the end does not change the size of the set.",
                "-   *",
                "-   * @param startIndex lower index",
                "-   * @param endIndex one-past the last bit to clear",
                "-   */",
                "-  public void clear(int startIndex, int endIndex)",
                "-  {",
                "-    if (endIndex <= startIndex) return;",
                "-",
                "-    int startWord = (startIndex>>6);",
                "-    if (startWord >= wlen) return;",
                "-",
                "-    // since endIndex is one past the end, this is index of the last",
                "-    // word to be changed.",
                "-    int endWord   = ((endIndex-1)>>6);",
                "-",
                "-    long startmask = -1L << startIndex;",
                "-    long endmask = -1L >>> -endIndex;  // 64-(endIndex&0x3f) is the same as -endIndex due to wrap",
                "-",
                "-    // invert masks since we are clearing",
                "-    startmask = ~startmask;",
                "-    endmask = ~endmask;",
                "-",
                "-    if (startWord == endWord)",
                "-    {",
                "-      bits[startWord / PAGE_SIZE][startWord % PAGE_SIZE] &= (startmask | endmask);",
                "-      return;",
                "-    }",
                "-",
                "-",
                "-    bits[startWord / PAGE_SIZE][startWord % PAGE_SIZE]  &= startmask;",
                "-",
                "-    int middle = Math.min(wlen, endWord);",
                "-    if (startWord / PAGE_SIZE == middle / PAGE_SIZE)",
                "-    {",
                "-        Arrays.fill(bits[startWord/PAGE_SIZE], (startWord+1) % PAGE_SIZE, middle % PAGE_SIZE, 0L);",
                "-    } else",
                "-    {",
                "-        while (++startWord<middle)",
                "-            bits[startWord / PAGE_SIZE][startWord % PAGE_SIZE] = 0L;",
                "-    }",
                "-    if (endWord < wlen)",
                "-    {",
                "-      bits[endWord / PAGE_SIZE][endWord % PAGE_SIZE] &= endmask;",
                "-    }",
                "-  }",
                "-",
                "-",
                "-  /** Clears a range of bits.  Clearing past the end does not change the size of the set.",
                "-   *",
                "-   * @param startIndex lower index",
                "-   * @param endIndex one-past the last bit to clear",
                "-   */",
                "-  public void clear(long startIndex, long endIndex)",
                "-  {",
                "-    if (endIndex <= startIndex) return;",
                "-",
                "-    int startWord = (int)(startIndex>>6);",
                "-    if (startWord >= wlen) return;",
                "-",
                "-    // since endIndex is one past the end, this is index of the last",
                "-    // word to be changed.",
                "-    int endWord   = (int)((endIndex-1)>>6);",
                "-",
                "-    long startmask = -1L << startIndex;",
                "-    long endmask = -1L >>> -endIndex;  // 64-(endIndex&0x3f) is the same as -endIndex due to wrap",
                "-",
                "-    // invert masks since we are clearing",
                "-    startmask = ~startmask;",
                "-    endmask = ~endmask;",
                "-",
                "-    if (startWord == endWord)",
                "-{",
                "-        bits[startWord / PAGE_SIZE][startWord % PAGE_SIZE] &= (startmask | endmask);",
                "-        return;",
                "-    }",
                "-",
                "-    bits[startWord / PAGE_SIZE][startWord % PAGE_SIZE]  &= startmask;",
                "-",
                "-    int middle = Math.min(wlen, endWord);",
                "-    if (startWord / PAGE_SIZE == middle / PAGE_SIZE)",
                "-    {",
                "-        Arrays.fill(bits[startWord/PAGE_SIZE], (startWord+1) % PAGE_SIZE, middle % PAGE_SIZE, 0L);",
                "-    } else",
                "-    {",
                "-        while (++startWord<middle)",
                "-            bits[startWord / PAGE_SIZE][startWord % PAGE_SIZE] = 0L;",
                "-    }",
                "-    if (endWord < wlen)",
                "-    {",
                "-        bits[endWord / PAGE_SIZE][endWord % PAGE_SIZE] &= endmask;",
                "-    }",
                "-  }",
                "-",
                "-  /** @return the number of set bits */",
                "-  public long cardinality()",
                "-  {",
                "-    long bitCount = 0L;",
                "-    for (int i=getPageCount();i-->0;)",
                "-        bitCount+=BitUtil.pop_array(bits[i],0,wlen);",
                "-",
                "-    return bitCount;",
                "-  }",
                "-",
                "-  /** this = this AND other */",
                "-  public void intersect(OpenBitSet other)",
                "-  {",
                "-    int newLen= Math.min(this.wlen,other.wlen);",
                "-    long[][] thisArr = this.bits;",
                "-    long[][] otherArr = other.bits;",
                "-    int thisPageSize = PAGE_SIZE;",
                "-    int otherPageSize = OpenBitSet.PAGE_SIZE;",
                "-    // testing against zero can be more efficient",
                "-    int pos=newLen;",
                "-    while(--pos>=0)",
                "-    {",
                "-      thisArr[pos / thisPageSize][ pos % thisPageSize] &= otherArr[pos / otherPageSize][pos % otherPageSize];",
                "-    }",
                "-",
                "-    if (this.wlen > newLen)",
                "-    {",
                "-      // fill zeros from the new shorter length to the old length",
                "-      for (pos=wlen;pos-->newLen;)",
                "-          thisArr[pos / thisPageSize][ pos % thisPageSize] =0;",
                "-    }",
                "-    this.wlen = newLen;",
                "-  }",
                "-",
                "-  // some BitSet compatability methods",
                "-",
                "-  //** see {@link intersect} */",
                "-  public void and(OpenBitSet other)",
                "-  {",
                "-    intersect(other);",
                "-  }",
                "-",
                "-  /** Lowers numWords, the number of words in use,",
                "-   * by checking for trailing zero words.",
                "-   */",
                "-  public void trimTrailingZeros()",
                "-  {",
                "-    int idx = wlen-1;",
                "-    while (idx>=0 && bits[idx / PAGE_SIZE][idx % PAGE_SIZE]==0) idx--;",
                "-    wlen = idx+1;",
                "-  }",
                "-",
                "-  /** returns the number of 64 bit words it would take to hold numBits */",
                "-  public static long bits2words(long numBits)",
                "-  {",
                "-   return (((numBits-1)>>>6)+1);",
                "-  }",
                "-",
                "-  /** returns true if both sets have the same bits set */",
                "-  @Override",
                "-  public boolean equals(Object o)",
                "-  {",
                "-    if (this == o) return true;",
                "-    if (!(o instanceof OpenBitSet)) return false;",
                "-    OpenBitSet a;",
                "-    OpenBitSet b = (OpenBitSet)o;",
                "-    // make a the larger set.",
                "-    if (b.wlen > this.wlen)",
                "-    {",
                "-      a = b; b=this;",
                "-    }",
                "-    else",
                "-    {",
                "-      a=this;",
                "-    }",
                "-",
                "-    int aPageSize = OpenBitSet.PAGE_SIZE;",
                "-    int bPageSize = OpenBitSet.PAGE_SIZE;",
                "-",
                "-    // check for any set bits out of the range of b",
                "-    for (int i=a.wlen-1; i>=b.wlen; i--)",
                "-    {",
                "-      if (a.bits[i/aPageSize][i % aPageSize]!=0) return false;",
                "-    }",
                "-",
                "-    for (int i=b.wlen-1; i>=0; i--)",
                "-    {",
                "-      if (a.bits[i/aPageSize][i % aPageSize] != b.bits[i/bPageSize][i % bPageSize]) return false;",
                "-    }",
                "-",
                "-    return true;",
                "-  }",
                "-",
                "-",
                "-  @Override",
                "-  public int hashCode()",
                "-  {",
                "-    // Start with a zero hash and use a mix that results in zero if the input is zero.",
                "-    // This effectively truncates trailing zeros without an explicit check.",
                "-    long h = 0;",
                "-    for (int i = wlen; --i>=0;)",
                "-    {",
                "-      h ^= bits[i / PAGE_SIZE][i % PAGE_SIZE];",
                "-      h = (h << 1) | (h >>> 63); // rotate left",
                "-    }",
                "-    // fold leftmost bits into right and add a constant to prevent",
                "-    // empty sets from returning 0, which is too common.",
                "-    return (int)((h>>32) ^ h) + 0x98761234;",
                "-  }",
                "-",
                "-  public void close()",
                "-  {",
                "-    // noop, let GC do the cleanup.",
                "-  }",
                "-",
                "-  public void serialize(DataOutput out) throws IOException",
                "-  {",
                "-    int bitLength = getNumWords();",
                "-    int pageSize = getPageSize();",
                "-    int pageCount = getPageCount();",
                "-",
                "-    out.writeInt(bitLength);",
                "-    for (int p = 0; p < pageCount; p++)",
                "-    {",
                "-      long[] bits = getPage(p);",
                "-      for (int i = 0; i < pageSize && bitLength-- > 0; i++)",
                "-      {",
                "-        out.writeLong(bits[i]);",
                "-      }",
                "-    }",
                "-}",
                "-",
                "-  public long serializedSize()",
                "-  {",
                "-    int bitLength = getNumWords();",
                "-    int pageSize = getPageSize();",
                "-    int pageCount = getPageCount();",
                "-",
                "-    long size = TypeSizes.sizeof(bitLength); // length",
                "-    for (int p = 0; p < pageCount; p++)",
                "-    {",
                "-      long[] bits = getPage(p);",
                "-      for (int i = 0; i < pageSize && bitLength-- > 0; i++)",
                "-        size += TypeSizes.sizeof(bits[i]); // bucket",
                "-    }",
                "-    return size;",
                "-  }",
                "-",
                "-  public void clear()",
                "-  {",
                "-    clear(0, capacity());",
                "-  }",
                "-",
                "-  public static OpenBitSet deserialize(DataInput in) throws IOException",
                "-  {",
                "-    long bitLength = in.readInt();",
                "-",
                "-    OpenBitSet bs = new OpenBitSet(bitLength << 6);",
                "-    int pageSize = bs.getPageSize();",
                "-    int pageCount = bs.getPageCount();",
                "-",
                "-    for (int p = 0; p < pageCount; p++)",
                "-    {",
                "-      long[] bits = bs.getPage(p);",
                "-      for (int i = 0; i < pageSize && bitLength-- > 0; i++)",
                "-        bits[i] = in.readLong();",
                "-    }",
                "-    return bs;",
                "-  }",
                "-}",
                "diff --git a/test/long/org/apache/cassandra/utils/LongBitSetTest.java b/test/long/org/apache/cassandra/utils/LongBitSetTest.java",
                "deleted file mode 100644",
                "index f20a4f8421..0000000000",
                "--- a/test/long/org/apache/cassandra/utils/LongBitSetTest.java",
                "+++ /dev/null",
                "@@ -1,134 +0,0 @@",
                "-/*",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- *     http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-package org.apache.cassandra.utils;",
                "-",
                "-import java.util.Random;",
                "-import java.util.concurrent.TimeUnit;",
                "-",
                "-import org.junit.Assert;",
                "-",
                "-import org.apache.cassandra.utils.obs.OffHeapBitSet;",
                "-import org.apache.cassandra.utils.obs.OpenBitSet;",
                "-import org.junit.Test;",
                "-import org.slf4j.Logger;",
                "-import org.slf4j.LoggerFactory;",
                "-",
                "-public class LongBitSetTest",
                "-{",
                "-    private static final Logger logger = LoggerFactory.getLogger(LongBitSetTest.class);",
                "-    private static final Random random = new Random();",
                "-",
                "-    public void populateRandom(OffHeapBitSet offbs, OpenBitSet obs, long index)",
                "-    {",
                "-        if (random.nextBoolean())",
                "-        {",
                "-            offbs.set(index);",
                "-            obs.set(index);",
                "-        }",
                "-    }",
                "-",
                "-    public void compare(OffHeapBitSet offbs, OpenBitSet obs, long index)",
                "-    {",
                "-        if (offbs.get(index) != obs.get(index))",
                "-            throw new RuntimeException();",
                "-        Assert.assertEquals(offbs.get(index), obs.get(index));",
                "-    }",
                "-",
                "-    @Test",
                "-    public void testBitSetOperations()",
                "-    {",
                "-        long size_to_test = Integer.MAX_VALUE / 40;",
                "-        long size_and_excess = size_to_test + 20;",
                "-        OffHeapBitSet offbs = new OffHeapBitSet(size_and_excess);",
                "-        OpenBitSet obs = new OpenBitSet(size_and_excess);",
                "-        for (long i = 0; i < size_to_test; i++)",
                "-            populateRandom(offbs, obs, i);",
                "-",
                "-        for (long i = 0; i < size_to_test; i++)",
                "-            compare(offbs, obs, i);",
                "-    }",
                "-",
                "-    @Test",
                "-    public void timeit()",
                "-    {",
                "-        long size_to_test = Integer.MAX_VALUE / 10; // about 214 million",
                "-        long size_and_excess = size_to_test + 20;",
                "-",
                "-        OpenBitSet obs = new OpenBitSet(size_and_excess);",
                "-        OffHeapBitSet offbs = new OffHeapBitSet(size_and_excess);",
                "-        logger.info(\"||Open BS set's|Open BS get's|Open BS clear's|Offheap BS set's|Offheap BS get's|Offheap BS clear's|\");",
                "-        // System.out.println(\"||Open BS set's|Open BS get's|Open BS clear's|Offheap BS set's|Offheap BS get's|Offheap BS clear's|\");",
                "-        loopOnce(obs, offbs, size_to_test);",
                "-    }",
                "-",
                "-    public void loopOnce(OpenBitSet obs, OffHeapBitSet offbs, long size_to_test)",
                "-    {",
                "-        StringBuffer buffer = new StringBuffer();",
                "-        // start off fresh.",
                "-        System.gc();",
                "-        long start = System.nanoTime();",
                "-        for (long i = 0; i < size_to_test; i++)",
                "-            obs.set(i);",
                "-        buffer.append(\"||\").append(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));",
                "-",
                "-        start = System.nanoTime();",
                "-        for (long i = 0; i < size_to_test; i++)",
                "-            obs.get(i);",
                "-        buffer.append(\"|\").append(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));",
                "-",
                "-        start = System.nanoTime();",
                "-        for (long i = 0; i < size_to_test; i++)",
                "-            obs.clear(i);",
                "-        buffer.append(\"|\").append(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));",
                "-",
                "-        System.gc();",
                "-        start = System.nanoTime();",
                "-        for (long i = 0; i < size_to_test; i++)",
                "-            offbs.set(i);",
                "-        buffer.append(\"|\").append(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));",
                "-",
                "-        start = System.nanoTime();",
                "-        for (long i = 0; i < size_to_test; i++)",
                "-            offbs.get(i);",
                "-        buffer.append(\"|\").append(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));",
                "-",
                "-        start = System.nanoTime();",
                "-        for (long i = 0; i < size_to_test; i++)",
                "-            offbs.clear(i);",
                "-        buffer.append(\"|\").append(TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start)).append(\"|\");",
                "-        logger.info(buffer.toString());",
                "-        // System.out.println(buffer.toString());",
                "-    }",
                "-",
                "-    /**",
                "-     * Just to make sure JIT doesn't come on our way",
                "-     */",
                "-    @Test",
                "-    // @Ignore",
                "-    public void loopIt()",
                "-    {",
                "-        long size_to_test = Integer.MAX_VALUE / 10; // about 214 million",
                "-        long size_and_excess = size_to_test + 20;",
                "-",
                "-        OpenBitSet obs = new OpenBitSet(size_and_excess);",
                "-        OffHeapBitSet offbs = new OffHeapBitSet(size_and_excess);",
                "-        for (int i = 0; i < 10; i++)",
                "-            // 10 times to do approx 2B keys each.",
                "-            loopOnce(obs, offbs, size_to_test);",
                "-    }",
                "-}",
                "diff --git a/test/long/org/apache/cassandra/utils/LongBloomFilterTest.java b/test/long/org/apache/cassandra/utils/LongBloomFilterTest.java",
                "index 10dd5a64fe..d998e4d8cf 100644",
                "--- a/test/long/org/apache/cassandra/utils/LongBloomFilterTest.java",
                "+++ b/test/long/org/apache/cassandra/utils/LongBloomFilterTest.java",
                "@@ -41,3 +41,3 @@ public class LongBloomFilterTest",
                "         int size = 10 * 1000 * 1000;",
                "-        IFilter bf = getFilter(size, FilterTestHelper.spec.bucketsPerElement, false);",
                "+        IFilter bf = getFilter(size, FilterTestHelper.spec.bucketsPerElement);",
                "         double fp = testFalsePositives(bf,",
                "@@ -52,3 +52,3 @@ public class LongBloomFilterTest",
                "         int size = 10 * 1000 * 1000;",
                "-        IFilter bf = getFilter(size, FilterTestHelper.spec.bucketsPerElement, false);",
                "+        IFilter bf = getFilter(size, FilterTestHelper.spec.bucketsPerElement);",
                "         double fp = testFalsePositives(bf,",
                "@@ -66,3 +66,3 @@ public class LongBloomFilterTest",
                "         int size = 10 * 1000 * 1000;",
                "-        try (IFilter bf = getFilter(size, 0.01, false))",
                "+        try (IFilter bf = getFilter(size, 0.01))",
                "         {",
                "@@ -79,3 +79,3 @@ public class LongBloomFilterTest",
                "         {",
                "-            try (IFilter bf = getFilter(elements, targetFp, false);)",
                "+            try (IFilter bf = getFilter(elements, targetFp))",
                "             {",
                "@@ -162,3 +162,3 @@ public class LongBloomFilterTest",
                "         int size = 300 * FilterTestHelper.ELEMENTS;",
                "-        IFilter bf = getFilter(size, FilterTestHelper.spec.bucketsPerElement, false);",
                "+        IFilter bf = getFilter(size, FilterTestHelper.spec.bucketsPerElement);",
                "         double sumfp = 0;",
                "diff --git a/test/unit/org/apache/cassandra/utils/BitSetTest.java b/test/unit/org/apache/cassandra/utils/BitSetTest.java",
                "deleted file mode 100644",
                "index 4dab17ecee..0000000000",
                "--- a/test/unit/org/apache/cassandra/utils/BitSetTest.java",
                "+++ /dev/null",
                "@@ -1,133 +0,0 @@",
                "-/*",
                "- * Licensed to the Apache Software Foundation (ASF) under one",
                "- * or more contributor license agreements.  See the NOTICE file",
                "- * distributed with this work for additional information",
                "- * regarding copyright ownership.  The ASF licenses this file",
                "- * to you under the Apache License, Version 2.0 (the",
                "- * \"License\"); you may not use this file except in compliance",
                "- * with the License.  You may obtain a copy of the License at",
                "- *",
                "- *     http://www.apache.org/licenses/LICENSE-2.0",
                "- *",
                "- * Unless required by applicable law or agreed to in writing, software",
                "- * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "- * See the License for the specific language governing permissions and",
                "- * limitations under the License.",
                "- */",
                "-package org.apache.cassandra.utils;",
                "-",
                "-import java.io.ByteArrayInputStream;",
                "-import java.io.DataInputStream;",
                "-import java.io.IOException;",
                "-import java.util.List;",
                "-import java.util.Random;",
                "-",
                "-import com.google.common.collect.Lists;",
                "-import org.junit.Assert;",
                "-import org.junit.Test;",
                "-",
                "-import org.apache.cassandra.io.util.DataOutputBuffer;",
                "-import org.apache.cassandra.utils.IFilter.FilterKey;",
                "-import org.apache.cassandra.utils.KeyGenerator.RandomStringGenerator;",
                "-import org.apache.cassandra.utils.obs.IBitSet;",
                "-import org.apache.cassandra.utils.obs.OffHeapBitSet;",
                "-import org.apache.cassandra.utils.obs.OpenBitSet;",
                "-",
                "-import static org.junit.Assert.assertEquals;",
                "-",
                "-public class BitSetTest",
                "-{",
                "-    /**",
                "-     * Test bitsets in a \"real-world\" environment, i.e., bloom filters",
                "-     */",
                "-    @Test",
                "-    public void compareBitSets()",
                "-    {",
                "-        BloomFilter bf2 = (BloomFilter) FilterFactory.getFilter(FilterTestHelper.ELEMENTS / 2, FilterTestHelper.MAX_FAILURE_RATE, false);",
                "-        BloomFilter bf3 = (BloomFilter) FilterFactory.getFilter(FilterTestHelper.ELEMENTS / 2, FilterTestHelper.MAX_FAILURE_RATE, true);",
                "-",
                "-        RandomStringGenerator gen1 = new KeyGenerator.RandomStringGenerator(new Random().nextInt(), FilterTestHelper.ELEMENTS);",
                "-",
                "-        // make sure both bitsets are empty.",
                "-        compare(bf2.bitset, bf3.bitset);",
                "-",
                "-        while (gen1.hasNext())",
                "-        {",
                "-            FilterKey key = FilterTestHelper.wrap(gen1.next());",
                "-            bf2.add(key);",
                "-            bf3.add(key);",
                "-        }",
                "-",
                "-        compare(bf2.bitset, bf3.bitset);",
                "-    }",
                "-",
                "-    private static final Random random = new Random();",
                "-",
                "-    /**",
                "-     * Test serialization and de-serialization in-memory",
                "-     */",
                "-    @Test",
                "-    public void testOffHeapSerialization() throws IOException",
                "-    {",
                "-        try (OffHeapBitSet bs = new OffHeapBitSet(100000))",
                "-        {",
                "-            populateAndReserialize(bs);",
                "-        }",
                "-    }",
                "-",
                "-    @Test",
                "-    public void testOffHeapCompatibility() throws IOException",
                "-    {",
                "-        try (OpenBitSet bs = new OpenBitSet(100000))",
                "-        {",
                "-            populateAndReserialize(bs);",
                "-        }",
                "-    }",
                "-",
                "-    private static void populateAndReserialize(IBitSet bs) throws IOException",
                "-    {",
                "-        for (long i = 0; i < bs.capacity(); i++)",
                "-            if (random.nextBoolean())",
                "-                bs.set(i);",
                "-",
                "-        DataOutputBuffer out = new DataOutputBuffer();",
                "-        bs.serialize(out);",
                "-        DataInputStream in = new DataInputStream(new ByteArrayInputStream(out.getData()));",
                "-        try (OffHeapBitSet newbs = OffHeapBitSet.deserialize(in))",
                "-        {",
                "-            compare(bs, newbs);",
                "-        }",
                "-    }",
                "-",
                "-    static void compare(IBitSet bs, IBitSet newbs)",
                "-    {",
                "-        assertEquals(bs.capacity(), newbs.capacity());",
                "-        for (long i = 0; i < bs.capacity(); i++)",
                "-            Assert.assertEquals(bs.get(i), newbs.get(i));",
                "-    }",
                "-",
                "-    @Test",
                "-    public void testBitClear()",
                "-    {",
                "-        int size = Integer.MAX_VALUE / 4000;",
                "-        try (OffHeapBitSet bitset = new OffHeapBitSet(size))",
                "-        {",
                "-            List<Integer> randomBits = Lists.newArrayList();",
                "-            for (int i = 0; i < 10; i++)",
                "-                randomBits.add(random.nextInt(size));",
                "-    ",
                "-            for (long randomBit : randomBits)",
                "-                bitset.set(randomBit);",
                "-    ",
                "-            for (long randomBit : randomBits)",
                "-                Assert.assertEquals(true, bitset.get(randomBit));",
                "-    ",
                "-            for (long randomBit : randomBits)",
                "-                bitset.clear(randomBit);",
                "-    ",
                "-            for (long randomBit : randomBits)",
                "-                Assert.assertEquals(false, bitset.get(randomBit));",
                "-        }",
                "-    }",
                "-}",
                "diff --git a/test/unit/org/apache/cassandra/utils/BloomFilterTest.java b/test/unit/org/apache/cassandra/utils/BloomFilterTest.java",
                "index 337e38774b..581248c297 100644",
                "--- a/test/unit/org/apache/cassandra/utils/BloomFilterTest.java",
                "+++ b/test/unit/org/apache/cassandra/utils/BloomFilterTest.java",
                "@@ -37,2 +37,5 @@ import org.apache.cassandra.utils.IFilter.FilterKey;",
                " import org.apache.cassandra.utils.KeyGenerator.RandomStringGenerator;",
                "+import org.apache.cassandra.utils.obs.IBitSet;",
                "+",
                "+import static org.junit.Assert.assertEquals;",
                "@@ -54,3 +57,3 @@ public class BloomFilterTest",
                "         ByteArrayInputStream in = new ByteArrayInputStream(out.getData(), 0, out.getLength());",
                "-        IFilter f2 = FilterFactory.deserialize(new DataInputStream(in), true);",
                "+        IFilter f2 = FilterFactory.deserialize(new DataInputStream(in));",
                "@@ -61,2 +64,8 @@ public class BloomFilterTest",
                "+    static void compare(IBitSet bs, IBitSet newbs)",
                "+    {",
                "+        assertEquals(bs.capacity(), newbs.capacity());",
                "+        for (long i = 0; i < bs.capacity(); i++)",
                "+            assertEquals(bs.get(i), newbs.get(i));",
                "+    }",
                "@@ -65,3 +74,3 @@ public class BloomFilterTest",
                "     {",
                "-        bfInvHashes = FilterFactory.getFilter(10000L, FilterTestHelper.MAX_FAILURE_RATE, true);",
                "+        bfInvHashes = FilterFactory.getFilter(10000L, FilterTestHelper.MAX_FAILURE_RATE);",
                "     }",
                "@@ -114,3 +123,3 @@ public class BloomFilterTest",
                "         }",
                "-        IFilter bf2 = FilterFactory.getFilter(KeyGenerator.WordGenerator.WORDS / 2, FilterTestHelper.MAX_FAILURE_RATE, true);",
                "+        IFilter bf2 = FilterFactory.getFilter(KeyGenerator.WordGenerator.WORDS / 2, FilterTestHelper.MAX_FAILURE_RATE);",
                "         int skipEven = KeyGenerator.WordGenerator.WORDS % 2 == 0 ? 0 : 2;",
                "@@ -144,3 +153,3 @@ public class BloomFilterTest",
                "             FilterKey buf = FilterTestHelper.wrap(keys.next());",
                "-            BloomFilter bf = (BloomFilter) FilterFactory.getFilter(10, 1, false);",
                "+            BloomFilter bf = (BloomFilter) FilterFactory.getFilter(10, 1);",
                "             for (long hashIndex : bf.getHashBuckets(buf, MAX_HASH_COUNT, 1024 * 1024))",
                "@@ -159,3 +168,3 @@ public class BloomFilterTest",
                "         long numKeys = ((long)Integer.MAX_VALUE) * 64L + 1L; // approx 128 Billion",
                "-        FilterFactory.getFilter(numKeys, 0.01d, true).close();",
                "+        FilterFactory.getFilter(numKeys, 0.01d).close();",
                "     }",
                "@@ -165,5 +174,5 @@ public class BloomFilterTest",
                "     {",
                "-        try (BloomFilter bf1 = (BloomFilter) FilterFactory.getFilter(FilterTestHelper.ELEMENTS / 2, FilterTestHelper.MAX_FAILURE_RATE, false);",
                "-             BloomFilter bf2 = (BloomFilter) FilterFactory.getFilter(FilterTestHelper.ELEMENTS / 2, FilterTestHelper.MAX_FAILURE_RATE, false);",
                "-             BloomFilter bf3 = (BloomFilter) FilterFactory.getFilter(FilterTestHelper.ELEMENTS / 2, FilterTestHelper.MAX_FAILURE_RATE, false))",
                "+        try (BloomFilter bf1 = (BloomFilter) FilterFactory.getFilter(FilterTestHelper.ELEMENTS / 2, FilterTestHelper.MAX_FAILURE_RATE);",
                "+             BloomFilter bf2 = (BloomFilter) FilterFactory.getFilter(FilterTestHelper.ELEMENTS / 2, FilterTestHelper.MAX_FAILURE_RATE);",
                "+             BloomFilter bf3 = (BloomFilter) FilterFactory.getFilter(FilterTestHelper.ELEMENTS / 2, FilterTestHelper.MAX_FAILURE_RATE))",
                "         {",
                "@@ -172,4 +181,4 @@ public class BloomFilterTest",
                "             // make sure all bitsets are empty.",
                "-            BitSetTest.compare(bf1.bitset, bf2.bitset);",
                "-            BitSetTest.compare(bf1.bitset, bf3.bitset);",
                "+            compare(bf1.bitset, bf2.bitset);",
                "+            compare(bf1.bitset, bf3.bitset);",
                "@@ -184,4 +193,4 @@ public class BloomFilterTest",
                "-            BitSetTest.compare(bf1.bitset, bf2.bitset);",
                "-            BitSetTest.compare(bf1.bitset, bf3.bitset);",
                "+            compare(bf1.bitset, bf2.bitset);",
                "+            compare(bf1.bitset, bf3.bitset);",
                "         }",
                "@@ -196,3 +205,3 @@ public class BloomFilterTest",
                "         File file = FileUtils.createDeletableTempFile(\"bloomFilterTest-\", \".dat\");",
                "-        BloomFilter filter = (BloomFilter) FilterFactory.getFilter(((long) Integer.MAX_VALUE / 8) + 1, 0.01d, true);",
                "+        BloomFilter filter = (BloomFilter) FilterFactory.getFilter(((long) Integer.MAX_VALUE / 8) + 1, 0.01d);",
                "         filter.add(FilterTestHelper.wrap(test));",
                "@@ -205,3 +214,3 @@ public class BloomFilterTest",
                "         DataInputStream in = new DataInputStream(new FileInputStream(file));",
                "-        BloomFilter filter2 = (BloomFilter) FilterFactory.deserialize(in, true);",
                "+        BloomFilter filter2 = (BloomFilter) FilterFactory.deserialize(in);",
                "         Assert.assertTrue(filter2.isPresent(FilterTestHelper.wrap(test)));",
                "diff --git a/test/unit/org/apache/cassandra/utils/SerializationsTest.java b/test/unit/org/apache/cassandra/utils/SerializationsTest.java",
                "index 79739642ad..8da4a92e83 100644",
                "--- a/test/unit/org/apache/cassandra/utils/SerializationsTest.java",
                "+++ b/test/unit/org/apache/cassandra/utils/SerializationsTest.java",
                "@@ -48,19 +48,5 @@ public class SerializationsTest extends AbstractSerializationsTester",
                "-    private static void testBloomFilterWrite(boolean offheap) throws IOException",
                "+    private static void testBloomFilterWrite1000() throws IOException",
                "     {",
                "-        IPartitioner partitioner = Util.testPartitioner();",
                "-        try (IFilter bf = FilterFactory.getFilter(1000000, 0.0001, offheap))",
                "-        {",
                "-            for (int i = 0; i < 100; i++)",
                "-                bf.add(partitioner.decorateKey(partitioner.getTokenFactory().toByteArray(partitioner.getRandomToken())));",
                "-            try (DataOutputStreamPlus out = getOutput(\"3.0\", \"utils.BloomFilter.bin\"))",
                "-            {",
                "-                FilterFactory.serialize(bf, out);",
                "-            }",
                "-        }",
                "-    }",
                "-",
                "-    private static void testBloomFilterWrite1000(boolean offheap) throws IOException",
                "-    {",
                "-        try (IFilter bf = FilterFactory.getFilter(1000000, 0.0001, offheap))",
                "+        try (IFilter bf = FilterFactory.getFilter(1000000, 0.0001))",
                "         {",
                "@@ -79,6 +65,6 @@ public class SerializationsTest extends AbstractSerializationsTester",
                "         if (EXECUTE_WRITES)",
                "-            testBloomFilterWrite1000(true);",
                "+            testBloomFilterWrite1000();",
                "         try (DataInputStream in = getInput(\"3.0\", \"utils.BloomFilter1000.bin\");",
                "-             IFilter filter = FilterFactory.deserialize(in, true))",
                "+             IFilter filter = FilterFactory.deserialize(in))",
                "         {",
                "@@ -109,3 +95,3 @@ public class SerializationsTest extends AbstractSerializationsTester",
                "         try (DataInputStream in = new DataInputStream(new FileInputStream(new File(file)));",
                "-             IFilter filter = FilterFactory.deserialize(in, true))",
                "+             IFilter filter = FilterFactory.deserialize(in))",
                "         {",
                "diff --git a/test/unit/org/apache/cassandra/utils/obs/OffHeapBitSetTest.java b/test/unit/org/apache/cassandra/utils/obs/OffHeapBitSetTest.java",
                "new file mode 100644",
                "index 0000000000..f0325da41f",
                "--- /dev/null",
                "+++ b/test/unit/org/apache/cassandra/utils/obs/OffHeapBitSetTest.java",
                "@@ -0,0 +1,134 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ *     http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+",
                "+package org.apache.cassandra.utils.obs;",
                "+",
                "+import java.io.ByteArrayInputStream;",
                "+import java.io.DataInputStream;",
                "+import java.io.IOException;",
                "+import java.util.List;",
                "+import java.util.Random;",
                "+",
                "+import com.google.common.collect.Lists;",
                "+import org.junit.Assert;",
                "+import org.junit.Test;",
                "+import org.junit.rules.ExpectedException;",
                "+",
                "+import org.apache.cassandra.io.util.DataOutputBuffer;",
                "+",
                "+import static org.junit.Assert.assertEquals;",
                "+import static org.junit.Assert.assertTrue;",
                "+import static org.junit.Assert.fail;",
                "+",
                "+public class OffHeapBitSetTest",
                "+{",
                "+    private static final Random random = new Random();",
                "+",
                "+    static void compare(IBitSet bs, IBitSet newbs)",
                "+    {",
                "+        assertEquals(bs.capacity(), newbs.capacity());",
                "+        for (long i = 0; i < bs.capacity(); i++)",
                "+            Assert.assertEquals(bs.get(i), newbs.get(i));",
                "+    }",
                "+",
                "+    @Test",
                "+    public void testOffHeapSerialization() throws IOException",
                "+    {",
                "+        try (OffHeapBitSet bs = new OffHeapBitSet(100000))",
                "+        {",
                "+            for (long i = 0; i < bs.capacity(); i++)",
                "+                if (random.nextBoolean())",
                "+                    bs.set(i);",
                "+",
                "+            DataOutputBuffer out = new DataOutputBuffer();",
                "+            bs.serialize(out);",
                "+",
                "+            DataInputStream in = new DataInputStream(new ByteArrayInputStream(out.getData()));",
                "+            try (OffHeapBitSet newbs = OffHeapBitSet.deserialize(in))",
                "+            {",
                "+                compare(bs, newbs);",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "+    @Test",
                "+    public void testBitSetGetClear()",
                "+    {",
                "+        int size = Integer.MAX_VALUE / 4000;",
                "+        try (OffHeapBitSet bs = new OffHeapBitSet(size))",
                "+        {",
                "+            List<Integer> randomBits = Lists.newArrayList();",
                "+            for (int i = 0; i < 10; i++)",
                "+                randomBits.add(random.nextInt(size));",
                "+",
                "+            for (long randomBit : randomBits)",
                "+                bs.set(randomBit);",
                "+",
                "+            for (long randomBit : randomBits)",
                "+                assertEquals(true, bs.get(randomBit));",
                "+",
                "+            for (long randomBit : randomBits)",
                "+                bs.clear(randomBit);",
                "+",
                "+            for (long randomBit : randomBits)",
                "+                assertEquals(false, bs.get(randomBit));",
                "+        }",
                "+    }",
                "+",
                "+    @Test(expected = UnsupportedOperationException.class)",
                "+    public void testUnsupportedLargeSize()",
                "+    {",
                "+        long size = 64L * Integer.MAX_VALUE + 1; // Max size 16G * 8 bits",
                "+        OffHeapBitSet bs = new OffHeapBitSet(size);",
                "+    }",
                "+",
                "+    @Test",
                "+    public void testInvalidIndex()",
                "+    {",
                "+        OffHeapBitSet bs = new OffHeapBitSet(10);",
                "+        int invalidIdx[] = {-1, 64, 1000};",
                "+",
                "+        for (int i : invalidIdx)",
                "+        {",
                "+            try",
                "+            {",
                "+                bs.set(i);",
                "+            }",
                "+            catch (AssertionError e)",
                "+            {",
                "+                assertTrue(e.getMessage().startsWith(\"Illegal bounds\"));",
                "+                continue;",
                "+            }",
                "+            fail(String.format(\"expect exception for index %d\", i));",
                "+        }",
                "+",
                "+        for (int i : invalidIdx)",
                "+        {",
                "+            try",
                "+            {",
                "+                bs.get(i);",
                "+            }",
                "+            catch (AssertionError e)",
                "+            {",
                "+                assertTrue(e.getMessage().startsWith(\"Illegal bounds\"));",
                "+                continue;",
                "+            }",
                "+            fail(String.format(\"expect exception for index %d\", i));",
                "+        }",
                "+    }",
                "+}"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java",
                "src/java/org/apache/cassandra/io/sstable/format/big/BigTableWriter.java",
                "src/java/org/apache/cassandra/utils/BloomFilterSerializer.java",
                "src/java/org/apache/cassandra/utils/FilterFactory.java",
                "src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java",
                "src/java/org/apache/cassandra/utils/obs/OpenBitSet.java",
                "test/long/org/apache/cassandra/utils/LongBitSetTest.java",
                "test/long/org/apache/cassandra/utils/LongBloomFilterTest.java",
                "test/unit/org/apache/cassandra/utils/BitSetTest.java",
                "test/unit/org/apache/cassandra/utils/BloomFilterTest.java",
                "test/unit/org/apache/cassandra/utils/SerializationsTest.java",
                "test/unit/org/apache/cassandra/utils/obs/OffHeapBitSetTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14152": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: implement",
                    "relevance": 4
                },
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14152",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "3891598e61ecb3fe2a029e27061ac28f4b58c29e",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518018563,
            "hunks": 2,
            "message": "Fix mistakenly unused variable introduced by CASSANDRA-7544 and potential CME in iterator Patch by Ariel Weisberg; Reviewed by Jason Brown for CASSANDRA-7544",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/service/StorageService.java b/src/java/org/apache/cassandra/service/StorageService.java",
                "index 16b6a81839..09edc35905 100644",
                "--- a/src/java/org/apache/cassandra/service/StorageService.java",
                "+++ b/src/java/org/apache/cassandra/service/StorageService.java",
                "@@ -173,3 +173,3 @@ public class StorageService extends NotificationBroadcasterSupport implements IE",
                "-    private final Set<InetAddressAndPort> replicatingNodes = Collections.synchronizedSet(new HashSet<InetAddressAndPort>());",
                "+    private final Set<InetAddressAndPort> replicatingNodes = Sets.newConcurrentHashSet();",
                "     private CassandraDaemon daemon;",
                "@@ -532,3 +532,3 @@ public class StorageService extends NotificationBroadcasterSupport implements IE",
                "-                String[] pieces = splitValue(entry.getValue().getApplicationState(ApplicationState.STATUS));",
                "+                String[] pieces = splitValue(value);",
                "                 assert (pieces.length > 0);"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/service/StorageService.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-7544": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: introduce",
                    "relevance": 4
                },
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-7544",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6eb65e5a23d3af30b2ecc1f5ea25c30c3b14e284",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518189632,
            "hunks": 2,
            "message": "Merge branch 'cassandra-3.0' into cassandra-3.11",
            "diff": [
                "diff --cc CHANGES.txt",
                "index 4eb03e586f,f42f3f45cb..c38b69b171",
                "--- a/CHANGES.txt",
                "+++ b/CHANGES.txt",
                "@@@ -1,25 -1,3 +1,25 @@@",
                " -3.0.16",
                " +3.11.2",
                " + * Remove dependencies on JVM internal classes from JMXServerUtils (CASSANDRA-14173) ",
                " + * Add DEFAULT, UNSET, MBEAN and MBEANS to `ReservedKeywords` (CASSANDRA-14205)",
                " + * Add Unittest for schema migration fix (CASSANDRA-14140)",
                " + * Print correct snitch info from nodetool describecluster (CASSANDRA-13528)",
                " + * Close socket on error during connect on OutboundTcpConnection (CASSANDRA-9630)",
                " + * Enable CDC unittest (CASSANDRA-14141)",
                " + * Acquire read lock before accessing CompactionStrategyManager fields (CASSANDRA-14139)",
                " + * Split CommitLogStressTest to avoid timeout (CASSANDRA-14143)",
                " + * Avoid invalidating disk boundaries unnecessarily (CASSANDRA-14083)",
                " + * Avoid exposing compaction strategy index externally (CASSANDRA-14082)",
                " + * Prevent continuous schema exchange between 3.0 and 3.11 nodes (CASSANDRA-14109)",
                " + * Fix imbalanced disks when replacing node with same address with JBOD (CASSANDRA-14084)",
                " + * Reload compaction strategies when disk boundaries are invalidated (CASSANDRA-13948)",
                " + * Remove OpenJDK log warning (CASSANDRA-13916)",
                " + * Prevent compaction strategies from looping indefinitely (CASSANDRA-14079)",
                " + * Cache disk boundaries (CASSANDRA-13215)",
                " + * Add asm jar to build.xml for maven builds (CASSANDRA-11193)",
                " + * Round buffer size to powers of 2 for the chunk cache (CASSANDRA-13897)",
                " + * Update jackson JSON jars (CASSANDRA-13949)",
                " + * Avoid locks when checking LCS fanout and if we should defrag (CASSANDRA-13930)",
                " + * Correctly count range tombstones in traces and tombstone thresholds (CASSANDRA-8527)",
                "- ",
                " +Merged from 3.0:",
                "+  * Use the correct digest file and reload sstable metadata in nodetool verify (CASSANDRA-14217)",
                "   * Handle failure when mutating repaired status in Verifier (CASSANDRA-13933)"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "CHANGES_RELEVANT_CODE",
                    "message": "The commit modifies code containing relevant filename or methods: CASSANDRA-14173",
                    "relevance": 8
                }
            ]
        },
        {
            "commit_id": "6b00767427706124e016e4f471c2266899387163",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517406798,
            "hunks": 1,
            "message": "Add missed DEFAULT, UNSET, MBEAN and MBEANS keywords to `ReservedKeywords`",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/cql3/ReservedKeywords.java b/src/java/org/apache/cassandra/cql3/ReservedKeywords.java",
                "index ee052a7376..30b1a6ed91 100644",
                "--- a/src/java/org/apache/cassandra/cql3/ReservedKeywords.java",
                "+++ b/src/java/org/apache/cassandra/cql3/ReservedKeywords.java",
                "@@ -87,3 +87,7 @@ class ReservedKeywords",
                "                                                      \"OR\",",
                "-                                                     \"REPLACE\" };",
                "+                                                     \"REPLACE\",",
                "+                                                     \"DEFAULT\",",
                "+                                                     \"UNSET\",",
                "+                                                     \"MBEAN\",",
                "+                                                     \"MBEANS\"};"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/cql3/ReservedKeywords.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_MSG",
                    "message": "The commit message and the advisory description contain the following keywords: default",
                    "relevance": 4
                },
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                }
            ]
        },
        {
            "commit_id": "21978bf9bba520c3d7e838ee6b15536d5b807ef4",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517927107,
            "hunks": 8,
            "message": "Use the correct digest file and reload sstable metadata in nodetool verify Patch by marcuse; reviewed by Ariel Weisberg for CASSANDRA-14217",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/db/compaction/Verifier.java b/src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "index 86bc377d5b..586c7547b1 100644",
                "--- a/src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "@@ -261,2 +261,4 @@ public class Verifier implements Closeable",
                "                 sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, ActiveRepairService.UNREPAIRED_SSTABLE);",
                "+                sstable.reloadSSTableMetadata();",
                "+                cfs.getTracker().notifySSTableRepairedStatusChanged(Collections.singleton(sstable));",
                "             }",
                "diff --git a/src/java/org/apache/cassandra/io/util/DataIntegrityMetadata.java b/src/java/org/apache/cassandra/io/util/DataIntegrityMetadata.java",
                "index 0a89d74b6f..cbf5753edd 100644",
                "--- a/src/java/org/apache/cassandra/io/util/DataIntegrityMetadata.java",
                "+++ b/src/java/org/apache/cassandra/io/util/DataIntegrityMetadata.java",
                "@@ -112,3 +112,3 @@ public class DataIntegrityMetadata",
                "             checksum = descriptor.version.uncompressedChecksumType().newInstance();",
                "-            digestReader = RandomAccessReader.open(new File(descriptor.filenameFor(Component.digestFor(descriptor.version.uncompressedChecksumType()))));",
                "+            digestReader = RandomAccessReader.open(new File(descriptor.filenameFor(descriptor.digestComponent)));",
                "             dataReader = RandomAccessReader.open(new File(descriptor.filenameFor(Component.DATA)));",
                "diff --git a/test/unit/org/apache/cassandra/db/VerifyTest.java b/test/unit/org/apache/cassandra/db/VerifyTest.java",
                "index fc875204b3..0748270202 100644",
                "--- a/test/unit/org/apache/cassandra/db/VerifyTest.java",
                "+++ b/test/unit/org/apache/cassandra/db/VerifyTest.java",
                "@@ -26,2 +26,3 @@ import org.apache.cassandra.Util;",
                " import org.apache.cassandra.UpdateBuilder;",
                "+import org.apache.cassandra.db.compaction.AbstractCompactionStrategy;",
                " import org.apache.cassandra.db.compaction.CompactionManager;",
                "@@ -46,2 +47,5 @@ import java.io.*;",
                " import java.nio.file.Files;",
                "+import java.util.Collections;",
                "+import java.util.List;",
                "+import java.util.concurrent.ExecutionException;",
                " import java.util.zip.CRC32;",
                "@@ -49,2 +53,4 @@ import java.util.zip.CheckedInputStream;",
                "+import static org.junit.Assert.assertFalse;",
                "+import static org.junit.Assert.assertTrue;",
                " import static org.junit.Assert.fail;",
                "@@ -370,2 +376,35 @@ public class VerifyTest",
                "+    @Test",
                "+    public void testMutateRepair() throws IOException, ExecutionException, InterruptedException",
                "+    {",
                "+        CompactionManager.instance.disableAutoCompaction();",
                "+        Keyspace keyspace = Keyspace.open(KEYSPACE);",
                "+        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CORRUPT_CF2);",
                "+",
                "+        fillCF(cfs, 2);",
                "+",
                "+        SSTableReader sstable = cfs.getLiveSSTables().iterator().next();",
                "+        sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, 1);",
                "+        sstable.reloadSSTableMetadata();",
                "+        cfs.getTracker().notifySSTableRepairedStatusChanged(Collections.singleton(sstable));",
                "+        assertTrue(sstable.isRepaired());",
                "+        cfs.forceMajorCompaction();",
                "+",
                "+        sstable = cfs.getLiveSSTables().iterator().next();",
                "+        Long correctChecksum;",
                "+        try (RandomAccessFile file = new RandomAccessFile(sstable.descriptor.filenameFor(sstable.descriptor.digestComponent), \"rw\"))",
                "+        {",
                "+            correctChecksum = Long.parseLong(file.readLine());",
                "+        }",
                "+        writeChecksum(++correctChecksum, sstable.descriptor.filenameFor(sstable.descriptor.digestComponent));",
                "+        try (Verifier verifier = new Verifier(cfs, sstable, false))",
                "+        {",
                "+            verifier.verify(false);",
                "+            fail(\"should be corrupt\");",
                "+        }",
                "+        catch (CorruptSSTableException e)",
                "+        {}",
                "+        assertFalse(sstable.isRepaired());",
                "+    }",
                "+",
                "diff --git a/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java b/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java",
                "index f4f6e851dc..ede4ab6ade 100644",
                "--- a/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java",
                "+++ b/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java",
                "@@ -42,2 +42,3 @@ import org.apache.cassandra.db.ColumnFamilyStore;",
                " import org.apache.cassandra.db.Keyspace;",
                "+import org.apache.cassandra.db.compaction.Verifier;",
                " import org.apache.cassandra.dht.IPartitioner;",
                "@@ -188,2 +189,19 @@ public class LegacySSTableTest",
                "+    @Test",
                "+    public void verifyOldSSTables() throws Exception",
                "+    {",
                "+        for (String legacyVersion : legacyVersions)",
                "+        {",
                "+            loadLegacyTables(legacyVersion);",
                "+            ColumnFamilyStore cfs = Keyspace.open(\"legacy_tables\").getColumnFamilyStore(String.format(\"legacy_%s_simple\", legacyVersion));",
                "+            for (SSTableReader sstable : cfs.getLiveSSTables())",
                "+            {",
                "+                try (Verifier verifier = new Verifier(cfs, sstable, false))",
                "+                {",
                "+                    verifier.verify(true);",
                "+                }",
                "+            }",
                "+        }",
                "+    }",
                "+",
                "     private void streamLegacyTables(String legacyVersion) throws Exception"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "src/java/org/apache/cassandra/io/util/DataIntegrityMetadata.java",
                "test/unit/org/apache/cassandra/db/VerifyTest.java",
                "test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14217": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14217",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d6cc5943250a7c19adb42ef86dc9a186d4e52166",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1515991377,
            "hunks": 65,
            "message": "New Bloomfilter format without changing the byte ordering patch by Jay Zhuang; reviewed by jasobrown for CASSANDRA-9067",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java b/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java",
                "index 6722e1bfc4..757b9a87cf 100644",
                "--- a/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java",
                "+++ b/src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java",
                "@@ -77,2 +77,3 @@ import org.apache.cassandra.utils.concurrent.Ref;",
                " import org.apache.cassandra.utils.concurrent.SelfRefCounted;",
                "+import org.apache.cassandra.utils.BloomFilterSerializer;",
                "@@ -730,3 +731,3 @@ public abstract class SSTableReader extends SSTable implements SelfRefCounted<SS",
                "             load(false, true);",
                "-            loadBloomFilter();",
                "+            loadBloomFilter(descriptor.version.hasOldBfFormat());",
                "         }",
                "@@ -738,4 +739,5 @@ public abstract class SSTableReader extends SSTable implements SelfRefCounted<SS",
                "      * @throws IOException",
                "+     * @param oldBfFormat",
                "      */",
                "-    private void loadBloomFilter() throws IOException",
                "+    private void loadBloomFilter(boolean oldBfFormat) throws IOException",
                "     {",
                "@@ -743,3 +745,3 @@ public abstract class SSTableReader extends SSTable implements SelfRefCounted<SS",
                "         {",
                "-            bf = FilterFactory.deserialize(stream);",
                "+            bf = BloomFilterSerializer.deserialize(stream, oldBfFormat);",
                "         }",
                "diff --git a/src/java/org/apache/cassandra/io/sstable/format/Version.java b/src/java/org/apache/cassandra/io/sstable/format/Version.java",
                "index e8721a6e1c..1d965ce681 100644",
                "--- a/src/java/org/apache/cassandra/io/sstable/format/Version.java",
                "+++ b/src/java/org/apache/cassandra/io/sstable/format/Version.java",
                "@@ -59,2 +59,9 @@ public abstract class Version",
                "+    /**",
                "+     * The old bloomfilter format serializes the data as BIG_ENDIAN long's, the new one uses the",
                "+     * same format as in memory (serializes as bytes).",
                "+     * @return True if the bloomfilter file is old serialization format",
                "+     */",
                "+    public abstract boolean hasOldBfFormat();",
                "+",
                "     public String getVersion()",
                "diff --git a/src/java/org/apache/cassandra/io/sstable/format/big/BigFormat.java b/src/java/org/apache/cassandra/io/sstable/format/big/BigFormat.java",
                "index 5f6dd536e4..db73b4f0bd 100644",
                "--- a/src/java/org/apache/cassandra/io/sstable/format/big/BigFormat.java",
                "+++ b/src/java/org/apache/cassandra/io/sstable/format/big/BigFormat.java",
                "@@ -122,3 +122,3 @@ public class BigFormat implements SSTableFormat",
                "-        // na (4.0.0): uncompressed chunks, pending repair session, checksummed sstable metadata file",
                "+        // na (4.0.0): uncompressed chunks, pending repair session, checksummed sstable metadata file, new Bloomfilter format",
                "         //",
                "@@ -133,2 +133,7 @@ public class BigFormat implements SSTableFormat",
                "         private final boolean hasMetadataChecksum;",
                "+        /**",
                "+         * CASSANDRA-9067: 4.0 bloom filter representation changed (two longs just swapped)",
                "+         * have no 'static' bits caused by using the same upper bits for both bloom filter and token distribution.",
                "+         */",
                "+        private final boolean hasOldBfFormat;",
                "@@ -146,2 +151,3 @@ public class BigFormat implements SSTableFormat",
                "             hasMetadataChecksum = version.compareTo(\"na\") >= 0;",
                "+            hasOldBfFormat = version.compareTo(\"na\") < 0;",
                "         }",
                "@@ -199,2 +205,8 @@ public class BigFormat implements SSTableFormat",
                "         }",
                "+",
                "+        @Override",
                "+        public boolean hasOldBfFormat()",
                "+        {",
                "+            return hasOldBfFormat;",
                "+        }",
                "     }",
                "diff --git a/src/java/org/apache/cassandra/io/sstable/format/big/BigTableWriter.java b/src/java/org/apache/cassandra/io/sstable/format/big/BigTableWriter.java",
                "index 04c7bbfd14..b5488ed346 100644",
                "--- a/src/java/org/apache/cassandra/io/sstable/format/big/BigTableWriter.java",
                "+++ b/src/java/org/apache/cassandra/io/sstable/format/big/BigTableWriter.java",
                "@@ -486,3 +486,3 @@ public class BigTableWriter extends SSTableWriter",
                "                     // bloom filter",
                "-                    FilterFactory.serialize(bf, stream);",
                "+                    BloomFilterSerializer.serialize((BloomFilter) bf, stream);",
                "                     stream.flush();",
                "diff --git a/src/java/org/apache/cassandra/utils/BloomFilterSerializer.java b/src/java/org/apache/cassandra/utils/BloomFilterSerializer.java",
                "index 9e9d15ae92..d3c08b53cb 100644",
                "--- a/src/java/org/apache/cassandra/utils/BloomFilterSerializer.java",
                "+++ b/src/java/org/apache/cassandra/utils/BloomFilterSerializer.java",
                "@@ -19,3 +19,3 @@ package org.apache.cassandra.utils;",
                "-import java.io.DataInput;",
                "+import java.io.DataInputStream;",
                " import java.io.IOException;",
                "@@ -27,3 +27,3 @@ import org.apache.cassandra.utils.obs.OffHeapBitSet;",
                "-final class BloomFilterSerializer",
                "+public final class BloomFilterSerializer",
                " {",
                "@@ -40,6 +40,6 @@ final class BloomFilterSerializer",
                "     @SuppressWarnings(\"resource\")",
                "-    public static BloomFilter deserialize(DataInput in) throws IOException",
                "+    public static BloomFilter deserialize(DataInputStream in, boolean oldBfFormat) throws IOException",
                "     {",
                "         int hashes = in.readInt();",
                "-        IBitSet bs = OffHeapBitSet.deserialize(in);",
                "+        IBitSet bs = OffHeapBitSet.deserialize(in, oldBfFormat);",
                "diff --git a/src/java/org/apache/cassandra/utils/FilterFactory.java b/src/java/org/apache/cassandra/utils/FilterFactory.java",
                "index 947945228c..4cf0cbf74d 100644",
                "--- a/src/java/org/apache/cassandra/utils/FilterFactory.java",
                "+++ b/src/java/org/apache/cassandra/utils/FilterFactory.java",
                "@@ -19,5 +19,2 @@ package org.apache.cassandra.utils;",
                "-import java.io.DataInput;",
                "-import java.io.IOException;",
                "-",
                " import org.slf4j.Logger;",
                "@@ -25,3 +22,2 @@ import org.slf4j.LoggerFactory;",
                "-import org.apache.cassandra.io.util.DataOutputPlus;",
                " import org.apache.cassandra.utils.obs.IBitSet;",
                "@@ -36,12 +32,2 @@ public class FilterFactory",
                "-    public static void serialize(IFilter bf, DataOutputPlus output) throws IOException",
                "-    {",
                "-        BloomFilterSerializer.serialize((BloomFilter) bf, output);",
                "-    }",
                "-",
                "-    public static IFilter deserialize(DataInput input) throws IOException",
                "-    {",
                "-        return BloomFilterSerializer.deserialize(input);",
                "-    }",
                "-",
                "     /**",
                "diff --git a/src/java/org/apache/cassandra/utils/obs/IBitSet.java b/src/java/org/apache/cassandra/utils/obs/IBitSet.java",
                "index 15ff361fbd..b262cf5f87 100644",
                "--- a/src/java/org/apache/cassandra/utils/obs/IBitSet.java",
                "+++ b/src/java/org/apache/cassandra/utils/obs/IBitSet.java",
                "@@ -20,5 +20,5 @@ package org.apache.cassandra.utils.obs;",
                " import java.io.Closeable;",
                "-import java.io.DataOutput;",
                " import java.io.IOException;",
                "+import org.apache.cassandra.io.util.DataOutputPlus;",
                " import org.apache.cassandra.utils.concurrent.Ref;",
                "@@ -46,3 +46,3 @@ public interface IBitSet extends Closeable",
                "-    public void serialize(DataOutput out) throws IOException;",
                "+    public void serialize(DataOutputPlus out) throws IOException;",
                "diff --git a/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java b/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java",
                "index d2c15ca93d..486ec388d8 100644",
                "--- a/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java",
                "+++ b/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java",
                "@@ -20,2 +20,3 @@ package org.apache.cassandra.utils.obs;",
                " import java.io.DataInput;",
                "+import java.io.DataInputStream;",
                " import java.io.DataOutput;",
                "@@ -23,4 +24,9 @@ import java.io.IOException;",
                "+import com.google.common.annotations.VisibleForTesting;",
                "+",
                " import org.apache.cassandra.db.TypeSizes;",
                "+import org.apache.cassandra.io.util.DataOutputPlus;",
                " import org.apache.cassandra.io.util.Memory;",
                "+import org.apache.cassandra.io.util.MemoryOutputStream;",
                "+import org.apache.cassandra.utils.FBUtilities;",
                " import org.apache.cassandra.utils.concurrent.Ref;",
                "@@ -111,15 +117,22 @@ public class OffHeapBitSet implements IBitSet",
                "-    public void serialize(DataOutput out) throws IOException",
                "+    public void serialize(DataOutputPlus out) throws IOException",
                "     {",
                "         out.writeInt((int) (bytes.size() / 8));",
                "-        for (long i = 0; i < bytes.size();)",
                "+        out.write(bytes, 0, bytes.size());",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    public void serializeOldBfFormat(DataOutputPlus out) throws IOException",
                "+    {",
                "+        out.writeInt((int) (bytes.size() / 8));",
                "+        for (long i = 0; i < bytes.size(); )",
                "         {",
                "             long value = ((bytes.getByte(i++) & 0xff) << 0)",
                "-                       + ((bytes.getByte(i++) & 0xff) << 8)",
                "-                       + ((bytes.getByte(i++) & 0xff) << 16)",
                "-                       + ((long) (bytes.getByte(i++) & 0xff) << 24)",
                "-                       + ((long) (bytes.getByte(i++) & 0xff) << 32)",
                "-                       + ((long) (bytes.getByte(i++) & 0xff) << 40)",
                "-                       + ((long) (bytes.getByte(i++) & 0xff) << 48)",
                "-                       + ((long) bytes.getByte(i++) << 56);",
                "+                         + ((bytes.getByte(i++) & 0xff) << 8)",
                "+                         + ((bytes.getByte(i++) & 0xff) << 16)",
                "+                         + ((long) (bytes.getByte(i++) & 0xff) << 24)",
                "+                         + ((long) (bytes.getByte(i++) & 0xff) << 32)",
                "+                         + ((long) (bytes.getByte(i++) & 0xff) << 40)",
                "+                         + ((long) (bytes.getByte(i++) & 0xff) << 48)",
                "+                         + ((long) bytes.getByte(i++) << 56);",
                "             out.writeLong(value);",
                "@@ -134,3 +147,3 @@ public class OffHeapBitSet implements IBitSet",
                "     @SuppressWarnings(\"resource\")",
                "-    public static OffHeapBitSet deserialize(DataInput in) throws IOException",
                "+    public static OffHeapBitSet deserialize(DataInputStream in, boolean oldBfFormat) throws IOException",
                "     {",
                "@@ -138,13 +151,20 @@ public class OffHeapBitSet implements IBitSet",
                "         Memory memory = Memory.allocate(byteCount);",
                "-        for (long i = 0; i < byteCount;)",
                "+        if (oldBfFormat)",
                "+        {",
                "+            for (long i = 0; i < byteCount; )",
                "+            {",
                "+                long v = in.readLong();",
                "+                memory.setByte(i++, (byte) (v >>> 0));",
                "+                memory.setByte(i++, (byte) (v >>> 8));",
                "+                memory.setByte(i++, (byte) (v >>> 16));",
                "+                memory.setByte(i++, (byte) (v >>> 24));",
                "+                memory.setByte(i++, (byte) (v >>> 32));",
                "+                memory.setByte(i++, (byte) (v >>> 40));",
                "+                memory.setByte(i++, (byte) (v >>> 48));",
                "+                memory.setByte(i++, (byte) (v >>> 56));",
                "+            }",
                "+        }",
                "+        else",
                "         {",
                "-            long v = in.readLong();",
                "-            memory.setByte(i++, (byte) (v >>> 0));",
                "-            memory.setByte(i++, (byte) (v >>> 8));",
                "-            memory.setByte(i++, (byte) (v >>> 16));",
                "-            memory.setByte(i++, (byte) (v >>> 24));",
                "-            memory.setByte(i++, (byte) (v >>> 32));",
                "-            memory.setByte(i++, (byte) (v >>> 40));",
                "-            memory.setByte(i++, (byte) (v >>> 48));",
                "-            memory.setByte(i++, (byte) (v >>> 56));",
                "+            FBUtilities.copy(in, new MemoryOutputStream(memory), byteCount);",
                "         }",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-CompressionInfo.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-CompressionInfo.db",
                "index 935d8630bf..ceaa5a3b35 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-CompressionInfo.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-CompressionInfo.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Data.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Data.db",
                "index 9982578898..6968720768 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Data.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Data.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Digest.crc32 b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Digest.crc32",
                "index 1e6cbfc56c..f1c192bebe 100644",
                "--- a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Digest.crc32",
                "+++ b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Digest.crc32",
                "@@ -1 +1 @@",
                "-4285275084",
                "\\ No newline at end of file",
                "+4004129384",
                "\\ No newline at end of file",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Filter.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Filter.db",
                "index 2e1d5d29ca..8868e5c180 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Filter.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Filter.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Index.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Index.db",
                "index 25b063cf25..af16195ea0 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Index.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Index.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Statistics.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Statistics.db",
                "index 59969c279d..970e3851b3 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Statistics.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Statistics.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-CompressionInfo.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-CompressionInfo.db",
                "index d6a1ff866f..f5ad4d0cb1 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-CompressionInfo.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-CompressionInfo.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Data.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Data.db",
                "index 03babd4bac..721771665b 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Data.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Data.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Digest.crc32 b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Digest.crc32",
                "index 7bd9e39838..4f1391a4d5 100644",
                "--- a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Digest.crc32",
                "+++ b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Digest.crc32",
                "@@ -1 +1 @@",
                "-1093063834",
                "\\ No newline at end of file",
                "+4072239034",
                "\\ No newline at end of file",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Filter.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Filter.db",
                "index 2e1d5d29ca..8868e5c180 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Filter.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Filter.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Index.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Index.db",
                "index 5ad5400ad1..6dd3da6e4b 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Index.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Index.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Statistics.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Statistics.db",
                "index 9347a2c7d7..3a0e63f884 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Statistics.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Statistics.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Data.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Data.db",
                "index 414ff070a5..c665dfb00e 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Data.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Data.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Digest.crc32 b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Digest.crc32",
                "index 1dfec4f889..c6c24a74bb 100644",
                "--- a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Digest.crc32",
                "+++ b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Digest.crc32",
                "@@ -1 +1 @@",
                "-2354437953",
                "\\ No newline at end of file",
                "+3772296151",
                "\\ No newline at end of file",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Filter.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Filter.db",
                "index 2e1d5d29ca..8868e5c180 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Filter.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Filter.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Statistics.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Statistics.db",
                "index 33f0516456..67414306d8 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Statistics.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Statistics.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Data.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Data.db",
                "index ed0dadb825..d9fe576f95 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Data.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Data.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Digest.crc32 b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Digest.crc32",
                "index 1480f5db63..de7baed426 100644",
                "--- a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Digest.crc32",
                "+++ b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Digest.crc32",
                "@@ -1 +1 @@",
                "-2023874595",
                "\\ No newline at end of file",
                "+4035692752",
                "\\ No newline at end of file",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Filter.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Filter.db",
                "index 2e1d5d29ca..8868e5c180 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Filter.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Filter.db differ",
                "diff --git a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Statistics.db b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Statistics.db",
                "index 124dacdd5c..e9556d1b7c 100644",
                "Binary files a/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Statistics.db and b/test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Statistics.db differ",
                "diff --git a/test/data/serialization/4.0/utils.BloomFilter1000.bin b/test/data/serialization/4.0/utils.BloomFilter1000.bin",
                "new file mode 100644",
                "index 0000000000..b1bfe99a99",
                "Binary files /dev/null and b/test/data/serialization/4.0/utils.BloomFilter1000.bin differ",
                "diff --git a/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java b/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java",
                "index 48921842ce..6e7d1730bc 100644",
                "--- a/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java",
                "+++ b/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java",
                "@@ -96,3 +96,3 @@ public class LegacySSTableTest",
                "         Assert.assertNotNull(\"System property \" + LEGACY_SSTABLE_ROOT + \" not set\", scp);",
                "-        ",
                "+",
                "         LEGACY_SSTABLE_ROOT = new File(scp).getAbsoluteFile();",
                "@@ -272,3 +272,2 @@ public class LegacySSTableTest",
                "                 String pkValue = Integer.toString(pk);",
                "-                UntypedResultSet rs;",
                "                 if (ck == 0)",
                "diff --git a/test/unit/org/apache/cassandra/utils/BloomFilterTest.java b/test/unit/org/apache/cassandra/utils/BloomFilterTest.java",
                "index 581248c297..1c3afff2ef 100644",
                "--- a/test/unit/org/apache/cassandra/utils/BloomFilterTest.java",
                "+++ b/test/unit/org/apache/cassandra/utils/BloomFilterTest.java",
                "@@ -45,8 +45,5 @@ public class BloomFilterTest",
                "-    public BloomFilterTest()",
                "-    {",
                "-    }",
                "-    public static IFilter testSerialize(IFilter f) throws IOException",
                "+    public static IFilter testSerialize(IFilter f, boolean oldBfFormat) throws IOException",
                "     {",
                "@@ -54,6 +51,13 @@ public class BloomFilterTest",
                "         DataOutputBuffer out = new DataOutputBuffer();",
                "-        FilterFactory.serialize(f, out);",
                "+        if (oldBfFormat)",
                "+        {",
                "+            SerializationsTest.serializeOldBfFormat((BloomFilter) f, out);",
                "+        }",
                "+        else",
                "+        {",
                "+            BloomFilterSerializer.serialize((BloomFilter) f, out);",
                "+        }",
                "         ByteArrayInputStream in = new ByteArrayInputStream(out.getData(), 0, out.getLength());",
                "-        IFilter f2 = FilterFactory.deserialize(new DataInputStream(in));",
                "+        IFilter f2 = BloomFilterSerializer.deserialize(new DataInputStream(in), oldBfFormat);",
                "@@ -134,3 +138,4 @@ public class BloomFilterTest",
                "     {",
                "-        BloomFilterTest.testSerialize(bfInvHashes).close();",
                "+        BloomFilterTest.testSerialize(bfInvHashes, true).close();",
                "+        BloomFilterTest.testSerialize(bfInvHashes, false).close();",
                "     }",
                "@@ -208,4 +213,3 @@ public class BloomFilterTest",
                "         DataOutputStreamPlus out = new BufferedDataOutputStreamPlus(new FileOutputStream(file));",
                "-        FilterFactory.serialize(filter, out);",
                "-        filter.bitset.serialize(out);",
                "+        BloomFilterSerializer.serialize(filter, out);",
                "         out.close();",
                "@@ -214,3 +218,3 @@ public class BloomFilterTest",
                "         DataInputStream in = new DataInputStream(new FileInputStream(file));",
                "-        BloomFilter filter2 = (BloomFilter) FilterFactory.deserialize(in);",
                "+        BloomFilter filter2 = BloomFilterSerializer.deserialize(in, false);",
                "         Assert.assertTrue(filter2.isPresent(FilterTestHelper.wrap(test)));",
                "diff --git a/test/unit/org/apache/cassandra/utils/SerializationsTest.java b/test/unit/org/apache/cassandra/utils/SerializationsTest.java",
                "index 8da4a92e83..f26042850d 100644",
                "--- a/test/unit/org/apache/cassandra/utils/SerializationsTest.java",
                "+++ b/test/unit/org/apache/cassandra/utils/SerializationsTest.java",
                "@@ -33,5 +33,6 @@ import org.apache.cassandra.db.marshal.Int32Type;",
                " import org.apache.cassandra.io.util.DataInputPlus.DataInputStreamPlus;",
                "+import org.apache.cassandra.io.util.DataOutputPlus;",
                " import org.apache.cassandra.io.util.DataOutputStreamPlus;",
                "-import org.apache.cassandra.dht.IPartitioner;",
                " import org.apache.cassandra.dht.Murmur3Partitioner;",
                "+import org.apache.cassandra.utils.obs.OffHeapBitSet;",
                "@@ -42,2 +43,10 @@ public class SerializationsTest extends AbstractSerializationsTester",
                " {",
                "+    // Helper function to serialize old Bloomfilter format, should be removed once the old format is not supported",
                "+    public static void serializeOldBfFormat(BloomFilter bf, DataOutputPlus out) throws IOException",
                "+    {",
                "+        out.writeInt(bf.hashCount);",
                "+        Assert.assertTrue(bf.bitset instanceof OffHeapBitSet);",
                "+        ((OffHeapBitSet) bf.bitset).serializeOldBfFormat(out);",
                "+    }",
                "+",
                "     @BeforeClass",
                "@@ -48,3 +57,3 @@ public class SerializationsTest extends AbstractSerializationsTester",
                "-    private static void testBloomFilterWrite1000() throws IOException",
                "+    private static void testBloomFilterWrite1000(boolean oldBfFormat) throws IOException",
                "     {",
                "@@ -54,5 +63,8 @@ public class SerializationsTest extends AbstractSerializationsTester",
                "                 bf.add(Util.dk(Int32Type.instance.decompose(i)));",
                "-            try (DataOutputStreamPlus out = getOutput(\"3.0\", \"utils.BloomFilter1000.bin\"))",
                "+            try (DataOutputStreamPlus out = getOutput(oldBfFormat ? \"3.0\" : \"4.0\", \"utils.BloomFilter1000.bin\"))",
                "             {",
                "-                FilterFactory.serialize(bf, out);",
                "+                if (oldBfFormat)",
                "+                    serializeOldBfFormat((BloomFilter) bf, out);",
                "+                else",
                "+                    BloomFilterSerializer.serialize((BloomFilter) bf, out);",
                "             }",
                "@@ -65,6 +77,25 @@ public class SerializationsTest extends AbstractSerializationsTester",
                "         if (EXECUTE_WRITES)",
                "-            testBloomFilterWrite1000();",
                "+        {",
                "+            testBloomFilterWrite1000(false);",
                "+            testBloomFilterWrite1000(true);",
                "+        }",
                "+",
                "+        try (DataInputStream in = getInput(\"4.0\", \"utils.BloomFilter1000.bin\");",
                "+             IFilter filter = BloomFilterSerializer.deserialize(in, false))",
                "+        {",
                "+            boolean present;",
                "+            for (int i = 0 ; i < 1000 ; i++)",
                "+            {",
                "+                present = filter.isPresent(Util.dk(Int32Type.instance.decompose(i)));",
                "+                Assert.assertTrue(present);",
                "+            }",
                "+            for (int i = 1000 ; i < 2000 ; i++)",
                "+            {",
                "+                present = filter.isPresent(Util.dk(Int32Type.instance.decompose(i)));",
                "+                Assert.assertFalse(present);",
                "+            }",
                "+        }",
                "         try (DataInputStream in = getInput(\"3.0\", \"utils.BloomFilter1000.bin\");",
                "-             IFilter filter = FilterFactory.deserialize(in))",
                "+             IFilter filter = BloomFilterSerializer.deserialize(in, true))",
                "         {",
                "@@ -87,6 +118,6 @@ public class SerializationsTest extends AbstractSerializationsTester",
                "     {",
                "-        testBloomFilterTable(\"test/data/bloom-filter/la/foo/la-1-big-Filter.db\");",
                "+        testBloomFilterTable(\"test/data/bloom-filter/la/foo/la-1-big-Filter.db\", true);",
                "     }",
                "-    private static void testBloomFilterTable(String file) throws Exception",
                "+    private static void testBloomFilterTable(String file, boolean oldBfFormat) throws Exception",
                "     {",
                "@@ -95,3 +126,3 @@ public class SerializationsTest extends AbstractSerializationsTester",
                "         try (DataInputStream in = new DataInputStream(new FileInputStream(new File(file)));",
                "-             IFilter filter = FilterFactory.deserialize(in))",
                "+             IFilter filter = BloomFilterSerializer.deserialize(in, oldBfFormat))",
                "         {",
                "diff --git a/test/unit/org/apache/cassandra/utils/obs/OffHeapBitSetTest.java b/test/unit/org/apache/cassandra/utils/obs/OffHeapBitSetTest.java",
                "index f0325da41f..49b4c94dd3 100644",
                "--- a/test/unit/org/apache/cassandra/utils/obs/OffHeapBitSetTest.java",
                "+++ b/test/unit/org/apache/cassandra/utils/obs/OffHeapBitSetTest.java",
                "@@ -27,7 +27,5 @@ import java.util.Random;",
                " import com.google.common.collect.Lists;",
                "+import org.apache.cassandra.io.util.DataOutputBuffer;",
                " import org.junit.Assert;",
                " import org.junit.Test;",
                "-import org.junit.rules.ExpectedException;",
                "-",
                "-import org.apache.cassandra.io.util.DataOutputBuffer;",
                "@@ -48,4 +46,3 @@ public class OffHeapBitSetTest",
                "-    @Test",
                "-    public void testOffHeapSerialization() throws IOException",
                "+    private void testOffHeapSerialization(boolean oldBfFormat) throws IOException",
                "     {",
                "@@ -58,6 +55,9 @@ public class OffHeapBitSetTest",
                "             DataOutputBuffer out = new DataOutputBuffer();",
                "-            bs.serialize(out);",
                "+            if (oldBfFormat)",
                "+                bs.serializeOldBfFormat(out);",
                "+            else",
                "+                bs.serialize(out);",
                "             DataInputStream in = new DataInputStream(new ByteArrayInputStream(out.getData()));",
                "-            try (OffHeapBitSet newbs = OffHeapBitSet.deserialize(in))",
                "+            try (OffHeapBitSet newbs = OffHeapBitSet.deserialize(in, oldBfFormat))",
                "             {",
                "@@ -68,2 +68,9 @@ public class OffHeapBitSetTest",
                "+    @Test",
                "+    public void testSerialization() throws IOException",
                "+    {",
                "+        testOffHeapSerialization(true);",
                "+        testOffHeapSerialization(false);",
                "+    }",
                "+",
                "     @Test"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/io/sstable/format/SSTableReader.java",
                "src/java/org/apache/cassandra/io/sstable/format/Version.java",
                "src/java/org/apache/cassandra/io/sstable/format/big/BigFormat.java",
                "src/java/org/apache/cassandra/io/sstable/format/big/BigTableWriter.java",
                "src/java/org/apache/cassandra/utils/BloomFilterSerializer.java",
                "src/java/org/apache/cassandra/utils/FilterFactory.java",
                "src/java/org/apache/cassandra/utils/obs/IBitSet.java",
                "src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-CompressionInfo.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Data.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Digest.crc32",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Filter.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Index.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust/na-1-big-Statistics.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-CompressionInfo.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Data.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Digest.crc32",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Filter.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Index.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_clust_counter/na-1-big-Statistics.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Data.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Digest.crc32",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Filter.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_simple/na-1-big-Statistics.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Data.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Digest.crc32",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Filter.db",
                "test/data/legacy-sstables/na/legacy_tables/legacy_na_simple_counter/na-1-big-Statistics.db",
                "test/data/serialization/4.0/utils.BloomFilter1000.bin",
                "test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java",
                "test/unit/org/apache/cassandra/utils/BloomFilterTest.java",
                "test/unit/org/apache/cassandra/utils/SerializationsTest.java",
                "test/unit/org/apache/cassandra/utils/obs/OffHeapBitSetTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-9067": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-9067",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "714703a08dfb965df40f4dad6ba83196ff95156f",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1513815928,
            "hunks": 1,
            "message": "Fix sstablemetadata date string for minLocalDeletionTime patch by Vince White; reviewed by jasobrown for CASSANDRA-14132",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/tools/SSTableMetadataViewer.java b/src/java/org/apache/cassandra/tools/SSTableMetadataViewer.java",
                "index 5a8a0bcdbf..8ff964f0d5 100755",
                "--- a/src/java/org/apache/cassandra/tools/SSTableMetadataViewer.java",
                "+++ b/src/java/org/apache/cassandra/tools/SSTableMetadataViewer.java",
                "@@ -427,3 +427,3 @@ public class SSTableMetadataViewer",
                "             field(\"EncodingStats minLocalDeletionTime\", encodingStats.minLocalDeletionTime,",
                "-                    toDateString(encodingStats.minLocalDeletionTime, TimeUnit.MILLISECONDS));",
                "+                    toDateString(encodingStats.minLocalDeletionTime, TimeUnit.SECONDS));",
                "             field(\"EncodingStats minTimestamp\", encodingStats.minTimestamp,"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/tools/SSTableMetadataViewer.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14132": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14132",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "b2949439ec62077128103540e42570238520f4ee",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517464888,
            "hunks": 95,
            "message": "Protect against overflow of local expiration time Patch by Paulo Motta; Reviewed by Sam Tunnicliffe for CASSANDRA-14092",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/cql/AbstractModification.java b/src/java/org/apache/cassandra/cql/AbstractModification.java",
                "index 8da2611d72..e98764bb1d 100644",
                "--- a/src/java/org/apache/cassandra/cql/AbstractModification.java",
                "+++ b/src/java/org/apache/cassandra/cql/AbstractModification.java",
                "@@ -22,2 +22,4 @@ import java.util.List;",
                "+import org.apache.cassandra.config.CFMetaData;",
                "+import org.apache.cassandra.config.Schema;",
                " import org.apache.cassandra.db.IMutation;",
                "@@ -27,2 +29,3 @@ import org.apache.cassandra.exceptions.UnauthorizedException;",
                " import org.apache.cassandra.thrift.ThriftClientState;",
                "+import org.w3c.dom.Attr;",
                "@@ -91,2 +94,3 @@ public abstract class AbstractModification",
                "     {",
                "+        Attributes.maybeApplyExpirationDateOverflowPolicy(keyspace, columnFamily, timeToLive);",
                "         return timeToLive;",
                "diff --git a/src/java/org/apache/cassandra/cql/Attributes.java b/src/java/org/apache/cassandra/cql/Attributes.java",
                "index faee3b8a00..c1c37efa8a 100644",
                "--- a/src/java/org/apache/cassandra/cql/Attributes.java",
                "+++ b/src/java/org/apache/cassandra/cql/Attributes.java",
                "@@ -19,3 +19,6 @@ package org.apache.cassandra.cql;",
                "+import org.apache.cassandra.config.CFMetaData;",
                "+import org.apache.cassandra.config.Schema;",
                " import org.apache.cassandra.db.ConsistencyLevel;",
                "+import org.apache.cassandra.exceptions.InvalidRequestException;",
                "@@ -75,2 +78,18 @@ public class Attributes",
                "+    public static void maybeApplyExpirationDateOverflowPolicy(String keyspace, String columnFamily, Integer timeToLive)",
                "+    {",
                "+        CFMetaData metadata = Schema.instance.getCFMetaData(keyspace, columnFamily);",
                "+        if (metadata != null)",
                "+        {",
                "+            try",
                "+            {",
                "+                org.apache.cassandra.cql3.Attributes.maybeApplyExpirationDateOverflowPolicy(metadata, timeToLive, false);",
                "+            }",
                "+            catch (InvalidRequestException e)",
                "+            {",
                "+                throw new RuntimeException(e);",
                "+            }",
                "+        }",
                "+    }",
                "+",
                " }",
                "diff --git a/src/java/org/apache/cassandra/cql/BatchStatement.java b/src/java/org/apache/cassandra/cql/BatchStatement.java",
                "index b141bcc5d8..e5a95b8e7f 100644",
                "--- a/src/java/org/apache/cassandra/cql/BatchStatement.java",
                "+++ b/src/java/org/apache/cassandra/cql/BatchStatement.java",
                "@@ -74,2 +74,6 @@ public class BatchStatement",
                "     {",
                "+        for (AbstractModification statement : statements)",
                "+        {",
                "+            Attributes.maybeApplyExpirationDateOverflowPolicy(statement.keyspace, statement.columnFamily, timeToLive);",
                "+        }",
                "         return timeToLive;",
                "diff --git a/src/java/org/apache/cassandra/cql/CFPropDefs.java b/src/java/org/apache/cassandra/cql/CFPropDefs.java",
                "index f65cb94b27..a0c8d0d877 100644",
                "--- a/src/java/org/apache/cassandra/cql/CFPropDefs.java",
                "+++ b/src/java/org/apache/cassandra/cql/CFPropDefs.java",
                "@@ -30,2 +30,3 @@ import org.slf4j.LoggerFactory;",
                " import org.apache.cassandra.config.CFMetaData;",
                "+import org.apache.cassandra.db.ExpiringCell;",
                " import org.apache.cassandra.exceptions.ConfigurationException;",
                "@@ -193,2 +194,8 @@ public class CFPropDefs {",
                "                         CFMetaData.DEFAULT_DEFAULT_TIME_TO_LIVE));",
                "+",
                "+            if (defaultTimeToLive > ExpiringCell.MAX_TTL)",
                "+                throw new InvalidRequestException(String.format(\"%s must be less than or equal to %d (got %s)\",",
                "+                                                                KW_DEFAULT_TIME_TO_LIVE,",
                "+                                                                ExpiringCell.MAX_TTL,",
                "+                                                                defaultTimeToLive));",
                "         }",
                "diff --git a/src/java/org/apache/cassandra/cql3/Attributes.java b/src/java/org/apache/cassandra/cql3/Attributes.java",
                "index 435757b7a7..23571cab3c 100644",
                "--- a/src/java/org/apache/cassandra/cql3/Attributes.java",
                "+++ b/src/java/org/apache/cassandra/cql3/Attributes.java",
                "@@ -20,4 +20,9 @@ package org.apache.cassandra.cql3;",
                " import java.nio.ByteBuffer;",
                "-import java.util.List;",
                "+import java.util.concurrent.TimeUnit;",
                "+import com.google.common.annotations.VisibleForTesting;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "+",
                "+import org.apache.cassandra.config.CFMetaData;",
                " import org.apache.cassandra.db.ExpiringCell;",
                "@@ -27,2 +32,3 @@ import org.apache.cassandra.exceptions.InvalidRequestException;",
                " import org.apache.cassandra.serializers.MarshalException;",
                "+import org.apache.cassandra.utils.NoSpamLogger;",
                "@@ -34,2 +40,37 @@ public class Attributes",
                " {",
                "+    private static final int EXPIRATION_OVERFLOW_WARNING_INTERVAL_MINUTES = Integer.getInteger(\"cassandra.expiration_overflow_warning_interval_minutes\", 5);",
                "+",
                "+    private static final Logger logger = LoggerFactory.getLogger(Attributes.class);",
                "+",
                "+    public enum ExpirationDateOverflowPolicy",
                "+    {",
                "+        REJECT, CAP",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    public static ExpirationDateOverflowPolicy policy;",
                "+",
                "+    static {",
                "+        String policyAsString = System.getProperty(\"cassandra.expiration_date_overflow_policy\", ExpirationDateOverflowPolicy.REJECT.name());",
                "+        try",
                "+        {",
                "+            policy = ExpirationDateOverflowPolicy.valueOf(policyAsString.toUpperCase());",
                "+        }",
                "+        catch (RuntimeException e)",
                "+        {",
                "+            logger.warn(\"Invalid expiration date overflow policy: {}. Using default: {}\", policyAsString, ExpirationDateOverflowPolicy.REJECT.name());",
                "+            policy = ExpirationDateOverflowPolicy.REJECT;",
                "+        }",
                "+    }",
                "+",
                "+    public static final String MAXIMUM_EXPIRATION_DATE_EXCEEDED_WARNING = \"Request on table {}.{} with {}ttl of {} seconds exceeds maximum supported expiration \" +",
                "+                                                                          \"date of 2038-01-19T03:14:06+00:00 and will have its expiration capped to that date. \" +",
                "+                                                                          \"In order to avoid this use a lower TTL or upgrade to a version where this limitation \" +",
                "+                                                                          \"is fixed. See CASSANDRA-14092 for more details.\";",
                "+",
                "+    public static final String MAXIMUM_EXPIRATION_DATE_EXCEEDED_REJECT_MESSAGE = \"Request on table %s.%s with %sttl of %d seconds exceeds maximum supported expiration \" +",
                "+                                                                                 \"date of 2038-01-19T03:14:06+00:00. In order to avoid this use a lower TTL, change \" +",
                "+                                                                                 \"the expiration date overflow policy or upgrade to a version where this limitation \" +",
                "+                                                                                 \"is fixed. See CASSANDRA-14092 for more details.\";",
                "+",
                "     private final Term timestamp;",
                "@@ -79,6 +120,9 @@ public class Attributes",
                "-    public int getTimeToLive(QueryOptions options) throws InvalidRequestException",
                "+    public int getTimeToLive(QueryOptions options, CFMetaData metadata) throws InvalidRequestException",
                "     {",
                "         if (timeToLive == null)",
                "-            return 0;",
                "+        {",
                "+            maybeApplyExpirationDateOverflowPolicy(metadata, metadata.getDefaultTimeToLive(), true);",
                "+            return metadata.getDefaultTimeToLive();",
                "+        }",
                "@@ -104,2 +148,4 @@ public class Attributes",
                "+        maybeApplyExpirationDateOverflowPolicy(metadata, ttl, false);",
                "+",
                "         return ttl;",
                "@@ -137,2 +183,31 @@ public class Attributes",
                "     }",
                "+",
                "+    public static void maybeApplyExpirationDateOverflowPolicy(CFMetaData metadata, int ttl, boolean isDefaultTTL) throws InvalidRequestException",
                "+    {",
                "+        if (ttl == 0)",
                "+            return;",
                "+",
                "+        // Check for localExpirationTime overflow (CASSANDRA-14092)",
                "+        int nowInSecs = (int)(System.currentTimeMillis() / 1000);",
                "+        if (ttl + nowInSecs < 0)",
                "+        {",
                "+            switch (policy)",
                "+            {",
                "+                case CAP:",
                "+                    /**",
                "+                     * Capping at this stage is basically not rejecting the request. The actual capping is done",
                "+                     * by {@link org.apache.cassandra.db.BufferExpiringCell#computeLocalExpirationTime(int)},",
                "+                     * which converts the negative TTL to {@link org.apache.cassandra.db.BufferExpiringCell#MAX_DELETION_TIME}",
                "+                     */",
                "+                    NoSpamLogger.log(logger, NoSpamLogger.Level.WARN, EXPIRATION_OVERFLOW_WARNING_INTERVAL_MINUTES,",
                "+                                     TimeUnit.MINUTES, MAXIMUM_EXPIRATION_DATE_EXCEEDED_WARNING,",
                "+                                     metadata.ksName, metadata.cfName, isDefaultTTL? \"default \" : \"\", ttl);",
                "+                    return;",
                "+",
                "+                default: //REJECT",
                "+                    throw new InvalidRequestException(String.format(MAXIMUM_EXPIRATION_DATE_EXCEEDED_REJECT_MESSAGE, metadata.ksName, metadata.cfName,",
                "+                                                                    isDefaultTTL? \"default \" : \"\", ttl));",
                "+            }",
                "+        }",
                "+    }",
                " }",
                "diff --git a/src/java/org/apache/cassandra/cql3/statements/CFPropDefs.java b/src/java/org/apache/cassandra/cql3/statements/CFPropDefs.java",
                "index 17edd6d086..27dd57f2ef 100644",
                "--- a/src/java/org/apache/cassandra/cql3/statements/CFPropDefs.java",
                "+++ b/src/java/org/apache/cassandra/cql3/statements/CFPropDefs.java",
                "@@ -24,2 +24,3 @@ import org.apache.cassandra.config.CFMetaData;",
                " import org.apache.cassandra.config.CFMetaData.SpeculativeRetry;",
                "+import org.apache.cassandra.db.ExpiringCell;",
                " import org.apache.cassandra.db.compaction.AbstractCompactionStrategy;",
                "@@ -129,2 +130,8 @@ public class CFPropDefs extends PropertyDefinitions",
                "         validateMinimumInt(KW_DEFAULT_TIME_TO_LIVE, 0, CFMetaData.DEFAULT_DEFAULT_TIME_TO_LIVE);",
                "+        Integer defaultTimeToLive = getInt(KW_DEFAULT_TIME_TO_LIVE, 0);",
                "+        if (defaultTimeToLive > ExpiringCell.MAX_TTL)",
                "+            throw new ConfigurationException(String.format(\"%s must be less than or equal to %d (got %s)\",",
                "+                                                           KW_DEFAULT_TIME_TO_LIVE,",
                "+                                                           ExpiringCell.MAX_TTL,",
                "+                                                           defaultTimeToLive));",
                "diff --git a/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java b/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java",
                "index f84188a60a..8038c6c3fa 100644",
                "--- a/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java",
                "+++ b/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java",
                "@@ -126,3 +126,3 @@ public abstract class ModificationStatement implements CQLStatement",
                "     {",
                "-        return attrs.getTimeToLive(options);",
                "+        return attrs.getTimeToLive(options, cfm);",
                "     }",
                "diff --git a/src/java/org/apache/cassandra/db/AbstractNativeCell.java b/src/java/org/apache/cassandra/db/AbstractNativeCell.java",
                "index e01d860e69..1b2c384f1c 100644",
                "--- a/src/java/org/apache/cassandra/db/AbstractNativeCell.java",
                "+++ b/src/java/org/apache/cassandra/db/AbstractNativeCell.java",
                "@@ -577,2 +577,8 @@ public abstract class AbstractNativeCell extends AbstractCell implements CellNam",
                "+    @Override",
                "+    public Cell withUpdatedTimestampAndLocalDeletionTime(long newTimestamp, int newLocalDeletionTime)",
                "+    {",
                "+        throw new UnsupportedOperationException();",
                "+    }",
                "+",
                "     protected long internalSize()",
                "diff --git a/src/java/org/apache/cassandra/db/BufferCell.java b/src/java/org/apache/cassandra/db/BufferCell.java",
                "index a7d632d908..ee5fe418c2 100644",
                "--- a/src/java/org/apache/cassandra/db/BufferCell.java",
                "+++ b/src/java/org/apache/cassandra/db/BufferCell.java",
                "@@ -70,2 +70,8 @@ public class BufferCell extends AbstractCell",
                "+    @Override",
                "+    public Cell withUpdatedTimestampAndLocalDeletionTime(long newTimestamp, int newLocalDeletionTime)",
                "+    {",
                "+        throw new UnsupportedOperationException();",
                "+    }",
                "+",
                "     @Override",
                "diff --git a/src/java/org/apache/cassandra/db/BufferDeletedCell.java b/src/java/org/apache/cassandra/db/BufferDeletedCell.java",
                "index a38f322bb2..3762e1f399 100644",
                "--- a/src/java/org/apache/cassandra/db/BufferDeletedCell.java",
                "+++ b/src/java/org/apache/cassandra/db/BufferDeletedCell.java",
                "@@ -55,2 +55,8 @@ public class BufferDeletedCell extends BufferCell implements DeletedCell",
                "+    @Override",
                "+    public Cell withUpdatedTimestampAndLocalDeletionTime(long newTimestamp, int newLocalDeletionTime)",
                "+    {",
                "+        throw new UnsupportedOperationException();",
                "+    }",
                "+",
                "     @Override",
                "diff --git a/src/java/org/apache/cassandra/db/BufferExpiringCell.java b/src/java/org/apache/cassandra/db/BufferExpiringCell.java",
                "index 25172c830c..ea406767c9 100644",
                "--- a/src/java/org/apache/cassandra/db/BufferExpiringCell.java",
                "+++ b/src/java/org/apache/cassandra/db/BufferExpiringCell.java",
                "@@ -33,2 +33,4 @@ public class BufferExpiringCell extends BufferCell implements ExpiringCell",
                " {",
                "+    public static final int MAX_DELETION_TIME = Integer.MAX_VALUE - 1;",
                "+",
                "     private final int localExpirationTime;",
                "@@ -38,3 +40,6 @@ public class BufferExpiringCell extends BufferCell implements ExpiringCell",
                "     {",
                "-        this(name, value, timestamp, timeToLive, (int) (System.currentTimeMillis() / 1000) + timeToLive);",
                "+        super(name, value, timestamp);",
                "+        assert timeToLive > 0 : timeToLive;",
                "+        this.timeToLive = timeToLive;",
                "+        this.localExpirationTime = computeLocalExpirationTime(timeToLive);",
                "     }",
                "@@ -45,3 +50,2 @@ public class BufferExpiringCell extends BufferCell implements ExpiringCell",
                "         assert timeToLive > 0 : timeToLive;",
                "-        assert localExpirationTime > 0 : localExpirationTime;",
                "         this.timeToLive = timeToLive;",
                "@@ -67,2 +71,8 @@ public class BufferExpiringCell extends BufferCell implements ExpiringCell",
                "+    @Override",
                "+    public Cell withUpdatedTimestampAndLocalDeletionTime(long newTimestamp, int newLocalDeletionTime)",
                "+    {",
                "+        return new BufferExpiringCell(name(), value(), newTimestamp, timeToLive, newLocalDeletionTime);",
                "+    }",
                "+",
                "     @Override",
                "@@ -178,3 +188,5 @@ public class BufferExpiringCell extends BufferCell implements ExpiringCell",
                "     {",
                "-        if (localExpirationTime >= expireBefore || flag == ColumnSerializer.Flag.PRESERVE_SIZE)",
                "+        // CASSANDRA-14092 may have written rows with negative localExpirationTime, so we don't turn them into tombstones yet",
                "+        // to be able to recover them with scrub.",
                "+        if (localExpirationTime < 0 || localExpirationTime >= expireBefore || flag == ColumnSerializer.Flag.PRESERVE_SIZE)",
                "             return new BufferExpiringCell(name, value, timestamp, timeToLive, localExpirationTime);",
                "@@ -186,2 +198,18 @@ public class BufferExpiringCell extends BufferCell implements ExpiringCell",
                "     }",
                "+",
                "+    /**",
                "+     * This method computes the {@link #localExpirationTime}, maybe capping to the maximum representable value",
                "+     * which is {@link #MAX_DELETION_TIME}.",
                "+     *",
                "+     * Please note that the {@link org.apache.cassandra.cql3.Attributes.ExpirationDateOverflowPolicy} is applied",
                "+     * during {@link org.apache.cassandra.cql3.Attributes#maybeApplyExpirationDateOverflowPolicy(CFMetaData, int, boolean)},",
                "+     * so if the request was not denied it means it's expiration date should be capped.",
                "+     *",
                "+     * See CASSANDRA-14092",
                "+     */",
                "+    private int computeLocalExpirationTime(int timeToLive)",
                "+    {",
                "+        int localExpirationTime =  (int) (System.currentTimeMillis() / 1000) + timeToLive;",
                "+        return localExpirationTime >= 0? localExpirationTime : MAX_DELETION_TIME;",
                "+    }",
                " }",
                "diff --git a/src/java/org/apache/cassandra/db/Cell.java b/src/java/org/apache/cassandra/db/Cell.java",
                "index 7c3926ac9d..274f369c9f 100644",
                "--- a/src/java/org/apache/cassandra/db/Cell.java",
                "+++ b/src/java/org/apache/cassandra/db/Cell.java",
                "@@ -40,2 +40,4 @@ public interface Cell extends OnDiskAtom",
                "+    public Cell withUpdatedTimestampAndLocalDeletionTime(long newTimestamp, int newLocalDeletionTime);",
                "+",
                "     @Override",
                "diff --git a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "index 6e82745f8f..2989b9d6ea 100644",
                "--- a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "+++ b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "@@ -1518,3 +1518,3 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean",
                "-    public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, int jobs) throws ExecutionException, InterruptedException",
                "+    public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs) throws ExecutionException, InterruptedException",
                "     {",
                "@@ -1523,3 +1523,3 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean",
                "             snapshotWithoutFlush(\"pre-scrub-\" + System.currentTimeMillis());",
                "-        return CompactionManager.instance.performScrub(ColumnFamilyStore.this, skipCorrupted, checkData, jobs);",
                "+        return CompactionManager.instance.performScrub(ColumnFamilyStore.this, skipCorrupted, checkData, reinsertOverflowedTTLRows, jobs);",
                "     }",
                "diff --git a/src/java/org/apache/cassandra/db/DeletionTime.java b/src/java/org/apache/cassandra/db/DeletionTime.java",
                "index 99cfe35753..c10a15f1a1 100644",
                "--- a/src/java/org/apache/cassandra/db/DeletionTime.java",
                "+++ b/src/java/org/apache/cassandra/db/DeletionTime.java",
                "@@ -62,2 +62,3 @@ public class DeletionTime implements Comparable<DeletionTime>, IMeasurableMemory",
                "     {",
                "+        assert localDeletionTime >= 0 : localDeletionTime;",
                "         this.markedForDeleteAt = markedForDeleteAt;",
                "diff --git a/src/java/org/apache/cassandra/db/compaction/CompactionManager.java b/src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "index 87819baa60..6e3634a1db 100644",
                "--- a/src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "@@ -360,3 +360,10 @@ public class CompactionManager implements CompactionManagerMBean",
                "+    @Deprecated",
                "     public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData, int jobs) throws InterruptedException, ExecutionException",
                "+    {",
                "+        return performScrub(cfs, skipCorrupted, checkData, false, jobs);",
                "+    }",
                "+",
                "+    public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData,",
                "+                                           final boolean reinsertOverflowedTTLRows, int jobs) throws InterruptedException, ExecutionException",
                "     {",
                "@@ -374,3 +381,3 @@ public class CompactionManager implements CompactionManagerMBean",
                "             {",
                "-                scrubOne(cfs, input, skipCorrupted, checkData);",
                "+                scrubOne(cfs, input, skipCorrupted, checkData, reinsertOverflowedTTLRows);",
                "             }",
                "@@ -712,5 +719,5 @@ public class CompactionManager implements CompactionManagerMBean",
                "-    private void scrubOne(ColumnFamilyStore cfs, SSTableReader sstable, boolean skipCorrupted, boolean checkData) throws IOException",
                "+    private void scrubOne(ColumnFamilyStore cfs, SSTableReader sstable, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows) throws IOException",
                "     {",
                "-        Scrubber scrubber = new Scrubber(cfs, sstable, skipCorrupted, false, checkData);",
                "+        Scrubber scrubber = new Scrubber(cfs, sstable, skipCorrupted, false, checkData, reinsertOverflowedTTLRows);",
                "@@ -1354,3 +1361,3 @@ public class CompactionManager implements CompactionManagerMBean",
                "             DebuggableThreadPoolExecutor.maybeResetTraceSessionWrapper(r);",
                "-    ",
                "+",
                "             if (t == null)",
                "diff --git a/src/java/org/apache/cassandra/db/compaction/Scrubber.java b/src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "index 2df3665ee6..6d4537cc19 100644",
                "--- a/src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "@@ -30,2 +30,3 @@ import org.apache.cassandra.db.*;",
                " import org.apache.cassandra.db.columniterator.OnDiskAtomIterator;",
                "+import org.apache.cassandra.db.composites.CellNames;",
                " import org.apache.cassandra.io.sstable.*;",
                "@@ -37,2 +38,3 @@ import org.apache.cassandra.utils.JVMStabilityInspector;",
                " import org.apache.cassandra.utils.OutputHandler;",
                "+import org.apache.cassandra.utils.memory.HeapAllocator;",
                "@@ -45,2 +47,3 @@ public class Scrubber implements Closeable",
                "     public final boolean validateColumns;",
                "+    private final boolean reinsertOverflowedTTLRows;",
                "@@ -69,2 +72,3 @@ public class Scrubber implements Closeable",
                "     private final OutputHandler outputHandler;",
                "+    private NegativeLocalDeletionInfoMetrics negativeLocalDeletionInfoMetrics = new NegativeLocalDeletionInfoMetrics();",
                "@@ -81,6 +85,13 @@ public class Scrubber implements Closeable",
                "     {",
                "-        this(cfs, sstable, skipCorrupted, new OutputHandler.LogOutput(), isOffline, checkData);",
                "+        this(cfs, sstable, skipCorrupted, isOffline, checkData, false);",
                "     }",
                "-    public Scrubber(ColumnFamilyStore cfs, SSTableReader sstable, boolean skipCorrupted, OutputHandler outputHandler, boolean isOffline, boolean checkData) throws IOException",
                "+    public Scrubber(ColumnFamilyStore cfs, SSTableReader sstable, boolean skipCorrupted, boolean isOffline, boolean checkData,",
                "+                    boolean reinsertOverflowedTTLRows) throws IOException",
                "+    {",
                "+        this(cfs, sstable, skipCorrupted, new OutputHandler.LogOutput(), isOffline, checkData, reinsertOverflowedTTLRows);",
                "+    }",
                "+",
                "+    public Scrubber(ColumnFamilyStore cfs, SSTableReader sstable, boolean skipCorrupted, OutputHandler outputHandler, boolean isOffline, boolean checkData,",
                "+                    boolean reinsertOverflowedTTLRows) throws IOException",
                "     {",
                "@@ -92,2 +103,3 @@ public class Scrubber implements Closeable",
                "         this.validateColumns = checkData;",
                "+        this.reinsertOverflowedTTLRows = reinsertOverflowedTTLRows;",
                "@@ -133,2 +145,5 @@ public class Scrubber implements Closeable",
                "         this.nextRowPositionFromIndex = 0;",
                "+",
                "+        if (reinsertOverflowedTTLRows)",
                "+            outputHandler.output(\"Starting scrub with reinsert overflowed TTL option\");",
                "     }",
                "@@ -313,2 +328,4 @@ public class Scrubber implements Closeable",
                "             outputHandler.output(\"Scrub of \" + sstable + \" complete: \" + goodRows + \" rows in new sstable and \" + emptyRows + \" empty (tombstoned) rows dropped\");",
                "+            if (negativeLocalDeletionInfoMetrics.fixedRows > 0)",
                "+                outputHandler.output(\"Fixed \" + negativeLocalDeletionInfoMetrics.fixedRows + \" rows with overflowed local deletion time.\");",
                "             if (badRows > 0)",
                "@@ -324,3 +341,3 @@ public class Scrubber implements Closeable",
                "         // to the outOfOrderRows that will be later written to a new SSTable.",
                "-        OrderCheckerIterator atoms = new OrderCheckerIterator(new SSTableIdentityIterator(sstable, dataFile, key, dataSize, validateColumns),",
                "+        OrderCheckerIterator atoms = new OrderCheckerIterator(getIterator(key, dataSize),",
                "                                                               cfs.metadata.comparator.onDiskAtomComparator());",
                "@@ -344,2 +361,14 @@ public class Scrubber implements Closeable",
                "+    /**",
                "+     * Only wrap with {@link FixNegativeLocalDeletionTimeIterator} if {@link #reinsertOverflowedTTLRows} option",
                "+     * is specified",
                "+     */",
                "+    private OnDiskAtomIterator getIterator(DecoratedKey key, long dataSize)",
                "+    {",
                "+        SSTableIdentityIterator sstableIdentityIterator = new SSTableIdentityIterator(sstable, dataFile, key, dataSize, validateColumns);",
                "+        return reinsertOverflowedTTLRows ? new FixNegativeLocalDeletionTimeIterator(sstableIdentityIterator,",
                "+                                                                                    outputHandler,",
                "+                                                                                    negativeLocalDeletionInfoMetrics) : sstableIdentityIterator;",
                "+    }",
                "+",
                "     private void updateIndexKey()",
                "@@ -518,2 +547,7 @@ public class Scrubber implements Closeable",
                "+    public class NegativeLocalDeletionInfoMetrics",
                "+    {",
                "+        public volatile int fixedRows = 0;",
                "+    }",
                "+",
                "     /**",
                "@@ -603,2 +637,59 @@ public class Scrubber implements Closeable",
                "     }",
                "+",
                "+    /**",
                "+     * This iterator converts negative {@link BufferExpiringCell#getLocalDeletionTime()} into {@link BufferExpiringCell#MAX_DELETION_TIME}",
                "+     *",
                "+     * This is to recover entries with overflowed localExpirationTime due to CASSANDRA-14092",
                "+     */",
                "+    private static final class FixNegativeLocalDeletionTimeIterator extends AbstractIterator<OnDiskAtom> implements OnDiskAtomIterator",
                "+    {",
                "+        /**",
                "+         * The decorated iterator.",
                "+         */",
                "+        private final OnDiskAtomIterator iterator;",
                "+",
                "+        private final OutputHandler outputHandler;",
                "+        private final NegativeLocalDeletionInfoMetrics negativeLocalExpirationTimeMetrics;",
                "+",
                "+        public FixNegativeLocalDeletionTimeIterator(OnDiskAtomIterator iterator, OutputHandler outputHandler,",
                "+                                                    NegativeLocalDeletionInfoMetrics negativeLocalDeletionInfoMetrics)",
                "+        {",
                "+            this.iterator = iterator;",
                "+            this.outputHandler = outputHandler;",
                "+            this.negativeLocalExpirationTimeMetrics = negativeLocalDeletionInfoMetrics;",
                "+        }",
                "+",
                "+        public ColumnFamily getColumnFamily()",
                "+        {",
                "+            return iterator.getColumnFamily();",
                "+        }",
                "+",
                "+        public DecoratedKey getKey()",
                "+        {",
                "+            return iterator.getKey();",
                "+        }",
                "+",
                "+        public void close() throws IOException",
                "+        {",
                "+            iterator.close();",
                "+        }",
                "+",
                "+        @Override",
                "+        protected OnDiskAtom computeNext()",
                "+        {",
                "+            if (!iterator.hasNext())",
                "+                return endOfData();",
                "+",
                "+            OnDiskAtom next = iterator.next();",
                "+",
                "+            if (next instanceof ExpiringCell && next.getLocalDeletionTime() < 0)",
                "+            {",
                "+                outputHandler.debug(String.format(\"Found cell with negative local expiration time: %s\", ((ExpiringCell) next).getString(getColumnFamily().getComparator()), getColumnFamily()));",
                "+                negativeLocalExpirationTimeMetrics.fixedRows++;",
                "+                next = ((Cell) next).localCopy(getColumnFamily().metadata(), HeapAllocator.instance).withUpdatedTimestampAndLocalDeletionTime(next.timestamp() + 1, BufferExpiringCell.MAX_DELETION_TIME);",
                "+            }",
                "+",
                "+            return next;",
                "+        }",
                "+    }",
                " }",
                "diff --git a/src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java b/src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java",
                "index 557c3deb0e..d7187657f7 100644",
                "--- a/src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java",
                "+++ b/src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java",
                "@@ -30,2 +30,3 @@ import org.apache.cassandra.config.CFMetaData;",
                " import org.apache.cassandra.config.DatabaseDescriptor;",
                "+import org.apache.cassandra.cql3.Attributes;",
                " import org.apache.cassandra.db.*;",
                "@@ -33,2 +34,3 @@ import org.apache.cassandra.db.context.CounterContext;",
                " import org.apache.cassandra.dht.IPartitioner;",
                "+import org.apache.cassandra.exceptions.InvalidRequestException;",
                " import org.apache.cassandra.io.sstable.metadata.MetadataCollector;",
                "@@ -157,3 +159,16 @@ public abstract class AbstractSSTableSimpleWriter implements Closeable",
                "     {",
                "-        addColumn(new BufferExpiringCell(metadata.comparator.cellFromByteBuffer(name), value, timestamp, ttl, (int)(expirationTimestampMS / 1000)));",
                "+        int localExpirationTime = (int) (expirationTimestampMS / 1000);",
                "+        try",
                "+        {",
                "+            // This will throw exception if policy is REJECT and now() + ttl is higher than MAX_DELETION_TIME",
                "+            Attributes.maybeApplyExpirationDateOverflowPolicy(metadata, ttl, false);",
                "+            // If exception was not thrown, this means the policy was CAP, so we check for overflow and cap if that's the case",
                "+            if (localExpirationTime < 0)",
                "+                localExpirationTime = BufferExpiringCell.MAX_DELETION_TIME;",
                "+        }",
                "+        catch (InvalidRequestException e)",
                "+        {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+        addColumn(new BufferExpiringCell(metadata.comparator.cellFromByteBuffer(name), value, timestamp, ttl, localExpirationTime));",
                "     }",
                "diff --git a/src/java/org/apache/cassandra/service/StorageService.java b/src/java/org/apache/cassandra/service/StorageService.java",
                "index 27939f9b26..a7a8ca7079 100644",
                "--- a/src/java/org/apache/cassandra/service/StorageService.java",
                "+++ b/src/java/org/apache/cassandra/service/StorageService.java",
                "@@ -2416,2 +2416,8 @@ public class StorageService extends NotificationBroadcasterSupport implements IE",
                "     public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException",
                "+    {",
                "+        return scrub(disableSnapshot, skipCorrupted, checkData, false, jobs, keyspaceName, columnFamilies);",
                "+    }",
                "+",
                "+    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows,",
                "+                     int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException",
                "     {",
                "@@ -2420,3 +2426,3 @@ public class StorageService extends NotificationBroadcasterSupport implements IE",
                "         {",
                "-            CompactionManager.AllSSTableOpStatus oneStatus = cfStore.scrub(disableSnapshot, skipCorrupted, checkData, jobs);",
                "+            CompactionManager.AllSSTableOpStatus oneStatus = cfStore.scrub(disableSnapshot, skipCorrupted, checkData, reinsertOverflowedTTLRows, jobs);",
                "             if (oneStatus != CompactionManager.AllSSTableOpStatus.SUCCESSFUL)",
                "diff --git a/src/java/org/apache/cassandra/service/StorageServiceMBean.java b/src/java/org/apache/cassandra/service/StorageServiceMBean.java",
                "index d3a1725a78..90c0fb5972 100644",
                "--- a/src/java/org/apache/cassandra/service/StorageServiceMBean.java",
                "+++ b/src/java/org/apache/cassandra/service/StorageServiceMBean.java",
                "@@ -221,3 +221,3 @@ public interface StorageServiceMBean extends NotificationEmitter",
                "      * Takes the snapshot of a multiple column family from different keyspaces. A snapshot name must be specified.",
                "-     * ",
                "+     *",
                "      * @param tag",
                "@@ -228,5 +228,5 @@ public interface StorageServiceMBean extends NotificationEmitter",
                "     public void takeMultipleColumnFamilySnapshot(String tag, String... columnFamilyList) throws IOException;",
                "-    ",
                "-    ",
                "-    ",
                "+",
                "+",
                "+",
                "     /**",
                "@@ -276,4 +276,7 @@ public interface StorageServiceMBean extends NotificationEmitter",
                "     public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException;",
                "+    @Deprecated",
                "     public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException;",
                "+    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException;",
                "+",
                "     /**",
                "@@ -383,7 +386,7 @@ public interface StorageServiceMBean extends NotificationEmitter",
                "      * The logback configuration should have < jmxConfigurator /> set",
                "-     * ",
                "+     *",
                "      * @param classQualifier The logger's classQualifer",
                "      * @param level The log level",
                "-     * @throws Exception ",
                "-     * ",
                "+     * @throws Exception",
                "+     *",
                "      *  @see ch.qos.logback.classic.Level#toLevel(String)",
                "diff --git a/src/java/org/apache/cassandra/thrift/ThriftValidation.java b/src/java/org/apache/cassandra/thrift/ThriftValidation.java",
                "index d5d9f73b91..10e7185d46 100644",
                "--- a/src/java/org/apache/cassandra/thrift/ThriftValidation.java",
                "+++ b/src/java/org/apache/cassandra/thrift/ThriftValidation.java",
                "@@ -26,2 +26,3 @@ import org.slf4j.LoggerFactory;",
                " import org.apache.cassandra.config.*;",
                "+import org.apache.cassandra.cql3.Attributes;",
                " import org.apache.cassandra.cql3.ColumnIdentifier;",
                "@@ -316,3 +317,3 @@ public class ThriftValidation",
                "-            validateTtl(cosc.column);",
                "+            validateTtl(metadata, cosc.column);",
                "             validateColumnPath(metadata, new ColumnPath(metadata.cfName).setSuper_column((ByteBuffer)null).setColumn(cosc.column.name));",
                "@@ -351,3 +352,3 @@ public class ThriftValidation",
                "-    private static void validateTtl(Column column) throws org.apache.cassandra.exceptions.InvalidRequestException",
                "+    private static void validateTtl(CFMetaData metadata, Column column) throws org.apache.cassandra.exceptions.InvalidRequestException",
                "     {",
                "@@ -360,2 +361,3 @@ public class ThriftValidation",
                "                 throw new org.apache.cassandra.exceptions.InvalidRequestException(String.format(\"ttl is too large. requested (%d) maximum (%d)\", column.ttl, ExpiringCell.MAX_TTL));",
                "+            Attributes.maybeApplyExpirationDateOverflowPolicy(metadata, column.ttl, false);",
                "         }",
                "@@ -363,2 +365,3 @@ public class ThriftValidation",
                "         {",
                "+            Attributes.maybeApplyExpirationDateOverflowPolicy(metadata, metadata.getDefaultTimeToLive(), true);",
                "             // if it's not set, then it should be zero -- here we are just checking to make sure Thrift doesn't change that contract with us.",
                "@@ -436,3 +439,3 @@ public class ThriftValidation",
                "     {",
                "-        validateTtl(column);",
                "+        validateTtl(metadata, column);",
                "         if (!column.isSetValue())",
                "diff --git a/src/java/org/apache/cassandra/tools/NodeProbe.java b/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "index 13c7acf414..fcd4110632 100644",
                "--- a/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "+++ b/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "@@ -245,5 +245,5 @@ public class NodeProbe implements AutoCloseable",
                "-    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException",
                "+    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException",
                "     {",
                "-        return ssProxy.scrub(disableSnapshot, skipCorrupted, checkData, jobs, keyspaceName, columnFamilies);",
                "+        return ssProxy.scrub(disableSnapshot, skipCorrupted, checkData, reinsertOverflowedTTLRows, jobs, keyspaceName, columnFamilies);",
                "     }",
                "@@ -270,6 +270,6 @@ public class NodeProbe implements AutoCloseable",
                "-    public void scrub(PrintStream out, boolean disableSnapshot, boolean skipCorrupted, boolean checkData, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException",
                "+    public void scrub(PrintStream out, boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException",
                "     {",
                "         checkJobs(out, jobs);",
                "-        if (scrub(disableSnapshot, skipCorrupted, checkData, jobs, keyspaceName, columnFamilies) != 0)",
                "+        if (scrub(disableSnapshot, skipCorrupted, checkData, reinsertOverflowedTTLRows, jobs, keyspaceName, columnFamilies) != 0)",
                "         {",
                "@@ -564,3 +564,3 @@ public class NodeProbe implements AutoCloseable",
                "      * Take a snapshot of all column family from different keyspaces.",
                "-     * ",
                "+     *",
                "      * @param snapshotName",
                "@@ -1304,3 +1304,3 @@ public class NodeProbe implements AutoCloseable",
                "         {",
                "-          throw new RuntimeException(\"Error setting log for \" + classQualifier +\" on level \" + level +\". Please check logback configuration and ensure to have <jmxConfigurator /> set\", e); ",
                "+          throw new RuntimeException(\"Error setting log for \" + classQualifier +\" on level \" + level +\". Please check logback configuration and ensure to have <jmxConfigurator /> set\", e);",
                "         }",
                "diff --git a/src/java/org/apache/cassandra/tools/NodeTool.java b/src/java/org/apache/cassandra/tools/NodeTool.java",
                "index d1afb6fe08..54d7fb7a05 100644",
                "--- a/src/java/org/apache/cassandra/tools/NodeTool.java",
                "+++ b/src/java/org/apache/cassandra/tools/NodeTool.java",
                "@@ -553,3 +553,3 @@ public class NodeTool",
                "                 ownerships = probe.effectiveOwnership(keyspace);",
                "-            } ",
                "+            }",
                "             catch (IllegalStateException ex)",
                "@@ -559,3 +559,3 @@ public class NodeTool",
                "                 showEffectiveOwnership = false;",
                "-            } ",
                "+            }",
                "             catch (IllegalArgumentException ex)",
                "@@ -566,3 +566,3 @@ public class NodeTool",
                "-            ",
                "+",
                "             System.out.println();",
                "@@ -1284,2 +1284,7 @@ public class NodeTool",
                "+        @Option(title = \"reinsert_overflowed_ttl\",",
                "+        name = {\"r\", \"--reinsert-overflowed-ttl\"},",
                "+        description = StandaloneScrubber.REINSERT_OVERFLOWED_TTL_OPTION_DESCRIPTION)",
                "+        private boolean reinsertOverflowedTTL = false;",
                "+",
                "         @Override",
                "@@ -1294,3 +1299,3 @@ public class NodeTool",
                "                 {",
                "-                    probe.scrub(System.out, disableSnapshot, skipCorrupted, !noValidation, jobs, keyspace, cfnames);",
                "+                    probe.scrub(System.out, disableSnapshot, skipCorrupted, reinsertOverflowedTTL   , !noValidation, jobs, keyspace, cfnames);",
                "                 } catch (Exception e)",
                "@@ -2199,3 +2204,3 @@ public class NodeTool",
                "             epSnitchInfo = probe.getEndpointSnitchInfoProxy();",
                "-            ",
                "+",
                "             StringBuffer errors = new StringBuffer();",
                "@@ -2251,5 +2256,5 @@ public class NodeTool",
                "             }",
                "-            ",
                "+",
                "             System.out.printf(\"%n\" + errors.toString());",
                "-            ",
                "+",
                "         }",
                "@@ -2728,3 +2733,3 @@ public class NodeTool",
                "     }",
                "-    ",
                "+",
                "     @Command(name = \"setlogginglevel\", description = \"Set the log level threshold for a given class. If both class and level are empty/null, it will reset to the initial configuration\")",
                "@@ -2743,3 +2748,3 @@ public class NodeTool",
                "     }",
                "-    ",
                "+",
                "     @Command(name = \"getlogginglevels\", description = \"Get the runtime logging levels\")",
                "diff --git a/src/java/org/apache/cassandra/tools/StandaloneScrubber.java b/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "index fdf6c8dd89..59d13d5519 100644",
                "--- a/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "+++ b/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "@@ -49,2 +49,8 @@ public class StandaloneScrubber",
                " {",
                "+    public static final String REINSERT_OVERFLOWED_TTL_OPTION_DESCRIPTION = \"Rewrites rows with overflowed expiration date affected by CASSANDRA-14092 with \" +",
                "+                                                                            \"the maximum supported expiration date of 2038-01-19T03:14:06+00:00. \" +",
                "+                                                                            \"The rows are rewritten with the original timestamp incremented by one millisecond \" +",
                "+                                                                            \"to override/supersede any potential tombstone that may have been generated \" +",
                "+                                                                            \"during compaction of the affected rows.\";",
                "+",
                "     private static final String TOOL_NAME = \"sstablescrub\";",
                "@@ -56,2 +62,3 @@ public class StandaloneScrubber",
                "     private static final String NO_VALIDATE_OPTION = \"no-validate\";",
                "+    private static final String REINSERT_OVERFLOWED_TTL_OPTION = \"reinsert-overflowed-ttl\";",
                "@@ -112,3 +119,3 @@ public class StandaloneScrubber",
                "                     {",
                "-                        Scrubber scrubber = new Scrubber(cfs, sstable, options.skipCorrupted, handler, true, !options.noValidate);",
                "+                        Scrubber scrubber = new Scrubber(cfs, sstable, options.skipCorrupted, handler, true, !options.noValidate, options.reinsertOverflowedTTL);",
                "                         try",
                "@@ -194,2 +201,3 @@ public class StandaloneScrubber",
                "         public boolean noValidate;",
                "+        public boolean reinsertOverflowedTTL;",
                "@@ -234,2 +242,3 @@ public class StandaloneScrubber",
                "                 opts.noValidate = cmd.hasOption(NO_VALIDATE_OPTION);",
                "+                opts.reinsertOverflowedTTL = cmd.hasOption(REINSERT_OVERFLOWED_TTL_OPTION);",
                "@@ -260,2 +269,3 @@ public class StandaloneScrubber",
                "             options.addOption(\"n\",  NO_VALIDATE_OPTION,    \"do not validate columns using column validator\");",
                "+            options.addOption(\"r\", REINSERT_OVERFLOWED_TTL_OPTION, REINSERT_OVERFLOWED_TTL_OPTION_DESCRIPTION);",
                "             return options;",
                "diff --git a/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-CompressionInfo.db b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-CompressionInfo.db",
                "new file mode 100644",
                "index 0000000000..d7cc13bc74",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-CompressionInfo.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Data.db b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Data.db",
                "new file mode 100644",
                "index 0000000000..0e3da66ced",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Data.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Digest.sha1 b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Digest.sha1",
                "new file mode 100644",
                "index 0000000000..8a6dcba895",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Digest.sha1",
                "@@ -0,0 +1 @@",
                "+4012184764",
                "\\ No newline at end of file",
                "diff --git a/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Filter.db b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Filter.db",
                "new file mode 100644",
                "index 0000000000..f8e53beeef",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Filter.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Index.db b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Index.db",
                "new file mode 100644",
                "index 0000000000..3ab96eee9d",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Index.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Statistics.db b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Statistics.db",
                "new file mode 100644",
                "index 0000000000..9bde77e31e",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Statistics.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Summary.db b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Summary.db",
                "new file mode 100644",
                "index 0000000000..788b66a06d",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Summary.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-CompressionInfo.db b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-CompressionInfo.db",
                "new file mode 100644",
                "index 0000000000..38373b4de9",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-CompressionInfo.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Data.db b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Data.db",
                "new file mode 100644",
                "index 0000000000..bdd4549dfd",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Data.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Digest.sha1 b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Digest.sha1",
                "new file mode 100644",
                "index 0000000000..f58914a466",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Digest.sha1",
                "@@ -0,0 +1 @@",
                "+3463582096",
                "\\ No newline at end of file",
                "diff --git a/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Filter.db b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Filter.db",
                "new file mode 100644",
                "index 0000000000..f8e53beeef",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Filter.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Index.db b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Index.db",
                "new file mode 100644",
                "index 0000000000..38a6e4cee1",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Index.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Statistics.db b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Statistics.db",
                "new file mode 100644",
                "index 0000000000..8ee9116ef5",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Statistics.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Summary.db b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Summary.db",
                "new file mode 100644",
                "index 0000000000..788b66a06d",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Summary.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-CompressionInfo.db b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-CompressionInfo.db",
                "new file mode 100644",
                "index 0000000000..04a738493d",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-CompressionInfo.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Data.db b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Data.db",
                "new file mode 100644",
                "index 0000000000..1fc8ba4190",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Data.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Digest.sha1 b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Digest.sha1",
                "new file mode 100644",
                "index 0000000000..cd091ad40a",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Digest.sha1",
                "@@ -0,0 +1 @@",
                "+1524836732",
                "\\ No newline at end of file",
                "diff --git a/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Filter.db b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Filter.db",
                "new file mode 100644",
                "index 0000000000..f8e53beeef",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Filter.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Index.db b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Index.db",
                "new file mode 100644",
                "index 0000000000..5fb34e86cf",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Index.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Statistics.db b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Statistics.db",
                "new file mode 100644",
                "index 0000000000..4d961fbde9",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Statistics.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Summary.db b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Summary.db",
                "new file mode 100644",
                "index 0000000000..788b66a06d",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Summary.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-CompressionInfo.db b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-CompressionInfo.db",
                "new file mode 100644",
                "index 0000000000..c814fef86a",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-CompressionInfo.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Data.db b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Data.db",
                "new file mode 100644",
                "index 0000000000..92032a7d87",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Data.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Digest.sha1 b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Digest.sha1",
                "new file mode 100644",
                "index 0000000000..a45d821b52",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Digest.sha1",
                "@@ -0,0 +1 @@",
                "+2189764235",
                "\\ No newline at end of file",
                "diff --git a/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Filter.db b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Filter.db",
                "new file mode 100644",
                "index 0000000000..f8e53beeef",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Filter.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Index.db b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Index.db",
                "new file mode 100644",
                "index 0000000000..8291383c15",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Index.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Statistics.db b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Statistics.db",
                "new file mode 100644",
                "index 0000000000..68f76aefab",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Statistics.db differ",
                "diff --git a/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Summary.db b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Summary.db",
                "new file mode 100644",
                "index 0000000000..788b66a06d",
                "Binary files /dev/null and b/test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Summary.db differ",
                "diff --git a/test/unit/org/apache/cassandra/cql3/validation/operations/TTLTest.java b/test/unit/org/apache/cassandra/cql3/validation/operations/TTLTest.java",
                "new file mode 100644",
                "index 0000000000..ab4ef21d0c",
                "--- /dev/null",
                "+++ b/test/unit/org/apache/cassandra/cql3/validation/operations/TTLTest.java",
                "@@ -0,0 +1,410 @@",
                "+package org.apache.cassandra.cql3.validation.operations;",
                "+",
                "+import java.io.File;",
                "+import java.io.FileInputStream;",
                "+import java.io.FileOutputStream;",
                "+import java.io.IOException;",
                "+",
                "+import static org.junit.Assert.assertEquals;",
                "+import static org.junit.Assert.assertTrue;",
                "+import static org.junit.Assert.fail;",
                "+",
                "+import org.apache.cassandra.cql3.Attributes;",
                "+import org.apache.cassandra.cql3.CQLTester;",
                "+import org.apache.cassandra.cql3.UntypedResultSet;",
                "+import org.apache.cassandra.db.BufferExpiringCell;",
                "+import org.apache.cassandra.db.ColumnFamilyStore;",
                "+import org.apache.cassandra.db.ExpiringCell;",
                "+import org.apache.cassandra.db.Keyspace;",
                "+import org.apache.cassandra.exceptions.InvalidRequestException;",
                "+import org.apache.cassandra.utils.FBUtilities;",
                "+",
                "+import org.junit.Test;",
                "+",
                "+public class TTLTest extends CQLTester",
                "+{",
                "+    public static String NEGATIVE_LOCAL_EXPIRATION_TEST_DIR = \"test/data/negative-local-expiration-test/%s\";",
                "+",
                "+    public static int MAX_TTL = ExpiringCell.MAX_TTL;",
                "+",
                "+    public static final String SIMPLE_NOCLUSTERING = \"table1\";",
                "+    public static final String SIMPLE_CLUSTERING = \"table2\";",
                "+    public static final String COMPLEX_NOCLUSTERING = \"table3\";",
                "+    public static final String COMPLEX_CLUSTERING = \"table4\";",
                "+",
                "+    @Test",
                "+    public void testTTLPerRequestLimit() throws Throwable",
                "+    {",
                "+        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int)\");",
                "+        // insert with low TTL should not be denied",
                "+        execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL ?\", 10); // max ttl",
                "+",
                "+        try",
                "+        {",
                "+            execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL ?\", MAX_TTL + 1);",
                "+            fail(\"Expect InvalidRequestException\");",
                "+        }",
                "+        catch (InvalidRequestException e)",
                "+        {",
                "+            assertTrue(e.getMessage().contains(\"ttl is too large.\"));",
                "+        }",
                "+",
                "+        try",
                "+        {",
                "+            execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL ?\", -1);",
                "+            fail(\"Expect InvalidRequestException\");",
                "+        }",
                "+        catch (InvalidRequestException e)",
                "+        {",
                "+            assertTrue(e.getMessage().contains(\"A TTL must be greater or equal to 0\"));",
                "+        }",
                "+        execute(\"TRUNCATE %s\");",
                "+",
                "+        // insert with low TTL should not be denied",
                "+        execute(\"UPDATE %s USING TTL ? SET i = 1 WHERE k = 2\", 5); // max ttl",
                "+",
                "+        try",
                "+        {",
                "+            execute(\"UPDATE %s USING TTL ? SET i = 1 WHERE k = 2\", MAX_TTL + 1);",
                "+            fail(\"Expect InvalidRequestException\");",
                "+        }",
                "+        catch (InvalidRequestException e)",
                "+        {",
                "+            assertTrue(e.getMessage().contains(\"ttl is too large.\"));",
                "+        }",
                "+",
                "+        try",
                "+        {",
                "+            execute(\"UPDATE %s USING TTL ? SET i = 1 WHERE k = 2\", -1);",
                "+            fail(\"Expect InvalidRequestException\");",
                "+        }",
                "+        catch (InvalidRequestException e)",
                "+        {",
                "+            assertTrue(e.getMessage().contains(\"A TTL must be greater or equal to 0\"));",
                "+        }",
                "+    }",
                "+",
                "+",
                "+    @Test",
                "+    public void testTTLDefaultLimit() throws Throwable",
                "+    {",
                "+        try",
                "+        {",
                "+            createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=-1\");",
                "+            fail(\"Expect Invalid schema\");",
                "+        }",
                "+        catch (RuntimeException e)",
                "+        {",
                "+            assertTrue(e.getCause()",
                "+                        .getCause()",
                "+                        .getMessage()",
                "+                        .contains(\"default_time_to_live cannot be smaller than 0\"));",
                "+        }",
                "+        try",
                "+        {",
                "+            createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\"",
                "+                        + (MAX_TTL + 1));",
                "+            fail(\"Expect Invalid schema\");",
                "+        }",
                "+        catch (RuntimeException e)",
                "+        {",
                "+            assertTrue(e.getCause()",
                "+                        .getCause()",
                "+                        .getMessage()",
                "+                        .contains(\"default_time_to_live must be less than or equal to \" + MAX_TTL + \" (got \"",
                "+                                  + (MAX_TTL + 1) + \")\"));",
                "+        }",
                "+",
                "+        // table with default low TTL should not be denied",
                "+        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\" + 5);",
                "+        execute(\"INSERT INTO %s (k, i) VALUES (1, 1)\");",
                "+    }",
                "+",
                "+    @Test",
                "+    public void testRejectExpirationDateOverflowPolicy() throws Throwable",
                "+    {",
                "+        Attributes.policy = Attributes.ExpirationDateOverflowPolicy.REJECT;",
                "+        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int)\");",
                "+        try",
                "+        {",
                "+            execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL \" + MAX_TTL);",
                "+        }",
                "+        catch (InvalidRequestException e)",
                "+        {",
                "+            assertTrue(e.getMessage().contains(\"exceeds maximum supported expiration date\"));",
                "+        }",
                "+        try",
                "+        {",
                "+            createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\" + MAX_TTL);",
                "+            execute(\"INSERT INTO %s (k, i) VALUES (1, 1)\");",
                "+        }",
                "+        catch (InvalidRequestException e)",
                "+        {",
                "+            assertTrue(e.getMessage().contains(\"exceeds maximum supported expiration date\"));",
                "+        }",
                "+    }",
                "+",
                "+    @Test",
                "+    public void testCapExpirationDatePolicyDefaultTTL() throws Throwable",
                "+    {",
                "+        Attributes.policy = Attributes.ExpirationDateOverflowPolicy.CAP;",
                "+        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\" + MAX_TTL);",
                "+        execute(\"INSERT INTO %s (k, i) VALUES (1, 1)\");",
                "+        checkTTLIsCapped(\"i\");",
                "+        Attributes.policy = Attributes.ExpirationDateOverflowPolicy.REJECT;",
                "+    }",
                "+",
                "+    @Test",
                "+    public void testCapExpirationDatePolicyPerRequest() throws Throwable",
                "+    {",
                "+        // Test cap policy",
                "+        Attributes.policy = Attributes.ExpirationDateOverflowPolicy.CAP;",
                "+",
                "+        // simple column, clustering, flush",
                "+        baseCapExpirationDateOverflowTest(true, true, true);",
                "+        // simple column, clustering, noflush",
                "+        baseCapExpirationDateOverflowTest(true, true, false);",
                "+        // simple column, noclustering, flush",
                "+        baseCapExpirationDateOverflowTest(true, false, true);",
                "+        // simple column, noclustering, noflush",
                "+        baseCapExpirationDateOverflowTest(true, false, false);",
                "+        // complex column, clustering, flush",
                "+        baseCapExpirationDateOverflowTest(false, true, true);",
                "+        // complex column, clustering, noflush",
                "+        baseCapExpirationDateOverflowTest(false, true, false);",
                "+        // complex column, noclustering, flush",
                "+        baseCapExpirationDateOverflowTest(false, false, true);",
                "+        // complex column, noclustering, noflush",
                "+        baseCapExpirationDateOverflowTest(false, false, false);",
                "+        // complex column, noclustering, flush",
                "+        baseCapExpirationDateOverflowTest(false, false, false);",
                "+",
                "+        // Return to previous policy",
                "+        Attributes.policy = Attributes.ExpirationDateOverflowPolicy.REJECT;",
                "+    }",
                "+",
                "+    @Test",
                "+    public void testRecoverOverflowedExpirationWithScrub() throws Throwable",
                "+    {",
                "+        createTable(true, true);",
                "+        createTable(true, false);",
                "+        createTable(false, true);",
                "+        createTable(false, false);",
                "+",
                "+        baseTestRecoverOverflowedExpiration(false, false);",
                "+        baseTestRecoverOverflowedExpiration(true, false);",
                "+        baseTestRecoverOverflowedExpiration(true, true);",
                "+    }",
                "+",
                "+    public void baseCapExpirationDateOverflowTest(boolean simple, boolean clustering, boolean flush) throws Throwable",
                "+    {",
                "+        // Create Table",
                "+        if (simple)",
                "+        {",
                "+            if (clustering)",
                "+                createTable(\"create table %s (k int, a int, b int, primary key(k, a))\");",
                "+            else",
                "+                createTable(\"create table %s (k int primary key, a int, b int)\");",
                "+        }",
                "+        else",
                "+        {",
                "+            if (clustering)",
                "+                createTable(\"create table %s (k int, a int, b set<text>, primary key(k, a))\");",
                "+            else",
                "+                createTable(\"create table %s (k int primary key, a int, b set<text>)\");",
                "+        }",
                "+",
                "+        // Insert data with INSERT and UPDATE",
                "+        if (simple)",
                "+        {",
                "+            execute(\"INSERT INTO %s (k, a, b) VALUES (?, ?, ?) USING TTL \" + MAX_TTL, 2, 2, 2);",
                "+            if (clustering)",
                "+                execute(\"UPDATE %s USING TTL \" + MAX_TTL + \" SET b = 1 WHERE k = 1 AND a = 1;\");",
                "+            else",
                "+                execute(\"UPDATE %s USING TTL \" + MAX_TTL + \" SET a = 1, b = 1 WHERE k = 1;\");",
                "+        }",
                "+        else",
                "+        {",
                "+            execute(\"INSERT INTO %s (k, a, b) VALUES (?, ?, ?) USING TTL \" + MAX_TTL, 2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\"));",
                "+            if (clustering)",
                "+                execute(\"UPDATE  %s USING TTL \" + MAX_TTL + \" SET b = ? WHERE k = 1 AND a = 1;\", set(\"v11\", \"v12\", \"v13\", \"v14\"));",
                "+            else",
                "+                execute(\"UPDATE  %s USING TTL \" + MAX_TTL + \" SET a = 1, b = ? WHERE k = 1;\", set(\"v11\", \"v12\", \"v13\", \"v14\"));",
                "+        }",
                "+",
                "+        // Maybe Flush",
                "+        Keyspace ks = Keyspace.open(keyspace());",
                "+        if (flush)",
                "+            FBUtilities.waitOnFutures(ks.flush());",
                "+",
                "+        // Verify data",
                "+        verifyData(simple);",
                "+",
                "+        // Maybe major compact",
                "+        if (flush)",
                "+        {",
                "+            // Major compact and check data is still present",
                "+            ks.getColumnFamilyStore(currentTable()).forceMajorCompaction();",
                "+",
                "+            // Verify data again",
                "+            verifyData(simple);",
                "+        }",
                "+    }",
                "+",
                "+    public void baseTestRecoverOverflowedExpiration(boolean runScrub, boolean reinsertOverflowedTTL) throws Throwable",
                "+    {",
                "+        // simple column, clustering",
                "+        testRecoverOverflowedExpirationWithScrub(true, true, runScrub, reinsertOverflowedTTL);",
                "+        // simple column, noclustering",
                "+        testRecoverOverflowedExpirationWithScrub(true, false, runScrub, reinsertOverflowedTTL);",
                "+        // complex column, clustering",
                "+        testRecoverOverflowedExpirationWithScrub(false, true, runScrub, reinsertOverflowedTTL);",
                "+        // complex column, noclustering",
                "+        testRecoverOverflowedExpirationWithScrub(false, false, runScrub, reinsertOverflowedTTL);",
                "+    }",
                "+",
                "+    private void verifyData(boolean simple) throws Throwable",
                "+    {",
                "+        if (simple)",
                "+        {",
                "+            assertRows(execute(\"SELECT * from %s\"), row(1, 1, 1), row(2, 2, 2));",
                "+        }",
                "+        else",
                "+        {",
                "+            assertRows(execute(\"SELECT * from %s\"), row(1, 1, set(\"v11\", \"v12\", \"v13\", \"v14\")), row(2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\")));",
                "+        }",
                "+        // Cannot retrieve TTL from collections",
                "+        if (simple)",
                "+            checkTTLIsCapped(\"b\");",
                "+    }",
                "+",
                "+    /**",
                "+     * Verify that the computed TTL is approximately equal to the maximum allowed ttl given the",
                "+     * {@link ExpiringCell#getLocalDeletionTime()} field limitation (CASSANDRA-14092)",
                "+     */",
                "+    private void checkTTLIsCapped(String field) throws Throwable",
                "+    {",
                "+",
                "+        // TTL is computed dynamically from row expiration time, so if it is",
                "+        // equal or higher to the minimum max TTL we compute before the query",
                "+        // we are fine.",
                "+        int minMaxTTL = computeMaxTTL();",
                "+        UntypedResultSet execute = execute(\"SELECT ttl(\" + field + \") FROM %s\");",
                "+        for (UntypedResultSet.Row row : execute)",
                "+        {",
                "+            int ttl = row.getInt(\"ttl(\" + field + \")\");",
                "+            assertTrue(ttl >= minMaxTTL);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * The max TTL is computed such that the TTL summed with the current time is equal to the maximum",
                "+     * allowed expiration time {@link BufferExpiringCell#getLocalDeletionTime()} (2038-01-19T03:14:06+00:00)",
                "+     */",
                "+    private int computeMaxTTL()",
                "+    {",
                "+        int nowInSecs = (int) (System.currentTimeMillis() / 1000);",
                "+        return BufferExpiringCell.MAX_DELETION_TIME - nowInSecs;",
                "+    }",
                "+",
                "+    public void testRecoverOverflowedExpirationWithScrub(boolean simple, boolean clustering, boolean runScrub, boolean reinsertOverflowedTTL) throws Throwable",
                "+    {",
                "+        if (reinsertOverflowedTTL)",
                "+        {",
                "+            assert runScrub;",
                "+        }",
                "+",
                "+        Keyspace keyspace = Keyspace.open(KEYSPACE);",
                "+        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(getTableName(simple, clustering));",
                "+",
                "+        assertEquals(0, cfs.getLiveSSTableCount());",
                "+",
                "+        copySSTablesToTableDir(simple, clustering);",
                "+",
                "+        cfs.loadNewSSTables();",
                "+",
                "+        if (runScrub)",
                "+        {",
                "+            cfs.scrub(true, false, false, reinsertOverflowedTTL, 1);",
                "+        }",
                "+",
                "+        if (reinsertOverflowedTTL)",
                "+        {",
                "+            if (simple)",
                "+            {",
                "+                UntypedResultSet execute = execute(String.format(\"SELECT * from %s.%s\", KEYSPACE, getTableName(simple, clustering)));",
                "+                assertRows(execute, row(1, 1, 1), row(2, 2, 2));",
                "+",
                "+            }",
                "+            else",
                "+                assertRows(execute(String.format(\"SELECT * from %s.%s\", KEYSPACE, getTableName(simple, clustering))), row(1, 1, set(\"v11\", \"v12\", \"v13\", \"v14\")), row(2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\")));",
                "+",
                "+            cfs.forceMajorCompaction();",
                "+",
                "+            if (simple)",
                "+                assertRows(execute(String.format(\"SELECT * from %s.%s\", KEYSPACE, getTableName(simple, clustering))), row(1, 1, 1), row(2, 2, 2));",
                "+            else",
                "+                assertRows(execute(String.format(\"SELECT * from %s.%s\", KEYSPACE, getTableName(simple, clustering))), row(1, 1, set(\"v11\", \"v12\", \"v13\", \"v14\")), row(2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\")));",
                "+        }",
                "+        else",
                "+        {",
                "+            assertEmpty(execute(String.format(\"SELECT * from %s.%s\", KEYSPACE, getTableName(simple, clustering))));",
                "+        }",
                "+        cfs.truncateBlocking(); //cleanup for next tests",
                "+    }",
                "+",
                "+    private void copySSTablesToTableDir(boolean simple, boolean clustering) throws IOException",
                "+    {",
                "+        File destDir = Keyspace.open(KEYSPACE).getColumnFamilyStore(getTableName(simple, clustering)).directories.getCFDirectories().iterator().next();",
                "+        File sourceDir = getTableDir(simple, clustering);",
                "+        for (File file : sourceDir.listFiles())",
                "+        {",
                "+            copyFile(file, destDir);",
                "+        }",
                "+    }",
                "+",
                "+    private void createTable(boolean simple, boolean clustering) throws Throwable",
                "+    {",
                "+        if (simple)",
                "+        {",
                "+            if (clustering)",
                "+                execute(String.format(\"create table %s.%s (k int, a int, b int, primary key(k, a))\", KEYSPACE, getTableName(simple, clustering)));",
                "+            else",
                "+                execute(String.format(\"create table %s.%s (k int primary key, a int, b int)\", KEYSPACE, getTableName(simple, clustering)));",
                "+        }",
                "+        else",
                "+        {",
                "+            if (clustering)",
                "+                execute(String.format(\"create table %s.%s (k int, a int, b set<text>, primary key(k, a))\", KEYSPACE, getTableName(simple, clustering)));",
                "+            else",
                "+                execute(String.format(\"create table %s.%s (k int primary key, a int, b set<text>)\", KEYSPACE, getTableName(simple, clustering)));",
                "+        }",
                "+    }",
                "+",
                "+    private static File getTableDir(boolean simple, boolean clustering)",
                "+    {",
                "+        return new File(String.format(NEGATIVE_LOCAL_EXPIRATION_TEST_DIR, getTableName(simple, clustering)));",
                "+    }",
                "+",
                "+    private static void copyFile(File src, File dest) throws IOException",
                "+    {",
                "+        byte[] buf = new byte[65536];",
                "+        if (src.isFile())",
                "+        {",
                "+            File target = new File(dest, src.getName());",
                "+            int rd;",
                "+            FileInputStream is = new FileInputStream(src);",
                "+            FileOutputStream os = new FileOutputStream(target);",
                "+            while ((rd = is.read(buf)) >= 0)",
                "+                os.write(buf, 0, rd);",
                "+        }",
                "+    }",
                "+",
                "+    public static String getTableName(boolean simple, boolean clustering)",
                "+    {",
                "+        if (simple)",
                "+            return clustering ? SIMPLE_CLUSTERING : SIMPLE_NOCLUSTERING;",
                "+        else",
                "+            return clustering ? COMPLEX_CLUSTERING : COMPLEX_NOCLUSTERING;",
                "+    }",
                "+}"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/cql/AbstractModification.java",
                "src/java/org/apache/cassandra/cql/Attributes.java",
                "src/java/org/apache/cassandra/cql/BatchStatement.java",
                "src/java/org/apache/cassandra/cql/CFPropDefs.java",
                "src/java/org/apache/cassandra/cql3/Attributes.java",
                "src/java/org/apache/cassandra/cql3/statements/CFPropDefs.java",
                "src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java",
                "src/java/org/apache/cassandra/db/AbstractNativeCell.java",
                "src/java/org/apache/cassandra/db/BufferCell.java",
                "src/java/org/apache/cassandra/db/BufferDeletedCell.java",
                "src/java/org/apache/cassandra/db/BufferExpiringCell.java",
                "src/java/org/apache/cassandra/db/Cell.java",
                "src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "src/java/org/apache/cassandra/db/DeletionTime.java",
                "src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java",
                "src/java/org/apache/cassandra/service/StorageService.java",
                "src/java/org/apache/cassandra/service/StorageServiceMBean.java",
                "src/java/org/apache/cassandra/thrift/ThriftValidation.java",
                "src/java/org/apache/cassandra/tools/NodeProbe.java",
                "src/java/org/apache/cassandra/tools/NodeTool.java",
                "src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-CompressionInfo.db",
                "test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Data.db",
                "test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Digest.sha1",
                "test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Filter.db",
                "test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Index.db",
                "test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Statistics.db",
                "test/data/negative-local-expiration-test/table1/cql_test_keyspace-table1-ka-1-Summary.db",
                "test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-CompressionInfo.db",
                "test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Data.db",
                "test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Digest.sha1",
                "test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Filter.db",
                "test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Index.db",
                "test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Statistics.db",
                "test/data/negative-local-expiration-test/table2/cql_test_keyspace-table2-ka-1-Summary.db",
                "test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-CompressionInfo.db",
                "test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Data.db",
                "test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Digest.sha1",
                "test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Filter.db",
                "test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Index.db",
                "test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Statistics.db",
                "test/data/negative-local-expiration-test/table3/cql_test_keyspace-table3-ka-1-Summary.db",
                "test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-CompressionInfo.db",
                "test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Data.db",
                "test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Digest.sha1",
                "test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Filter.db",
                "test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Index.db",
                "test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Statistics.db",
                "test/data/negative-local-expiration-test/table4/cql_test_keyspace-table4-ka-1-Summary.db",
                "test/unit/org/apache/cassandra/cql3/validation/operations/TTLTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14092": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-2.1.20",
                "cassandra-2.1.21",
                "cassandra-2.1.22",
                "cassandra-2.2.12",
                "cassandra-2.2.13",
                "cassandra-2.2.14",
                "cassandra-2.2.15",
                "cassandra-2.2.16",
                "cassandra-2.2.17",
                "cassandra-2.2.18",
                "cassandra-2.2.19",
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14092",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7e362e78c342d8ca016de12218732a3e5f7dcc36",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516657569,
            "hunks": 12,
            "message": "Nodetool tablehistograms to print statics for all the tables Patch by Jaydeepkumar Chovatia; Reviewed by Chris Lohfink for CASSANDRA-14185",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/tools/nodetool/TableHistograms.java b/src/java/org/apache/cassandra/tools/nodetool/TableHistograms.java",
                "index 5286e18410..910176ac9d 100644",
                "--- a/src/java/org/apache/cassandra/tools/nodetool/TableHistograms.java",
                "+++ b/src/java/org/apache/cassandra/tools/nodetool/TableHistograms.java",
                "@@ -25,4 +25,9 @@ import io.airlift.airline.Command;",
                " import java.util.ArrayList;",
                "+import java.util.Arrays;",
                "+import java.util.HashMap;",
                "+import java.util.Iterator;",
                " import java.util.List;",
                "+import java.util.Map;",
                "+import org.apache.cassandra.db.ColumnFamilyStoreMBean;",
                " import org.apache.cassandra.metrics.CassandraMetricsRegistry;",
                "@@ -36,3 +41,3 @@ public class TableHistograms extends NodeToolCmd",
                " {",
                "-    @Arguments(usage = \"<keyspace> <table> | <keyspace.table>\", description = \"The keyspace and table name\")",
                "+    @Arguments(usage = \"[<keyspace> <table> | <keyspace.table>]\", description = \"The keyspace and table name\")",
                "     private List<String> args = new ArrayList<>();",
                "@@ -42,7 +47,6 @@ public class TableHistograms extends NodeToolCmd",
                "     {",
                "-        String keyspace = null, table = null;",
                "+        Map<String, List<String>> tablesList = new HashMap<>();",
                "         if (args.size() == 2)",
                "         {",
                "-            keyspace = args.get(0);",
                "-            table = args.get(1);",
                "+            tablesList.put(args.get(0), new ArrayList<String>(Arrays.asList(args.get(1))));",
                "         }",
                "@@ -52,4 +56,3 @@ public class TableHistograms extends NodeToolCmd",
                "             checkArgument(input.length == 2, \"tablehistograms requires keyspace and table name arguments\");",
                "-            keyspace = input[0];",
                "-            table = input[1];",
                "+            tablesList.put(input[0], new ArrayList<String>(Arrays.asList(input[1])));",
                "         }",
                "@@ -57,83 +60,104 @@ public class TableHistograms extends NodeToolCmd",
                "         {",
                "-            checkArgument(false, \"tablehistograms requires keyspace and table name arguments\");",
                "-        }",
                "-",
                "-        // calculate percentile of row size and column count",
                "-        long[] estimatedPartitionSize = (long[]) probe.getColumnFamilyMetric(keyspace, table, \"EstimatedPartitionSizeHistogram\");",
                "-        long[] estimatedColumnCount = (long[]) probe.getColumnFamilyMetric(keyspace, table, \"EstimatedColumnCountHistogram\");",
                "-",
                "-        // build arrays to store percentile values",
                "-        double[] estimatedRowSizePercentiles = new double[7];",
                "-        double[] estimatedColumnCountPercentiles = new double[7];",
                "-        double[] offsetPercentiles = new double[]{0.5, 0.75, 0.95, 0.98, 0.99};",
                "-",
                "-        if (ArrayUtils.isEmpty(estimatedPartitionSize) || ArrayUtils.isEmpty(estimatedColumnCount))",
                "-        {",
                "-            System.err.println(\"No SSTables exists, unable to calculate 'Partition Size' and 'Cell Count' percentiles\");",
                "-",
                "-            for (int i = 0; i < 7; i++)",
                "+            // get a list of table stores",
                "+            Iterator<Map.Entry<String, ColumnFamilyStoreMBean>> tableMBeans = probe.getColumnFamilyStoreMBeanProxies();",
                "+            while (tableMBeans.hasNext())",
                "             {",
                "-                estimatedRowSizePercentiles[i] = Double.NaN;",
                "-                estimatedColumnCountPercentiles[i] = Double.NaN;",
                "+                Map.Entry<String, ColumnFamilyStoreMBean> entry = tableMBeans.next();",
                "+                String keyspaceName = entry.getKey();",
                "+                ColumnFamilyStoreMBean tableProxy = entry.getValue();",
                "+                if (!tablesList.containsKey(keyspaceName))",
                "+                {",
                "+                    tablesList.put(keyspaceName, new ArrayList<String>());",
                "+                }",
                "+                tablesList.get(keyspaceName).add(tableProxy.getTableName());",
                "             }",
                "         }",
                "-        else",
                "-        {",
                "-            EstimatedHistogram partitionSizeHist = new EstimatedHistogram(estimatedPartitionSize);",
                "-            EstimatedHistogram columnCountHist = new EstimatedHistogram(estimatedColumnCount);",
                "-            if (partitionSizeHist.isOverflowed())",
                "+        Iterator<Map.Entry<String, List<String>>> iter = tablesList.entrySet().iterator();",
                "+        while(iter.hasNext())",
                "+        {",
                "+            Map.Entry<String, List<String>> entry = iter.next();",
                "+            String keyspace = entry.getKey();",
                "+            for (String table : entry.getValue())",
                "             {",
                "-                System.err.println(String.format(\"Row sizes are larger than %s, unable to calculate percentiles\", partitionSizeHist.getLargestBucketOffset()));",
                "-                for (int i = 0; i < offsetPercentiles.length; i++)",
                "+                // calculate percentile of row size and column count",
                "+                long[] estimatedPartitionSize = (long[]) probe.getColumnFamilyMetric(keyspace, table, \"EstimatedPartitionSizeHistogram\");",
                "+                long[] estimatedColumnCount = (long[]) probe.getColumnFamilyMetric(keyspace, table, \"EstimatedColumnCountHistogram\");",
                "+",
                "+                // build arrays to store percentile values",
                "+                double[] estimatedRowSizePercentiles = new double[7];",
                "+                double[] estimatedColumnCountPercentiles = new double[7];",
                "+                double[] offsetPercentiles = new double[]{0.5, 0.75, 0.95, 0.98, 0.99};",
                "+",
                "+                if (ArrayUtils.isEmpty(estimatedPartitionSize) || ArrayUtils.isEmpty(estimatedColumnCount))",
                "+                {",
                "+                    System.out.println(\"No SSTables exists, unable to calculate 'Partition Size' and 'Cell Count' percentiles\");",
                "+",
                "+                    for (int i = 0; i < 7; i++)",
                "+                    {",
                "                         estimatedRowSizePercentiles[i] = Double.NaN;",
                "-            }",
                "-            else",
                "-            {",
                "-                for (int i = 0; i < offsetPercentiles.length; i++)",
                "-                    estimatedRowSizePercentiles[i] = partitionSizeHist.percentile(offsetPercentiles[i]);",
                "-            }",
                "+                        estimatedColumnCountPercentiles[i] = Double.NaN;",
                "+                    }",
                "+                }",
                "+                else",
                "+                {",
                "+                    EstimatedHistogram partitionSizeHist = new EstimatedHistogram(estimatedPartitionSize);",
                "+                    EstimatedHistogram columnCountHist = new EstimatedHistogram(estimatedColumnCount);",
                "-            if (columnCountHist.isOverflowed())",
                "-            {",
                "-                System.err.println(String.format(\"Column counts are larger than %s, unable to calculate percentiles\", columnCountHist.getLargestBucketOffset()));",
                "-                for (int i = 0; i < estimatedColumnCountPercentiles.length; i++)",
                "-                    estimatedColumnCountPercentiles[i] = Double.NaN;",
                "-            }",
                "-            else",
                "-            {",
                "-                for (int i = 0; i < offsetPercentiles.length; i++)",
                "-                    estimatedColumnCountPercentiles[i] = columnCountHist.percentile(offsetPercentiles[i]);",
                "-            }",
                "+                    if (partitionSizeHist.isOverflowed())",
                "+                    {",
                "+                        System.out.println(String.format(\"Row sizes are larger than %s, unable to calculate percentiles\", partitionSizeHist.getLargestBucketOffset()));",
                "+                        for (int i = 0; i < offsetPercentiles.length; i++)",
                "+                            estimatedRowSizePercentiles[i] = Double.NaN;",
                "+                    }",
                "+                    else",
                "+                    {",
                "+                        for (int i = 0; i < offsetPercentiles.length; i++)",
                "+                            estimatedRowSizePercentiles[i] = partitionSizeHist.percentile(offsetPercentiles[i]);",
                "+                    }",
                "-            // min value",
                "-            estimatedRowSizePercentiles[5] = partitionSizeHist.min();",
                "-            estimatedColumnCountPercentiles[5] = columnCountHist.min();",
                "-            // max value",
                "-            estimatedRowSizePercentiles[6] = partitionSizeHist.max();",
                "-            estimatedColumnCountPercentiles[6] = columnCountHist.max();",
                "-        }",
                "+                    if (columnCountHist.isOverflowed())",
                "+                    {",
                "+                        System.out.println(String.format(\"Column counts are larger than %s, unable to calculate percentiles\", columnCountHist.getLargestBucketOffset()));",
                "+                        for (int i = 0; i < estimatedColumnCountPercentiles.length; i++)",
                "+                            estimatedColumnCountPercentiles[i] = Double.NaN;",
                "+                    }",
                "+                    else",
                "+                    {",
                "+                        for (int i = 0; i < offsetPercentiles.length; i++)",
                "+                            estimatedColumnCountPercentiles[i] = columnCountHist.percentile(offsetPercentiles[i]);",
                "+                    }",
                "-        String[] percentiles = new String[]{\"50%\", \"75%\", \"95%\", \"98%\", \"99%\", \"Min\", \"Max\"};",
                "-        double[] readLatency = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxTimerMBean) probe.getColumnFamilyMetric(keyspace, table, \"ReadLatency\"));",
                "-        double[] writeLatency = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxTimerMBean) probe.getColumnFamilyMetric(keyspace, table, \"WriteLatency\"));",
                "-        double[] sstablesPerRead = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxHistogramMBean) probe.getColumnFamilyMetric(keyspace, table, \"SSTablesPerReadHistogram\"));",
                "+                    // min value",
                "+                    estimatedRowSizePercentiles[5] = partitionSizeHist.min();",
                "+                    estimatedColumnCountPercentiles[5] = columnCountHist.min();",
                "+                    // max value",
                "+                    estimatedRowSizePercentiles[6] = partitionSizeHist.max();",
                "+                    estimatedColumnCountPercentiles[6] = columnCountHist.max();",
                "+                }",
                "-        System.out.println(format(\"%s/%s histograms\", keyspace, table));",
                "-        System.out.println(format(\"%-10s%10s%18s%18s%18s%18s\",",
                "-                \"Percentile\", \"SSTables\", \"Write Latency\", \"Read Latency\", \"Partition Size\", \"Cell Count\"));",
                "-        System.out.println(format(\"%-10s%10s%18s%18s%18s%18s\",",
                "-                \"\", \"\", \"(micros)\", \"(micros)\", \"(bytes)\", \"\"));",
                "+                String[] percentiles = new String[]{\"50%\", \"75%\", \"95%\", \"98%\", \"99%\", \"Min\", \"Max\"};",
                "+                double[] readLatency = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxTimerMBean) probe.getColumnFamilyMetric(keyspace, table, \"ReadLatency\"));",
                "+                double[] writeLatency = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxTimerMBean) probe.getColumnFamilyMetric(keyspace, table, \"WriteLatency\"));",
                "+                double[] sstablesPerRead = probe.metricPercentilesAsArray((CassandraMetricsRegistry.JmxHistogramMBean) probe.getColumnFamilyMetric(keyspace, table, \"SSTablesPerReadHistogram\"));",
                "-        for (int i = 0; i < percentiles.length; i++)",
                "-        {",
                "-            System.out.println(format(\"%-10s%10.2f%18.2f%18.2f%18.0f%18.0f\",",
                "-                    percentiles[i],",
                "-                    sstablesPerRead[i],",
                "-                    writeLatency[i],",
                "-                    readLatency[i],",
                "-                    estimatedRowSizePercentiles[i],",
                "-                    estimatedColumnCountPercentiles[i]));",
                "+                System.out.println(format(\"%s/%s histograms\", keyspace, table));",
                "+                System.out.println(format(\"%-10s%10s%18s%18s%18s%18s\",",
                "+                        \"Percentile\", \"SSTables\", \"Write Latency\", \"Read Latency\", \"Partition Size\", \"Cell Count\"));",
                "+                System.out.println(format(\"%-10s%10s%18s%18s%18s%18s\",",
                "+                        \"\", \"\", \"(micros)\", \"(micros)\", \"(bytes)\", \"\"));",
                "+",
                "+                for (int i = 0; i < percentiles.length; i++)",
                "+                {",
                "+                    System.out.println(format(\"%-10s%10.2f%18.2f%18.2f%18.0f%18.0f\",",
                "+                            percentiles[i],",
                "+                            sstablesPerRead[i],",
                "+                            writeLatency[i],",
                "+                            readLatency[i],",
                "+                            estimatedRowSizePercentiles[i],",
                "+                            estimatedColumnCountPercentiles[i]));",
                "+                }",
                "+                System.out.println();",
                "+            }",
                "         }",
                "-        System.out.println();",
                "     }"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/tools/nodetool/TableHistograms.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14185": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14185",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d6e508f33c1a7274b5826ad9d5ce814d719bd848",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1515453155,
            "hunks": 16,
            "message": "Migrate dtests to use pytest and python3 Patch by Michael Kjellman; Reviewed by Ariel Weisberg for CASSANDRA-14134",
            "diff": [
                "diff --git a/.circleci/config.yml b/.circleci/config.yml",
                "new file mode 100644",
                "index 0000000000..f881b709bd",
                "--- /dev/null",
                "+++ b/.circleci/config.yml",
                "@@ -0,0 +1,339 @@",
                "+default_env_vars: &default_env_vars",
                "+    JAVA_HOME: /usr/local/openjdk8u154-cassandra-b02",
                "+    ANT_HOME: /usr/local/apache-ant-1.10.1",
                "+    LANG: en_US.UTF-8",
                "+    JDK_HOME: /usr/local/openjdk8u154-cassandra-b02",
                "+    JAVA8_HOME: /usr/local/openjdk8u154-cassandra-b02",
                "+    JAVA7_HOME: /usr/local/openjdk7u82-cassandra-b02",
                "+    KEEP_TEST_DIR: true",
                "+    DEFAULT_DIR: /home/cassandra/cassandra-dtest",
                "+    PYTHONIOENCODING: utf-8",
                "+    PYTHONUNBUFFERED: true",
                "+    CASS_DRIVER_NO_EXTENSIONS: true",
                "+    CASS_DRIVER_NO_CYTHON: true",
                "+# For environments with xlarge instances, use more memory",
                "+high_capacity_env_vars: &high_capacity_env_vars",
                "+    <<: *default_env_vars",
                "+    CCM_MAX_HEAP_SIZE: 2048M",
                "+    CCM_HEAP_NEWSIZE: 512M",
                "+# For environments with limited memory (e.g the free OSS CircleCI Tier)",
                "+resource_constrained_env_vars: &resource_constrained_env_vars",
                "+    <<: *default_env_vars",
                "+    CCM_MAX_HEAP_SIZE: 1024M",
                "+    CCM_HEAP_NEWSIZE: 256M",
                "+# Settings for users who do not have a paid CircleCI account",
                "+default_env_settings: &default_env_settings",
                "+    resource_class: medium",
                "+    parallelism: 4",
                "+# Settings for users with high-capacity, paid CircleCI account",
                "+high_capacity_env_settings: &high_capacity_env_settings",
                "+    resource_class: xlarge",
                "+    parallelism: 100",
                "+default_jobs: &default_jobs",
                "+        jobs:",
                "+            - build",
                "+            - unit_tests:",
                "+                  requires:",
                "+                      - build",
                "+with_dtests_jobs: &with_dtest_jobs",
                "+        jobs:",
                "+            - build",
                "+            - unit_tests:",
                "+                  requires:",
                "+                      - build",
                "+            - dtests-with-vnodes:",
                "+                  requires:",
                "+                      - build",
                "+            - dtests-no-vnodes:",
                "+                  requires:",
                "+                      - build",
                "+with_dtest_jobs_only: &with_dtest_jobs_only",
                "+        jobs:",
                "+            - build",
                "+            - dtests-with-vnodes:",
                "+                  requires:",
                "+                      - build",
                "+            - dtests-no-vnodes:",
                "+                  requires:",
                "+                      - build",
                "+# Set env_settings, env_vars, and workflows/build_and_run_tests based on environment",
                "+env_settings: &env_settings",
                "+    <<: *default_env_settings",
                "+    #<<: *high_capacity_env_settings",
                "+env_vars: &env_vars",
                "+    <<: *resource_constrained_env_vars",
                "+    #<<: *high_capacity_env_vars",
                "+workflows:",
                "+    version: 2",
                "+    build_and_run_tests: *default_jobs",
                "+    #build_and_run_tests: *with_dtest_jobs_only",
                "+    #build_and_run_tests: *with_dtest_jobs",
                "+docker_image: &docker_image kjellman/cassandra-test:0.4.3",
                "+version: 2",
                "+jobs:",
                "+  build:",
                "+    <<: *env_settings",
                "+    parallelism: 1 # This job doesn't benefit from parallelism",
                "+    working_directory: ~/",
                "+    shell: /bin/bash -eo pipefail -l",
                "+    docker:",
                "+      - image: *docker_image",
                "+        environment:",
                "+            <<: *env_vars",
                "+    steps:",
                "+      - run:",
                "+          name: Log Environment Information",
                "+          command: |",
                "+              echo '*** id ***'",
                "+              id",
                "+              echo '*** cat /proc/cpuinfo ***'",
                "+              cat /proc/cpuinfo",
                "+              echo '*** free -m ***'",
                "+              free -m",
                "+              echo '*** df -m ***'",
                "+              df -m",
                "+              echo '*** ifconfig -a ***'",
                "+              ifconfig -a",
                "+              echo '*** uname -a ***'",
                "+              uname -a",
                "+              echo '*** mount ***'",
                "+              mount",
                "+              echo '*** env ***'",
                "+              env",
                "+      - run:",
                "+          name: Clone Cassandra Repository (via git)",
                "+          command: |",
                "+            export LANG=en_US.UTF-8",
                "+            git clone --single-branch --depth 1 --branch $CIRCLE_BRANCH git://github.com/$CIRCLE_PROJECT_USERNAME/$CIRCLE_PROJECT_REPONAME.git ~/cassandra",
                "+      - run:",
                "+          name: Build Cassandra",
                "+          command: |",
                "+            export LANG=en_US.UTF-8",
                "+            export JAVA_TOOL_OPTIONS=\"-Dfile.encoding=UTF8\"",
                "+            export PATH=$PATH:$ANT_HOME/bin:$JAVA_HOME/bin",
                "+            cd ~/cassandra",
                "+            # Loop to prevent failure due to maven-ant-tasks not downloading a jar..",
                "+            for x in $(seq 1 3); do",
                "+                ${ANT_HOME}/bin/ant clean jar",
                "+                RETURN=\"$?\"",
                "+                if [ \"${RETURN}\" -eq \"0\" ]; then",
                "+                    break",
                "+                fi",
                "+            done",
                "+            # Exit, if we didn't build successfully",
                "+            if [ \"${RETURN}\" -ne \"0\" ]; then",
                "+                echo \"Build failed with exit code: ${RETURN}\"",
                "+                exit ${RETURN}",
                "+            fi",
                "+          no_output_timeout: 15m",
                "+      - persist_to_workspace:",
                "+            root: /home/cassandra",
                "+            paths:",
                "+                - cassandra",
                "+                - .m2",
                "+  unit_tests:",
                "+    <<: *env_settings",
                "+    working_directory: ~/",
                "+    shell: /bin/bash -eo pipefail -l",
                "+    docker:",
                "+      - image: *docker_image",
                "+        environment:",
                "+            <<: *env_vars",
                "+    steps:",
                "+      - attach_workspace:",
                "+          at: /home/cassandra",
                "+      - run:",
                "+          name: Determine Tests to Run",
                "+          no_output_timeout: 15m",
                "+          command: |",
                "+            # reminder: this code (along with all the steps) is independently executed on every circle container",
                "+            # so the goal here is to get the circleci script to return the tests *this* container will run",
                "+            # which we do via the `circleci` cli tool.",
                "+",
                "+            export LANG=en_US.UTF-8",
                "+            rm -fr ~/cassandra-dtest/upgrade_tests",
                "+            echo \"***java tests***\"",
                "+",
                "+            # get all of our unit test filenames",
                "+            set -eo pipefail && circleci tests glob \"$HOME/cassandra/test/unit/**/*.java\" > /tmp/all_java_unit_tests.txt",
                "+",
                "+            # split up the unit tests into groups based on the number of containers we have",
                "+            set -eo pipefail && circleci tests split --split-by=timings --timings-type=filename --index=${CIRCLE_NODE_INDEX} --total=${CIRCLE_NODE_TOTAL} /tmp/all_java_unit_tests.txt > /tmp/java_tests_${CIRCLE_NODE_INDEX}.txt",
                "+            set -eo pipefail && cat /tmp/java_tests_${CIRCLE_NODE_INDEX}.txt | cut -c 37-1000000 | grep \"Test\\.java$\" > /tmp/java_tests_${CIRCLE_NODE_INDEX}_final.txt",
                "+            echo \"** /tmp/java_tests_${CIRCLE_NODE_INDEX}_final.txt\"",
                "+            cat /tmp/java_tests_${CIRCLE_NODE_INDEX}_final.txt",
                "+      - run:",
                "+         name: Run Unit Tests",
                "+         command: |",
                "+            export LANG=en_US.UTF-8",
                "+            export JAVA_TOOL_OPTIONS=\"-Dfile.encoding=UTF8\"",
                "+            export PATH=$PATH:$ANT_HOME/bin:$JAVA_HOME/bin",
                "+            #Skip all syncing to disk to avoid performance issues in flaky CI environments",
                "+            export CASSANDRA_SKIP_SYNC=true",
                "+",
                "+            time mv ~/cassandra /tmp",
                "+            cd /tmp/cassandra",
                "+            ant testclasslist -Dtest.classlistfile=/tmp/java_tests_${CIRCLE_NODE_INDEX}_final.txt",
                "+         no_output_timeout: 15m",
                "+      - store_test_results:",
                "+            path: /tmp/cassandra/build/test/output/",
                "+      - store_artifacts:",
                "+          path: /tmp/cassandra/build/test/output",
                "+          destination: junitxml",
                "+      - store_artifacts:",
                "+          path: /tmp/cassandra/build/test/logs",
                "+          destination: logs",
                "+  dtests-with-vnodes:",
                "+    <<: *env_settings",
                "+    working_directory: ~/",
                "+    shell: /bin/bash -eo pipefail -l",
                "+    docker:",
                "+      - image: *docker_image",
                "+        environment:",
                "+            <<: *env_vars",
                "+    steps:",
                "+      - attach_workspace:",
                "+          at: /home/cassandra",
                "+      - run:",
                "+          name: Clone Cassandra dtest Repository (via git)",
                "+          command: |",
                "+            export LANG=en_US.UTF-8",
                "+            git clone --single-branch --branch master --depth 1 git://github.com/apache/cassandra-dtest.git ~/cassandra-dtest",
                "+      - run:",
                "+          name: Configure virtualenv and python Dependencies",
                "+          command: |",
                "+            # note, this should be super quick as all dependencies should be pre-installed in the docker image",
                "+            # if additional dependencies were added to requirmeents.txt and the docker image hasn't been updated",
                "+            # we'd have to install it here at runtime -- which will make things slow, so do yourself a favor and",
                "+            # rebuild the docker image! (it automatically pulls the latest requirements.txt on build)",
                "+            export LANG=en_US.UTF-8",
                "+            source ~/env/bin/activate",
                "+            export PATH=$PATH:$ANT_HOME/bin:$JAVA_HOME/bin",
                "+            export CASS_DRIVER_NO_EXTENSIONS=true",
                "+            export CASS_DRIVER_NO_CYTHON=true",
                "+            pip3 install --exists-action w -r ~/cassandra-dtest/requirements.txt",
                "+            pip3 freeze",
                "+      - run:",
                "+          name: Determine Tests to Run",
                "+          no_output_timeout: 5m",
                "+          command: |",
                "+            # reminder: this code (along with all the steps) is independently executed on every circle container",
                "+            # so the goal here is to get the circleci script to return the tests *this* container will run",
                "+            # which we do via the `circleci` cli tool.",
                "+",
                "+            export LANG=en_US.UTF-8",
                "+            cd cassandra-dtest",
                "+            source ~/env/bin/activate",
                "+            export PATH=$PATH:$ANT_HOME/bin:$JAVA_HOME/bin",
                "+",
                "+            echo \"***Collected DTests (with vnodes)***\"",
                "+            set -eo pipefail && ./run_dtests.py --use-vnodes --dtest-print-tests-only --skip-resource-intensive-tests --dtest-print-tests-output=/tmp/all_dtest_tests_with_vnodes",
                "+            set -eo pipefail && circleci tests split --split-by=timings --timings-type=classname /tmp/all_dtest_tests_with_vnodes > /tmp/split_dtest_tests_with_vnodes.txt",
                "+            cat /tmp/split_dtest_tests_with_vnodes.txt | tr '\\n' ' ' > /tmp/split_dtest_tests_with_vnodes_final.txt",
                "+            # cat /tmp/split_dtest_tests_with_vnodes.txt",
                "+            cat /tmp/split_dtest_tests_with_vnodes_final.txt",
                "+      - run:",
                "+          name: Run dtests (with vnodes)",
                "+          no_output_timeout: 15m",
                "+          command: |",
                "+            echo \"cat /tmp/split_dtest_tests_with_vnodes_final.txt\"",
                "+            cat /tmp/split_dtest_tests_with_vnodes_final.txt",
                "+",
                "+            source ~/env/bin/activate",
                "+            export PATH=$PATH:$ANT_HOME/bin:$JAVA_HOME/bin",
                "+",
                "+            cd ~/cassandra-dtest",
                "+            mkdir -p /tmp/dtest",
                "+",
                "+            echo \"env: $(env)\"",
                "+            echo \"** done env\"",
                "+            mkdir -p /tmp/results/dtests",
                "+            # we need the \"set -o pipefail\" here so that the exit code that circleci will actually use is from pytest and not the exit code from tee",
                "+            export SPLIT_TESTS=`cat /tmp/split_dtest_tests_with_vnodes_final.txt`",
                "+            #Skip all syncing to disk to avoid performance issues in flaky CI environments",
                "+            export CASSANDRA_SKIP_SYNC=true",
                "+            set -o pipefail && cd ~/cassandra-dtest && pytest --log-level=\"INFO\" --use-vnodes --num-tokens=32 --junit-xml=/tmp/results/dtests/pytest_result_with_vnodes.xml -s --cassandra-dir=/home/cassandra/cassandra --skip-resource-intensive-tests --keep-test-dir $SPLIT_TESTS 2>&1 | tee /tmp/dtest/stdout.txt",
                "+      - store_test_results:",
                "+          path: /tmp/results",
                "+      - store_artifacts:",
                "+          path: /tmp/dtest",
                "+          destination: dtest_with_vnodes",
                "+      - store_artifacts:",
                "+          path: ~/cassandra-dtest/logs",
                "+          destination: dtest_with_vnodes_logs",
                "+  dtests-no-vnodes:",
                "+    <<: *env_settings",
                "+    working_directory: ~/",
                "+    shell: /bin/bash -eo pipefail -l",
                "+    docker:",
                "+      - image: *docker_image",
                "+        environment:",
                "+            <<: *env_vars",
                "+    steps:",
                "+      - attach_workspace:",
                "+          at: /home/cassandra",
                "+      - run:",
                "+          name: Clone Cassandra dtest Repository (via git)",
                "+          command: |",
                "+            export LANG=en_US.UTF-8",
                "+            git clone --single-branch --branch master --depth 1 git://github.com/apache/cassandra-dtest.git ~/cassandra-dtest",
                "+      - run:",
                "+          name: Configure virtualenv and python Dependencies",
                "+          command: |",
                "+            # note, this should be super quick as all dependencies should be pre-installed in the docker image",
                "+            # if additional dependencies were added to requirmeents.txt and the docker image hasn't been updated",
                "+            # we'd have to install it here at runtime -- which will make things slow, so do yourself a favor and",
                "+            # rebuild the docker image! (it automatically pulls the latest requirements.txt on build)",
                "+            export LANG=en_US.UTF-8",
                "+            source ~/env/bin/activate",
                "+            export PATH=$PATH:$ANT_HOME/bin:$JAVA_HOME/bin",
                "+            export CASS_DRIVER_NO_EXTENSIONS=true",
                "+            export CASS_DRIVER_NO_CYTHON=true",
                "+            pip3 install --exists-action w -r ~/cassandra-dtest/requirements.txt",
                "+            pip3 freeze",
                "+      - run:",
                "+          name: Determine Tests to Run",
                "+          no_output_timeout: 5m",
                "+          command: |",
                "+            # reminder: this code (along with all the steps) is independently executed on every circle container",
                "+            # so the goal here is to get the circleci script to return the tests *this* container will run",
                "+            # which we do via the `circleci` cli tool.",
                "+",
                "+            export LANG=en_US.UTF-8",
                "+            cd cassandra-dtest",
                "+            source ~/env/bin/activate",
                "+            export PATH=$PATH:$ANT_HOME/bin:$JAVA_HOME/bin",
                "+",
                "+            echo \"***Collected DTests (without vnodes)***\"",
                "+            ./run_dtests.py --dtest-print-tests-only --skip-resource-intensive-tests --dtest-print-tests-output=/tmp/all_dtest_tests_without_vnodes",
                "+            set -eo pipefail && circleci tests split --split-by=timings --timings-type=classname /tmp/all_dtest_tests_without_vnodes > /tmp/split_dtest_tests_without_vnodes.txt",
                "+            cat /tmp/split_dtest_tests_without_vnodes.txt | tr '\\n' ' ' > /tmp/split_dtest_tests_without_vnodes_final.txt",
                "+            # cat /tmp/split_dtest_tests_without_vnodes.txt",
                "+            cat /tmp/split_dtest_tests_without_vnodes_final.txt",
                "+      - run:",
                "+          name: Run dtests (without vnodes)",
                "+          no_output_timeout: 15m",
                "+          command: |",
                "+            # for now require at least 50 circleci containers to run the dtests (with less resources the tests won't produce reliable results or will fail to complete)",
                "+            if [ $CIRCLE_NODE_TOTAL -gt 0 ]; then",
                "+                source ~/env/bin/activate",
                "+                export PATH=$PATH:$ANT_HOME/bin:$JAVA_HOME/bin",
                "+",
                "+                cd ~/cassandra-dtest",
                "+                mkdir -p /tmp/dtest",
                "+",
                "+                mkdir -p /tmp/results/dtests",
                "+                # we need the \"set -o pipefail\" here so that the exit code that circleci will actually use is from pytest and not the exit code from tee",
                "+                export SPLIT_TESTS=`cat /tmp/split_dtest_tests_without_vnodes_final.txt`",
                "+                #Skip all syncing to disk to avoid performance issues in flaky CI environments",
                "+                export CASSANDRA_SKIP_SYNC=true",
                "+                set -o pipefail && cd ~/cassandra-dtest && pytest --log-level=\"INFO\" --junit-xml=/tmp/results/dtests/pytest_result_novnodes.xml -s --cassandra-dir=/home/cassandra/cassandra --skip-resource-intensive-tests --keep-test-dir $SPLIT_TESTS 2>&1 | tee /tmp/dtest/stdout-novnodes.txt",
                "+            fi",
                "+      - store_test_results:",
                "+          path: /tmp/results",
                "+      - store_artifacts:",
                "+          path: /tmp/dtest",
                "+          destination: dtest_no_vnodes",
                "+      - store_artifacts:",
                "+          path: ~/cassandra-dtest/logs",
                "+          destination: dtest_no_vnodes_logs",
                "diff --git a/circle.yml b/circle.yml",
                "deleted file mode 100644",
                "index ae848aae8a..0000000000",
                "--- a/circle.yml",
                "+++ /dev/null",
                "@@ -1,18 +0,0 @@",
                "-machine:",
                "-  java:",
                "-    version: 'oraclejdk8'",
                "-",
                "-test:",
                "-  pre:",
                "-    - sudo apt-get update; sudo apt-get install wamerican:",
                "-        parallel: true",
                "-  override:",
                "-    - case $CIRCLE_NODE_INDEX in 0) ant eclipse-warnings; ant test -Dtest.runners=1;; 1) ant long-test ;; 2) ant test-compression ;; 3) ant stress-test ;;esac:",
                "-        parallel: true",
                "-",
                "-  post:",
                "-    - mkdir -p $CIRCLE_TEST_REPORTS/junit/:",
                "-        parallel: true",
                "-    - case $CIRCLE_NODE_INDEX in [0123]) find ./build/test/output/ -iname \"*.xml\" -exec cp {} $CIRCLE_TEST_REPORTS/junit/ \\; ;;esac:",
                "-        parallel: true",
                "-",
                "diff --git a/src/java/org/apache/cassandra/io/compress/CompressionMetadata.java b/src/java/org/apache/cassandra/io/compress/CompressionMetadata.java",
                "index 391baeeb74..fa0f9f7211 100644",
                "--- a/src/java/org/apache/cassandra/io/compress/CompressionMetadata.java",
                "+++ b/src/java/org/apache/cassandra/io/compress/CompressionMetadata.java",
                "@@ -57,2 +57,3 @@ import org.apache.cassandra.schema.CompressionParams;",
                " import org.apache.cassandra.utils.Pair;",
                "+import org.apache.cassandra.utils.SyncUtil;",
                " import org.apache.cassandra.utils.concurrent.Transactional;",
                "@@ -415,3 +416,3 @@ public class CompressionMetadata",
                "                 out.flush();",
                "-                fos.getFD().sync();",
                "+                SyncUtil.sync(fos);",
                "             }",
                "diff --git a/src/java/org/apache/cassandra/io/util/FileUtils.java b/src/java/org/apache/cassandra/io/util/FileUtils.java",
                "index 70595fcbd7..b8be84f76c 100644",
                "--- a/src/java/org/apache/cassandra/io/util/FileUtils.java",
                "+++ b/src/java/org/apache/cassandra/io/util/FileUtils.java",
                "@@ -21,2 +21,3 @@ import java.io.*;",
                " import java.nio.ByteBuffer;",
                "+import java.nio.channels.Channels;",
                " import java.nio.channels.FileChannel;",
                "@@ -31,4 +32,6 @@ import java.util.Arrays;",
                " import java.util.Collections;",
                "+import java.util.HashSet;",
                " import java.util.List;",
                " import java.util.Optional;",
                "+import java.util.Set;",
                " import java.util.concurrent.atomic.AtomicReference;",
                "@@ -37,2 +40,4 @@ import org.slf4j.Logger;",
                " import org.slf4j.LoggerFactory;",
                "+",
                "+import org.apache.cassandra.utils.SyncUtil;",
                " import sun.nio.ch.DirectBuffer;",
                "@@ -559,10 +564,42 @@ public final class FileUtils",
                "+    /**",
                "+     * Write lines to a file adding a newline to the end of each supplied line using the provided open options.",
                "+     *",
                "+     * If open option sync or dsync is provided this will not open the file with sync or dsync since it might end up syncing",
                "+     * many times for a lot of lines. Instead it will write all the lines and sync once at the end. Since the file is",
                "+     * never returned there is not much difference from the perspective of the caller.",
                "+     * @param file",
                "+     * @param lines",
                "+     * @param options",
                "+     */",
                "     public static void write(File file, List<String> lines, StandardOpenOption ... options)",
                "     {",
                "-        try",
                "+        Set<StandardOpenOption> optionsSet = new HashSet<>(Arrays.asList(options));",
                "+        //Emulate the old FileSystemProvider.newOutputStream behavior for open options.",
                "+        if (optionsSet.isEmpty())",
                "+        {",
                "+            optionsSet.add(StandardOpenOption.CREATE);",
                "+            optionsSet.add(StandardOpenOption.TRUNCATE_EXISTING);",
                "+        }",
                "+        boolean sync = optionsSet.remove(StandardOpenOption.SYNC);",
                "+        boolean dsync = optionsSet.remove(StandardOpenOption.DSYNC);",
                "+        optionsSet.add(StandardOpenOption.WRITE);",
                "+",
                "+        Path filePath = file.toPath();",
                "+        try (FileChannel fc = filePath.getFileSystem().provider().newFileChannel(filePath, optionsSet);",
                "+             BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(Channels.newOutputStream(fc), CHARSET.newEncoder())))",
                "         {",
                "-            Files.write(file.toPath(),",
                "-                        lines,",
                "-                        CHARSET,",
                "-                        options);",
                "+            for (CharSequence line: lines) {",
                "+                writer.append(line);",
                "+                writer.newLine();",
                "+            }",
                "+",
                "+            if (sync)",
                "+            {",
                "+                SyncUtil.force(fc, true);",
                "+            }",
                "+            else if (dsync)",
                "+            {",
                "+                SyncUtil.force(fc, false);",
                "+            }",
                "         }",
                "diff --git a/src/java/org/apache/cassandra/utils/SyncUtil.java b/src/java/org/apache/cassandra/utils/SyncUtil.java",
                "index 64d64cf9f8..1917e8bc58 100644",
                "--- a/src/java/org/apache/cassandra/utils/SyncUtil.java",
                "+++ b/src/java/org/apache/cassandra/utils/SyncUtil.java",
                "@@ -30,4 +30,7 @@ import java.util.concurrent.atomic.AtomicInteger;",
                " import org.apache.cassandra.config.Config;",
                "+import org.apache.cassandra.service.CassandraDaemon;",
                " import com.google.common.base.Preconditions;",
                "+import org.slf4j.Logger;",
                "+import org.slf4j.LoggerFactory;",
                "@@ -40,3 +43,3 @@ public class SyncUtil",
                " {",
                "-    public static boolean SKIP_SYNC = Boolean.getBoolean(Config.PROPERTY_PREFIX + \"skip_sync\");",
                "+    public static final boolean SKIP_SYNC;",
                "@@ -46,2 +49,4 @@ public class SyncUtil",
                "+    private static final Logger logger = LoggerFactory.getLogger(SyncUtil.class);",
                "+",
                "     static",
                "@@ -82,2 +87,11 @@ public class SyncUtil",
                "         fdUseCountField = fdUseCountTemp;",
                "+",
                "+        //If skipping syncing is requested by any means then skip them.",
                "+        boolean skipSyncProperty = Boolean.getBoolean(Config.PROPERTY_PREFIX + \"skip_sync\");",
                "+        boolean skipSyncEnv = Boolean.valueOf(System.getenv().getOrDefault(\"CASSANDRA_SKIP_SYNC\", \"false\"));",
                "+        SKIP_SYNC = skipSyncProperty || skipSyncEnv;",
                "+        if (SKIP_SYNC)",
                "+        {",
                "+            logger.info(\"Skip fsync enabled due to property {} and environment {}\", skipSyncProperty, skipSyncEnv);",
                "+        }",
                "     }"
            ],
            "changed_files": [
                ".circleci/config.yml",
                "circle.yml",
                "src/java/org/apache/cassandra/io/compress/CompressionMetadata.java",
                "src/java/org/apache/cassandra/io/util/FileUtils.java",
                "src/java/org/apache/cassandra/utils/SyncUtil.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14134": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14134",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "473e8dfd7be95815ee10502f021bd7deb8734fba",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518044881,
            "hunks": 23,
            "message": "Add hot reloading of SSL Certificates patch by Dinesh Joshi; reviewed by jasobrown for CASSANDRA-14222",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java b/src/java/org/apache/cassandra/config/DatabaseDescriptor.java",
                "index 8e831cf2b7..0714245643 100644",
                "--- a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java",
                "+++ b/src/java/org/apache/cassandra/config/DatabaseDescriptor.java",
                "@@ -62,2 +62,3 @@ import org.apache.cassandra.net.RateBasedBackPressure;",
                " import org.apache.cassandra.security.EncryptionContext;",
                "+import org.apache.cassandra.security.SSLFactory;",
                " import org.apache.cassandra.service.CacheService.CacheType;",
                "@@ -324,2 +325,4 @@ public class DatabaseDescriptor",
                "         applyEncryptionContext();",
                "+",
                "+        applySslContextHotReload();",
                "     }",
                "@@ -867,2 +870,7 @@ public class DatabaseDescriptor",
                "+    public static void applySslContextHotReload()",
                "+    {",
                "+        SSLFactory.initHotReloading(conf.server_encryption_options, conf.client_encryption_options, false);",
                "+    }",
                "+",
                "     public static void applySeedProvider()",
                "diff --git a/src/java/org/apache/cassandra/net/MessagingService.java b/src/java/org/apache/cassandra/net/MessagingService.java",
                "index 9f00d27c3e..8fdb3954e3 100644",
                "--- a/src/java/org/apache/cassandra/net/MessagingService.java",
                "+++ b/src/java/org/apache/cassandra/net/MessagingService.java",
                "@@ -103,2 +103,3 @@ import org.apache.cassandra.schema.MigrationManager;",
                " import org.apache.cassandra.schema.TableId;",
                "+import org.apache.cassandra.security.SSLFactory;",
                " import org.apache.cassandra.service.AbstractWriteResponseHandler;",
                "@@ -1666,2 +1667,8 @@ public final class MessagingService implements MessagingServiceMBean",
                "     }",
                "+",
                "+    @Override",
                "+    public void reloadSslCertificates()",
                "+    {",
                "+        SSLFactory.checkCertFilesForHotReloading();",
                "+    }",
                " }",
                "diff --git a/src/java/org/apache/cassandra/net/MessagingServiceMBean.java b/src/java/org/apache/cassandra/net/MessagingServiceMBean.java",
                "index f4a0c434cf..6adb891950 100644",
                "--- a/src/java/org/apache/cassandra/net/MessagingServiceMBean.java",
                "+++ b/src/java/org/apache/cassandra/net/MessagingServiceMBean.java",
                "@@ -131,2 +131,4 @@ public interface MessagingServiceMBean",
                "     public int getVersion(String address) throws UnknownHostException;",
                "+",
                "+    void reloadSslCertificates();",
                " }",
                "diff --git a/src/java/org/apache/cassandra/security/SSLFactory.java b/src/java/org/apache/cassandra/security/SSLFactory.java",
                "index a931f5fbdb..0bf769cdf7 100644",
                "--- a/src/java/org/apache/cassandra/security/SSLFactory.java",
                "+++ b/src/java/org/apache/cassandra/security/SSLFactory.java",
                "@@ -20,2 +20,3 @@ package org.apache.cassandra.security;",
                "+import java.io.File;",
                " import java.io.IOException;",
                "@@ -26,2 +27,3 @@ import java.security.KeyStore;",
                " import java.security.cert.X509Certificate;",
                "+import java.util.ArrayList;",
                " import java.util.Arrays;",
                "@@ -30,2 +32,4 @@ import java.util.Enumeration;",
                " import java.util.List;",
                "+import java.util.concurrent.CopyOnWriteArrayList;",
                "+import java.util.concurrent.TimeUnit;",
                " import java.util.concurrent.atomic.AtomicReference;",
                "@@ -39,2 +43,3 @@ import com.google.common.annotations.VisibleForTesting;",
                " import com.google.common.base.Predicates;",
                "+import com.google.common.collect.ImmutableList;",
                " import com.google.common.collect.ImmutableSet;",
                "@@ -51,3 +56,3 @@ import io.netty.handler.ssl.SslProvider;",
                " import io.netty.handler.ssl.SupportedCipherSuiteFilter;",
                "-import io.netty.util.ReferenceCountUtil;",
                "+import org.apache.cassandra.concurrent.ScheduledExecutors;",
                " import org.apache.cassandra.config.EncryptionOptions;",
                "@@ -79,2 +84,63 @@ public final class SSLFactory",
                "+    /**",
                "+     * List of files that trigger hot reloading of SSL certificates",
                "+     */",
                "+    private static volatile List<HotReloadableFile> hotReloadableFiles = ImmutableList.of();",
                "+",
                "+    /**",
                "+     * Default initial delay for hot reloading",
                "+     */",
                "+    public static final int DEFAULT_HOT_RELOAD_INITIAL_DELAY_SEC = 600;",
                "+",
                "+    /**",
                "+     * Default periodic check delay for hot reloading",
                "+     */",
                "+    public static final int DEFAULT_HOT_RELOAD_PERIOD_SEC = 600;",
                "+",
                "+    /**",
                "+     * State variable to maintain initialization invariant",
                "+     */",
                "+    private static boolean isHotReloadingInitialized = false;",
                "+",
                "+    /**",
                "+     * Helper class for hot reloading SSL Contexts",
                "+     */",
                "+    private static class HotReloadableFile",
                "+    {",
                "+        enum Type",
                "+        {",
                "+            SERVER,",
                "+            CLIENT",
                "+        }",
                "+",
                "+        private final File file;",
                "+        private volatile long lastModTime;",
                "+        private final Type certType;",
                "+",
                "+        HotReloadableFile(String path, Type type)",
                "+        {",
                "+            file = new File(path);",
                "+            lastModTime = file.lastModified();",
                "+            certType = type;",
                "+        }",
                "+",
                "+        boolean shouldReload()",
                "+        {",
                "+            long curModTime = file.lastModified();",
                "+            boolean result = curModTime != lastModTime;",
                "+            lastModTime = curModTime;",
                "+            return result;",
                "+        }",
                "+",
                "+        public boolean isServer()",
                "+        {",
                "+            return certType == Type.SERVER;",
                "+        }",
                "+",
                "+        public boolean isClient()",
                "+        {",
                "+            return certType == Type.CLIENT;",
                "+        }",
                "+    }",
                "+",
                "     /**",
                "@@ -178,6 +244,10 @@ public final class SSLFactory",
                "     {",
                "-        if (forServer && serverSslContext.get() != null)",
                "-            return serverSslContext.get();",
                "-        if (!forServer && clientSslContext.get() != null)",
                "-            return clientSslContext.get();",
                "+",
                "+        SslContext sslContext;",
                "+",
                "+        if (forServer && (sslContext = serverSslContext.get()) != null)",
                "+            return sslContext;",
                "+",
                "+        if (!forServer && (sslContext = clientSslContext.get()) != null)",
                "+            return sslContext;",
                "@@ -221,5 +291,71 @@ public final class SSLFactory",
                "-        ReferenceCountUtil.release(ctx);",
                "         return ref.get();",
                "     }",
                "+",
                "+    /**",
                "+     * Performs a lightweight check whether the certificate files have been refreshed.",
                "+     *",
                "+     * @throws IllegalStateException if {@link #initHotReloading(EncryptionOptions.ServerEncryptionOptions, EncryptionOptions, boolean)}",
                "+     * is not called first",
                "+     */",
                "+    public static void checkCertFilesForHotReloading()",
                "+    {",
                "+        if (!isHotReloadingInitialized)",
                "+            throw new IllegalStateException(\"Hot reloading functionality has not been initialized.\");",
                "+",
                "+        logger.trace(\"Checking whether certificates have been updated\");",
                "+",
                "+        if (hotReloadableFiles.stream().anyMatch(f -> f.isServer() && f.shouldReload()))",
                "+        {",
                "+            logger.info(\"Server ssl certificates have been updated. Reseting the context for new peer connections.\");",
                "+            serverSslContext.set(null);",
                "+        }",
                "+",
                "+        if (hotReloadableFiles.stream().anyMatch(f -> f.isClient() && f.shouldReload()))",
                "+        {",
                "+            logger.info(\"Client ssl certificates have been updated. Reseting the context for new client connections.\");",
                "+            clientSslContext.set(null);",
                "+        }",
                "+    }",
                "+",
                "+    /**",
                "+     * Determines whether to hot reload certificates and schedules a periodic task for it.",
                "+     *",
                "+     * @param serverEncryptionOptions",
                "+     * @param clientEncryptionOptions",
                "+     */",
                "+    public static synchronized void initHotReloading(EncryptionOptions.ServerEncryptionOptions serverEncryptionOptions,",
                "+                                                     EncryptionOptions clientEncryptionOptions,",
                "+                                                     boolean force)",
                "+    {",
                "+        if (isHotReloadingInitialized && !force)",
                "+            return;",
                "+",
                "+        logger.debug(\"Initializing hot reloading SSLContext\");",
                "+",
                "+        List<HotReloadableFile> fileList = new ArrayList<>();",
                "+",
                "+        if (serverEncryptionOptions.enabled)",
                "+        {",
                "+            fileList.add(new HotReloadableFile(serverEncryptionOptions.keystore, HotReloadableFile.Type.SERVER));",
                "+            fileList.add(new HotReloadableFile(serverEncryptionOptions.truststore, HotReloadableFile.Type.SERVER));",
                "+        }",
                "+",
                "+        if (clientEncryptionOptions.enabled)",
                "+        {",
                "+            fileList.add(new HotReloadableFile(clientEncryptionOptions.keystore, HotReloadableFile.Type.CLIENT));",
                "+            fileList.add(new HotReloadableFile(clientEncryptionOptions.truststore, HotReloadableFile.Type.CLIENT));",
                "+        }",
                "+",
                "+        hotReloadableFiles = ImmutableList.copyOf(fileList);",
                "+",
                "+        if (!isHotReloadingInitialized)",
                "+        {",
                "+            ScheduledExecutors.scheduledTasks.scheduleWithFixedDelay(SSLFactory::checkCertFilesForHotReloading,",
                "+                                                                     DEFAULT_HOT_RELOAD_INITIAL_DELAY_SEC,",
                "+                                                                     DEFAULT_HOT_RELOAD_PERIOD_SEC, TimeUnit.SECONDS);",
                "+        }",
                "+",
                "+        isHotReloadingInitialized = true;",
                "+    }",
                " }",
                "diff --git a/src/java/org/apache/cassandra/tools/NodeProbe.java b/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "index 69b64ab9c3..7ce534189f 100644",
                "--- a/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "+++ b/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "@@ -1649,2 +1649,7 @@ public class NodeProbe implements AutoCloseable",
                "     }",
                "+",
                "+    public void reloadSslCerts()",
                "+    {",
                "+        msProxy.reloadSslCertificates();",
                "+    }",
                " }",
                "diff --git a/src/java/org/apache/cassandra/tools/NodeTool.java b/src/java/org/apache/cassandra/tools/NodeTool.java",
                "index 81f2023afe..2b6fabf81e 100644",
                "--- a/src/java/org/apache/cassandra/tools/NodeTool.java",
                "+++ b/src/java/org/apache/cassandra/tools/NodeTool.java",
                "@@ -158,3 +158,4 @@ public class NodeTool",
                "                 ViewBuildStatus.class,",
                "-                HandoffWindow.class",
                "+                HandoffWindow.class,",
                "+                ReloadSslCertificates.class",
                "         );",
                "diff --git a/src/java/org/apache/cassandra/tools/ReloadSslCertificates.java b/src/java/org/apache/cassandra/tools/ReloadSslCertificates.java",
                "new file mode 100644",
                "index 0000000000..f38b8c059c",
                "--- /dev/null",
                "+++ b/src/java/org/apache/cassandra/tools/ReloadSslCertificates.java",
                "@@ -0,0 +1,30 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ *     http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+package org.apache.cassandra.tools;",
                "+",
                "+import io.airlift.airline.Command;",
                "+",
                "+@Command(name = \"reloadssl\", description = \"Signals Cassandra to reload SSL certificates\")",
                "+public class ReloadSslCertificates extends NodeTool.NodeToolCmd",
                "+{",
                "+    @Override",
                "+    public void execute(NodeProbe probe)",
                "+    {",
                "+        probe.reloadSslCerts();",
                "+    }",
                "+}",
                "\\ No newline at end of file",
                "diff --git a/test/unit/org/apache/cassandra/security/SSLFactoryTest.java b/test/unit/org/apache/cassandra/security/SSLFactoryTest.java",
                "index 61933a507a..5153a1170e 100644",
                "--- a/test/unit/org/apache/cassandra/security/SSLFactoryTest.java",
                "+++ b/test/unit/org/apache/cassandra/security/SSLFactoryTest.java",
                "@@ -20,12 +20,8 @@ package org.apache.cassandra.security;",
                "+import java.io.File;",
                " import java.io.IOException;",
                "-import java.net.InetAddress;",
                " import java.security.cert.CertificateException;",
                " import java.util.Arrays;",
                "-import javax.net.ssl.SSLServerSocket;",
                " import javax.net.ssl.TrustManagerFactory;",
                "-import com.google.common.base.Predicates;",
                "-import com.google.common.collect.Iterables;",
                "-import com.google.common.collect.Lists;",
                " import org.junit.Assert;",
                "@@ -164,2 +160,34 @@ public class SSLFactoryTest",
                "     }",
                "+",
                "+    @Test",
                "+    public void testSslContextReload_HappyPath() throws IOException, InterruptedException",
                "+    {",
                "+        try",
                "+        {",
                "+            EncryptionOptions options = addKeystoreOptions(encryptionOptions);",
                "+            options.enabled = true;",
                "+",
                "+            SSLFactory.initHotReloading((ServerEncryptionOptions) options, options, true);",
                "+",
                "+            SslContext oldCtx = SSLFactory.getSslContext(options, true, true, OpenSsl.isAvailable());",
                "+            File keystoreFile = new File(options.keystore);",
                "+",
                "+            SSLFactory.checkCertFilesForHotReloading();",
                "+            Thread.sleep(5000);",
                "+            keystoreFile.setLastModified(System.currentTimeMillis());",
                "+",
                "+            SSLFactory.checkCertFilesForHotReloading();",
                "+            SslContext newCtx = SSLFactory.getSslContext(options, true, true, OpenSsl.isAvailable());",
                "+",
                "+            Assert.assertNotSame(oldCtx, newCtx);",
                "+        }",
                "+        catch (Exception e)",
                "+        {",
                "+            throw e;",
                "+        }",
                "+        finally",
                "+        {",
                "+            DatabaseDescriptor.loadConfig();",
                "+        }",
                "+    }",
                " }"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/config/DatabaseDescriptor.java",
                "src/java/org/apache/cassandra/net/MessagingService.java",
                "src/java/org/apache/cassandra/net/MessagingServiceMBean.java",
                "src/java/org/apache/cassandra/security/SSLFactory.java",
                "src/java/org/apache/cassandra/tools/NodeProbe.java",
                "src/java/org/apache/cassandra/tools/NodeTool.java",
                "src/java/org/apache/cassandra/tools/ReloadSslCertificates.java",
                "test/unit/org/apache/cassandra/security/SSLFactoryTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14222": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14222",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "da58565ebc717b63fff4f4883559b5daf20cb6fa",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517505577,
            "hunks": 9,
            "message": "Revert ProtocolVersion changes from CASSANDRA-7544 Patch by Ariel Weisberg; Reviewd by Jason Brown for CASSANDRA-14211",
            "diff": [
                "diff --git a/build.xml b/build.xml",
                "index 579686888a..e04ce18a5b 100644",
                "--- a/build.xml",
                "+++ b/build.xml",
                "@@ -439,3 +439,3 @@",
                " \t  <!-- UPDATE AND UNCOMMENT ON THE DRIVER RELEASE, BEFORE 4.0 RELEASE",
                "-          <dependency groupId=\"com.datastax.cassandra\" artifactId=\"cassandra-driver-core\" version=\"4.0.0-SNAPSHOT\" classifier=\"shaded\">",
                "+          <dependency groupId=\"com.datastax.cassandra\" artifactId=\"cassandra-driver-core\" version=\"3.4.0-SNAPSHOT\" classifier=\"shaded\">",
                "             <exclusion groupId=\"io.netty\" artifactId=\"netty-buffer\"/>",
                "diff --git a/conf/cassandra.yaml b/conf/cassandra.yaml",
                "index 3bed3a6bd1..9acc6d6540 100644",
                "--- a/conf/cassandra.yaml",
                "+++ b/conf/cassandra.yaml",
                "@@ -947,9 +947,2 @@ dynamic_snitch_badness_threshold: 0.1",
                " #",
                "-# If you are taking advantage of StartTLS outbound connections will have the issue that they can't know",
                "-# what encrypted port to connect to in a foolproof way. outgoing_encrypted_port_source deals with this confusion",
                "-# by allowing you to specify how you want a node to pick an outgoing port for intra-cluster connections.",
                "-# Valid values are \"gossip\" and \"yaml\". Gossip will always connect to the storage port for a node that is",
                "-# published via a gossip which is always going to be the plain storage port. \"yaml\" will always select",
                "-# the port configured as ssl_storage_port on THIS node. If you want to use SSL and have different storage",
                "-# ports across the cluster you must select \"gossip\" and use StartTLS on storage_port.",
                " server_encryption_options:",
                "diff --git a/lib/cassandra-driver-core-3.4.0-SNAPSHOT-shaded.jar b/lib/cassandra-driver-core-3.4.0-SNAPSHOT-shaded.jar",
                "new file mode 100644",
                "index 0000000000..1290dc3e31",
                "Binary files /dev/null and b/lib/cassandra-driver-core-3.4.0-SNAPSHOT-shaded.jar differ",
                "diff --git a/lib/cassandra-driver-internal-only-3.12.0.post0-00f6f77e.zip b/lib/cassandra-driver-internal-only-3.12.0.post0-00f6f77e.zip",
                "new file mode 100644",
                "index 0000000000..e44da16941",
                "Binary files /dev/null and b/lib/cassandra-driver-internal-only-3.12.0.post0-00f6f77e.zip differ",
                "diff --git a/lib/cassandra-driver-internal-only-3.12.0.post0-9ee88ded.zip b/lib/cassandra-driver-internal-only-3.12.0.post0-9ee88ded.zip",
                "deleted file mode 100644",
                "index 4aa91b7046..0000000000",
                "Binary files a/lib/cassandra-driver-internal-only-3.12.0.post0-9ee88ded.zip and /dev/null differ",
                "diff --git a/src/java/org/apache/cassandra/cql3/functions/ScriptBasedUDFunction.java b/src/java/org/apache/cassandra/cql3/functions/ScriptBasedUDFunction.java",
                "index c568972f65..96a8b0d85a 100644",
                "--- a/src/java/org/apache/cassandra/cql3/functions/ScriptBasedUDFunction.java",
                "+++ b/src/java/org/apache/cassandra/cql3/functions/ScriptBasedUDFunction.java",
                "@@ -82,3 +82,5 @@ final class ScriptBasedUDFunction extends UDFunction",
                "     \"com.datastax.driver.core\",",
                "-    \"com.datastax.driver.core.utils\"",
                "+    \"com.datastax.driver.core.utils\",",
                "+    //Driver Metadata class requires hashmap from this",
                "+    \"com.datastax.shaded.netty.util.collection\"",
                "     };",
                "diff --git a/src/java/org/apache/cassandra/transport/ProtocolVersion.java b/src/java/org/apache/cassandra/transport/ProtocolVersion.java",
                "index 838176ac29..cd73c86255 100644",
                "--- a/src/java/org/apache/cassandra/transport/ProtocolVersion.java",
                "+++ b/src/java/org/apache/cassandra/transport/ProtocolVersion.java",
                "@@ -45,4 +45,3 @@ public enum ProtocolVersion implements Comparable<ProtocolVersion>",
                "     V4(4, \"v4\", false),",
                "-    V5(5, \"v5\", false),",
                "-    V6(6, \"v6-beta\", true);",
                "+    V5(5, \"v5-beta\", true);",
                "@@ -65,3 +64,3 @@ public enum ProtocolVersion implements Comparable<ProtocolVersion>",
                "     /** The supported versions stored as an array, these should be private and are required for fast decoding*/",
                "-    private final static ProtocolVersion[] SUPPORTED_VERSIONS = new ProtocolVersion[] { V3, V4, V5, V6 };",
                "+    private final static ProtocolVersion[] SUPPORTED_VERSIONS = new ProtocolVersion[] { V3, V4, V5 };",
                "     final static ProtocolVersion MIN_SUPPORTED_VERSION = SUPPORTED_VERSIONS[0];",
                "@@ -76,4 +75,4 @@ public enum ProtocolVersion implements Comparable<ProtocolVersion>",
                "     /** The preferred versions */",
                "-    public final static ProtocolVersion CURRENT = V5;",
                "-    public final static Optional<ProtocolVersion> BETA = Optional.of(V6);",
                "+    public final static ProtocolVersion CURRENT = V4;",
                "+    public final static Optional<ProtocolVersion> BETA = Optional.of(V5);",
                "diff --git a/test/unit/org/apache/cassandra/cql3/PreparedStatementsTest.java b/test/unit/org/apache/cassandra/cql3/PreparedStatementsTest.java",
                "index ce5de627d1..0a314da6dc 100644",
                "--- a/test/unit/org/apache/cassandra/cql3/PreparedStatementsTest.java",
                "+++ b/test/unit/org/apache/cassandra/cql3/PreparedStatementsTest.java",
                "@@ -225,2 +225,3 @@ public class PreparedStatementsTest extends CQLTester",
                "                                  .withoutJMXReporting()",
                "+                                 .allowBetaProtocolVersion()",
                "                                  .build())",
                "diff --git a/test/unit/org/apache/cassandra/service/ProtocolBetaVersionTest.java b/test/unit/org/apache/cassandra/service/ProtocolBetaVersionTest.java",
                "index a7551f42b3..4ade4adcfb 100644",
                "--- a/test/unit/org/apache/cassandra/service/ProtocolBetaVersionTest.java",
                "+++ b/test/unit/org/apache/cassandra/service/ProtocolBetaVersionTest.java",
                "@@ -112,3 +112,3 @@ public class ProtocolBetaVersionTest extends CQLTester",
                "         {",
                "-            assertEquals(\"Beta version of server used (6/v6-beta), but USE_BETA flag is not set\",",
                "+            assertEquals(\"Beta version of server used (5/v5-beta), but USE_BETA flag is not set\",",
                "                          e.getMessage());",
                "diff --git a/test/unit/org/apache/cassandra/transport/ProtocolVersionTest.java b/test/unit/org/apache/cassandra/transport/ProtocolVersionTest.java",
                "index 6b95c67985..0669699730 100644",
                "--- a/test/unit/org/apache/cassandra/transport/ProtocolVersionTest.java",
                "+++ b/test/unit/org/apache/cassandra/transport/ProtocolVersionTest.java",
                "@@ -64,4 +64,3 @@ public class ProtocolVersionTest",
                "         Assert.assertFalse(ProtocolVersion.V4.isBeta());",
                "-        Assert.assertFalse(ProtocolVersion.V5.isBeta());",
                "-        Assert.assertTrue(ProtocolVersion.V6.isBeta());",
                "+        Assert.assertTrue(ProtocolVersion.V5.isBeta());",
                "     }"
            ],
            "changed_files": [
                "build.xml",
                "conf/cassandra.yaml",
                "lib/cassandra-driver-core-3.4.0-SNAPSHOT-shaded.jar",
                "lib/cassandra-driver-internal-only-3.12.0.post0-00f6f77e.zip",
                "lib/cassandra-driver-internal-only-3.12.0.post0-9ee88ded.zip",
                "src/java/org/apache/cassandra/cql3/functions/ScriptBasedUDFunction.java",
                "src/java/org/apache/cassandra/transport/ProtocolVersion.java",
                "test/unit/org/apache/cassandra/cql3/PreparedStatementsTest.java",
                "test/unit/org/apache/cassandra/service/ProtocolBetaVersionTest.java",
                "test/unit/org/apache/cassandra/transport/ProtocolVersionTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-7544": "",
                "CASSANDRA-14211": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-7544, CASSANDRA-14211",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "4de7a65ed9f3c97658a80dd64032ad6e82e9d58b",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516624262,
            "hunks": 16,
            "message": "Make sub-range selection for non-frozen collections return null instead of empty patch by Benjamin Lerer; reviewed by Andr\u00c3\u00a9s de la Pe\u00c3\u00b1a for CASSANDRA-14182",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/cql3/selection/ElementsSelector.java b/src/java/org/apache/cassandra/cql3/selection/ElementsSelector.java",
                "index f94b5c84cc..9427c51d59 100644",
                "--- a/src/java/org/apache/cassandra/cql3/selection/ElementsSelector.java",
                "+++ b/src/java/org/apache/cassandra/cql3/selection/ElementsSelector.java",
                "@@ -306,3 +306,3 @@ abstract class ElementsSelector extends Selector",
                "         {",
                "-            return type.getSerializer().getSliceFromSerialized(collection, from, to, type.nameComparator());",
                "+            return type.getSerializer().getSliceFromSerialized(collection, from, to, type.nameComparator(), type.isFrozenCollection());",
                "         }",
                "diff --git a/src/java/org/apache/cassandra/serializers/CollectionSerializer.java b/src/java/org/apache/cassandra/serializers/CollectionSerializer.java",
                "index 9d261c650a..d988cc0f13 100644",
                "--- a/src/java/org/apache/cassandra/serializers/CollectionSerializer.java",
                "+++ b/src/java/org/apache/cassandra/serializers/CollectionSerializer.java",
                "@@ -136,2 +136,4 @@ public abstract class CollectionSerializer<T> implements TypeSerializer<T>",
                "      * Returns the slice of a collection directly from its serialized value.",
                "+     * <p>If the slice contains no elements an empty collection will be returned for frozen collections, and a ",
                "+     * {@code null} one for non-frozen collections.</p>",
                "      *",
                "@@ -143,6 +145,10 @@ public abstract class CollectionSerializer<T> implements TypeSerializer<T>",
                "      * in the collection.",
                "-     * @return a valid serialized collection (possibly empty) corresponding to slice {@code [from, to]}",
                "-     * of {@code collection}.",
                "+     * @param frozen {@code true} if the collection is a frozen one, {@code false} otherwise",
                "+     * @return a serialized collection corresponding to slice {@code [from, to]} of {@code collection}.",
                "      */",
                "-    public abstract ByteBuffer getSliceFromSerialized(ByteBuffer collection, ByteBuffer from, ByteBuffer to, AbstractType<?> comparator);",
                "+    public abstract ByteBuffer getSliceFromSerialized(ByteBuffer collection,",
                "+                                                      ByteBuffer from,",
                "+                                                      ByteBuffer to,",
                "+                                                      AbstractType<?> comparator,",
                "+                                                      boolean frozen);",
                "diff --git a/src/java/org/apache/cassandra/serializers/ListSerializer.java b/src/java/org/apache/cassandra/serializers/ListSerializer.java",
                "index 98ebd484a4..dd0bc9e8b7 100644",
                "--- a/src/java/org/apache/cassandra/serializers/ListSerializer.java",
                "+++ b/src/java/org/apache/cassandra/serializers/ListSerializer.java",
                "@@ -171,2 +171,3 @@ public class ListSerializer<T> extends CollectionSerializer<List<T>>",
                "+    @Override",
                "     public ByteBuffer getSerializedValue(ByteBuffer collection, ByteBuffer key, AbstractType<?> comparator)",
                "@@ -177,3 +178,8 @@ public class ListSerializer<T> extends CollectionSerializer<List<T>>",
                "-    public ByteBuffer getSliceFromSerialized(ByteBuffer collection, ByteBuffer from, ByteBuffer to, AbstractType<?> comparator)",
                "+    @Override",
                "+    public ByteBuffer getSliceFromSerialized(ByteBuffer collection,",
                "+                                             ByteBuffer from,",
                "+                                             ByteBuffer to,",
                "+                                             AbstractType<?> comparator,",
                "+                                             boolean frozen)",
                "     {",
                "diff --git a/src/java/org/apache/cassandra/serializers/MapSerializer.java b/src/java/org/apache/cassandra/serializers/MapSerializer.java",
                "index f4285027f1..c772a456ee 100644",
                "--- a/src/java/org/apache/cassandra/serializers/MapSerializer.java",
                "+++ b/src/java/org/apache/cassandra/serializers/MapSerializer.java",
                "@@ -131,2 +131,3 @@ public class MapSerializer<K, V> extends CollectionSerializer<Map<K, V>>",
                "+    @Override",
                "     public ByteBuffer getSerializedValue(ByteBuffer collection, ByteBuffer key, AbstractType<?> comparator)",
                "@@ -157,3 +158,8 @@ public class MapSerializer<K, V> extends CollectionSerializer<Map<K, V>>",
                "-    public ByteBuffer getSliceFromSerialized(ByteBuffer collection, ByteBuffer from, ByteBuffer to, AbstractType<?> comparator)",
                "+    @Override",
                "+    public ByteBuffer getSliceFromSerialized(ByteBuffer collection,",
                "+                                             ByteBuffer from,",
                "+                                             ByteBuffer to,",
                "+                                             AbstractType<?> comparator,",
                "+                                             boolean frozen)",
                "     {",
                "@@ -210,2 +216,6 @@ public class MapSerializer<K, V> extends CollectionSerializer<Map<K, V>>",
                "             }",
                "+",
                "+            if (count == 0 && !frozen)",
                "+                return null;",
                "+",
                "             return copyAsNewCollection(collection, count, startPos, input.position(), ProtocolVersion.V3);",
                "diff --git a/src/java/org/apache/cassandra/serializers/SetSerializer.java b/src/java/org/apache/cassandra/serializers/SetSerializer.java",
                "index e4e970cce6..2548219261 100644",
                "--- a/src/java/org/apache/cassandra/serializers/SetSerializer.java",
                "+++ b/src/java/org/apache/cassandra/serializers/SetSerializer.java",
                "@@ -142,2 +142,3 @@ public class SetSerializer<T> extends CollectionSerializer<Set<T>>",
                "+    @Override",
                "     public ByteBuffer getSerializedValue(ByteBuffer collection, ByteBuffer key, AbstractType<?> comparator)",
                "@@ -167,3 +168,8 @@ public class SetSerializer<T> extends CollectionSerializer<Set<T>>",
                "-    public ByteBuffer getSliceFromSerialized(ByteBuffer collection, ByteBuffer from, ByteBuffer to, AbstractType<?> comparator)",
                "+    @Override",
                "+    public ByteBuffer getSliceFromSerialized(ByteBuffer collection,",
                "+                                             ByteBuffer from,",
                "+                                             ByteBuffer to,",
                "+                                             AbstractType<?> comparator,",
                "+                                             boolean frozen)",
                "     {",
                "@@ -218,2 +224,6 @@ public class SetSerializer<T> extends CollectionSerializer<Set<T>>",
                "             }",
                "+",
                "+            if (count == 0 && !frozen)",
                "+                return null;",
                "+",
                "             return copyAsNewCollection(collection, count, startPos, input.position(), ProtocolVersion.V3);",
                "diff --git a/test/unit/org/apache/cassandra/cql3/validation/entities/CollectionsTest.java b/test/unit/org/apache/cassandra/cql3/validation/entities/CollectionsTest.java",
                "index e26f20777b..9ad47c05f1 100644",
                "--- a/test/unit/org/apache/cassandra/cql3/validation/entities/CollectionsTest.java",
                "+++ b/test/unit/org/apache/cassandra/cql3/validation/entities/CollectionsTest.java",
                "@@ -1200,3 +1200,3 @@ public class CollectionsTest extends CQLTester",
                "                    row(0, \"foobar\", map(\"22\", \"value22\"), 42),",
                "-                   row(0, \"foobar\", map(), 42)",
                "+                   row(0, \"foobar\", null, 42)",
                "         );",
                "@@ -1713,3 +1713,3 @@ public class CollectionsTest extends CQLTester",
                "                    row(0,",
                "-                       map(\"1\", \"one\", \"2\", \"two\"), \"two\", map(\"2\", \"two\"), map(\"1\", \"one\", \"2\", \"two\"), map(),",
                "+                       map(\"1\", \"one\", \"2\", \"two\"), \"two\", map(\"2\", \"two\"), map(\"1\", \"one\", \"2\", \"two\"), null,",
                "                        map(\"1\", \"one\", \"2\", \"two\"), \"two\", map(\"2\", \"two\"), map(\"1\", \"one\", \"2\", \"two\"), map(),",
                "@@ -1817,3 +1817,3 @@ public class CollectionsTest extends CQLTester",
                "         assertRows(execute(\"SELECT m[7..8] FROM %s WHERE k=?\", 0),",
                "-                   row(map()));",
                "+                   row((Map<Integer, Integer>) null));",
                "@@ -1917,2 +1917,66 @@ public class CollectionsTest extends CQLTester",
                "     }",
                "+",
                "+    @Test",
                "+    public void testSelectionOfEmptyCollections() throws Throwable",
                "+    {",
                "+        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, m frozen<map<text, int>>, s frozen<set<int>>)\");",
                "+",
                "+        execute(\"INSERT INTO %s(k) VALUES (0)\");",
                "+        execute(\"INSERT INTO %s(k, m, s) VALUES (1, {}, {})\");",
                "+        execute(\"INSERT INTO %s(k, m, s) VALUES (2, ?, ?)\", map(), set());",
                "+        execute(\"INSERT INTO %s(k, m, s) VALUES (3, {'2':2}, {2})\");",
                "+",
                "+        beforeAndAfterFlush(() ->",
                "+        {",
                "+            assertRows(execute(\"SELECT m, s FROM %s WHERE k = 0\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'], s[0] FROM %s WHERE k = 0\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1'], s[0..1] FROM %s WHERE k = 0\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1']['3'..'5'], s[0..1][3..5] FROM %s WHERE k = 0\"), row(null, null));",
                "+",
                "+            assertRows(execute(\"SELECT m, s FROM %s WHERE k = 1\"), row(map(), set()));",
                "+            assertRows(execute(\"SELECT m['0'], s[0] FROM %s WHERE k = 1\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1'], s[0..1] FROM %s WHERE k = 1\"), row(map(), set()));",
                "+            assertRows(execute(\"SELECT m['0'..'1']['3'..'5'], s[0..1][3..5] FROM %s WHERE k = 1\"), row(map(), set()));",
                "+",
                "+            assertRows(execute(\"SELECT m, s FROM %s WHERE k = 2\"), row(map(), set()));",
                "+            assertRows(execute(\"SELECT m['0'], s[0] FROM %s WHERE k = 2\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1'], s[0..1] FROM %s WHERE k = 2\"), row(map(), set()));",
                "+            assertRows(execute(\"SELECT m['0'..'1']['3'..'5'], s[0..1][3..5] FROM %s WHERE k = 2\"), row(map(), set()));",
                "+",
                "+            assertRows(execute(\"SELECT m, s FROM %s WHERE k = 3\"), row(map(\"2\", 2), set(2)));",
                "+            assertRows(execute(\"SELECT m['0'], s[0] FROM %s WHERE k = 3\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1'], s[0..1] FROM %s WHERE k = 3\"), row(map(), set()));",
                "+            assertRows(execute(\"SELECT m['0'..'1']['3'..'5'], s[0..1][3..5] FROM %s WHERE k = 3\"), row(map(), set()));",
                "+        });",
                "+",
                "+        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, m map<text, int>, s set<int>)\");",
                "+",
                "+        execute(\"INSERT INTO %s(k) VALUES (0)\");",
                "+        execute(\"INSERT INTO %s(k, m, s) VALUES (1, {}, {})\");",
                "+        execute(\"INSERT INTO %s(k, m, s) VALUES (2, ?, ?)\", map(), set());",
                "+        execute(\"INSERT INTO %s(k, m, s) VALUES (3, {'2':2}, {2})\");",
                "+",
                "+        beforeAndAfterFlush(() ->",
                "+        {",
                "+            assertRows(execute(\"SELECT m, s FROM %s WHERE k = 0\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'], s[0] FROM %s WHERE k = 0\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1'], s[0..1] FROM %s WHERE k = 0\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1']['3'..'5'], s[0..1][3..5] FROM %s WHERE k = 0\"), row(null, null));",
                "+",
                "+            assertRows(execute(\"SELECT m, s FROM %s WHERE k = 1\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'], s[0] FROM %s WHERE k = 1\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1'], s[0..1] FROM %s WHERE k = 1\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1']['3'..'5'], s[0..1][3..5] FROM %s WHERE k = 1\"), row(null, null));",
                "+",
                "+            assertRows(execute(\"SELECT m, s FROM %s WHERE k = 2\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'], s[0] FROM %s WHERE k = 2\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1'], s[0..1] FROM %s WHERE k = 2\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1']['3'..'5'], s[0..1][3..5] FROM %s WHERE k = 2\"), row(null, null));",
                "+",
                "+            assertRows(execute(\"SELECT m, s FROM %s WHERE k = 3\"), row(map(\"2\", 2), set(2)));",
                "+            assertRows(execute(\"SELECT m['0'], s[0] FROM %s WHERE k = 3\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1'], s[0..1] FROM %s WHERE k = 3\"), row(null, null));",
                "+            assertRows(execute(\"SELECT m['0'..'1']['3'..'5'], s[0..1][3..5] FROM %s WHERE k = 3\"), row(null, null));",
                "+        });",
                "+    }",
                " }"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/cql3/selection/ElementsSelector.java",
                "src/java/org/apache/cassandra/serializers/CollectionSerializer.java",
                "src/java/org/apache/cassandra/serializers/ListSerializer.java",
                "src/java/org/apache/cassandra/serializers/MapSerializer.java",
                "src/java/org/apache/cassandra/serializers/SetSerializer.java",
                "test/unit/org/apache/cassandra/cql3/validation/entities/CollectionsTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14182": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14182",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "fc3357a00e2b6e56d399f07c5b81a82780c1e143",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516227121,
            "hunks": 5,
            "message": "Print correct snitch info from nodetool describecluster patch by Lerh Chuan Low; reviewed by jasobrown for CASSANDRA-13528",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/locator/EndpointSnitchInfoMBean.java b/src/java/org/apache/cassandra/locator/EndpointSnitchInfoMBean.java",
                "index 6de5022c51..d6a18ff786 100644",
                "--- a/src/java/org/apache/cassandra/locator/EndpointSnitchInfoMBean.java",
                "+++ b/src/java/org/apache/cassandra/locator/EndpointSnitchInfoMBean.java",
                "@@ -55,3 +55,2 @@ public interface EndpointSnitchInfoMBean",
                "     public String getSnitchName();",
                "-",
                " }",
                "diff --git a/src/java/org/apache/cassandra/tools/NodeProbe.java b/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "index 8046c54029..f87de35c0e 100644",
                "--- a/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "+++ b/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "@@ -68,2 +68,3 @@ import org.apache.cassandra.gms.GossiperMBean;",
                " import org.apache.cassandra.db.HintedHandOffManager;",
                "+import org.apache.cassandra.locator.DynamicEndpointSnitchMBean;",
                " import org.apache.cassandra.locator.EndpointSnitchInfoMBean;",
                "@@ -825,2 +826,14 @@ public class NodeProbe implements AutoCloseable",
                "+    public DynamicEndpointSnitchMBean getDynamicEndpointSnitchInfoProxy()",
                "+    {",
                "+        try",
                "+        {",
                "+            return JMX.newMBeanProxy(mbeanServerConn, new ObjectName(\"org.apache.cassandra.db:type=DynamicEndpointSnitch\"), DynamicEndpointSnitchMBean.class);",
                "+        }",
                "+        catch (MalformedObjectNameException e)",
                "+        {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "     public ColumnFamilyStoreMBean getCfsProxy(String ks, String cf)",
                "diff --git a/src/java/org/apache/cassandra/tools/nodetool/DescribeCluster.java b/src/java/org/apache/cassandra/tools/nodetool/DescribeCluster.java",
                "index 81dee203a5..41789984df 100644",
                "--- a/src/java/org/apache/cassandra/tools/nodetool/DescribeCluster.java",
                "+++ b/src/java/org/apache/cassandra/tools/nodetool/DescribeCluster.java",
                "@@ -25,2 +25,3 @@ import java.util.Map;",
                "+import org.apache.cassandra.locator.DynamicEndpointSnitch;",
                " import org.apache.cassandra.tools.NodeProbe;",
                "@@ -37,3 +38,11 @@ public class DescribeCluster extends NodeToolCmd",
                "         System.out.println(\"\\tName: \" + probe.getClusterName());",
                "-        System.out.println(\"\\tSnitch: \" + probe.getEndpointSnitchInfoProxy().getSnitchName());",
                "+        String snitch = probe.getEndpointSnitchInfoProxy().getSnitchName();",
                "+        boolean dynamicSnitchEnabled = false;",
                "+        if (snitch.equals(DynamicEndpointSnitch.class.getName()))",
                "+        {",
                "+            snitch = probe.getDynamicEndpointSnitchInfoProxy().getSubsnitchClassName();",
                "+            dynamicSnitchEnabled = true;",
                "+        }",
                "+        System.out.println(\"\\tSnitch: \" + snitch);",
                "+        System.out.println(\"\\tDynamicEndPointSnitch: \" + (dynamicSnitchEnabled ? \"enabled\" : \"disabled\"));",
                "         System.out.println(\"\\tPartitioner: \" + probe.getPartitioner());"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/locator/EndpointSnitchInfoMBean.java",
                "src/java/org/apache/cassandra/tools/NodeProbe.java",
                "src/java/org/apache/cassandra/tools/nodetool/DescribeCluster.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-13528": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-13528",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "6fc3699adb4f609f78a40888af9797bb205216b0",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516911890,
            "hunks": 1,
            "message": "ninja: fix bad CASSANDRA-14082 merge from 3.11",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/db/compaction/CompactionStrategyManager.java b/src/java/org/apache/cassandra/db/compaction/CompactionStrategyManager.java",
                "index 7ad6e91f80..61f0630942 100644",
                "--- a/src/java/org/apache/cassandra/db/compaction/CompactionStrategyManager.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/CompactionStrategyManager.java",
                "@@ -747,3 +747,3 @@ public class CompactionStrategyManager implements INotificationConsumer",
                "         {",
                "-            getCompactionStrategyFor(deleted).removeSSTable(deleted);",
                "+            compactionStrategyFor(deleted).removeSSTable(deleted);",
                "         }"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/db/compaction/CompactionStrategyManager.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14082": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14082",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "69db2359ee0889cb4a57aec179b9821ff442d26b",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517183012,
            "hunks": 4,
            "message": "Reset CDCSpaceInMB after each test patch by Jay Zhuang; reviewed by jmckenzie for CASSANDRA-14195",
            "diff": [
                "diff --git a/test/unit/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDCTest.java b/test/unit/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDCTest.java",
                "index d9bf493c2b..8c0647c839 100644",
                "--- a/test/unit/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDCTest.java",
                "+++ b/test/unit/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDCTest.java",
                "@@ -190,2 +190,4 @@ public class CommitLogSegmentManagerCDCTest extends CQLTester",
                "         CommitLogSegment initialSegment = CommitLog.instance.segmentManager.allocatingFrom();",
                "+        Integer originalCDCSize = DatabaseDescriptor.getCDCSpaceInMB();",
                "+",
                "         DatabaseDescriptor.setCDCSpaceInMB(8);",
                "@@ -204,2 +206,6 @@ public class CommitLogSegmentManagerCDCTest extends CQLTester",
                "         }",
                "+        finally",
                "+        {",
                "+            DatabaseDescriptor.setCDCSpaceInMB(originalCDCSize);",
                "+        }",
                "@@ -277,2 +283,3 @@ public class CommitLogSegmentManagerCDCTest extends CQLTester",
                "         String table_name = createTable(\"CREATE TABLE %s (idx int, data text, primary key(idx)) WITH cdc=true;\");",
                "+        Integer originalCDCSize = DatabaseDescriptor.getCDCSpaceInMB();",
                "@@ -294,2 +301,6 @@ public class CommitLogSegmentManagerCDCTest extends CQLTester",
                "         }",
                "+        finally",
                "+        {",
                "+            DatabaseDescriptor.setCDCSpaceInMB(originalCDCSize);",
                "+        }"
            ],
            "changed_files": [
                "test/unit/org/apache/cassandra/db/commitlog/CommitLogSegmentManagerCDCTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14195": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14195",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7885a703d6dae8c3c6e5a6af632c6a23342593fc",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517990093,
            "hunks": 1,
            "message": "Handle error when mutating repairedAt in nodetool verify Patch by Sumanth Pasupuleti; reviewed by marcuse for CASSANDRA-13933",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/db/compaction/Verifier.java b/src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "index 68088b318a..86bc377d5b 100644",
                "--- a/src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "@@ -257,3 +257,12 @@ public class Verifier implements Closeable",
                "         if (mutateRepaired) // if we are able to mutate repaired flag, an incremental repair should be enough",
                "-            sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, ActiveRepairService.UNREPAIRED_SSTABLE);",
                "+        {",
                "+            try",
                "+            {",
                "+                sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, ActiveRepairService.UNREPAIRED_SSTABLE);",
                "+            }",
                "+            catch(IOException ioe)",
                "+            {",
                "+                outputHandler.output(\"Error mutating repairedAt for SSTable \" +  sstable.getFilename() + \", as part of markAndThrow\");",
                "+            }",
                "+        }",
                "         throw new CorruptSSTableException(new Exception(String.format(\"Invalid SSTable %s, please force %srepair\", sstable.getFilename(), mutateRepaired ? \"\" : \"a full \")), sstable.getFilename());"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/db/compaction/Verifier.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-13933": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-13933",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "fa39b6e2775abef6323598461f8fadbcbbe793e5",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518369431,
            "hunks": 4,
            "message": "ninja: fix bad #14092 merge from cassandra-3.0 to cassandra-3.11",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/tools/StandaloneScrubber.java b/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "index ead8fc57d4..f4a5f9b962 100644",
                "--- a/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "+++ b/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "@@ -45,2 +45,8 @@ public class StandaloneScrubber",
                " {",
                "+    public static final String REINSERT_OVERFLOWED_TTL_OPTION_DESCRIPTION = \"Rewrites rows with overflowed expiration date affected by CASSANDRA-14092 with \" +",
                "+                                                                            \"the maximum supported expiration date of 2038-01-19T03:14:06+00:00. \" +",
                "+                                                                            \"The rows are rewritten with the original timestamp incremented by one millisecond \" +",
                "+                                                                            \"to override/supersede any potential tombstone that may have been generated \" +",
                "+                                                                            \"during compaction of the affected rows.\";",
                "+",
                "     private static final String TOOL_NAME = \"sstablescrub\";",
                "@@ -271,3 +277,3 @@ public class StandaloneScrubber",
                "             options.addOption(\"n\",  NO_VALIDATE_OPTION,    \"do not validate columns using column validator\");",
                "-            options.addOption(\"r\",  REINSERT_OVERFLOWED_TTL_OPTION,    \"Reinsert found rows with overflowed TTL affected by CASSANDRA-14092\");",
                "+            options.addOption(\"r\", REINSERT_OVERFLOWED_TTL_OPTION, REINSERT_OVERFLOWED_TTL_OPTION_DESCRIPTION);",
                "             return options;",
                "diff --git a/src/java/org/apache/cassandra/tools/nodetool/Scrub.java b/src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "index 812202dc25..ead2fd4c37 100644",
                "--- a/src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "+++ b/src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "@@ -28,2 +28,3 @@ import org.apache.cassandra.tools.NodeProbe;",
                " import org.apache.cassandra.tools.NodeTool.NodeToolCmd;",
                "+import org.apache.cassandra.tools.StandaloneScrubber;",
                "@@ -52,3 +53,3 @@ public class Scrub extends NodeToolCmd",
                "     name = {\"r\", \"--reinsert-overflowed-ttl\"},",
                "-    description = \"Reinsert found rows with overflowed TTL affected by CASSANDRA-14092\")",
                "+    description = StandaloneScrubber.REINSERT_OVERFLOWED_TTL_OPTION_DESCRIPTION)",
                "     private boolean reinsertOverflowedTTL = false;"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "src/java/org/apache/cassandra/tools/nodetool/Scrub.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {
                "14092": ""
            },
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "GITHUB_ISSUE_IN_MESSAGE",
                    "message": "The commit message references some github issue: 14092",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d30bfcaf3430b8eb11cca913105b01364502348e",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517471453,
            "hunks": 18,
            "message": "Add nodetool clientlist patch by Chris Lohfink; reviewed by jasobrown for CASSANDRA-13665",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/metrics/ClientMetrics.java b/src/java/org/apache/cassandra/metrics/ClientMetrics.java",
                "index db6422c1ac..8ca3480376 100644",
                "--- a/src/java/org/apache/cassandra/metrics/ClientMetrics.java",
                "+++ b/src/java/org/apache/cassandra/metrics/ClientMetrics.java",
                "@@ -38,5 +38,5 @@ public class ClientMetrics",
                "-    public void addCounter(String name, final Callable<Integer> provider)",
                "+    public <T> void addGauge(String name, final Callable<T> provider)",
                "     {",
                "-        Metrics.register(factory.createMetricName(name), (Gauge<Integer>) () -> {",
                "+        Metrics.register(factory.createMetricName(name), (Gauge<T>) () -> {",
                "             try",
                "diff --git a/src/java/org/apache/cassandra/service/NativeTransportService.java b/src/java/org/apache/cassandra/service/NativeTransportService.java",
                "index b184442227..42764e1ed6 100644",
                "--- a/src/java/org/apache/cassandra/service/NativeTransportService.java",
                "+++ b/src/java/org/apache/cassandra/service/NativeTransportService.java",
                "@@ -20,2 +20,3 @@ package org.apache.cassandra.service;",
                " import java.net.InetAddress;",
                "+import java.util.ArrayList;",
                " import java.util.Arrays;",
                "@@ -23,2 +24,6 @@ import java.util.Collection;",
                " import java.util.Collections;",
                "+import java.util.HashMap;",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import java.util.Map.Entry;",
                " import java.util.concurrent.TimeUnit;",
                "@@ -26,2 +31,4 @@ import java.util.concurrent.TimeUnit;",
                " import com.google.common.annotations.VisibleForTesting;",
                "+import com.google.common.collect.Lists;",
                "+",
                " import org.slf4j.Logger;",
                "@@ -111,3 +118,3 @@ public class NativeTransportService",
                "         // register metrics",
                "-        ClientMetrics.instance.addCounter(\"connectedNativeClients\", () ->",
                "+        ClientMetrics.instance.addGauge(\"connectedNativeClients\", () ->",
                "         {",
                "@@ -118,3 +125,28 @@ public class NativeTransportService",
                "         });",
                "+        ClientMetrics.instance.addGauge(\"connectedNativeClientsByUser\", () ->",
                "+        {",
                "+            Map<String, Integer> result = new HashMap<>();",
                "+            for (Server server : servers)",
                "+            {",
                "+                for (Entry<String, Integer> e : server.getConnectedClientsByUser().entrySet())",
                "+                {",
                "+                    String user = e.getKey();",
                "+                    result.put(user, result.getOrDefault(user, 0) + e.getValue());",
                "+                }",
                "+            }",
                "+            return result;",
                "+        });",
                "+        ClientMetrics.instance.addGauge(\"connections\", () ->",
                "+        {",
                "+            List<Map<String, String>> result = new ArrayList<>();",
                "+            for (Server server : servers)",
                "+            {",
                "+                for (Map<String, String> e : server.getConnectionStates())",
                "+                {",
                "+                    result.add(e);",
                "+                }",
                "+            }",
                "+            return result;",
                "+        });",
                "         AuthMetrics.init();",
                "diff --git a/src/java/org/apache/cassandra/service/StorageProxy.java b/src/java/org/apache/cassandra/service/StorageProxy.java",
                "index dcf0cab52a..e2125d4994 100644",
                "--- a/src/java/org/apache/cassandra/service/StorageProxy.java",
                "+++ b/src/java/org/apache/cassandra/service/StorageProxy.java",
                "@@ -76,2 +76,3 @@ import org.apache.cassandra.net.MessagingService.Verb;",
                " import org.apache.cassandra.tracing.Tracing;",
                "+import org.apache.cassandra.transport.Server;",
                " import org.apache.cassandra.triggers.TriggerExecutor;",
                "diff --git a/src/java/org/apache/cassandra/tools/NodeProbe.java b/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "index ec8f7ba7bf..69b64ab9c3 100644",
                "--- a/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "+++ b/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "@@ -1493,2 +1493,28 @@ public class NodeProbe implements AutoCloseable",
                "+    /**",
                "+     * Retrieve Proxy metrics",
                "+     * @param connections, connectedNativeClients, connectedNativeClientsByUser",
                "+     */",
                "+    public Object getClientMetric(String metricName)",
                "+    {",
                "+        try",
                "+        {",
                "+            switch(metricName)",
                "+            {",
                "+                case \"connections\": // List<Map<String,String>> - list of all native connections and their properties",
                "+                case \"connectedNativeClients\": // number of connected native clients",
                "+                case \"connectedNativeClientsByUser\": // number of native clients by username",
                "+                    return JMX.newMBeanProxy(mbeanServerConn,",
                "+                            new ObjectName(\"org.apache.cassandra.metrics:type=Client,name=\" + metricName),",
                "+                            CassandraMetricsRegistry.JmxGaugeMBean.class).getValue();",
                "+                default:",
                "+                    throw new RuntimeException(\"Unknown client metric \" + metricName);",
                "+            }",
                "+        }",
                "+        catch (MalformedObjectNameException e)",
                "+        {",
                "+            throw new RuntimeException(e);",
                "+        }",
                "+    }",
                "+",
                "     /**",
                "diff --git a/src/java/org/apache/cassandra/tools/NodeTool.java b/src/java/org/apache/cassandra/tools/NodeTool.java",
                "index f7b7f76304..81f2023afe 100644",
                "--- a/src/java/org/apache/cassandra/tools/NodeTool.java",
                "+++ b/src/java/org/apache/cassandra/tools/NodeTool.java",
                "@@ -61,2 +61,3 @@ public class NodeTool",
                "                 ClearSnapshot.class,",
                "+                ClientStats.class,",
                "                 Compact.class,",
                "diff --git a/src/java/org/apache/cassandra/tools/nodetool/ClientStats.java b/src/java/org/apache/cassandra/tools/nodetool/ClientStats.java",
                "new file mode 100644",
                "index 0000000000..21915cb466",
                "--- /dev/null",
                "+++ b/src/java/org/apache/cassandra/tools/nodetool/ClientStats.java",
                "@@ -0,0 +1,69 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ *     http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+package org.apache.cassandra.tools.nodetool;",
                "+",
                "+import java.util.List;",
                "+import java.util.Map;",
                "+import java.util.Map.Entry;",
                "+",
                "+import org.apache.cassandra.tools.NodeProbe;",
                "+import org.apache.cassandra.tools.NodeTool.NodeToolCmd;",
                "+import org.apache.cassandra.tools.nodetool.formatter.TableBuilder;",
                "+",
                "+import io.airlift.airline.Command;",
                "+import io.airlift.airline.Option;",
                "+",
                "+@Command(name = \"clientstats\", description = \"Print information about connected clients\")",
                "+public class ClientStats extends NodeToolCmd",
                "+{",
                "+    @Option(title = \"list_connections\", name = \"--all\", description = \"Lists all connections\")",
                "+    private boolean listConnections = false;",
                "+",
                "+    @Override",
                "+    public void execute(NodeProbe probe)",
                "+    {",
                "+        if (listConnections)",
                "+        {",
                "+            List<Map<String, String>> clients = (List<Map<String, String>>) probe.getClientMetric(\"connections\");",
                "+            if (!clients.isEmpty())",
                "+            {",
                "+                TableBuilder table = new TableBuilder();",
                "+                table.add(\"Address\", \"SSL\", \"Version\", \"User\", \"Keyspace\", \"Requests\");",
                "+                for (Map<String, String> conn : clients)",
                "+                {",
                "+                    table.add(conn.get(\"address\"), conn.get(\"ssl\"), conn.get(\"version\"), ",
                "+                              conn.get(\"user\"), conn.get(\"keyspace\"), conn.get(\"requests\"));",
                "+                }",
                "+                table.printTo(System.out);",
                "+                System.out.println();",
                "+            }",
                "+        }",
                "+",
                "+        Map<String, Integer> connectionsByUser = (Map<String, Integer>) probe.getClientMetric(\"connectedNativeClientsByUser\");",
                "+        int total = connectionsByUser.values().stream().reduce(0, Integer::sum);",
                "+        System.out.println(\"Total connected clients: \" + total);",
                "+        System.out.println();",
                "+        TableBuilder table = new TableBuilder();",
                "+        table.add(\"User\", \"Connections\");",
                "+        for (Entry<String, Integer> entry : connectionsByUser.entrySet())",
                "+        {",
                "+            table.add(entry.getKey(), entry.getValue().toString());",
                "+        }",
                "+        table.printTo(System.out);",
                "+    }",
                "+}",
                "\\ No newline at end of file",
                "diff --git a/src/java/org/apache/cassandra/transport/Message.java b/src/java/org/apache/cassandra/transport/Message.java",
                "index 2da2ca7aca..f71d6401e6 100644",
                "--- a/src/java/org/apache/cassandra/transport/Message.java",
                "+++ b/src/java/org/apache/cassandra/transport/Message.java",
                "@@ -516,2 +516,3 @@ public abstract class Message",
                "                 logger.trace(\"Received: {}, v={}\", request, connection.getVersion());",
                "+                connection.requests.inc();",
                "                 response = request.execute(qstate, queryStartNanoTime);",
                "diff --git a/src/java/org/apache/cassandra/transport/Server.java b/src/java/org/apache/cassandra/transport/Server.java",
                "index cd04edcd32..2c5e28ad1b 100644",
                "--- a/src/java/org/apache/cassandra/transport/Server.java",
                "+++ b/src/java/org/apache/cassandra/transport/Server.java",
                "@@ -32,2 +32,4 @@ import org.slf4j.LoggerFactory;",
                "+import com.google.common.collect.ImmutableMap;",
                "+",
                " import io.netty.bootstrap.ServerBootstrap;",
                "@@ -175,2 +177,29 @@ public class Server implements CassandraDaemon.Server",
                "+    public Map<String, Integer> getConnectedClientsByUser()",
                "+    {",
                "+        return connectionTracker.getConnectedClientsByUser();",
                "+    }",
                "+",
                "+    public List<Map<String, String>> getConnectionStates()",
                "+    {",
                "+        List<Map<String, String>> result = new ArrayList<>();",
                "+        for(Channel c : connectionTracker.allChannels)",
                "+        {",
                "+            Connection connection = c.attr(Connection.attributeKey).get();",
                "+            if (connection instanceof ServerConnection)",
                "+            {",
                "+                ServerConnection conn = (ServerConnection) connection;",
                "+                result.add(new ImmutableMap.Builder<String, String>()",
                "+                        .put(\"user\", conn.getClientState().getUser().getName())",
                "+                        .put(\"keyspace\", conn.getClientState().getRawKeyspace() == null ? \"\" : conn.getClientState().getRawKeyspace())",
                "+                        .put(\"address\", conn.getClientState().getRemoteAddress().toString())",
                "+                        .put(\"version\", String.valueOf(conn.getVersion().asInt()))",
                "+                        .put(\"requests\", String.valueOf(conn.requests.getCount()))",
                "+                        .put(\"ssl\", conn.channel().pipeline().get(SslHandler.class) == null ? \"false\" : \"true\")",
                "+                        .build());",
                "+            }",
                "+        }",
                "+        return result;",
                "+    }",
                "+",
                "     private void close()",
                "@@ -287,2 +316,18 @@ public class Server implements CassandraDaemon.Server",
                "         }",
                "+",
                "+        public Map<String, Integer> getConnectedClientsByUser()",
                "+        {",
                "+            Map<String, Integer> result = new HashMap<>();",
                "+            for(Channel c : allChannels)",
                "+            {",
                "+                Connection connection = c.attr(Connection.attributeKey).get();",
                "+                if (connection instanceof ServerConnection)",
                "+                {",
                "+                    ServerConnection conn = (ServerConnection) connection;",
                "+                    String name = conn.getClientState().getUser().getName();",
                "+                    result.put(name, result.getOrDefault(name, 0) + 1);",
                "+                }",
                "+            }",
                "+            return result;",
                "+        }",
                "     }",
                "diff --git a/src/java/org/apache/cassandra/transport/ServerConnection.java b/src/java/org/apache/cassandra/transport/ServerConnection.java",
                "index 9374ca0638..1ebf81c609 100644",
                "--- a/src/java/org/apache/cassandra/transport/ServerConnection.java",
                "+++ b/src/java/org/apache/cassandra/transport/ServerConnection.java",
                "@@ -28,2 +28,4 @@ import org.apache.cassandra.service.QueryState;",
                "+import com.codahale.metrics.Counter;",
                "+",
                " public class ServerConnection extends Connection",
                "@@ -35,2 +37,3 @@ public class ServerConnection extends Connection",
                "     private volatile State state;",
                "+    public final Counter requests = new Counter();",
                "@@ -58,2 +61,7 @@ public class ServerConnection extends Connection",
                "+    public ClientState getClientState()",
                "+    {",
                "+        return clientState;",
                "+    }",
                "+",
                "     public QueryState validateNewMessage(Message.Type type, ProtocolVersion version, int streamId)"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/metrics/ClientMetrics.java",
                "src/java/org/apache/cassandra/service/NativeTransportService.java",
                "src/java/org/apache/cassandra/service/StorageProxy.java",
                "src/java/org/apache/cassandra/tools/NodeProbe.java",
                "src/java/org/apache/cassandra/tools/NodeTool.java",
                "src/java/org/apache/cassandra/tools/nodetool/ClientStats.java",
                "src/java/org/apache/cassandra/transport/Message.java",
                "src/java/org/apache/cassandra/transport/Server.java",
                "src/java/org/apache/cassandra/transport/ServerConnection.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-13665": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-13665",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "0f58f6c6501518e57ba021cb959b288fd533f472",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517819272,
            "hunks": 11,
            "message": "Make it possible to change neverPurgeTombstones during runtime Patch by marcuse; reviewed by Chris Lohfink for CASSANDRA-14214",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "index c12b4745b4..8e7b220638 100644",
                "--- a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "+++ b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "@@ -220,2 +220,4 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean",
                "+    private volatile boolean neverPurgeTombstones = false;",
                "+",
                "     public static void shutdownPostFlushExecutor() throws InterruptedException",
                "@@ -2638,2 +2640,19 @@ public class ColumnFamilyStore implements ColumnFamilyStoreMBean",
                "     }",
                "+",
                "+    @Override",
                "+    public void setNeverPurgeTombstones(boolean value)",
                "+    {",
                "+        if (neverPurgeTombstones != value)",
                "+            logger.info(\"Changing neverPurgeTombstones for {}.{} from {} to {}\", keyspace.getName(), getTableName(), neverPurgeTombstones, value);",
                "+        else",
                "+            logger.info(\"Not changing neverPurgeTombstones for {}.{}, it is {}\", keyspace.getName(), getTableName(), neverPurgeTombstones);",
                "+",
                "+        neverPurgeTombstones = value;",
                "+    }",
                "+",
                "+    @Override",
                "+    public boolean getNeverPurgeTombstones()",
                "+    {",
                "+        return neverPurgeTombstones;",
                "+    }",
                " }",
                "diff --git a/src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java b/src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java",
                "index e361ffe2ad..bdb842dfad 100644",
                "--- a/src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java",
                "+++ b/src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java",
                "@@ -197,2 +197,6 @@ public interface ColumnFamilyStoreMBean",
                "     public void compactionDiskSpaceCheck(boolean enable);",
                "+",
                "+    public void setNeverPurgeTombstones(boolean value);",
                "+",
                "+    public boolean getNeverPurgeTombstones();",
                " }",
                "diff --git a/src/java/org/apache/cassandra/db/compaction/AbstractCompactionStrategy.java b/src/java/org/apache/cassandra/db/compaction/AbstractCompactionStrategy.java",
                "index e88524f9b4..a43761f080 100644",
                "--- a/src/java/org/apache/cassandra/db/compaction/AbstractCompactionStrategy.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/AbstractCompactionStrategy.java",
                "@@ -352,3 +352,3 @@ public abstract class AbstractCompactionStrategy",
                "     {",
                "-        if (disableTombstoneCompactions || CompactionController.NEVER_PURGE_TOMBSTONES)",
                "+        if (disableTombstoneCompactions || CompactionController.NEVER_PURGE_TOMBSTONES || cfs.getNeverPurgeTombstones())",
                "             return false;",
                "diff --git a/src/java/org/apache/cassandra/db/compaction/CompactionController.java b/src/java/org/apache/cassandra/db/compaction/CompactionController.java",
                "index 32ce67a247..59bba0a5cc 100644",
                "--- a/src/java/org/apache/cassandra/db/compaction/CompactionController.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/CompactionController.java",
                "@@ -114,2 +114,8 @@ public class CompactionController implements AutoCloseable",
                "+        if (cfs.getNeverPurgeTombstones())",
                "+        {",
                "+            logger.debug(\"not refreshing overlaps for {}.{} - neverPurgeTombstones is enabled\", cfs.keyspace.getName(), cfs.getTableName());",
                "+            return;",
                "+        }",
                "+",
                "         for (SSTableReader reader : overlappingSSTables)",
                "@@ -126,3 +132,3 @@ public class CompactionController implements AutoCloseable",
                "     {",
                "-        if (NEVER_PURGE_TOMBSTONES)",
                "+        if (NEVER_PURGE_TOMBSTONES || cfs.getNeverPurgeTombstones())",
                "             return;",
                "@@ -169,4 +175,4 @@ public class CompactionController implements AutoCloseable",
                "-        if (NEVER_PURGE_TOMBSTONES || compacting == null)",
                "-            return Collections.emptySet();",
                "+        if (NEVER_PURGE_TOMBSTONES || compacting == null || cfStore.getNeverPurgeTombstones())",
                "+            return Collections.<SSTableReader>emptySet();",
                "@@ -261,3 +267,3 @@ public class CompactionController implements AutoCloseable",
                "     {",
                "-        if (NEVER_PURGE_TOMBSTONES || !compactingRepaired())",
                "+        if (NEVER_PURGE_TOMBSTONES || !compactingRepaired() || cfs.getNeverPurgeTombstones())",
                "             return time -> false;",
                "@@ -323,3 +329,3 @@ public class CompactionController implements AutoCloseable",
                "     {",
                "-        if (!provideTombstoneSources() || !compactingRepaired() || NEVER_PURGE_TOMBSTONES)",
                "+        if (!provideTombstoneSources() || !compactingRepaired() || NEVER_PURGE_TOMBSTONES || cfs.getNeverPurgeTombstones())",
                "             return null;",
                "diff --git a/test/unit/org/apache/cassandra/db/compaction/CompactionsCQLTest.java b/test/unit/org/apache/cassandra/db/compaction/CompactionsCQLTest.java",
                "index 7873ac9b70..4d5215e838 100644",
                "--- a/test/unit/org/apache/cassandra/db/compaction/CompactionsCQLTest.java",
                "+++ b/test/unit/org/apache/cassandra/db/compaction/CompactionsCQLTest.java",
                "@@ -27,3 +27,10 @@ import org.apache.cassandra.cql3.CQLTester;",
                " import org.apache.cassandra.cql3.UntypedResultSet;",
                "+import org.apache.cassandra.db.rows.Cell;",
                "+import org.apache.cassandra.db.rows.Row;",
                "+import org.apache.cassandra.db.rows.Unfiltered;",
                "+import org.apache.cassandra.db.rows.UnfilteredRowIterator;",
                "+import org.apache.cassandra.io.sstable.ISSTableScanner;",
                "+import org.apache.cassandra.io.sstable.format.SSTableReader;",
                "+import static org.junit.Assert.assertEquals;",
                " import static org.junit.Assert.assertFalse;",
                "@@ -228,2 +235,78 @@ public class CompactionsCQLTest extends CQLTester",
                "+    @Test",
                "+    public void testPerCFSNeverPurgeTombstonesCell() throws Throwable",
                "+    {",
                "+        testPerCFSNeverPurgeTombstonesHelper(true);",
                "+    }",
                "+",
                "+    @Test",
                "+    public void testPerCFSNeverPurgeTombstones() throws Throwable",
                "+    {",
                "+        testPerCFSNeverPurgeTombstonesHelper(false);",
                "+    }",
                "+",
                "+",
                "+    public void testPerCFSNeverPurgeTombstonesHelper(boolean deletedCell) throws Throwable",
                "+    {",
                "+        createTable(\"CREATE TABLE %s (id int primary key, b text) with gc_grace_seconds = 0\");",
                "+        for (int i = 0; i < 100; i++)",
                "+        {",
                "+            execute(\"INSERT INTO %s (id, b) VALUES (?, ?)\", i, String.valueOf(i));",
                "+        }",
                "+        getCurrentColumnFamilyStore().forceBlockingFlush();",
                "+",
                "+        assertTombstones(getCurrentColumnFamilyStore().getLiveSSTables().iterator().next(), false);",
                "+        if (deletedCell)",
                "+            execute(\"UPDATE %s SET b=null WHERE id = ?\", 50);",
                "+        else",
                "+            execute(\"DELETE FROM %s WHERE id = ?\", 50);",
                "+        getCurrentColumnFamilyStore().setNeverPurgeTombstones(false);",
                "+        getCurrentColumnFamilyStore().forceBlockingFlush();",
                "+        Thread.sleep(2000); // wait for gcgs to pass",
                "+        getCurrentColumnFamilyStore().forceMajorCompaction();",
                "+        assertTombstones(getCurrentColumnFamilyStore().getLiveSSTables().iterator().next(), false);",
                "+        if (deletedCell)",
                "+            execute(\"UPDATE %s SET b=null WHERE id = ?\", 44);",
                "+        else",
                "+            execute(\"DELETE FROM %s WHERE id = ?\", 44);",
                "+        getCurrentColumnFamilyStore().setNeverPurgeTombstones(true);",
                "+        getCurrentColumnFamilyStore().forceBlockingFlush();",
                "+        Thread.sleep(1100);",
                "+        getCurrentColumnFamilyStore().forceMajorCompaction();",
                "+        assertTombstones(getCurrentColumnFamilyStore().getLiveSSTables().iterator().next(), true);",
                "+        // disable it again and make sure the tombstone is gone:",
                "+        getCurrentColumnFamilyStore().setNeverPurgeTombstones(false);",
                "+        getCurrentColumnFamilyStore().forceMajorCompaction();",
                "+        assertTombstones(getCurrentColumnFamilyStore().getLiveSSTables().iterator().next(), false);",
                "+        getCurrentColumnFamilyStore().truncateBlocking();",
                "+    }",
                "+",
                "+    private void assertTombstones(SSTableReader sstable, boolean expectTS)",
                "+    {",
                "+        boolean foundTombstone = false;",
                "+        try(ISSTableScanner scanner = sstable.getScanner())",
                "+        {",
                "+            while (scanner.hasNext())",
                "+            {",
                "+                try (UnfilteredRowIterator iter = scanner.next())",
                "+                {",
                "+                    if (!iter.partitionLevelDeletion().isLive())",
                "+                        foundTombstone = true;",
                "+                    while (iter.hasNext())",
                "+                    {",
                "+                        Unfiltered unfiltered = iter.next();",
                "+                        assertTrue(unfiltered instanceof Row);",
                "+                        for (Cell c : ((Row)unfiltered).cells())",
                "+                        {",
                "+                            if (c.isTombstone())",
                "+                                foundTombstone = true;",
                "+                        }",
                "+",
                "+                    }",
                "+                }",
                "+            }",
                "+        }",
                "+        assertEquals(expectTS, foundTombstone);",
                "+    }",
                "+",
                "     public boolean verifyStrategies(CompactionStrategyManager manager, Class<? extends AbstractCompactionStrategy> expected)"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "src/java/org/apache/cassandra/db/ColumnFamilyStoreMBean.java",
                "src/java/org/apache/cassandra/db/compaction/AbstractCompactionStrategy.java",
                "src/java/org/apache/cassandra/db/compaction/CompactionController.java",
                "test/unit/org/apache/cassandra/db/compaction/CompactionsCQLTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14214": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14214",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "d0b34d383c20f5add8b8d7d454b4460aace0c939",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518022007,
            "hunks": 4,
            "message": "node map does not handle InetAddressAndPort correctly. patch by dbrosius, reviewed by aweisberg for CASSANDRA-14216",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/service/StorageService.java b/src/java/org/apache/cassandra/service/StorageService.java",
                "index cda575aef9..16b6a81839 100644",
                "--- a/src/java/org/apache/cassandra/service/StorageService.java",
                "+++ b/src/java/org/apache/cassandra/service/StorageService.java",
                "@@ -4768,4 +4768,4 @@ public class StorageService extends NotificationBroadcasterSupport implements IE",
                "             Float tokenOwnership = entry.getValue();",
                "-            if (nodeMap.containsKey(endpoint))",
                "-                nodeMap.put(endpoint.address, nodeMap.get(endpoint) + tokenOwnership);",
                "+            if (nodeMap.containsKey(endpoint.address))",
                "+                nodeMap.put(endpoint.address, nodeMap.get(endpoint.address) + tokenOwnership);",
                "             else",
                "@@ -4786,4 +4786,4 @@ public class StorageService extends NotificationBroadcasterSupport implements IE",
                "             Float tokenOwnership = entry.getValue();",
                "-            if (nodeMap.containsKey(endpoint))",
                "-                nodeMap.put(endpoint.toString(), nodeMap.get(endpoint) + tokenOwnership);",
                "+            if (nodeMap.containsKey(endpoint.toString()))",
                "+                nodeMap.put(endpoint.toString(), nodeMap.get(endpoint.toString()) + tokenOwnership);",
                "             else",
                "diff --git a/src/java/org/apache/cassandra/tools/nodetool/SetHostStatWithPort.java b/src/java/org/apache/cassandra/tools/nodetool/SetHostStatWithPort.java",
                "index 67cd46498e..6ac02583b6 100644",
                "--- a/src/java/org/apache/cassandra/tools/nodetool/SetHostStatWithPort.java",
                "+++ b/src/java/org/apache/cassandra/tools/nodetool/SetHostStatWithPort.java",
                "@@ -52,3 +52,3 @@ public class SetHostStatWithPort implements Iterable<HostStatWithPort>",
                "         InetAddressAndPort endpoint = InetAddressAndPort.getByName(host);",
                "-        Float owns = ownerships.get(endpoint);",
                "+        Float owns = ownerships.get(endpoint.toString());",
                "         hostStats.add(new HostStatWithPort(token, endpoint, resolveIp, owns));",
                "diff --git a/src/java/org/apache/cassandra/tools/nodetool/Status.java b/src/java/org/apache/cassandra/tools/nodetool/Status.java",
                "index 49724a5dfc..10c7fdff05 100644",
                "--- a/src/java/org/apache/cassandra/tools/nodetool/Status.java",
                "+++ b/src/java/org/apache/cassandra/tools/nodetool/Status.java",
                "@@ -118,3 +118,3 @@ public class Status extends NodeToolCmd",
                "                 {",
                "-                    Float owns = ownerships.get(endpoint);",
                "+                    Float owns = ownerships.get(endpoint.toString());",
                "                     List<HostStatWithPort> tokens = hostToTokens.get(endpoint);"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/service/StorageService.java",
                "src/java/org/apache/cassandra/tools/nodetool/SetHostStatWithPort.java",
                "src/java/org/apache/cassandra/tools/nodetool/Status.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14216": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14216",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "bfecdf52054a4da472af22b0c35c5db5f1132bbc",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516818534,
            "hunks": 21,
            "message": "Add nodetool getseeds and reloadseeds commands patch by Samuel Fink; reviewed by jasobrown for CASSANDRA-14190",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java b/src/java/org/apache/cassandra/config/DatabaseDescriptor.java",
                "index 9012e3ac88..8e831cf2b7 100644",
                "--- a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java",
                "+++ b/src/java/org/apache/cassandra/config/DatabaseDescriptor.java",
                "@@ -1663,2 +1663,12 @@ public class DatabaseDescriptor",
                "+    public static SeedProvider getSeedProvider()",
                "+    {",
                "+        return seedProvider;",
                "+    }",
                "+",
                "+    public static void setSeedProvider(SeedProvider newSeedProvider)",
                "+    {",
                "+        seedProvider = newSeedProvider;",
                "+    }",
                "+",
                "     public static InetAddress getListenAddress()",
                "diff --git a/src/java/org/apache/cassandra/gms/Gossiper.java b/src/java/org/apache/cassandra/gms/Gossiper.java",
                "index eb6c500701..a4e46f20af 100644",
                "--- a/src/java/org/apache/cassandra/gms/Gossiper.java",
                "+++ b/src/java/org/apache/cassandra/gms/Gossiper.java",
                "@@ -37,2 +37,3 @@ import com.google.common.util.concurrent.Uninterruptibles;",
                " import org.apache.cassandra.locator.InetAddressAndPort;",
                "+import org.apache.cassandra.locator.SeedProvider;",
                " import org.apache.cassandra.utils.CassandraVersion;",
                "@@ -91,3 +92,3 @@ public class Gossiper implements IFailureDetectionEventListener, GossiperMBean",
                "     private static final Logger logger = LoggerFactory.getLogger(Gossiper.class);",
                "-    public static final Gossiper instance = new Gossiper();",
                "+    public static final Gossiper instance = new Gossiper(true);",
                "@@ -201,3 +202,3 @@ public class Gossiper implements IFailureDetectionEventListener, GossiperMBean",
                "-    private Gossiper()",
                "+    Gossiper(boolean registerJmx)",
                "     {",
                "@@ -209,10 +210,13 @@ public class Gossiper implements IFailureDetectionEventListener, GossiperMBean",
                "         // Register this instance with JMX",
                "-        try",
                "-        {",
                "-            MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();",
                "-            mbs.registerMBean(this, new ObjectName(MBEAN_NAME));",
                "-        }",
                "-        catch (Exception e)",
                "+        if (registerJmx)",
                "         {",
                "-            throw new RuntimeException(e);",
                "+            try",
                "+            {",
                "+                MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();",
                "+                mbs.registerMBean(this, new ObjectName(MBEAN_NAME));",
                "+            }",
                "+            catch (Exception e)",
                "+            {",
                "+                throw new RuntimeException(e);",
                "+            }",
                "         }",
                "@@ -1470,2 +1474,63 @@ public class Gossiper implements IFailureDetectionEventListener, GossiperMBean",
                "+    /**",
                "+     * JMX interface for triggering an update of the seed node list.",
                "+     */",
                "+    public List<String> reloadSeeds()",
                "+    {",
                "+        logger.trace(\"Triggering reload of seed node list\");",
                "+",
                "+        // Get the new set in the same that buildSeedsList does",
                "+        Set<InetAddressAndPort> tmp = new HashSet<>();",
                "+        try",
                "+        {",
                "+            for (InetAddressAndPort seed : DatabaseDescriptor.getSeeds())",
                "+            {",
                "+                if (seed.equals(FBUtilities.getBroadcastAddressAndPort()))",
                "+                    continue;",
                "+                tmp.add(seed);",
                "+            }",
                "+        }",
                "+        // If using the SimpleSeedProvider invalid yaml added to the config since startup could",
                "+        // cause this to throw. Additionally, third party seed providers may throw exceptions.",
                "+        // Handle the error and return a null to indicate that there was a problem.",
                "+        catch (Throwable e)",
                "+        {",
                "+            JVMStabilityInspector.inspectThrowable(e);",
                "+            logger.warn(\"Error while getting seed node list: {}\", e.getLocalizedMessage());",
                "+            return null;",
                "+        }",
                "+",
                "+        if (tmp.size() == 0)",
                "+        {",
                "+            logger.trace(\"New seed node list is empty. Not updating seed list.\");",
                "+            return getSeeds();",
                "+        }",
                "+",
                "+        if (tmp.equals(seeds))",
                "+        {",
                "+            logger.trace(\"New seed node list matches the existing list.\");",
                "+            return getSeeds();",
                "+        }",
                "+",
                "+        // Add the new entries",
                "+        seeds.addAll(tmp);",
                "+        // Remove the old entries",
                "+        seeds.retainAll(tmp);",
                "+        logger.trace(\"New seed node list after reload {}\", seeds);",
                "+        return getSeeds();",
                "+    }",
                "+",
                "+    /**",
                "+     * JMX endpoint for getting the list of seeds from the node",
                "+     */",
                "+    public List<String> getSeeds()",
                "+    {",
                "+        List<String> seedList = new ArrayList<String>();",
                "+        for (InetAddressAndPort seed : seeds)",
                "+        {",
                "+            seedList.add(seed.toString());",
                "+        }",
                "+        return seedList;",
                "+    }",
                "+",
                "     // initialize local HB state if needed, i.e., if gossiper has never been started before.",
                "diff --git a/src/java/org/apache/cassandra/gms/GossiperMBean.java b/src/java/org/apache/cassandra/gms/GossiperMBean.java",
                "index c4b244c41d..1b116e1a4b 100644",
                "--- a/src/java/org/apache/cassandra/gms/GossiperMBean.java",
                "+++ b/src/java/org/apache/cassandra/gms/GossiperMBean.java",
                "@@ -20,2 +20,3 @@ package org.apache.cassandra.gms;",
                " import java.net.UnknownHostException;",
                "+import java.util.List;",
                "@@ -31,2 +32,6 @@ public interface GossiperMBean",
                "+    public List<String> reloadSeeds();",
                "+",
                "+    public List<String> getSeeds();",
                "+",
                " }",
                "\\ No newline at end of file",
                "diff --git a/src/java/org/apache/cassandra/tools/NodeProbe.java b/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "index d330ed40c6..ec8f7ba7bf 100644",
                "--- a/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "+++ b/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "@@ -713,2 +713,12 @@ public class NodeProbe implements AutoCloseable",
                "+    public List<String> reloadSeeds()",
                "+    {",
                "+        return gossProxy.reloadSeeds();",
                "+    }",
                "+",
                "+    public List<String> getSeeds()",
                "+    {",
                "+        return gossProxy.getSeeds();",
                "+    }",
                "+",
                "     /**",
                "diff --git a/src/java/org/apache/cassandra/tools/NodeTool.java b/src/java/org/apache/cassandra/tools/NodeTool.java",
                "index d707499f6b..f7b7f76304 100644",
                "--- a/src/java/org/apache/cassandra/tools/NodeTool.java",
                "+++ b/src/java/org/apache/cassandra/tools/NodeTool.java",
                "@@ -89,2 +89,3 @@ public class NodeTool",
                "                 GetEndpoints.class,",
                "+                GetSeeds.class,",
                "                 GetSSTables.class,",
                "@@ -104,2 +105,3 @@ public class NodeTool",
                "                 Assassinate.class,",
                "+                ReloadSeeds.class,",
                "                 ResetFullQueryLog.class,",
                "diff --git a/src/java/org/apache/cassandra/tools/nodetool/GetSeeds.java b/src/java/org/apache/cassandra/tools/nodetool/GetSeeds.java",
                "new file mode 100644",
                "index 0000000000..207363cdff",
                "--- /dev/null",
                "+++ b/src/java/org/apache/cassandra/tools/nodetool/GetSeeds.java",
                "@@ -0,0 +1,44 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ *     http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+package org.apache.cassandra.tools.nodetool;",
                "+",
                "+import java.util.List;",
                "+",
                "+import io.airlift.airline.Command;",
                "+",
                "+import org.apache.cassandra.tools.NodeProbe;",
                "+import org.apache.cassandra.tools.NodeTool.NodeToolCmd;",
                "+",
                "+@Command(name = \"getseeds\", description = \"Get the currently in use seed node IP list excluding the node IP\")",
                "+public class GetSeeds extends NodeToolCmd",
                "+{",
                "+    @Override",
                "+    public void execute(NodeProbe probe)",
                "+    {",
                "+        List<String> seedList = probe.getSeeds();",
                "+        if (seedList.isEmpty())",
                "+        {",
                "+            System.out.println(\"Seed node list does not contain any remote node IPs\");",
                "+        }",
                "+        else",
                "+        {",
                "+            System.out.println(\"Current list of seed node IPs, excluding the current node's IP: \" + String.join(\" \", seedList));",
                "+        }",
                "+",
                "+    }",
                "+}",
                "\\ No newline at end of file",
                "diff --git a/src/java/org/apache/cassandra/tools/nodetool/ReloadSeeds.java b/src/java/org/apache/cassandra/tools/nodetool/ReloadSeeds.java",
                "new file mode 100644",
                "index 0000000000..b9682cfb75",
                "--- /dev/null",
                "+++ b/src/java/org/apache/cassandra/tools/nodetool/ReloadSeeds.java",
                "@@ -0,0 +1,47 @@",
                "+/*",
                "+ * Licensed to the Apache Software Foundation (ASF) under one",
                "+ * or more contributor license agreements.  See the NOTICE file",
                "+ * distributed with this work for additional information",
                "+ * regarding copyright ownership.  The ASF licenses this file",
                "+ * to you under the Apache License, Version 2.0 (the",
                "+ * \"License\"); you may not use this file except in compliance",
                "+ * with the License.  You may obtain a copy of the License at",
                "+ *",
                "+ *     http://www.apache.org/licenses/LICENSE-2.0",
                "+ *",
                "+ * Unless required by applicable law or agreed to in writing, software",
                "+ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+ * See the License for the specific language governing permissions and",
                "+ * limitations under the License.",
                "+ */",
                "+package org.apache.cassandra.tools.nodetool;",
                "+",
                "+import java.util.List;",
                "+",
                "+import io.airlift.airline.Command;",
                "+",
                "+import org.apache.cassandra.tools.NodeProbe;",
                "+import org.apache.cassandra.tools.NodeTool.NodeToolCmd;",
                "+",
                "+@Command(name = \"reloadseeds\", description = \"Reload the seed node list from the seed node provider\")",
                "+public class ReloadSeeds extends NodeToolCmd",
                "+{",
                "+    @Override",
                "+    public void execute(NodeProbe probe)",
                "+    {",
                "+        List<String> seedList = probe.reloadSeeds();",
                "+        if (seedList == null)",
                "+        {",
                "+            System.out.println(\"Failed to reload the seed node list.\");",
                "+        }",
                "+        else if (seedList.isEmpty())",
                "+        {",
                "+            System.out.println(\"Seed node list does not contain any remote node IPs\");",
                "+        }",
                "+        else",
                "+        {",
                "+            System.out.println(\"Updated seed node IP list, excluding the current node's IP: \" + String.join(\" \", seedList));",
                "+        }",
                "+    }",
                "+}",
                "diff --git a/test/unit/org/apache/cassandra/gms/GossiperTest.java b/test/unit/org/apache/cassandra/gms/GossiperTest.java",
                "index 8c65cb4d7f..b856983925 100644",
                "--- a/test/unit/org/apache/cassandra/gms/GossiperTest.java",
                "+++ b/test/unit/org/apache/cassandra/gms/GossiperTest.java",
                "@@ -20,2 +20,3 @@ package org.apache.cassandra.gms;",
                "+import java.net.InetAddress;",
                " import java.net.UnknownHostException;",
                "@@ -26,2 +27,5 @@ import java.util.UUID;",
                " import com.google.common.collect.ImmutableMap;",
                "+import com.google.common.net.InetAddresses;",
                "+import org.junit.After;",
                "+import org.junit.Assert;",
                " import org.junit.Before;",
                "@@ -35,2 +39,3 @@ import org.apache.cassandra.dht.Token;",
                " import org.apache.cassandra.locator.InetAddressAndPort;",
                "+import org.apache.cassandra.locator.SeedProvider;",
                " import org.apache.cassandra.locator.TokenMetadata;",
                "@@ -46,2 +51,3 @@ public class GossiperTest",
                "     }",
                "+",
                "     static final IPartitioner partitioner = new RandomPartitioner();",
                "@@ -54,2 +60,4 @@ public class GossiperTest",
                "+    private SeedProvider originalSeedProvider;",
                "+",
                "     @Before",
                "@@ -58,3 +66,10 @@ public class GossiperTest",
                "         tmd.clearUnsafe();",
                "-    };",
                "+        originalSeedProvider = DatabaseDescriptor.getSeedProvider();",
                "+    }",
                "+",
                "+    @After",
                "+    public void tearDown()",
                "+    {",
                "+        DatabaseDescriptor.setSeedProvider(originalSeedProvider);",
                "+    }",
                "@@ -92,2 +107,125 @@ public class GossiperTest",
                "     }",
                "+",
                "+    // Note: This test might fail if for some reason the node broadcast address is in 127.99.0.0/16",
                "+    @Test",
                "+    public void testReloadSeeds() throws UnknownHostException",
                "+    {",
                "+        Gossiper gossiper = new Gossiper(false);",
                "+        List<String> loadedList;",
                "+",
                "+        // Initialize the seed list directly to a known set to start with",
                "+        gossiper.seeds.clear();",
                "+        InetAddressAndPort addr = InetAddressAndPort.getByAddress(InetAddress.getByName(\"127.99.1.1\"));",
                "+        int nextSize = 4;",
                "+        List<InetAddressAndPort> nextSeeds = new ArrayList<>(nextSize);",
                "+        for (int i = 0; i < nextSize; i ++)",
                "+        {",
                "+            gossiper.seeds.add(addr);",
                "+            nextSeeds.add(addr);",
                "+            addr = InetAddressAndPort.getByAddress(InetAddresses.increment(addr.address));",
                "+        }",
                "+        Assert.assertEquals(nextSize, gossiper.seeds.size());",
                "+",
                "+        // Add another unique address to the list",
                "+        addr = InetAddressAndPort.getByAddress(InetAddresses.increment(addr.address));",
                "+        nextSeeds.add(addr);",
                "+        nextSize++;",
                "+        DatabaseDescriptor.setSeedProvider(new TestSeedProvider(nextSeeds));",
                "+        loadedList = gossiper.reloadSeeds();",
                "+",
                "+        // Check that the new entry was added",
                "+        Assert.assertEquals(nextSize, loadedList.size());",
                "+        for (InetAddressAndPort a : nextSeeds)",
                "+            Assert.assertTrue(loadedList.contains(a.toString()));",
                "+",
                "+        // Check that the return value of the reloadSeeds matches the content of the getSeeds call",
                "+        // and that they both match the internal contents of the Gossiper seeds list",
                "+        Assert.assertEquals(loadedList.size(), gossiper.getSeeds().size());",
                "+        for (InetAddressAndPort a : gossiper.seeds)",
                "+        {",
                "+            Assert.assertTrue(loadedList.contains(a.toString()));",
                "+            Assert.assertTrue(gossiper.getSeeds().contains(a.toString()));",
                "+        }",
                "+",
                "+        // Add a duplicate of the last address to the seed provider list",
                "+        int uniqueSize = nextSize;",
                "+        nextSeeds.add(addr);",
                "+        nextSize++;",
                "+        DatabaseDescriptor.setSeedProvider(new TestSeedProvider(nextSeeds));",
                "+        loadedList = gossiper.reloadSeeds();",
                "+",
                "+        // Check that the number of seed nodes reported hasn't increased",
                "+        Assert.assertEquals(uniqueSize, loadedList.size());",
                "+        for (InetAddressAndPort a : nextSeeds)",
                "+            Assert.assertTrue(loadedList.contains(a.toString()));",
                "+",
                "+        // Create a new list that has no overlaps with the previous list",
                "+        addr = InetAddressAndPort.getByAddress(InetAddress.getByName(\"127.99.2.1\"));",
                "+        int disjointSize = 3;",
                "+        List<InetAddressAndPort> disjointSeeds = new ArrayList<>(disjointSize);",
                "+        for (int i = 0; i < disjointSize; i ++)",
                "+        {",
                "+            disjointSeeds.add(addr);",
                "+            addr = InetAddressAndPort.getByAddress(InetAddresses.increment(addr.address));",
                "+        }",
                "+        DatabaseDescriptor.setSeedProvider(new TestSeedProvider(disjointSeeds));",
                "+        loadedList = gossiper.reloadSeeds();",
                "+",
                "+        // Check that the list now contains exactly the new other list.",
                "+        Assert.assertEquals(disjointSize, gossiper.getSeeds().size());",
                "+        Assert.assertEquals(disjointSize, loadedList.size());",
                "+        for (InetAddressAndPort a : disjointSeeds)",
                "+        {",
                "+            Assert.assertTrue(gossiper.getSeeds().contains(a.toString()));",
                "+            Assert.assertTrue(loadedList.contains(a.toString()));",
                "+        }",
                "+",
                "+        // Set the seed node provider to return an empty list",
                "+        DatabaseDescriptor.setSeedProvider(new TestSeedProvider(new ArrayList<InetAddressAndPort>()));",
                "+        loadedList = gossiper.reloadSeeds();",
                "+",
                "+        // Check that the in memory seed node list was not modified",
                "+        Assert.assertEquals(disjointSize, loadedList.size());",
                "+        for (InetAddressAndPort a : disjointSeeds)",
                "+            Assert.assertTrue(loadedList.contains(a.toString()));",
                "+",
                "+        // Change the seed provider to one that throws an unchecked exception",
                "+        DatabaseDescriptor.setSeedProvider(new ErrorSeedProvider());",
                "+        loadedList = gossiper.reloadSeeds();",
                "+",
                "+        // Check for the expected null response from a reload error",
                "+        Assert.assertNull(loadedList);",
                "+",
                "+        // Check that the in memory seed node list was not modified and the exception was caught",
                "+        Assert.assertEquals(disjointSize, gossiper.getSeeds().size());",
                "+        for (InetAddressAndPort a : disjointSeeds)",
                "+            Assert.assertTrue(gossiper.getSeeds().contains(a.toString()));",
                "+    }",
                "+",
                "+    static class TestSeedProvider implements SeedProvider",
                "+    {",
                "+        private List<InetAddressAndPort> seeds;",
                "+",
                "+        TestSeedProvider(List<InetAddressAndPort> seeds)",
                "+        {",
                "+            this.seeds = seeds;",
                "+        }",
                "+",
                "+        @Override",
                "+        public List<InetAddressAndPort> getSeeds()",
                "+        {",
                "+            return seeds;",
                "+        }",
                "+    }",
                "+",
                "+    // A seed provider for testing which throws assertion errors when queried",
                "+    static class ErrorSeedProvider implements SeedProvider",
                "+    {",
                "+        @Override",
                "+        public List<InetAddressAndPort> getSeeds()",
                "+        {",
                "+            assert(false);",
                "+            return new ArrayList<>();",
                "+        }",
                "+    }",
                " }"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/config/DatabaseDescriptor.java",
                "src/java/org/apache/cassandra/gms/Gossiper.java",
                "src/java/org/apache/cassandra/gms/GossiperMBean.java",
                "src/java/org/apache/cassandra/tools/NodeProbe.java",
                "src/java/org/apache/cassandra/tools/NodeTool.java",
                "src/java/org/apache/cassandra/tools/nodetool/GetSeeds.java",
                "src/java/org/apache/cassandra/tools/nodetool/ReloadSeeds.java",
                "test/unit/org/apache/cassandra/gms/GossiperTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14190": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14190",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "f2fc2e96738505118d9ad161ec77d66a2369fe44",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517953126,
            "hunks": 3,
            "message": "Remove GossipDigestSynVerbHandler#doSort() patch by jasobrown; reviewed by Joel Knighton for CASSANDRA-14174",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/gms/GossipDigestSynVerbHandler.java b/src/java/org/apache/cassandra/gms/GossipDigestSynVerbHandler.java",
                "index 9619f4e107..b06c24dcdf 100644",
                "--- a/src/java/org/apache/cassandra/gms/GossipDigestSynVerbHandler.java",
                "+++ b/src/java/org/apache/cassandra/gms/GossipDigestSynVerbHandler.java",
                "@@ -21,4 +21,2 @@ import java.util.*;",
                "-import com.google.common.collect.Maps;",
                "-",
                " import org.slf4j.Logger;",
                "@@ -101,4 +99,2 @@ public class GossipDigestSynVerbHandler implements IVerbHandler<GossipDigestSyn>",
                "-        doSort(gDigestList);",
                "-",
                "         List<GossipDigest> deltaGossipDigestList = new ArrayList<GossipDigest>();",
                "@@ -114,45 +110,2 @@ public class GossipDigestSynVerbHandler implements IVerbHandler<GossipDigestSyn>",
                "     }",
                "-",
                "-    /*",
                "-     * First construct a map whose key is the endpoint in the GossipDigest and the value is the",
                "-     * GossipDigest itself. Then build a list of version differences i.e difference between the",
                "-     * version in the GossipDigest and the version in the local state for a given InetAddressAndPort.",
                "-     * Sort this list. Now loop through the sorted list and retrieve the GossipDigest corresponding",
                "-     * to the endpoint from the map that was initially constructed.",
                "-    */",
                "-    private void doSort(List<GossipDigest> gDigestList)",
                "-    {",
                "-        /* Construct a map of endpoint to GossipDigest. */",
                "-        Map<InetAddressAndPort, GossipDigest> epToDigestMap = Maps.newHashMapWithExpectedSize(gDigestList.size());",
                "-        for (GossipDigest gDigest : gDigestList)",
                "-        {",
                "-            epToDigestMap.put(gDigest.getEndpoint(), gDigest);",
                "-        }",
                "-",
                "-        /*",
                "-         * These digests have their maxVersion set to the difference of the version",
                "-         * of the local EndpointState and the version found in the GossipDigest.",
                "-        */",
                "-        List<GossipDigest> diffDigests = new ArrayList<GossipDigest>(gDigestList.size());",
                "-        for (GossipDigest gDigest : gDigestList)",
                "-        {",
                "-            InetAddressAndPort ep = gDigest.getEndpoint();",
                "-            EndpointState epState = Gossiper.instance.getEndpointStateForEndpoint(ep);",
                "-            int version = (epState != null) ? Gossiper.instance.getMaxEndpointStateVersion(epState) : 0;",
                "-            int diffVersion = Math.abs(version - gDigest.getMaxVersion());",
                "-            diffDigests.add(new GossipDigest(ep, gDigest.getGeneration(), diffVersion));",
                "-        }",
                "-",
                "-        gDigestList.clear();",
                "-        Collections.sort(diffDigests);",
                "-        int size = diffDigests.size();",
                "-        /*",
                "-         * Report the digests in descending order. This takes care of the endpoints",
                "-         * that are far behind w.r.t this local endpoint",
                "-        */",
                "-        for (int i = size - 1; i >= 0; --i)",
                "-        {",
                "-            gDigestList.add(epToDigestMap.get(diffDigests.get(i).getEndpoint()));",
                "-        }",
                "-    }",
                " }"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/gms/GossipDigestSynVerbHandler.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14174": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14174",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "e7b9c1f50a9875682b480a3ab69e662f4b097d4d",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1514417136,
            "hunks": 27,
            "message": "Add Unittest for schema migration fix patch by Jay Zhuang; reviewed by jasobrown for CASSANDRA-14140",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/gms/VersionedValue.java b/src/java/org/apache/cassandra/gms/VersionedValue.java",
                "index d9c8d0bc50..0ec1712aae 100644",
                "--- a/src/java/org/apache/cassandra/gms/VersionedValue.java",
                "+++ b/src/java/org/apache/cassandra/gms/VersionedValue.java",
                "@@ -26,2 +26,3 @@ import static java.nio.charset.StandardCharsets.ISO_8859_1;",
                "+import com.google.common.annotations.VisibleForTesting;",
                " import com.google.common.collect.Iterables;",
                "@@ -252,3 +253,9 @@ public class VersionedValue implements Comparable<VersionedValue>",
                "         {",
                "-            return new VersionedValue(FBUtilities.getReleaseVersionString());",
                "+            return releaseVersion(FBUtilities.getReleaseVersionString());",
                "+        }",
                "+",
                "+        @VisibleForTesting",
                "+        public VersionedValue releaseVersion(String version)",
                "+        {",
                "+            return new VersionedValue(version);",
                "         }",
                "diff --git a/src/java/org/apache/cassandra/schema/SchemaKeyspace.java b/src/java/org/apache/cassandra/schema/SchemaKeyspace.java",
                "index b6add96b40..164e32d0f6 100644",
                "--- a/src/java/org/apache/cassandra/schema/SchemaKeyspace.java",
                "+++ b/src/java/org/apache/cassandra/schema/SchemaKeyspace.java",
                "@@ -27,2 +27,3 @@ import java.util.stream.Collectors;",
                "+import com.google.common.annotations.VisibleForTesting;",
                " import com.google.common.collect.*;",
                "@@ -111,3 +112,3 @@ public final class SchemaKeyspace",
                "     public static final ImmutableList<String> ALL_FOR_DIGEST =",
                "-        ImmutableList.of(KEYSPACES, TABLES, COLUMNS, DROPPED_COLUMNS, TRIGGERS, VIEWS, TYPES, FUNCTIONS, AGGREGATES, INDEXES);",
                "+        ImmutableList.of(KEYSPACES, TABLES, COLUMNS, TRIGGERS, VIEWS, TYPES, FUNCTIONS, AGGREGATES, INDEXES);",
                "@@ -318,2 +319,10 @@ public final class SchemaKeyspace",
                "     public static Pair<UUID, UUID> calculateSchemaDigest()",
                "+    {",
                "+        Set<ByteBuffer> cdc = Collections.singleton(ByteBufferUtil.bytes(\"cdc\"));",
                "+",
                "+        return calculateSchemaDigest(cdc);",
                "+    }",
                "+",
                "+    @VisibleForTesting",
                "+    static Pair<UUID, UUID> calculateSchemaDigest(Set<ByteBuffer> columnsToExclude)",
                "     {",
                "@@ -330,3 +339,2 @@ public final class SchemaKeyspace",
                "         }",
                "-        Set<ByteBuffer> cdc = Collections.singleton(ByteBufferUtil.bytes(\"cdc\"));",
                "@@ -334,7 +342,2 @@ public final class SchemaKeyspace",
                "         {",
                "-            // Due to CASSANDRA-11050 we want to exclude DROPPED_COLUMNS for schema digest computation. We can and",
                "-            // should remove that in the next major release (so C* 4.0).",
                "-            if (table.equals(DROPPED_COLUMNS))",
                "-                continue;",
                "-",
                "             ReadCommand cmd = getReadCommandForTableSchema(table);",
                "@@ -349,3 +352,3 @@ public final class SchemaKeyspace",
                "                         {",
                "-                            RowIterators.digest(partition, digest, digest30, cdc);",
                "+                            RowIterators.digest(partition, digest, digest30, columnsToExclude);",
                "                         }",
                "diff --git a/src/java/org/apache/cassandra/service/StorageService.java b/src/java/org/apache/cassandra/service/StorageService.java",
                "index c5e2912511..15027b2015 100644",
                "--- a/src/java/org/apache/cassandra/service/StorageService.java",
                "+++ b/src/java/org/apache/cassandra/service/StorageService.java",
                "@@ -833,27 +833,2 @@ public class StorageService extends NotificationBroadcasterSupport implements IE",
                "-    public void waitForSchema(int delay)",
                "-    {",
                "-        logger.debug(\"Waiting for schema (max {} seconds)\", delay);",
                "-        // first sleep the delay to make sure we see all our peers",
                "-        for (int i = 0; i < delay; i += 1000)",
                "-        {",
                "-            // if we see schema, we can proceed to the next check directly",
                "-            if (!Schema.instance.isEmpty())",
                "-            {",
                "-                logger.debug(\"current schema version: {} (3.0 compatible: {})\", Schema.instance.getRealVersion(), Schema.instance.getAltVersion());",
                "-                break;",
                "-            }",
                "-            Uninterruptibles.sleepUninterruptibly(1, TimeUnit.SECONDS);",
                "-        }",
                "-        // if our schema hasn't matched yet, wait until it has",
                "-        // we do this by waiting for all in-flight migration requests and responses to complete",
                "-        // (post CASSANDRA-1391 we don't expect this to be necessary very often, but it doesn't hurt to be careful)",
                "-        if (!MigrationManager.isReadyForBootstrap())",
                "-        {",
                "-            setMode(Mode.JOINING, \"waiting for schema information to complete\", true);",
                "-            MigrationManager.waitUntilReadyForBootstrap();",
                "-        }",
                "-        logger.info(\"Has schema with version {}\", Schema.instance.getVersion());",
                "-    }",
                "-",
                "     private void joinTokenRing(int delay) throws ConfigurationException",
                "diff --git a/test/unit/org/apache/cassandra/gms/GossiperTest.java b/test/unit/org/apache/cassandra/gms/GossiperTest.java",
                "index 83f12d13a0..def0530cb3 100644",
                "--- a/test/unit/org/apache/cassandra/gms/GossiperTest.java",
                "+++ b/test/unit/org/apache/cassandra/gms/GossiperTest.java",
                "@@ -28,6 +28,9 @@ import com.google.common.collect.ImmutableMap;",
                " import org.junit.Before;",
                "+import org.junit.BeforeClass;",
                " import org.junit.Test;",
                "+import org.apache.cassandra.SchemaLoader;",
                " import org.apache.cassandra.Util;",
                " import org.apache.cassandra.config.DatabaseDescriptor;",
                "+import org.apache.cassandra.config.Schema;",
                " import org.apache.cassandra.dht.IPartitioner;",
                "@@ -36,2 +39,4 @@ import org.apache.cassandra.dht.Token;",
                " import org.apache.cassandra.locator.TokenMetadata;",
                "+import org.apache.cassandra.net.MessagingService;",
                "+import org.apache.cassandra.schema.KeyspaceParams;",
                " import org.apache.cassandra.service.StorageService;",
                "@@ -39,2 +44,4 @@ import org.apache.cassandra.service.StorageService;",
                " import static org.junit.Assert.assertEquals;",
                "+import static org.junit.Assert.assertFalse;",
                "+import static org.junit.Assert.assertTrue;",
                "@@ -42,6 +49,12 @@ public class GossiperTest",
                " {",
                "-    static",
                "+    @BeforeClass",
                "+    public static void before()",
                "     {",
                "         DatabaseDescriptor.daemonInitialization();",
                "+        SchemaLoader.prepareServer();",
                "+        SchemaLoader.createKeyspace(\"schema_test_ks\",",
                "+                                    KeyspaceParams.simple(1),",
                "+                                    SchemaLoader.standardCFMD(\"schema_test_ks\", \"schema_test_cf\"));",
                "     }",
                "+",
                "     static final IPartitioner partitioner = new RandomPartitioner();",
                "@@ -58,6 +71,6 @@ public class GossiperTest",
                "         tmd.clearUnsafe();",
                "-    };",
                "+    }",
                "     @Test",
                "-    public void testLargeGenerationJump() throws UnknownHostException, InterruptedException",
                "+    public void testLargeGenerationJump() throws UnknownHostException",
                "     {",
                "@@ -92,2 +105,52 @@ public class GossiperTest",
                "     }",
                "+",
                "+    @Test",
                "+    public void testSchemaVersionUpdate() throws UnknownHostException, InterruptedException",
                "+    {",
                "+        Util.createInitialRing(ss, partitioner, endpointTokens, keyTokens, hosts, hostIds, 2);",
                "+        MessagingService.instance().listen();",
                "+        Gossiper.instance.start(1);",
                "+        InetAddress remoteHostAddress = hosts.get(1);",
                "+",
                "+        EndpointState initialRemoteState = Gossiper.instance.getEndpointStateForEndpoint(remoteHostAddress);",
                "+        // Set to any 3.0 version",
                "+        Gossiper.instance.injectApplicationState(remoteHostAddress, ApplicationState.RELEASE_VERSION, StorageService.instance.valueFactory.releaseVersion(\"3.0.14\"));",
                "+",
                "+        Gossiper.instance.applyStateLocally(ImmutableMap.of(remoteHostAddress, initialRemoteState));",
                "+",
                "+        // wait until the schema is set",
                "+        VersionedValue schema = null;",
                "+        for (int i = 0; i < 10; i++)",
                "+        {",
                "+            EndpointState localState = Gossiper.instance.getEndpointStateForEndpoint(hosts.get(0));",
                "+            schema = localState.getApplicationState(ApplicationState.SCHEMA);",
                "+            if (schema != null)",
                "+                break;",
                "+            Thread.sleep(1000);",
                "+        }",
                "+",
                "+        // schema is set and equals to \"alternative\" version",
                "+        assertTrue(schema != null);",
                "+        assertEquals(schema.value, Schema.instance.getAltVersion().toString());",
                "+",
                "+        // Upgrade remote host version to the latest one (3.11)",
                "+        Gossiper.instance.injectApplicationState(remoteHostAddress, ApplicationState.RELEASE_VERSION, StorageService.instance.valueFactory.releaseVersion());",
                "+",
                "+        Gossiper.instance.applyStateLocally(ImmutableMap.of(remoteHostAddress, initialRemoteState));",
                "+",
                "+        // wait until the schema change",
                "+        VersionedValue newSchema = null;",
                "+        for (int i = 0; i < 10; i++)",
                "+        {",
                "+            EndpointState localState = Gossiper.instance.getEndpointStateForEndpoint(hosts.get(0));",
                "+            newSchema = localState.getApplicationState(ApplicationState.SCHEMA);",
                "+            if (!schema.value.equals(newSchema.value))",
                "+                break;",
                "+            Thread.sleep(1000);",
                "+        }",
                "+",
                "+        // schema is changed and equals to real version",
                "+        assertFalse(schema.value.equals(newSchema.value));",
                "+        assertEquals(newSchema.value, Schema.instance.getRealVersion().toString());",
                "+    }",
                " }",
                "diff --git a/test/unit/org/apache/cassandra/schema/SchemaKeyspaceTest.java b/test/unit/org/apache/cassandra/schema/SchemaKeyspaceTest.java",
                "index 2f834982cd..550deedd79 100644",
                "--- a/test/unit/org/apache/cassandra/schema/SchemaKeyspaceTest.java",
                "+++ b/test/unit/org/apache/cassandra/schema/SchemaKeyspaceTest.java",
                "@@ -23,2 +23,3 @@ import java.nio.ByteBuffer;",
                " import java.util.ArrayList;",
                "+import java.util.Collection;",
                " import java.util.Collections;",
                "@@ -27,2 +28,3 @@ import java.util.List;",
                " import java.util.Set;",
                "+import java.util.UUID;",
                "@@ -36,2 +38,3 @@ import org.apache.cassandra.config.CFMetaData;",
                " import org.apache.cassandra.config.ColumnDefinition;",
                "+import org.apache.cassandra.config.DatabaseDescriptor;",
                " import org.apache.cassandra.config.Schema;",
                "@@ -48,2 +51,5 @@ import org.apache.cassandra.db.rows.UnfilteredRowIterators;",
                " import org.apache.cassandra.exceptions.ConfigurationException;",
                "+import org.apache.cassandra.io.util.DataInputBuffer;",
                "+import org.apache.cassandra.io.util.DataOutputBuffer;",
                "+import org.apache.cassandra.net.MessagingService;",
                " import org.apache.cassandra.thrift.CfDef;",
                "@@ -54,4 +60,6 @@ import org.apache.cassandra.utils.ByteBufferUtil;",
                " import org.apache.cassandra.utils.FBUtilities;",
                "+import org.apache.cassandra.utils.Pair;",
                " import static org.junit.Assert.assertEquals;",
                "+import static org.junit.Assert.assertFalse;",
                " import static org.junit.Assert.assertTrue;",
                "@@ -208,2 +216,96 @@ public class SchemaKeyspaceTest",
                "     }",
                "+",
                "+    private static boolean hasCDC(Mutation m)",
                "+    {",
                "+        for (PartitionUpdate p : m.getPartitionUpdates())",
                "+        {",
                "+            for (ColumnDefinition cd : p.columns())",
                "+            {",
                "+                if (cd.name.toString().equals(\"cdc\"))",
                "+                    return true;",
                "+            }",
                "+        }",
                "+        return false;",
                "+    }",
                "+",
                "+    private static boolean hasSchemaTables(Mutation m)",
                "+    {",
                "+        for (PartitionUpdate p : m.getPartitionUpdates())",
                "+        {",
                "+            if (p.metadata().cfName.equals(SchemaKeyspace.TABLES))",
                "+                return true;",
                "+        }",
                "+        return false;",
                "+    }",
                "+",
                "+    @Test",
                "+    public void testConvertSchemaToMutationsWithoutCDC() throws IOException",
                "+    {",
                "+        boolean oldCDCOption = DatabaseDescriptor.isCDCEnabled();",
                "+        try",
                "+        {",
                "+            DatabaseDescriptor.setCDCEnabled(false);",
                "+            Collection<Mutation> mutations = SchemaKeyspace.convertSchemaToMutations();",
                "+            boolean foundTables = false;",
                "+            for (Mutation m : mutations)",
                "+            {",
                "+                if (hasSchemaTables(m))",
                "+                {",
                "+                    foundTables = true;",
                "+                    assertFalse(hasCDC(m));",
                "+                    try (DataOutputBuffer output = new DataOutputBuffer())",
                "+                    {",
                "+                        Mutation.serializer.serialize(m, output, MessagingService.current_version);",
                "+                        try (DataInputBuffer input = new DataInputBuffer(output.getData()))",
                "+                        {",
                "+                            Mutation out = Mutation.serializer.deserialize(input, MessagingService.current_version);",
                "+                            assertFalse(hasCDC(out));",
                "+                        }",
                "+                    }",
                "+                }",
                "+            }",
                "+            assertTrue(foundTables);",
                "+        }",
                "+        finally",
                "+        {",
                "+            DatabaseDescriptor.setCDCEnabled(oldCDCOption);",
                "+        }",
                "+    }",
                "+",
                "+    @Test",
                "+    public void testConvertSchemaToMutationsWithCDC()",
                "+    {",
                "+        boolean oldCDCOption = DatabaseDescriptor.isCDCEnabled();",
                "+        try",
                "+        {",
                "+            DatabaseDescriptor.setCDCEnabled(true);",
                "+            Collection<Mutation> mutations = SchemaKeyspace.convertSchemaToMutations();",
                "+            boolean foundTables = false;",
                "+            for (Mutation m : mutations)",
                "+            {",
                "+                if (hasSchemaTables(m))",
                "+                {",
                "+                    foundTables = true;",
                "+                    assertTrue(hasCDC(m));",
                "+                }",
                "+            }",
                "+            assertTrue(foundTables);",
                "+        }",
                "+        finally",
                "+        {",
                "+            DatabaseDescriptor.setCDCEnabled(oldCDCOption);",
                "+        }",
                "+    }",
                "+",
                "+    @Test",
                "+    public void testSchemaDigest()",
                "+    {",
                "+        Set<ByteBuffer> abc = Collections.singleton(ByteBufferUtil.bytes(\"abc\"));",
                "+        Pair<UUID, UUID> versions = SchemaKeyspace.calculateSchemaDigest(abc);",
                "+        assertTrue(versions.left.equals(versions.right));",
                "+",
                "+        Set<ByteBuffer> cdc = Collections.singleton(ByteBufferUtil.bytes(\"cdc\"));",
                "+        versions = SchemaKeyspace.calculateSchemaDigest(cdc);",
                "+        assertFalse(versions.left.equals(versions.right));",
                "+    }",
                " }"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/gms/VersionedValue.java",
                "src/java/org/apache/cassandra/schema/SchemaKeyspace.java",
                "src/java/org/apache/cassandra/service/StorageService.java",
                "test/unit/org/apache/cassandra/gms/GossiperTest.java",
                "test/unit/org/apache/cassandra/schema/SchemaKeyspaceTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14140": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                },
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14140",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "7df36056b12a13b60097b7a9a4f8155a1d02ff62",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517888270,
            "hunks": 2,
            "message": "fix parameters to exception message",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/cql3/statements/AlterTableStatement.java b/src/java/org/apache/cassandra/cql3/statements/AlterTableStatement.java",
                "index a5fa12d8c0..59c4e08ce9 100644",
                "--- a/src/java/org/apache/cassandra/cql3/statements/AlterTableStatement.java",
                "+++ b/src/java/org/apache/cassandra/cql3/statements/AlterTableStatement.java",
                "@@ -239,5 +239,5 @@ public class AlterTableStatement extends SchemaAlteringStatement",
                "                 if (!Iterables.isEmpty(views))",
                "-                    throw new InvalidRequestException(String.format(\"Cannot drop column %s on base table with materialized views.\",",
                "+                    throw new InvalidRequestException(String.format(\"Cannot drop column %s on base table %s with materialized views.\",",
                "                                                                     columnName.toString(),",
                "-                                                                    keyspace()));",
                "+                                                                    columnFamily()));",
                "                 break;"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/cql3/statements/AlterTableStatement.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                }
            ]
        },
        {
            "commit_id": "0d26879a955a3045441a5016c73c750faab03c46",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517886186,
            "hunks": 1,
            "message": "fix comparison of addresses for event filtering in repair",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/repair/RepairRunnable.java b/src/java/org/apache/cassandra/repair/RepairRunnable.java",
                "index 5121874092..ebe736d0f0 100644",
                "--- a/src/java/org/apache/cassandra/repair/RepairRunnable.java",
                "+++ b/src/java/org/apache/cassandra/repair/RepairRunnable.java",
                "@@ -747,3 +747,3 @@ public class RepairRunnable extends WrappedRunnable implements ProgressEventNoti",
                "                     {",
                "-                        if (source.equals(r.getInetAddress(\"source\")))",
                "+                        if (source.address.equals(r.getInetAddress(\"source\")))",
                "                             continue;"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/repair/RepairRunnable.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                }
            ]
        },
        {
            "commit_id": "3fc381d5cb603e81eb70c80788aee02af7ba6d31",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517885759,
            "hunks": 1,
            "message": "fix comparison of addresses for message cross node check",
            "diff": [
                "diff --git a/src/java/org/apache/cassandra/net/MessageIn.java b/src/java/org/apache/cassandra/net/MessageIn.java",
                "index a426ef02e4..ab77f3313c 100644",
                "--- a/src/java/org/apache/cassandra/net/MessageIn.java",
                "+++ b/src/java/org/apache/cassandra/net/MessageIn.java",
                "@@ -182,3 +182,3 @@ public class MessageIn<T>",
                "     {",
                "-        return !from.equals(DatabaseDescriptor.getBroadcastAddress());",
                "+        return !from.address.equals(DatabaseDescriptor.getBroadcastAddress());",
                "     }"
            ],
            "changed_files": [
                "src/java/org/apache/cassandra/net/MessageIn.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                }
            ]
        },
        {
            "commit_id": "40148a178bd9b74b731591aa46b4158efb16b742",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518130180,
            "hunks": 2,
            "message": "fix test due to change in exception message",
            "diff": [
                "diff --git a/test/unit/org/apache/cassandra/cql3/ViewComplexTest.java b/test/unit/org/apache/cassandra/cql3/ViewComplexTest.java",
                "index d0f80f815b..464cc39d6b 100644",
                "--- a/test/unit/org/apache/cassandra/cql3/ViewComplexTest.java",
                "+++ b/test/unit/org/apache/cassandra/cql3/ViewComplexTest.java",
                "@@ -426,3 +426,3 @@ public class ViewComplexTest extends CQLTester",
                "         executeNet(protocolVersion, \"USE \" + keyspace());",
                "-        createTable(\"CREATE TABLE %s (k int, c int, a int, b int, l list<int>, s set<int>, m map<int,int>, PRIMARY KEY (k, c))\");",
                "+        String baseTableName = createTable(\"CREATE TABLE %s (k int, c int, a int, b int, l list<int>, s set<int>, m map<int,int>, PRIMARY KEY (k, c))\");",
                "         createView(\"mv\",",
                "@@ -462,3 +462,3 @@ public class ViewComplexTest extends CQLTester",
                "-        assertInvalidMessage(\"Cannot drop column m on base table with materialized views\", \"ALTER TABLE %s DROP m\");",
                "+        assertInvalidMessage(\"Cannot drop column m on base table \" + baseTableName + \" with materialized views\", \"ALTER TABLE %s DROP m\");",
                "         // executeNet(protocolVersion, \"ALTER TABLE %s DROP m\");"
            ],
            "changed_files": [
                "test/unit/org/apache/cassandra/cql3/ViewComplexTest.java"
            ],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "ADV_KEYWORDS_IN_FILES",
                    "message": "An advisory keyword is contained in the changed files: java",
                    "relevance": 4
                }
            ]
        },
        {
            "commit_id": "0628520a9be69bb42a0ba73859888a5a8af83b27",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516624271,
            "hunks": 1,
            "message": "Add ant-junit as SRPM build requirement patch by Troels Arvin; reviewed by Stefan Podkowinski for CASSANDRA-14180",
            "diff": [
                "diff --git a/redhat/cassandra.spec b/redhat/cassandra.spec",
                "index de79bdb043..a483411cf4 100644",
                "--- a/redhat/cassandra.spec",
                "+++ b/redhat/cassandra.spec",
                "@@ -24,2 +24,3 @@ BuildRoot:     %{_tmppath}/%{relname}root-%(%{__id_u} -n)",
                " BuildRequires: ant >= 1.9",
                "+BuildRequires: ant-junit >= 1.9"
            ],
            "changed_files": [
                "redhat/cassandra.spec"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14180": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14180",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "5ba9e6da94ed74c11f6ea37199dbe8a501859e7c",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516626842,
            "hunks": 1,
            "message": "RPM: fix permissions for installed jars and config files patch by Troels Arvin; reviewed by Stefan Podkowinski for CASSANDRA-14181",
            "diff": [
                "diff --git a/redhat/cassandra.spec b/redhat/cassandra.spec",
                "index 05f1b0bfe3..a3f09b0fb6 100644",
                "--- a/redhat/cassandra.spec",
                "+++ b/redhat/cassandra.spec",
                "@@ -128,6 +128,6 @@ exit 0",
                " %attr(755,root,root) /%{_sysconfdir}/rc.d/init.d/%{username}",
                "-%attr(755,root,root) /%{_sysconfdir}/default/%{username}",
                "-%attr(755,root,root) /%{_sysconfdir}/security/limits.d/%{username}.conf",
                "-%attr(755,root,root) /usr/share/%{username}*",
                "-%attr(755,root,root) %config(noreplace) /%{_sysconfdir}/%{username}",
                "+%{_sysconfdir}/default/%{username}",
                "+%{_sysconfdir}/security/limits.d/%{username}.conf",
                "+/usr/share/%{username}*",
                "+%config(noreplace) /%{_sysconfdir}/%{username}",
                " %attr(755,%{username},%{username}) %config(noreplace) /var/lib/%{username}/*"
            ],
            "changed_files": [
                "redhat/cassandra.spec"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14181": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-2.1.20",
                "cassandra-2.1.21",
                "cassandra-2.1.22",
                "cassandra-2.2.12",
                "cassandra-2.2.13",
                "cassandra-2.2.14",
                "cassandra-2.2.15",
                "cassandra-2.2.16",
                "cassandra-2.2.17",
                "cassandra-2.2.18",
                "cassandra-2.2.19",
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14181",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "0d728c194349b210ffc85f892c8492dca39b31f6",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516367565,
            "hunks": 1,
            "message": "ninja-fix a minor documentation oversight in bin/cassandra (CASSANDRA-14175)",
            "diff": [
                "diff --git a/bin/cassandra b/bin/cassandra",
                "index 1305db9e28..95ab4c07ec 100755",
                "--- a/bin/cassandra",
                "+++ b/bin/cassandra",
                "@@ -35,3 +35,3 @@",
                " # more of these variables. This so-called `include' can be placed in a ",
                "-# number of locations and will be searched for in order. The lowest ",
                "+# number of locations and will be searched for in order. The highest ",
                " # priority search path is the same directory as the startup script, and"
            ],
            "changed_files": [
                "bin/cassandra"
            ],
            "message_reference_content": [],
            "jira_refs": {
                "CASSANDRA-14175": ""
            },
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": [
                {
                    "id": "BUG_IN_MESSAGE",
                    "message": "The commit message references some bug tracking ticket: CASSANDRA-14175",
                    "relevance": 2
                }
            ]
        },
        {
            "commit_id": "010c477e4c1b8b452cc0fa33b3fdb6c286e4037d",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516627271,
            "hunks": 0,
            "message": "Merge branch 'cassandra-2.2' into cassandra-3.0",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "c181943a46bda93f8543019046f783ca045c8921",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516624526,
            "hunks": 0,
            "message": "Merge branch 'cassandra-3.0' into cassandra-3.11",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "f428c42622910528e3eb924fca41b2375d6cb351",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516278847,
            "hunks": 1,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [
                "diff --cc CHANGES.txt",
                "index 0d6d486a9b,c06ad0dd84..65a7a40b58",
                "--- a/CHANGES.txt",
                "+++ b/CHANGES.txt",
                "@@@ -181,2 -1,4 +181,3 @@@",
                "  3.11.2",
                "+  * Print correct snitch info from nodetool describecluster (CASSANDRA-13528)",
                " - * Close socket on error during connect on OutboundTcpConnection (CASSANDRA-9630)",
                "   * Enable CDC unittest (CASSANDRA-14141)"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "43aa79d3cd96087dd8aa07592a7d0e87a1bd6824",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518130248,
            "hunks": 0,
            "message": "Merge branch 'cassandra-3.0' into cassandra-3.11",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "de6c62dd6ce60d8493319deb05b71f90d85bc2b0",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517888456,
            "hunks": 2,
            "message": "Merge branch cassandra-3.0 into cassandra-3.11",
            "diff": [
                "diff --cc src/java/org/apache/cassandra/cql3/statements/AlterTableStatement.java",
                "index de426471a1,59c4e08ce9..12abba7ba5",
                "--- a/src/java/org/apache/cassandra/cql3/statements/AlterTableStatement.java",
                "+++ b/src/java/org/apache/cassandra/cql3/statements/AlterTableStatement.java",
                "@@@ -202,45 -238,6 +202,45 @@@ public class AlterTableStatement extend",
                " -                if (!Iterables.isEmpty(views))",
                " +                    switch (def.kind)",
                " +                    {",
                " +                         case PARTITION_KEY:",
                " +                         case CLUSTERING:",
                " +                              throw new InvalidRequestException(String.format(\"Cannot drop PRIMARY KEY part %s\", columnName));",
                " +                         case REGULAR:",
                " +                         case STATIC:",
                " +                              ColumnDefinition toDelete = null;",
                " +                              for (ColumnDefinition columnDef : cfm.partitionColumns())",
                " +                              {",
                " +                                   if (columnDef.name.equals(columnName))",
                " +                                   {",
                " +                                       toDelete = columnDef;",
                " +                                       break;",
                " +                                   }",
                " +                               }",
                " +                             assert toDelete != null;",
                " +                             cfm.removeColumnDefinition(toDelete);",
                " +                             cfm.recordColumnDrop(toDelete, deleteTimestamp  == null ? queryState.getTimestamp() : deleteTimestamp);",
                " +                             break;",
                " +                    }",
                " +",
                " +                    // If the dropped column is required by any secondary indexes",
                " +                    // we reject the operation, as the indexes must be dropped first",
                " +                    Indexes allIndexes = cfm.getIndexes();",
                " +                    if (!allIndexes.isEmpty())",
                " +                    {",
                " +                        ColumnFamilyStore store = Keyspace.openAndGetStore(cfm);",
                " +                        Set<IndexMetadata> dependentIndexes = store.indexManager.getDependentIndexes(def);",
                " +                        if (!dependentIndexes.isEmpty())",
                " +                            throw new InvalidRequestException(String.format(\"Cannot drop column %s because it has \" +",
                " +                                                                            \"dependent secondary indexes (%s)\",",
                " +                                                                            def,",
                " +                                                                            dependentIndexes.stream()",
                " +                                                                                            .map(i -> i.name)",
                " +                                                                                            .collect(Collectors.joining(\",\"))));",
                " +                    }",
                " +",
                " +                    if (!Iterables.isEmpty(views))",
                "-                         throw new InvalidRequestException(String.format(\"Cannot drop column %s on base table with materialized views.\",",
                "+                     throw new InvalidRequestException(String.format(\"Cannot drop column %s on base table %s with materialized views.\",",
                " -                                                                    columnName.toString(),",
                " -                                                                    columnFamily()));",
                " +                                                                        columnName.toString(),",
                "-                                                                         keyspace()));",
                "++                                                                        columnFamily()));",
                " +                }",
                "                  break;"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "1602e606348959aead18531cb8027afb15f276e7",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518281600,
            "hunks": 78,
            "message": "Merge branch 'cassandra-2.1' into cassandra-2.2",
            "diff": [
                "diff --cc CHANGES.txt",
                "index 0f6e61c7e2,9332354804..82da6ad26e",
                "--- a/CHANGES.txt",
                "+++ b/CHANGES.txt",
                "@@@ -1,7 -1,3 +1,8 @@@",
                " -2.1.20",
                " +2.2.12",
                " + * Fix the inspectJvmOptions startup check (CASSANDRA-14112)",
                " + * Fix race that prevents submitting compaction for a table when executor is full (CASSANDRA-13801)",
                " + * Rely on the JVM to handle OutOfMemoryErrors (CASSANDRA-13006)",
                " + * Grab refs during scrub/index redistribution/cleanup (CASSANDRA-13873)",
                " +Merged from 2.1:",
                "+  * Protect against overflow of local expiration time (CASSANDRA-14092)",
                "   * More PEP8 compliance for cqlsh (CASSANDRA-14021)",
                "diff --cc NEWS.txt",
                "index 57479412b9,fb6b4ee38b..4fe3508bea",
                "--- a/NEWS.txt",
                "+++ b/NEWS.txt",
                "@@@ -20,11 -40,5 +40,12 @@@ Upgradin",
                "  ---------",
                " -   - See MAXIMUM TTL EXPIRATION DATE NOTICE above.",
                " -",
                " -2.1.19",
                "++    - See MAXIMUM TTL EXPIRATION DATE NOTICE above.",
                " +    - Cassandra is now relying on the JVM options to properly shutdown on OutOfMemoryError. By default it will",
                " +      rely on the OnOutOfMemoryError option as the ExitOnOutOfMemoryError and CrashOnOutOfMemoryError options",
                " +      are not supported by the older 1.7 and 1.8 JVMs. A warning will be logged at startup if none of those JVM",
                " +      options are used. See CASSANDRA-13006 for more details.",
                " +    - Cassandra is not logging anymore by default an Heap histogram on OutOfMemoryError. To enable that behavior",
                " +      set the 'cassandra.printHeapHistogramOnOutOfMemoryError' System property to 'true'. See CASSANDRA-13006",
                " +      for more details.",
                " +",
                " +2.2.11",
                "  ======",
                "diff --cc debian/rules",
                "index 35f5a51e40,70db61c054..ff1d64df1b",
                "--- a/debian/rules",
                "+++ b/debian/rules",
                "@@@ -65,3 -66,3 +65,3 @@@ binary-indep: build instal",
                "  \tdh_installinit -u'start 50 2 3 4 5 . stop 50 0 1 6 .'",
                "- \tdh_installdocs README.asc CHANGES.txt NEWS.txt doc/cql3/CQL.css doc/cql3/CQL.html",
                " -\tdh_installdocs README.asc CHANGES.txt NEWS.txt",
                "++\tdh_installdocs README.asc CHANGES.txt NEWS.txt doc/cql3/CQL.css doc/cql3/CQL.html CASSANDRA-14092.txt",
                "  \tdh_installexamples tools/*.yaml",
                "diff --cc redhat/cassandra.spec",
                "index 0d4b271545,a3f09b0fb6..07c3dc57ae",
                "--- a/redhat/cassandra.spec",
                "+++ b/redhat/cassandra.spec",
                "@@@ -116,3 -115,4 +116,3 @@@ exit ",
                "  %defattr(0644,root,root,0755)",
                "--%doc CHANGES.txt LICENSE.txt README.asc NEWS.txt NOTICE.txt",
                " -%attr(755,root,root) %{_bindir}/cassandra-cli",
                "++%doc CHANGES.txt LICENSE.txt README.asc NEWS.txt NOTICE.txt CASSANDRA-14092.txt",
                "  %attr(755,root,root) %{_bindir}/cassandra-stress",
                "diff --cc src/java/org/apache/cassandra/cql3/Attributes.java",
                "index 7b38e9faee,23571cab3c..84f423acb6",
                "--- a/src/java/org/apache/cassandra/cql3/Attributes.java",
                "+++ b/src/java/org/apache/cassandra/cql3/Attributes.java",
                "@@@ -20,7 -20,9 +20,12 @@@ package org.apache.cassandra.cql3",
                "  import java.nio.ByteBuffer;",
                " +import java.util.Collections;",
                "+ import java.util.concurrent.TimeUnit;",
                " +import com.google.common.collect.Iterables;",
                "+ import com.google.common.annotations.VisibleForTesting;",
                "+ import org.slf4j.Logger;",
                "+ import org.slf4j.LoggerFactory;",
                "+ import org.apache.cassandra.config.CFMetaData;",
                " +import org.apache.cassandra.cql3.functions.Function;",
                "  import org.apache.cassandra.db.ExpiringCell;",
                "@@@ -30,3 -32,3 +35,4 @@@ import org.apache.cassandra.exceptions.",
                "  import org.apache.cassandra.serializers.MarshalException;",
                " +import org.apache.cassandra.utils.ByteBufferUtil;",
                "+ import org.apache.cassandra.utils.NoSpamLogger;",
                "diff --cc src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "index 7e36e11b6a,2989b9d6ea..45908deb6a",
                "--- a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "+++ b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "@@@ -913,16 -887,3 +913,16 @@@ public class ColumnFamilyStore implemen",
                "              @SuppressWarnings(\"unchecked\")",
                "-             ListenableFuture<ReplayPosition> future = ",
                " -            ListenableFuture<?> future = Futures.allAsList(flushTask, task);",
                "++            ListenableFuture<ReplayPosition> future =",
                " +                    // If either of the two tasks errors out, resulting future must also error out.",
                " +                    // Combine the two futures and only return post-flush result after both have completed.",
                " +                    // Note that flushTask will always yield null, but Futures.allAsList is",
                " +                    // order preserving, which is why the transform function returns the result",
                " +                    // from item 1 in it's input list (i.e. what was yielded by task).",
                " +                    Futures.transform(Futures.allAsList(flushTask, task),",
                " +                                      new Function<List<Object>, ReplayPosition>()",
                " +                                      {",
                " +                                          public ReplayPosition apply(List<Object> input)",
                " +                                          {",
                " +                                              return (ReplayPosition) input.get(1);",
                " +                                          }",
                " +                                      });",
                "              return future;",
                "@@@ -1614,7 -1520,30 +1614,6 @@@",
                "      {",
                "-         return scrub(disableSnapshot, skipCorrupted, false, checkData, jobs);",
                " -        // skip snapshot creation during scrub, SEE JIRA 5891",
                " -        if(!disableSnapshot)",
                " -            snapshotWithoutFlush(\"pre-scrub-\" + System.currentTimeMillis());",
                " -        return CompactionManager.instance.performScrub(ColumnFamilyStore.this, skipCorrupted, checkData, reinsertOverflowedTTLRows, jobs);",
                " -    }",
                " -",
                " -    public CompactionManager.AllSSTableOpStatus sstablesRewrite(boolean excludeCurrentVersion, int jobs) throws ExecutionException, InterruptedException",
                " -    {",
                " -        return CompactionManager.instance.performSSTableRewrite(ColumnFamilyStore.this, excludeCurrentVersion, jobs);",
                " -    }",
                " -",
                " -    public void markObsolete(Collection<SSTableReader> sstables, OperationType compactionType)",
                " -    {",
                " -        assert !sstables.isEmpty();",
                " -        data.markObsolete(sstables, compactionType);",
                " -    }",
                " -",
                " -    void replaceFlushed(Memtable memtable, SSTableReader sstable)",
                " -    {",
                " -        compactionStrategyWrapper.replaceFlushed(memtable, sstable);",
                " -    }",
                " -",
                " -    public boolean isValid()",
                " -    {",
                " -        return valid;",
                "++        return scrub(disableSnapshot, skipCorrupted, false, checkData, reinsertOverflowedTTLRows, jobs);",
                "      }",
                "-     @VisibleForTesting",
                "-     public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean alwaysFail, boolean checkData, int jobs) throws ExecutionException, InterruptedException",
                " -    public long getMemtableColumnsCount()",
                "++    public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean alwaysFail, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs) throws ExecutionException, InterruptedException",
                "      {",
                "@@@ -1624,10 -1552,6 +1623,10 @@@",
                " -    public long getMemtableDataSize()",
                " -    {",
                " -        return metric.memtableOnHeapSize.value();",
                " -    }",
                " +        try",
                " +        {",
                "-             return CompactionManager.instance.performScrub(ColumnFamilyStore.this, skipCorrupted, checkData, jobs);",
                "++            return CompactionManager.instance.performScrub(ColumnFamilyStore.this, skipCorrupted, checkData, reinsertOverflowedTTLRows, jobs);",
                " +        }",
                " +        catch(Throwable t)",
                " +        {",
                " +            if (!rebuildOnFailedScrub(t))",
                " +                throw t;",
                "diff --cc src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "index 3350b2091c,6e3634a1db..d90abe9fe7",
                "--- a/src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "@@@ -350,4 -360,10 +350,10 @@@ public class CompactionManager implemen",
                " -    @Deprecated",
                " -    public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData, int jobs) throws InterruptedException, ExecutionException",
                " +    public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData, int jobs)",
                " +    throws InterruptedException, ExecutionException",
                "+     {",
                "+         return performScrub(cfs, skipCorrupted, checkData, false, jobs);",
                "+     }",
                "+ ",
                " -    public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData,",
                " -                                           final boolean reinsertOverflowedTTLRows, int jobs) throws InterruptedException, ExecutionException",
                "++    public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData, final boolean reinsertOverflowedTTLRows, int jobs)",
                "++    throws InterruptedException, ExecutionException",
                "      {",
                "@@@ -732,3 -719,3 +738,3 @@@",
                "-     private void scrubOne(ColumnFamilyStore cfs, LifecycleTransaction modifier, boolean skipCorrupted, boolean checkData) throws IOException",
                " -    private void scrubOne(ColumnFamilyStore cfs, SSTableReader sstable, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows) throws IOException",
                "++    private void scrubOne(ColumnFamilyStore cfs, LifecycleTransaction modifier, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows) throws IOException",
                "      {",
                "@@@ -736,3 -723,5 +742,3 @@@",
                "-         try (Scrubber scrubber = new Scrubber(cfs, modifier, skipCorrupted, checkData))",
                " -        CompactionInfo.Holder scrubInfo = scrubber.getScrubInfo();",
                " -        metrics.beginCompaction(scrubInfo);",
                " -        try",
                "++        try (Scrubber scrubber = new Scrubber(cfs, modifier, skipCorrupted, checkData, reinsertOverflowedTTLRows))",
                "          {",
                "diff --cc src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "index b6b20fbbea,6d4537cc19..affee119fd",
                "--- a/src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "@@@ -41,4 -38,3 +42,5 @@@ import org.apache.cassandra.utils.JVMSt",
                "  import org.apache.cassandra.utils.OutputHandler;",
                "+ import org.apache.cassandra.utils.memory.HeapAllocator;",
                " +import org.apache.cassandra.utils.UUIDGen;",
                " +import org.apache.cassandra.utils.concurrent.Refs;",
                "@@@ -51,2 -46,4 +53,3 @@@ public class Scrubber implements Closea",
                "      private final boolean skipCorrupted;",
                " -    public final boolean validateColumns;",
                "+     private final boolean reinsertOverflowedTTLRows;",
                "@@@ -88,7 -85,13 +92,14 @@@",
                "      {",
                "-         this(cfs, transaction, skipCorrupted, new OutputHandler.LogOutput(), checkData);",
                " -        this(cfs, sstable, skipCorrupted, isOffline, checkData, false);",
                "++        this(cfs, transaction, skipCorrupted, new OutputHandler.LogOutput(), checkData, false);",
                "+     }",
                "+ ",
                " -    public Scrubber(ColumnFamilyStore cfs, SSTableReader sstable, boolean skipCorrupted, boolean isOffline, boolean checkData,",
                "++    public Scrubber(ColumnFamilyStore cfs, LifecycleTransaction transaction, boolean skipCorrupted, boolean checkData,",
                "+                     boolean reinsertOverflowedTTLRows) throws IOException",
                "+     {",
                " -        this(cfs, sstable, skipCorrupted, new OutputHandler.LogOutput(), isOffline, checkData, reinsertOverflowedTTLRows);",
                "++        this(cfs, transaction, skipCorrupted, new OutputHandler.LogOutput(), checkData, reinsertOverflowedTTLRows);",
                "      }",
                " -    public Scrubber(ColumnFamilyStore cfs, SSTableReader sstable, boolean skipCorrupted, OutputHandler outputHandler, boolean isOffline, boolean checkData,",
                " +    @SuppressWarnings(\"resource\")",
                "-     public Scrubber(ColumnFamilyStore cfs, LifecycleTransaction transaction, boolean skipCorrupted, OutputHandler outputHandler, boolean checkData) throws IOException",
                "++    public Scrubber(ColumnFamilyStore cfs, LifecycleTransaction transaction, boolean skipCorrupted, OutputHandler outputHandler, boolean checkData,",
                "+                     boolean reinsertOverflowedTTLRows) throws IOException",
                "      {",
                "@@@ -99,3 -101,5 +110,4 @@@",
                "          this.skipCorrupted = skipCorrupted;",
                " -        this.isOffline = isOffline;",
                " -        this.validateColumns = checkData;",
                " +        this.rowIndexEntrySerializer = sstable.descriptor.version.getSSTableFormat().getIndexSerializer(sstable.metadata);",
                "+         this.reinsertOverflowedTTLRows = reinsertOverflowedTTLRows;",
                "@@@ -324,3 -341,3 +341,3 @@@",
                "          // to the outOfOrderRows that will be later written to a new SSTable.",
                "-         OrderCheckerIterator atoms = new OrderCheckerIterator(new SSTableIdentityIterator(sstable, dataFile, key, checkData),",
                " -        OrderCheckerIterator atoms = new OrderCheckerIterator(getIterator(key, dataSize),",
                "++        OrderCheckerIterator atoms = new OrderCheckerIterator(getIterator(key),",
                "                                                                cfs.metadata.comparator.onDiskAtomComparator());",
                "@@@ -344,2 -361,14 +361,14 @@@",
                "+     /**",
                "+      * Only wrap with {@link FixNegativeLocalDeletionTimeIterator} if {@link #reinsertOverflowedTTLRows} option",
                "+      * is specified",
                "+      */",
                " -    private OnDiskAtomIterator getIterator(DecoratedKey key, long dataSize)",
                "++    private OnDiskAtomIterator getIterator(DecoratedKey key)",
                "+     {",
                " -        SSTableIdentityIterator sstableIdentityIterator = new SSTableIdentityIterator(sstable, dataFile, key, dataSize, validateColumns);",
                "++        SSTableIdentityIterator sstableIdentityIterator = new SSTableIdentityIterator(sstable, dataFile, key, checkData);",
                "+         return reinsertOverflowedTTLRows ? new FixNegativeLocalDeletionTimeIterator(sstableIdentityIterator,",
                "+                                                                                     outputHandler,",
                "+                                                                                     negativeLocalDeletionInfoMetrics) : sstableIdentityIterator;",
                "+     }",
                "+ ",
                "      private void updateIndexKey()",
                "diff --cc src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java",
                "index 6896062272,d7187657f7..e416c7b32c",
                "--- a/src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java",
                "+++ b/src/java/org/apache/cassandra/io/sstable/AbstractSSTableSimpleWriter.java",
                "@@@ -33,4 -34,4 +34,5 @@@ import org.apache.cassandra.db.context.",
                "  import org.apache.cassandra.dht.IPartitioner;",
                "+ import org.apache.cassandra.exceptions.InvalidRequestException;",
                " -import org.apache.cassandra.io.sstable.metadata.MetadataCollector;",
                " +import org.apache.cassandra.io.sstable.format.SSTableFormat;",
                " +import org.apache.cassandra.io.sstable.format.SSTableWriter;",
                "  import org.apache.cassandra.service.ActiveRepairService;",
                "diff --cc src/java/org/apache/cassandra/service/StorageServiceMBean.java",
                "index 761eed6f33,90c0fb5972..f336bcc495",
                "--- a/src/java/org/apache/cassandra/service/StorageServiceMBean.java",
                "+++ b/src/java/org/apache/cassandra/service/StorageServiceMBean.java",
                "@@@ -269,9 -279,3 +270,11 @@@ public interface StorageServiceMBean ex",
                " -    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException;",
                "++public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException;",
                "++",
                " +    /**",
                " +     * Verify (checksums of) the given keyspace.",
                " +     * If columnFamilies array is empty, all CFs are verified.",
                " +     *",
                " +     * The entire sstable will be read to ensure each cell validates if extendedVerify is true",
                " +     */",
                " +    public int verify(boolean extendedVerify, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException;",
                "diff --cc src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "index a486a1399c,59d13d5519..f5e84c5f0e",
                "--- a/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "+++ b/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "@@@ -123,4 -119,4 +130,4 @@@ public class StandaloneScrubbe",
                "                      {",
                " -                        Scrubber scrubber = new Scrubber(cfs, sstable, options.skipCorrupted, handler, true, !options.noValidate, options.reinsertOverflowedTTL);",
                " -                        try",
                " +                        txn.obsoleteOriginals(); // make sure originals are deleted and avoid NPE if index is missing, CASSANDRA-9591",
                "-                         try (Scrubber scrubber = new Scrubber(cfs, txn, options.skipCorrupted, handler, !options.noValidate))",
                "++                        try (Scrubber scrubber = new Scrubber(cfs, txn, options.skipCorrupted, handler, !options.noValidate, options.reinsertOverflowedTTL))",
                "                          {",
                "diff --cc src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "index dafe8d1718,0000000000..50224a0137",
                "mode 100644,000000..100644",
                "--- a/src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "+++ b/src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "@@@ -1,76 -1,0 +1,82 @@@",
                " +/*",
                " + * Licensed to the Apache Software Foundation (ASF) under one",
                " + * or more contributor license agreements.  See the NOTICE file",
                " + * distributed with this work for additional information",
                " + * regarding copyright ownership.  The ASF licenses this file",
                " + * to you under the Apache License, Version 2.0 (the",
                " + * \"License\"); you may not use this file except in compliance",
                " + * with the License.  You may obtain a copy of the License at",
                " + *",
                " + *     http://www.apache.org/licenses/LICENSE-2.0",
                " + *",
                " + * Unless required by applicable law or agreed to in writing, software",
                " + * distributed under the License is distributed on an \"AS IS\" BASIS,",
                " + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                " + * See the License for the specific language governing permissions and",
                " + * limitations under the License.",
                " + */",
                " +package org.apache.cassandra.tools.nodetool;",
                " +",
                " +import io.airlift.command.Arguments;",
                " +import io.airlift.command.Command;",
                " +import io.airlift.command.Option;",
                " +",
                " +import java.util.ArrayList;",
                " +import java.util.List;",
                " +",
                " +import org.apache.cassandra.tools.NodeProbe;",
                " +import org.apache.cassandra.tools.NodeTool.NodeToolCmd;",
                "++import org.apache.cassandra.tools.StandaloneScrubber;",
                " +",
                " +@Command(name = \"scrub\", description = \"Scrub (rebuild sstables for) one or more tables\")",
                " +public class Scrub extends NodeToolCmd",
                " +{",
                " +    @Arguments(usage = \"[<keyspace> <tables>...]\", description = \"The keyspace followed by one or many tables\")",
                " +    private List<String> args = new ArrayList<>();",
                " +",
                " +    @Option(title = \"disable_snapshot\",",
                " +            name = {\"-ns\", \"--no-snapshot\"},",
                " +            description = \"Scrubbed CFs will be snapshotted first, if disableSnapshot is false. (default false)\")",
                " +    private boolean disableSnapshot = false;",
                " +",
                " +    @Option(title = \"skip_corrupted\",",
                " +            name = {\"-s\", \"--skip-corrupted\"},",
                " +            description = \"Skip corrupted partitions even when scrubbing counter tables. (default false)\")",
                " +    private boolean skipCorrupted = false;",
                " +",
                " +    @Option(title = \"no_validate\",",
                " +                   name = {\"-n\", \"--no-validate\"},",
                " +                   description = \"Do not validate columns using column validator\")",
                " +    private boolean noValidation = false;",
                " +",
                " +    @Option(title = \"jobs\",",
                " +            name = {\"-j\", \"--jobs\"},",
                " +            description = \"Number of sstables to scrub simultanously, set to 0 to use all available compaction threads\")",
                " +    private int jobs = 2;",
                " +",
                "++    @Option(title = \"reinsert_overflowed_ttl\",",
                "++    name = {\"r\", \"--reinsert-overflowed-ttl\"},",
                "++    description = StandaloneScrubber.REINSERT_OVERFLOWED_TTL_OPTION_DESCRIPTION)",
                "++    private boolean reinsertOverflowedTTL = false;",
                "++",
                " +    @Override",
                " +    public void execute(NodeProbe probe)",
                " +    {",
                " +        List<String> keyspaces = parseOptionalKeyspace(args, probe);",
                " +        String[] cfnames = parseOptionalColumnFamilies(args);",
                " +",
                " +        for (String keyspace : keyspaces)",
                " +        {",
                " +            try",
                " +            {",
                "-                 probe.scrub(System.out, disableSnapshot, skipCorrupted, !noValidation, jobs, keyspace, cfnames);",
                "++                probe.scrub(System.out, disableSnapshot, skipCorrupted, !noValidation, reinsertOverflowedTTL, jobs, keyspace, cfnames);",
                " +            } catch (IllegalArgumentException e)",
                " +            {",
                " +                throw e;",
                " +            } catch (Exception e)",
                " +            {",
                " +                throw new RuntimeException(\"Error occurred during scrubbing\", e);",
                " +            }",
                " +        }",
                " +    }",
                " +}",
                "diff --cc test/data/negative-local-expiration-test/table1/lb-1-big-CompressionInfo.db",
                "index 0000000000,d7cc13bc74..d7cc13bc74",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/lb-1-big-Data.db",
                "index 0000000000,0000000000..51213c2f21",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/lb-1-big-Digest.adler32",
                "index 0000000000,0000000000..d5b12df4a2",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table1/lb-1-big-Digest.adler32",
                "@@@ -1,0 -1,0 +1,1 @@@",
                "++2292388625",
                "diff --cc test/data/negative-local-expiration-test/table1/lb-1-big-Filter.db",
                "index 0000000000,f8e53beeef..f8e53beeef",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/lb-1-big-Index.db",
                "index 0000000000,3ab96eee9d..3ab96eee9d",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/lb-1-big-Statistics.db",
                "index 0000000000,9bde77e31e..e8cc7e0ba3",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/lb-1-big-Summary.db",
                "index 0000000000,788b66a06d..1a3f81f76e",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/lb-1-big-TOC.txt",
                "index 0000000000,4b6cff8157..26c7025634",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table1/lb-1-big-TOC.txt",
                "+++ b/test/data/negative-local-expiration-test/table1/lb-1-big-TOC.txt",
                "@@@ -1,0 -1,8 +1,8 @@@",
                " -Index.db",
                " -Data.db",
                "+ Statistics.db",
                " -TOC.txt",
                "+ Summary.db",
                " -Digest.sha1",
                "++Digest.adler32",
                "++Data.db",
                "++Index.db",
                "+ CompressionInfo.db",
                "+ Filter.db",
                "++TOC.txt",
                "diff --cc test/data/negative-local-expiration-test/table2/lb-1-big-CompressionInfo.db",
                "index 0000000000,38373b4de9..38373b4de9",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/lb-1-big-Data.db",
                "index 0000000000,0000000000..762a2297f3",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/lb-1-big-Digest.adler32",
                "index 0000000000,0000000000..ae89849b88",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table2/lb-1-big-Digest.adler32",
                "@@@ -1,0 -1,0 +1,1 @@@",
                "++3829731931",
                "diff --cc test/data/negative-local-expiration-test/table2/lb-1-big-Filter.db",
                "index 0000000000,f8e53beeef..f8e53beeef",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/lb-1-big-Index.db",
                "index 0000000000,38a6e4cee1..38a6e4cee1",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/lb-1-big-Statistics.db",
                "index 0000000000,8ee9116ef5..64dab4377c",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/lb-1-big-Summary.db",
                "index 0000000000,788b66a06d..1a3f81f76e",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/lb-1-big-TOC.txt",
                "index 0000000000,4b6cff8157..26c7025634",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table2/lb-1-big-TOC.txt",
                "+++ b/test/data/negative-local-expiration-test/table2/lb-1-big-TOC.txt",
                "@@@ -1,0 -1,8 +1,8 @@@",
                " -Index.db",
                " -Data.db",
                "+ Statistics.db",
                " -TOC.txt",
                "+ Summary.db",
                " -Digest.sha1",
                "++Digest.adler32",
                "++Data.db",
                "++Index.db",
                "+ CompressionInfo.db",
                "+ Filter.db",
                "++TOC.txt",
                "diff --cc test/data/negative-local-expiration-test/table3/lb-1-big-CompressionInfo.db",
                "index 0000000000,04a738493d..04a738493d",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/lb-1-big-Data.db",
                "index 0000000000,0000000000..33145dfb7c",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/lb-1-big-Digest.adler32",
                "index 0000000000,0000000000..2a542cda55",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table3/lb-1-big-Digest.adler32",
                "@@@ -1,0 -1,0 +1,1 @@@",
                "++3574474340",
                "diff --cc test/data/negative-local-expiration-test/table3/lb-1-big-Filter.db",
                "index 0000000000,f8e53beeef..f8e53beeef",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/lb-1-big-Index.db",
                "index 0000000000,5fb34e86cf..5fb34e86cf",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/lb-1-big-Statistics.db",
                "index 0000000000,4d961fbde9..51203ae035",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/lb-1-big-Summary.db",
                "index 0000000000,788b66a06d..1a3f81f76e",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/lb-1-big-TOC.txt",
                "index 0000000000,4b6cff8157..26c7025634",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table3/lb-1-big-TOC.txt",
                "+++ b/test/data/negative-local-expiration-test/table3/lb-1-big-TOC.txt",
                "@@@ -1,0 -1,8 +1,8 @@@",
                " -Index.db",
                " -Data.db",
                "+ Statistics.db",
                " -TOC.txt",
                "+ Summary.db",
                " -Digest.sha1",
                "++Digest.adler32",
                "++Data.db",
                "++Index.db",
                "+ CompressionInfo.db",
                "+ Filter.db",
                "++TOC.txt",
                "diff --cc test/data/negative-local-expiration-test/table4/lb-1-big-CompressionInfo.db",
                "index 0000000000,c814fef86a..c814fef86a",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/lb-1-big-Data.db",
                "index 0000000000,0000000000..f40e71f59e",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/lb-1-big-Digest.adler32",
                "index 0000000000,0000000000..e6675e4e91",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table4/lb-1-big-Digest.adler32",
                "@@@ -1,0 -1,0 +1,1 @@@",
                "++2405377913",
                "diff --cc test/data/negative-local-expiration-test/table4/lb-1-big-Filter.db",
                "index 0000000000,f8e53beeef..f8e53beeef",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/lb-1-big-Index.db",
                "index 0000000000,8291383c15..8291383c15",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/lb-1-big-Statistics.db",
                "index 0000000000,68f76aefab..2217c2d85d",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/lb-1-big-Summary.db",
                "index 0000000000,788b66a06d..1a3f81f76e",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/lb-1-big-TOC.txt",
                "index 0000000000,4b6cff8157..26c7025634",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table4/lb-1-big-TOC.txt",
                "+++ b/test/data/negative-local-expiration-test/table4/lb-1-big-TOC.txt",
                "@@@ -1,0 -1,8 +1,8 @@@",
                " -Index.db",
                " -Data.db",
                "+ Statistics.db",
                " -TOC.txt",
                "+ Summary.db",
                " -Digest.sha1",
                "++Digest.adler32",
                "++Data.db",
                "++Index.db",
                "+ CompressionInfo.db",
                "+ Filter.db",
                "++TOC.txt",
                "diff --cc test/unit/org/apache/cassandra/cql3/validation/operations/TTLTest.java",
                "index 0000000000,ab4ef21d0c..b1eaac10b0",
                "mode 000000,100644..100644",
                "--- a/test/unit/org/apache/cassandra/cql3/validation/operations/TTLTest.java",
                "+++ b/test/unit/org/apache/cassandra/cql3/validation/operations/TTLTest.java",
                "@@@ -1,0 -1,410 +1,405 @@@",
                "+ package org.apache.cassandra.cql3.validation.operations;",
                "+ ",
                "+ import java.io.File;",
                "+ import java.io.FileInputStream;",
                "+ import java.io.FileOutputStream;",
                "+ import java.io.IOException;",
                "+ ",
                "+ import static org.junit.Assert.assertEquals;",
                "+ import static org.junit.Assert.assertTrue;",
                "+ import static org.junit.Assert.fail;",
                "+ ",
                "+ import org.apache.cassandra.cql3.Attributes;",
                "+ import org.apache.cassandra.cql3.CQLTester;",
                "+ import org.apache.cassandra.cql3.UntypedResultSet;",
                "+ import org.apache.cassandra.db.BufferExpiringCell;",
                "+ import org.apache.cassandra.db.ColumnFamilyStore;",
                "+ import org.apache.cassandra.db.ExpiringCell;",
                "+ import org.apache.cassandra.db.Keyspace;",
                "+ import org.apache.cassandra.exceptions.InvalidRequestException;",
                "+ import org.apache.cassandra.utils.FBUtilities;",
                "+ ",
                "+ import org.junit.Test;",
                "+ ",
                "+ public class TTLTest extends CQLTester",
                "+ {",
                "+     public static String NEGATIVE_LOCAL_EXPIRATION_TEST_DIR = \"test/data/negative-local-expiration-test/%s\";",
                "+ ",
                "+     public static int MAX_TTL = ExpiringCell.MAX_TTL;",
                "+ ",
                "+     public static final String SIMPLE_NOCLUSTERING = \"table1\";",
                "+     public static final String SIMPLE_CLUSTERING = \"table2\";",
                "+     public static final String COMPLEX_NOCLUSTERING = \"table3\";",
                "+     public static final String COMPLEX_CLUSTERING = \"table4\";",
                "+ ",
                "+     @Test",
                "+     public void testTTLPerRequestLimit() throws Throwable",
                "+     {",
                "+         createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int)\");",
                "+         // insert with low TTL should not be denied",
                "+         execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL ?\", 10); // max ttl",
                "+ ",
                "+         try",
                "+         {",
                "+             execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL ?\", MAX_TTL + 1);",
                "+             fail(\"Expect InvalidRequestException\");",
                "+         }",
                "+         catch (InvalidRequestException e)",
                "+         {",
                "+             assertTrue(e.getMessage().contains(\"ttl is too large.\"));",
                "+         }",
                "+ ",
                "+         try",
                "+         {",
                "+             execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL ?\", -1);",
                "+             fail(\"Expect InvalidRequestException\");",
                "+         }",
                "+         catch (InvalidRequestException e)",
                "+         {",
                "+             assertTrue(e.getMessage().contains(\"A TTL must be greater or equal to 0\"));",
                "+         }",
                "+         execute(\"TRUNCATE %s\");",
                "+ ",
                "+         // insert with low TTL should not be denied",
                "+         execute(\"UPDATE %s USING TTL ? SET i = 1 WHERE k = 2\", 5); // max ttl",
                "+ ",
                "+         try",
                "+         {",
                "+             execute(\"UPDATE %s USING TTL ? SET i = 1 WHERE k = 2\", MAX_TTL + 1);",
                "+             fail(\"Expect InvalidRequestException\");",
                "+         }",
                "+         catch (InvalidRequestException e)",
                "+         {",
                "+             assertTrue(e.getMessage().contains(\"ttl is too large.\"));",
                "+         }",
                "+ ",
                "+         try",
                "+         {",
                "+             execute(\"UPDATE %s USING TTL ? SET i = 1 WHERE k = 2\", -1);",
                "+             fail(\"Expect InvalidRequestException\");",
                "+         }",
                "+         catch (InvalidRequestException e)",
                "+         {",
                "+             assertTrue(e.getMessage().contains(\"A TTL must be greater or equal to 0\"));",
                "+         }",
                "+     }",
                "+ ",
                "+ ",
                "+     @Test",
                "+     public void testTTLDefaultLimit() throws Throwable",
                "+     {",
                "+         try",
                "+         {",
                "+             createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=-1\");",
                "+             fail(\"Expect Invalid schema\");",
                "+         }",
                "+         catch (RuntimeException e)",
                "+         {",
                "+             assertTrue(e.getCause()",
                " -                        .getCause()",
                "+                         .getMessage()",
                "+                         .contains(\"default_time_to_live cannot be smaller than 0\"));",
                "+         }",
                "+         try",
                "+         {",
                "+             createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\"",
                "+                         + (MAX_TTL + 1));",
                "+             fail(\"Expect Invalid schema\");",
                "+         }",
                "+         catch (RuntimeException e)",
                "+         {",
                "+             assertTrue(e.getCause()",
                " -                        .getCause()",
                "+                         .getMessage()",
                "+                         .contains(\"default_time_to_live must be less than or equal to \" + MAX_TTL + \" (got \"",
                "+                                   + (MAX_TTL + 1) + \")\"));",
                "+         }",
                "+ ",
                "+         // table with default low TTL should not be denied",
                "+         createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\" + 5);",
                "+         execute(\"INSERT INTO %s (k, i) VALUES (1, 1)\");",
                "+     }",
                "+ ",
                "+     @Test",
                "+     public void testRejectExpirationDateOverflowPolicy() throws Throwable",
                "+     {",
                "+         Attributes.policy = Attributes.ExpirationDateOverflowPolicy.REJECT;",
                "+         createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int)\");",
                "+         try",
                "+         {",
                "+             execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL \" + MAX_TTL);",
                "+         }",
                "+         catch (InvalidRequestException e)",
                "+         {",
                "+             assertTrue(e.getMessage().contains(\"exceeds maximum supported expiration date\"));",
                "+         }",
                "+         try",
                "+         {",
                "+             createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\" + MAX_TTL);",
                "+             execute(\"INSERT INTO %s (k, i) VALUES (1, 1)\");",
                "+         }",
                "+         catch (InvalidRequestException e)",
                "+         {",
                "+             assertTrue(e.getMessage().contains(\"exceeds maximum supported expiration date\"));",
                "+         }",
                "+     }",
                "+ ",
                "+     @Test",
                "+     public void testCapExpirationDatePolicyDefaultTTL() throws Throwable",
                "+     {",
                "+         Attributes.policy = Attributes.ExpirationDateOverflowPolicy.CAP;",
                "+         createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\" + MAX_TTL);",
                "+         execute(\"INSERT INTO %s (k, i) VALUES (1, 1)\");",
                "+         checkTTLIsCapped(\"i\");",
                "+         Attributes.policy = Attributes.ExpirationDateOverflowPolicy.REJECT;",
                "+     }",
                "+ ",
                "+     @Test",
                "+     public void testCapExpirationDatePolicyPerRequest() throws Throwable",
                "+     {",
                "+         // Test cap policy",
                "+         Attributes.policy = Attributes.ExpirationDateOverflowPolicy.CAP;",
                "+ ",
                "+         // simple column, clustering, flush",
                "+         baseCapExpirationDateOverflowTest(true, true, true);",
                "+         // simple column, clustering, noflush",
                "+         baseCapExpirationDateOverflowTest(true, true, false);",
                "+         // simple column, noclustering, flush",
                "+         baseCapExpirationDateOverflowTest(true, false, true);",
                "+         // simple column, noclustering, noflush",
                "+         baseCapExpirationDateOverflowTest(true, false, false);",
                "+         // complex column, clustering, flush",
                "+         baseCapExpirationDateOverflowTest(false, true, true);",
                "+         // complex column, clustering, noflush",
                "+         baseCapExpirationDateOverflowTest(false, true, false);",
                "+         // complex column, noclustering, flush",
                "+         baseCapExpirationDateOverflowTest(false, false, true);",
                "+         // complex column, noclustering, noflush",
                "+         baseCapExpirationDateOverflowTest(false, false, false);",
                "+         // complex column, noclustering, flush",
                "+         baseCapExpirationDateOverflowTest(false, false, false);",
                "+ ",
                "+         // Return to previous policy",
                "+         Attributes.policy = Attributes.ExpirationDateOverflowPolicy.REJECT;",
                "+     }",
                "+ ",
                "+     @Test",
                "+     public void testRecoverOverflowedExpirationWithScrub() throws Throwable",
                "+     {",
                " -        createTable(true, true);",
                " -        createTable(true, false);",
                " -        createTable(false, true);",
                " -        createTable(false, false);",
                " -",
                "+         baseTestRecoverOverflowedExpiration(false, false);",
                "+         baseTestRecoverOverflowedExpiration(true, false);",
                "+         baseTestRecoverOverflowedExpiration(true, true);",
                "+     }",
                "+ ",
                "+     public void baseCapExpirationDateOverflowTest(boolean simple, boolean clustering, boolean flush) throws Throwable",
                "+     {",
                "+         // Create Table",
                "+         if (simple)",
                "+         {",
                "+             if (clustering)",
                "+                 createTable(\"create table %s (k int, a int, b int, primary key(k, a))\");",
                "+             else",
                "+                 createTable(\"create table %s (k int primary key, a int, b int)\");",
                "+         }",
                "+         else",
                "+         {",
                "+             if (clustering)",
                "+                 createTable(\"create table %s (k int, a int, b set<text>, primary key(k, a))\");",
                "+             else",
                "+                 createTable(\"create table %s (k int primary key, a int, b set<text>)\");",
                "+         }",
                "+ ",
                "+         // Insert data with INSERT and UPDATE",
                "+         if (simple)",
                "+         {",
                "+             execute(\"INSERT INTO %s (k, a, b) VALUES (?, ?, ?) USING TTL \" + MAX_TTL, 2, 2, 2);",
                "+             if (clustering)",
                "+                 execute(\"UPDATE %s USING TTL \" + MAX_TTL + \" SET b = 1 WHERE k = 1 AND a = 1;\");",
                "+             else",
                "+                 execute(\"UPDATE %s USING TTL \" + MAX_TTL + \" SET a = 1, b = 1 WHERE k = 1;\");",
                "+         }",
                "+         else",
                "+         {",
                "+             execute(\"INSERT INTO %s (k, a, b) VALUES (?, ?, ?) USING TTL \" + MAX_TTL, 2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\"));",
                "+             if (clustering)",
                "+                 execute(\"UPDATE  %s USING TTL \" + MAX_TTL + \" SET b = ? WHERE k = 1 AND a = 1;\", set(\"v11\", \"v12\", \"v13\", \"v14\"));",
                "+             else",
                "+                 execute(\"UPDATE  %s USING TTL \" + MAX_TTL + \" SET a = 1, b = ? WHERE k = 1;\", set(\"v11\", \"v12\", \"v13\", \"v14\"));",
                "+         }",
                "+ ",
                "+         // Maybe Flush",
                "+         Keyspace ks = Keyspace.open(keyspace());",
                "+         if (flush)",
                "+             FBUtilities.waitOnFutures(ks.flush());",
                "+ ",
                "+         // Verify data",
                "+         verifyData(simple);",
                "+ ",
                "+         // Maybe major compact",
                "+         if (flush)",
                "+         {",
                "+             // Major compact and check data is still present",
                "+             ks.getColumnFamilyStore(currentTable()).forceMajorCompaction();",
                "+ ",
                "+             // Verify data again",
                "+             verifyData(simple);",
                "+         }",
                "+     }",
                "+ ",
                "+     public void baseTestRecoverOverflowedExpiration(boolean runScrub, boolean reinsertOverflowedTTL) throws Throwable",
                "+     {",
                "+         // simple column, clustering",
                "+         testRecoverOverflowedExpirationWithScrub(true, true, runScrub, reinsertOverflowedTTL);",
                "+         // simple column, noclustering",
                "+         testRecoverOverflowedExpirationWithScrub(true, false, runScrub, reinsertOverflowedTTL);",
                "+         // complex column, clustering",
                "+         testRecoverOverflowedExpirationWithScrub(false, true, runScrub, reinsertOverflowedTTL);",
                "+         // complex column, noclustering",
                "+         testRecoverOverflowedExpirationWithScrub(false, false, runScrub, reinsertOverflowedTTL);",
                "+     }",
                "+ ",
                "+     private void verifyData(boolean simple) throws Throwable",
                "+     {",
                "+         if (simple)",
                "+         {",
                "+             assertRows(execute(\"SELECT * from %s\"), row(1, 1, 1), row(2, 2, 2));",
                "+         }",
                "+         else",
                "+         {",
                "+             assertRows(execute(\"SELECT * from %s\"), row(1, 1, set(\"v11\", \"v12\", \"v13\", \"v14\")), row(2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\")));",
                "+         }",
                "+         // Cannot retrieve TTL from collections",
                "+         if (simple)",
                "+             checkTTLIsCapped(\"b\");",
                "+     }",
                "+ ",
                "+     /**",
                "+      * Verify that the computed TTL is approximately equal to the maximum allowed ttl given the",
                "+      * {@link ExpiringCell#getLocalDeletionTime()} field limitation (CASSANDRA-14092)",
                "+      */",
                "+     private void checkTTLIsCapped(String field) throws Throwable",
                "+     {",
                "+ ",
                "+         // TTL is computed dynamically from row expiration time, so if it is",
                "+         // equal or higher to the minimum max TTL we compute before the query",
                "+         // we are fine.",
                "+         int minMaxTTL = computeMaxTTL();",
                "+         UntypedResultSet execute = execute(\"SELECT ttl(\" + field + \") FROM %s\");",
                "+         for (UntypedResultSet.Row row : execute)",
                "+         {",
                "+             int ttl = row.getInt(\"ttl(\" + field + \")\");",
                "+             assertTrue(ttl >= minMaxTTL);",
                "+         }",
                "+     }",
                "+ ",
                "+     /**",
                "+      * The max TTL is computed such that the TTL summed with the current time is equal to the maximum",
                "+      * allowed expiration time {@link BufferExpiringCell#getLocalDeletionTime()} (2038-01-19T03:14:06+00:00)",
                "+      */",
                "+     private int computeMaxTTL()",
                "+     {",
                "+         int nowInSecs = (int) (System.currentTimeMillis() / 1000);",
                "+         return BufferExpiringCell.MAX_DELETION_TIME - nowInSecs;",
                "+     }",
                "+ ",
                "+     public void testRecoverOverflowedExpirationWithScrub(boolean simple, boolean clustering, boolean runScrub, boolean reinsertOverflowedTTL) throws Throwable",
                "+     {",
                "+         if (reinsertOverflowedTTL)",
                "+         {",
                "+             assert runScrub;",
                "+         }",
                "+ ",
                "++        createTable(simple, clustering);",
                "++",
                "+         Keyspace keyspace = Keyspace.open(KEYSPACE);",
                " -        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(getTableName(simple, clustering));",
                "++        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(currentTable());",
                "+ ",
                " -        assertEquals(0, cfs.getLiveSSTableCount());",
                "++        assertEquals(0, cfs.getSSTables().size());",
                "+ ",
                " -        copySSTablesToTableDir(simple, clustering);",
                "++        copySSTablesToTableDir(currentTable(), simple, clustering);",
                "+ ",
                "+         cfs.loadNewSSTables();",
                "+ ",
                "+         if (runScrub)",
                "+         {",
                "+             cfs.scrub(true, false, false, reinsertOverflowedTTL, 1);",
                "+         }",
                "+ ",
                "+         if (reinsertOverflowedTTL)",
                "+         {",
                "+             if (simple)",
                " -            {",
                " -                UntypedResultSet execute = execute(String.format(\"SELECT * from %s.%s\", KEYSPACE, getTableName(simple, clustering)));",
                " -                assertRows(execute, row(1, 1, 1), row(2, 2, 2));",
                " -",
                " -            }",
                "++                assertRows(execute(\"SELECT * from %s\"), row(1, 1, 1), row(2, 2, 2));",
                "+             else",
                " -                assertRows(execute(String.format(\"SELECT * from %s.%s\", KEYSPACE, getTableName(simple, clustering))), row(1, 1, set(\"v11\", \"v12\", \"v13\", \"v14\")), row(2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\")));",
                "++                assertRows(execute(\"SELECT * from %s\"), row(1, 1, set(\"v11\", \"v12\", \"v13\", \"v14\")), row(2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\")));",
                "+ ",
                "+             cfs.forceMajorCompaction();",
                "+ ",
                "+             if (simple)",
                " -                assertRows(execute(String.format(\"SELECT * from %s.%s\", KEYSPACE, getTableName(simple, clustering))), row(1, 1, 1), row(2, 2, 2));",
                "++                assertRows(execute(\"SELECT * from %s\"), row(1, 1, 1), row(2, 2, 2));",
                "+             else",
                " -                assertRows(execute(String.format(\"SELECT * from %s.%s\", KEYSPACE, getTableName(simple, clustering))), row(1, 1, set(\"v11\", \"v12\", \"v13\", \"v14\")), row(2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\")));",
                "++                assertRows(execute(\"SELECT * from %s\"), row(1, 1, set(\"v11\", \"v12\", \"v13\", \"v14\")), row(2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\")));",
                "+         }",
                "+         else",
                "+         {",
                " -            assertEmpty(execute(String.format(\"SELECT * from %s.%s\", KEYSPACE, getTableName(simple, clustering))));",
                "++            assertEmpty(execute(\"SELECT * from %s\"));",
                "+         }",
                " -        cfs.truncateBlocking(); //cleanup for next tests",
                "+     }",
                "+ ",
                " -    private void copySSTablesToTableDir(boolean simple, boolean clustering) throws IOException",
                "++    private void copySSTablesToTableDir(String table, boolean simple, boolean clustering) throws IOException",
                "+     {",
                " -        File destDir = Keyspace.open(KEYSPACE).getColumnFamilyStore(getTableName(simple, clustering)).directories.getCFDirectories().iterator().next();",
                " -        File sourceDir = getTableDir(simple, clustering);",
                "++        File destDir = Keyspace.open(keyspace()).getColumnFamilyStore(table).directories.getCFDirectories().iterator().next();",
                "++        File sourceDir = getTableDir(table, simple, clustering);",
                "+         for (File file : sourceDir.listFiles())",
                "+         {",
                "+             copyFile(file, destDir);",
                "+         }",
                "+     }",
                "+ ",
                " -    private void createTable(boolean simple, boolean clustering) throws Throwable",
                "++    private static File getTableDir(String table, boolean simple, boolean clustering)",
                "++    {",
                "++        return new File(String.format(NEGATIVE_LOCAL_EXPIRATION_TEST_DIR, getTableName(simple, clustering)));",
                "++    }",
                "++",
                "++    private void createTable(boolean simple, boolean clustering)",
                "+     {",
                "+         if (simple)",
                "+         {",
                "+             if (clustering)",
                " -                execute(String.format(\"create table %s.%s (k int, a int, b int, primary key(k, a))\", KEYSPACE, getTableName(simple, clustering)));",
                "++                createTable(\"create table %s (k int, a int, b int, primary key(k, a))\");",
                "+             else",
                " -                execute(String.format(\"create table %s.%s (k int primary key, a int, b int)\", KEYSPACE, getTableName(simple, clustering)));",
                "++                createTable(\"create table %s (k int primary key, a int, b int)\");",
                "+         }",
                "+         else",
                "+         {",
                "+             if (clustering)",
                " -                execute(String.format(\"create table %s.%s (k int, a int, b set<text>, primary key(k, a))\", KEYSPACE, getTableName(simple, clustering)));",
                "++                createTable(\"create table %s (k int, a int, b set<text>, primary key(k, a))\");",
                "+             else",
                " -                execute(String.format(\"create table %s.%s (k int primary key, a int, b set<text>)\", KEYSPACE, getTableName(simple, clustering)));",
                "++                createTable(\"create table %s (k int primary key, a int, b set<text>)\");",
                "+         }",
                "+     }",
                "+ ",
                "+     private static File getTableDir(boolean simple, boolean clustering)",
                "+     {",
                "+         return new File(String.format(NEGATIVE_LOCAL_EXPIRATION_TEST_DIR, getTableName(simple, clustering)));",
                "+     }",
                "+ ",
                "+     private static void copyFile(File src, File dest) throws IOException",
                "+     {",
                "+         byte[] buf = new byte[65536];",
                "+         if (src.isFile())",
                "+         {",
                "+             File target = new File(dest, src.getName());",
                "+             int rd;",
                "+             FileInputStream is = new FileInputStream(src);",
                "+             FileOutputStream os = new FileOutputStream(target);",
                "+             while ((rd = is.read(buf)) >= 0)",
                "+                 os.write(buf, 0, rd);",
                "+         }",
                "+     }",
                "+ ",
                "+     public static String getTableName(boolean simple, boolean clustering)",
                "+     {",
                "+         if (simple)",
                "+             return clustering ? SIMPLE_CLUSTERING : SIMPLE_NOCLUSTERING;",
                "+         else",
                "+             return clustering ? COMPLEX_CLUSTERING : COMPLEX_NOCLUSTERING;",
                "+     }",
                "+ }",
                "diff --cc test/unit/org/apache/cassandra/db/ScrubTest.java",
                "index 4cca7ff0f4,4efd082fcf..9b1ede421d",
                "--- a/test/unit/org/apache/cassandra/db/ScrubTest.java",
                "+++ b/test/unit/org/apache/cassandra/db/ScrubTest.java",
                "@@@ -656,127 -567,2 +656,127 @@@ public class ScrubTes",
                "      }",
                " +",
                " +    @Test /* CASSANDRA-5174 */",
                " +    public void testScrubKeysIndex_preserveOrder() throws IOException, ExecutionException, InterruptedException",
                " +    {",
                " +        //If the partitioner preserves the order then SecondaryIndex uses BytesType comparator,",
                " +        // otherwise it uses LocalByPartitionerType",
                " +        setKeyComparator(BytesType.instance);",
                " +        testScrubIndex(CF_INDEX1, COL_KEYS_INDEX, false, true);",
                " +    }",
                " +",
                " +    @Test /* CASSANDRA-5174 */",
                " +    public void testScrubCompositeIndex_preserveOrder() throws IOException, ExecutionException, InterruptedException",
                " +    {",
                " +        setKeyComparator(BytesType.instance);",
                " +        testScrubIndex(CF_INDEX2, COL_COMPOSITES_INDEX, true, true);",
                " +    }",
                " +",
                " +    @Test /* CASSANDRA-5174 */",
                " +    public void testScrubKeysIndex() throws IOException, ExecutionException, InterruptedException",
                " +    {",
                " +        setKeyComparator(new LocalByPartionerType(StorageService.getPartitioner()));",
                " +        testScrubIndex(CF_INDEX1, COL_KEYS_INDEX, false, true);",
                " +    }",
                " +",
                " +    @Test /* CASSANDRA-5174 */",
                " +    public void testScrubCompositeIndex() throws IOException, ExecutionException, InterruptedException",
                " +    {",
                " +        setKeyComparator(new LocalByPartionerType(StorageService.getPartitioner()));",
                " +        testScrubIndex(CF_INDEX2, COL_COMPOSITES_INDEX, true, true);",
                " +    }",
                " +",
                " +    @Test /* CASSANDRA-5174 */",
                " +    public void testFailScrubKeysIndex() throws IOException, ExecutionException, InterruptedException",
                " +    {",
                " +        testScrubIndex(CF_INDEX1, COL_KEYS_INDEX, false, false);",
                " +    }",
                " +",
                " +    @Test /* CASSANDRA-5174 */",
                " +    public void testFailScrubCompositeIndex() throws IOException, ExecutionException, InterruptedException",
                " +    {",
                " +        testScrubIndex(CF_INDEX2, COL_COMPOSITES_INDEX, true, false);",
                " +    }",
                " +",
                " +    @Test /* CASSANDRA-5174 */",
                " +    public void testScrubTwice() throws IOException, ExecutionException, InterruptedException",
                " +    {",
                " +        testScrubIndex(CF_INDEX1, COL_KEYS_INDEX, false, true, true);",
                " +    }",
                " +",
                " +    /** The SecondaryIndex class is used for custom indexes so to avoid",
                " +     * making a public final field into a private field with getters",
                " +     * and setters, we resort to this hack in order to test it properly",
                " +     * since it can have two values which influence the scrubbing behavior.",
                " +     * @param comparator - the key comparator we want to test",
                " +     */",
                " +    private void setKeyComparator(AbstractType<?> comparator)",
                " +    {",
                " +        try",
                " +        {",
                " +            Field keyComparator = SecondaryIndex.class.getDeclaredField(\"keyComparator\");",
                " +            keyComparator.setAccessible(true);",
                " +            int modifiers = keyComparator.getModifiers();",
                " +            Field modifierField = keyComparator.getClass().getDeclaredField(\"modifiers\");",
                " +            modifiers = modifiers & ~Modifier.FINAL;",
                " +            modifierField.setAccessible(true);",
                " +            modifierField.setInt(keyComparator, modifiers);",
                " +",
                " +            keyComparator.set(null, comparator);",
                " +        }",
                " +        catch (Exception ex)",
                " +        {",
                " +            fail(\"Failed to change key comparator in secondary index : \" + ex.getMessage());",
                " +            ex.printStackTrace();",
                " +        }",
                " +    }",
                " +",
                " +    private void testScrubIndex(String cfName, String colName, boolean composite, boolean ... scrubs)",
                " +            throws IOException, ExecutionException, InterruptedException",
                " +    {",
                " +        CompactionManager.instance.disableAutoCompaction();",
                " +        Keyspace keyspace = Keyspace.open(KEYSPACE);",
                " +        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(cfName);",
                " +        cfs.clearUnsafe();",
                " +",
                " +        int numRows = 1000;",
                " +        long[] colValues = new long [numRows * 2]; // each row has two columns",
                " +        for (int i = 0; i < colValues.length; i+=2)",
                " +        {",
                " +            colValues[i] = (i % 4 == 0 ? 1L : 2L); // index column",
                " +            colValues[i+1] = 3L; //other column",
                " +        }",
                " +        fillIndexCF(cfs, composite, colValues);",
                " +",
                " +        // check index",
                " +        IndexExpression expr = new IndexExpression(ByteBufferUtil.bytes(colName), Operator.EQ, ByteBufferUtil.bytes(1L));",
                " +        List<Row> rows = cfs.search(Util.range(\"\", \"\"), Arrays.asList(expr), new IdentityQueryFilter(), numRows);",
                " +        assertNotNull(rows);",
                " +        assertEquals(numRows / 2, rows.size());",
                " +",
                " +        // scrub index",
                " +        Set<ColumnFamilyStore> indexCfss = cfs.indexManager.getIndexesBackedByCfs();",
                " +        assertTrue(indexCfss.size() == 1);",
                " +        for(ColumnFamilyStore indexCfs : indexCfss)",
                " +        {",
                " +            for (int i = 0; i < scrubs.length; i++)",
                " +            {",
                " +                boolean failure = !scrubs[i];",
                " +                if (failure)",
                " +                { //make sure the next scrub fails",
                " +                    overrideWithGarbage(indexCfs.getSSTables().iterator().next(), ByteBufferUtil.bytes(1L), ByteBufferUtil.bytes(2L));",
                " +                }",
                "-                 CompactionManager.AllSSTableOpStatus result = indexCfs.scrub(false, false, true, true, 0);",
                "++                CompactionManager.AllSSTableOpStatus result = indexCfs.scrub(false, false, true, true, true, 0);",
                " +                assertEquals(failure ?",
                " +                             CompactionManager.AllSSTableOpStatus.ABORTED :",
                " +                             CompactionManager.AllSSTableOpStatus.SUCCESSFUL,",
                " +                                result);",
                " +            }",
                " +        }",
                " +",
                " +",
                " +        // check index is still working",
                " +        rows = cfs.search(Util.range(\"\", \"\"), Arrays.asList(expr), new IdentityQueryFilter(), numRows);",
                " +        assertNotNull(rows);",
                " +        assertEquals(numRows / 2, rows.size());",
                " +    }",
                "  }"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-2.2.12",
                "cassandra-2.2.13",
                "cassandra-2.2.14",
                "cassandra-2.2.15",
                "cassandra-2.2.16",
                "cassandra-2.2.17",
                "cassandra-2.2.18",
                "cassandra-2.2.19",
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "716d08f9090deb4c0c48096b5044186d270feb45",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516371611,
            "hunks": 0,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "b8c12fba064fb0b6a3b6306b2670497434471920",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516627330,
            "hunks": 1,
            "message": "Merge branch 'cassandra-3.0' into cassandra-3.11",
            "diff": [
                "diff --cc CHANGES.txt",
                "index 5e4c40a09a,0eaab6ee1e..81b358d158",
                "--- a/CHANGES.txt",
                "+++ b/CHANGES.txt",
                "@@@ -47,19 -28,7 +47,20 @@@ Merged from 2.2",
                "  Merged from 2.1:",
                " - * More PEP8 compliance for cqlsh (CASSANDRA-14021)",
                "+  * RPM package spec: fix permissions for installed jars and config files (CASSANDRA-14181)",
                " -",
                " -",
                " -3.0.15",
                " + * More PEP8 compiance for cqlsh (CASSANDRA-14021)",
                " +",
                " +",
                " +3.11.1",
                " + * Fix the computation of cdc_total_space_in_mb for exabyte filesystems (CASSANDRA-13808)",
                " + * AbstractTokenTreeBuilder#serializedSize returns wrong value when there is a single leaf and overflow collisions (CASSANDRA-13869)",
                " + * Add a compaction option to TWCS to ignore sstables overlapping checks (CASSANDRA-13418)",
                " + * BTree.Builder memory leak (CASSANDRA-13754)",
                " + * Revert CASSANDRA-10368 of supporting non-pk column filtering due to correctness (CASSANDRA-13798)",
                " + * Add a skip read validation flag to cassandra-stress (CASSANDRA-13772)",
                " + * Fix cassandra-stress hang issues when an error during cluster connection happens (CASSANDRA-12938)",
                " + * Better bootstrap failure message when blocked by (potential) range movement (CASSANDRA-13744)",
                " + * \"ignore\" option is ignored in sstableloader (CASSANDRA-13721)",
                " + * Deadlock in AbstractCommitLogSegmentManager (CASSANDRA-13652)",
                " + * Duplicate the buffer before passing it to analyser in SASI operation (CASSANDRA-13512)",
                " + * Properly evict pstmts from prepared statements cache (CASSANDRA-13641)",
                " +Merged from 3.0:",
                "   * Improve TRUNCATE performance (CASSANDRA-13909)"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "5c3e33b17c59a1f8e4eb8eee60cf4ea59accb6e0",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517991033,
            "hunks": 15,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [
                "diff --cc src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "index 046ae2969c,e158982db5..c5e59581bf",
                "--- a/src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "@@@ -412,3 -400,3 +412,3 @@@ public class CompactionManager implemen",
                "              @Override",
                "--            public void execute(LifecycleTransaction input) throws IOException",
                "++            public void execute(LifecycleTransaction input)",
                "              {",
                "@@@ -987,3 -988,3 +987,3 @@@",
                "--    private void verifyOne(ColumnFamilyStore cfs, SSTableReader sstable, boolean extendedVerify) throws IOException",
                "++    private void verifyOne(ColumnFamilyStore cfs, SSTableReader sstable, boolean extendedVerify)",
                "      {",
                "diff --cc src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "index af009150c5,44a585078b..5a2a82809c",
                "--- a/src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "@@@ -89,3 -88,3 +89,3 @@@ public class Verifier implements Closea",
                "--    public void verify(boolean extended) throws IOException",
                "++    public void verify(boolean extended)",
                "      {",
                "@@@ -250,3 -250,3 +250,3 @@@",
                "--    private void markAndThrow() throws IOException",
                "++    private void markAndThrow()",
                "      {",
                "@@@ -255,6 -255,15 +255,15 @@@",
                "--    private void markAndThrow(boolean mutateRepaired) throws IOException",
                "++    private void markAndThrow(boolean mutateRepaired)",
                "      {",
                "          if (mutateRepaired) // if we are able to mutate repaired flag, an incremental repair should be enough",
                "-             sstable.descriptor.getMetadataSerializer().mutateRepaired(sstable.descriptor, ActiveRepairService.UNREPAIRED_SSTABLE, sstable.getSSTableMetadata().pendingRepair);",
                "+         {",
                "+             try",
                "+             {",
                " -                sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, ActiveRepairService.UNREPAIRED_SSTABLE);",
                "++                sstable.descriptor.getMetadataSerializer().mutateRepaired(sstable.descriptor, ActiveRepairService.UNREPAIRED_SSTABLE, sstable.getSSTableMetadata().pendingRepair);",
                "+             }",
                "+             catch(IOException ioe)",
                "+             {",
                "+                 outputHandler.output(\"Error mutating repairedAt for SSTable \" +  sstable.getFilename() + \", as part of markAndThrow\");",
                "+             }",
                "+         }",
                "          throw new CorruptSSTableException(new Exception(String.format(\"Invalid SSTable %s, please force %srepair\", sstable.getFilename(), mutateRepaired ? \"\" : \"a full \")), sstable.getFilename());",
                "diff --cc test/unit/org/apache/cassandra/db/VerifyTest.java",
                "index 94fb9ec582,6808c47f39..2eb741cf23",
                "--- a/test/unit/org/apache/cassandra/db/VerifyTest.java",
                "+++ b/test/unit/org/apache/cassandra/db/VerifyTest.java",
                "@@@ -99,3 -96,3 +99,3 @@@ public class VerifyTes",
                "      @Test",
                "--    public void testVerifyCorrect() throws IOException",
                "++    public void testVerifyCorrect()",
                "      {",
                "@@@ -120,3 -117,3 +120,3 @@@",
                "      @Test",
                "--    public void testVerifyCounterCorrect() throws IOException",
                "++    public void testVerifyCounterCorrect()",
                "      {",
                "@@@ -141,3 -138,3 +141,3 @@@",
                "      @Test",
                "--    public void testExtendedVerifyCorrect() throws IOException",
                "++    public void testExtendedVerifyCorrect()",
                "      {",
                "@@@ -162,3 -159,3 +162,3 @@@",
                "      @Test",
                "--    public void testExtendedVerifyCounterCorrect() throws IOException",
                "++    public void testExtendedVerifyCounterCorrect()",
                "      {",
                "@@@ -183,3 -180,3 +183,3 @@@",
                "      @Test",
                "--    public void testVerifyCorrectUncompressed() throws IOException",
                "++    public void testVerifyCorrectUncompressed()",
                "      {",
                "@@@ -204,3 -201,3 +204,3 @@@",
                "      @Test",
                "--    public void testVerifyCounterCorrectUncompressed() throws IOException",
                "++    public void testVerifyCounterCorrectUncompressed()",
                "      {",
                "@@@ -225,3 -222,3 +225,3 @@@",
                "      @Test",
                "--    public void testExtendedVerifyCorrectUncompressed() throws IOException",
                "++    public void testExtendedVerifyCorrectUncompressed()",
                "      {",
                "@@@ -246,3 -243,3 +246,3 @@@",
                "      @Test",
                "--    public void testExtendedVerifyCounterCorrectUncompressed() throws IOException",
                "++    public void testExtendedVerifyCounterCorrectUncompressed()",
                "      {"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "161c37da8fa93f572e3c9474cb37fc6d96058ff6",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518189984,
            "hunks": 10,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [
                "diff --cc CHANGES.txt",
                "index e8602659d0,c38b69b171..0f7b7fef5a",
                "--- a/CHANGES.txt",
                "+++ b/CHANGES.txt",
                "@@@ -209,3 -21,5 +209,4 @@@",
                "   * Avoid locks when checking LCS fanout and if we should defrag (CASSANDRA-13930)",
                " - * Correctly count range tombstones in traces and tombstone thresholds (CASSANDRA-8527)",
                "  Merged from 3.0:",
                "+  * Use the correct digest file and reload sstable metadata in nodetool verify (CASSANDRA-14217)",
                "   * Handle failure when mutating repaired status in Verifier (CASSANDRA-13933)",
                "diff --cc src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "index 5a2a82809c,f0309b7829..01e465ef32",
                "--- a/src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/Verifier.java",
                "@@@ -261,3 -261,5 +261,5 @@@ public class Verifier implements Closea",
                "              {",
                " -                sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, ActiveRepairService.UNREPAIRED_SSTABLE);",
                " +                sstable.descriptor.getMetadataSerializer().mutateRepaired(sstable.descriptor, ActiveRepairService.UNREPAIRED_SSTABLE, sstable.getSSTableMetadata().pendingRepair);",
                "+                 sstable.reloadSSTableMetadata();",
                "+                 cfs.getTracker().notifySSTableRepairedStatusChanged(Collections.singleton(sstable));",
                "              }",
                "diff --cc test/unit/org/apache/cassandra/db/VerifyTest.java",
                "index 2eb741cf23,a332f743df..8b9b437ab7",
                "--- a/test/unit/org/apache/cassandra/db/VerifyTest.java",
                "+++ b/test/unit/org/apache/cassandra/db/VerifyTest.java",
                "@@@ -46,2 -48,5 +47,4 @@@ import java.io.*",
                "  import java.nio.file.Files;",
                "+ import java.util.Collections;",
                " -import java.util.List;",
                "+ import java.util.concurrent.ExecutionException;",
                "  import java.util.zip.CRC32;",
                "@@@ -49,6 -54,4 +52,8 @@@ import java.util.zip.CheckedInputStream",
                " +import static org.apache.cassandra.SchemaLoader.counterCFMD;",
                " +import static org.apache.cassandra.SchemaLoader.createKeyspace;",
                " +import static org.apache.cassandra.SchemaLoader.loadSchema;",
                " +import static org.apache.cassandra.SchemaLoader.standardCFMD;",
                "+ import static org.junit.Assert.assertFalse;",
                "+ import static org.junit.Assert.assertTrue;",
                "  import static org.junit.Assert.fail;",
                "@@@ -377,2 -380,35 +382,35 @@@ public class VerifyTes",
                "+     @Test",
                "+     public void testMutateRepair() throws IOException, ExecutionException, InterruptedException",
                "+     {",
                "+         CompactionManager.instance.disableAutoCompaction();",
                "+         Keyspace keyspace = Keyspace.open(KEYSPACE);",
                "+         ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(CORRUPT_CF2);",
                "+ ",
                "+         fillCF(cfs, 2);",
                "+ ",
                "+         SSTableReader sstable = cfs.getLiveSSTables().iterator().next();",
                " -        sstable.descriptor.getMetadataSerializer().mutateRepairedAt(sstable.descriptor, 1);",
                "++        sstable.descriptor.getMetadataSerializer().mutateRepaired(sstable.descriptor, 1, sstable.getSSTableMetadata().pendingRepair);",
                "+         sstable.reloadSSTableMetadata();",
                "+         cfs.getTracker().notifySSTableRepairedStatusChanged(Collections.singleton(sstable));",
                "+         assertTrue(sstable.isRepaired());",
                "+         cfs.forceMajorCompaction();",
                "+ ",
                "+         sstable = cfs.getLiveSSTables().iterator().next();",
                "+         Long correctChecksum;",
                " -        try (RandomAccessFile file = new RandomAccessFile(sstable.descriptor.filenameFor(sstable.descriptor.digestComponent), \"rw\"))",
                "++        try (RandomAccessFile file = new RandomAccessFile(sstable.descriptor.filenameFor(Component.DIGEST), \"rw\"))",
                "+         {",
                "+             correctChecksum = Long.parseLong(file.readLine());",
                "+         }",
                " -        writeChecksum(++correctChecksum, sstable.descriptor.filenameFor(sstable.descriptor.digestComponent));",
                "++        writeChecksum(++correctChecksum, sstable.descriptor.filenameFor(Component.DIGEST));",
                "+         try (Verifier verifier = new Verifier(cfs, sstable, false))",
                "+         {",
                "+             verifier.verify(false);",
                "+             fail(\"should be corrupt\");",
                "+         }",
                "+         catch (CorruptSSTableException e)",
                "+         {}",
                "+         assertFalse(sstable.isRepaired());",
                "+     }",
                "+ ",
                "diff --cc test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java",
                "index 8fc69bfdbb,478327dbf8..f669cd7d75",
                "--- a/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java",
                "+++ b/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java",
                "@@@ -195,3 -169,59 +196,20 @@@ public class LegacySSTableTes",
                "      }",
                " -    @Test",
                " -    public void testReverseIterationOfLegacyIndexedSSTable() throws Exception",
                " -    {",
                " -        // During upgrades from 2.1 to 3.0, reverse queries can drop rows before upgradesstables is completed",
                " -        QueryProcessor.executeInternal(\"CREATE TABLE legacy_tables.legacy_ka_indexed (\" +",
                " -                                       \"  p int,\" +",
                " -                                       \"  c int,\" +",
                " -                                       \"  v1 int,\" +",
                " -                                       \"  v2 int,\" +",
                " -                                       \"  PRIMARY KEY(p, c)\" +",
                " -                                       \")\");",
                " -        loadLegacyTable(\"legacy_%s_indexed%s\", \"ka\", \"\");",
                " -        UntypedResultSet rs = QueryProcessor.executeInternal(\"SELECT * \" +",
                " -                                                             \"FROM legacy_tables.legacy_ka_indexed \" +",
                " -                                                             \"WHERE p=1 \" +",
                " -                                                             \"ORDER BY c DESC\");",
                " -        Assert.assertEquals(5000, rs.size());",
                " -    }",
                " -",
                " -    @Test",
                " -    public void testReadingLegacyIndexedSSTableWithStaticColumns() throws Exception",
                " -    {",
                " -        // During upgrades from 2.1 to 3.0, reading from tables with static columns errors before upgradesstables",
                " -        // is completed",
                " -        QueryProcessor.executeInternal(\"CREATE TABLE legacy_tables.legacy_ka_indexed_static (\" +",
                " -                                       \"  p int,\" +",
                " -                                       \"  c int,\" +",
                " -                                       \"  v1 int,\" +",
                " -                                       \"  v2 int,\" +",
                " -                                       \"  s1 int static,\" +",
                " -                                       \"  s2 int static,\" +",
                " -                                       \"  PRIMARY KEY(p, c)\" +",
                " -                                       \")\");",
                " -        loadLegacyTable(\"legacy_%s_indexed_static%s\", \"ka\", \"\");",
                " -        UntypedResultSet rs = QueryProcessor.executeInternal(\"SELECT * \" +",
                " -                                                             \"FROM legacy_tables.legacy_ka_indexed_static \" +",
                " -                                                             \"WHERE p=1 \");",
                " -        Assert.assertEquals(5000, rs.size());",
                " -    }",
                "+     @Test",
                "+     public void verifyOldSSTables() throws Exception",
                "+     {",
                "+         for (String legacyVersion : legacyVersions)",
                "+         {",
                "+             loadLegacyTables(legacyVersion);",
                "+             ColumnFamilyStore cfs = Keyspace.open(\"legacy_tables\").getColumnFamilyStore(String.format(\"legacy_%s_simple\", legacyVersion));",
                "+             for (SSTableReader sstable : cfs.getLiveSSTables())",
                "+             {",
                "+                 try (Verifier verifier = new Verifier(cfs, sstable, false))",
                "+                 {",
                "+                     verifier.verify(true);",
                "+                 }",
                "+             }",
                "+         }",
                "+     }",
                "+ ",
                "      private void streamLegacyTables(String legacyVersion) throws Exception"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "d369190964be9a6011f707a7e584446f9ec57a73",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517990779,
            "hunks": 1,
            "message": "Merge branch 'cassandra-3.0' into cassandra-3.11",
            "diff": [
                "diff --cc CHANGES.txt",
                "index 30ca8a8bf0,4e8f2ac9ec..8c0d8f048a",
                "--- a/CHANGES.txt",
                "+++ b/CHANGES.txt",
                "@@@ -6,18 -3,2 +6,19 @@@",
                "   * Close socket on error during connect on OutboundTcpConnection (CASSANDRA-9630)",
                " + * Enable CDC unittest (CASSANDRA-14141)",
                " + * Acquire read lock before accessing CompactionStrategyManager fields (CASSANDRA-14139)",
                " + * Split CommitLogStressTest to avoid timeout (CASSANDRA-14143)",
                " + * Avoid invalidating disk boundaries unnecessarily (CASSANDRA-14083)",
                " + * Avoid exposing compaction strategy index externally (CASSANDRA-14082)",
                " + * Prevent continuous schema exchange between 3.0 and 3.11 nodes (CASSANDRA-14109)",
                " + * Fix imbalanced disks when replacing node with same address with JBOD (CASSANDRA-14084)",
                " + * Reload compaction strategies when disk boundaries are invalidated (CASSANDRA-13948)",
                " + * Remove OpenJDK log warning (CASSANDRA-13916)",
                " + * Prevent compaction strategies from looping indefinitely (CASSANDRA-14079)",
                " + * Cache disk boundaries (CASSANDRA-13215)",
                " + * Add asm jar to build.xml for maven builds (CASSANDRA-11193)",
                " + * Round buffer size to powers of 2 for the chunk cache (CASSANDRA-13897)",
                " + * Update jackson JSON jars (CASSANDRA-13949)",
                " + * Avoid locks when checking LCS fanout and if we should defrag (CASSANDRA-13930)",
                " +Merged from 3.0:",
                "++ * Handle failure when mutating repaired status in Verifier (CASSANDRA-13933)",
                "   * Set encoding for javadoc generation (CASSANDRA-14154)"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "aa831c98f7a6268046915036bdc2d2f3b537dd78",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516627104,
            "hunks": 0,
            "message": "Merge branch 'cassandra-2.1' into cassandra-2.2",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-2.2.12",
                "cassandra-2.2.13",
                "cassandra-2.2.14",
                "cassandra-2.2.15",
                "cassandra-2.2.16",
                "cassandra-2.2.17",
                "cassandra-2.2.18",
                "cassandra-2.2.19",
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "a4cf29fe95f5f93896a31683c57cdd9bed0447b9",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518189479,
            "hunks": 0,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "71445706c49025e77502cb3a79257fb70583fbc2",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517573069,
            "hunks": 0,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "59a4624d5f9b2c414b200e65b45beed9c5f4db52",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517516981,
            "hunks": 1,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [
                "diff --cc CHANGES.txt",
                "index a2e36542dc,2d7d8f7515..91b3ed8ee2",
                "--- a/CHANGES.txt",
                "+++ b/CHANGES.txt",
                "@@@ -187,3 -1,6 +187,4 @@@",
                "  3.11.2",
                "+  * Add DEFAULT, UNSET, MBEAN and MBEANS to `ReservedKeywords` (CASSANDRA-14205)",
                " - * Add Unittest for schema migration fix (CASSANDRA-14140)",
                "   * Print correct snitch info from nodetool describecluster (CASSANDRA-13528)",
                " - * Close socket on error during connect on OutboundTcpConnection (CASSANDRA-9630)",
                "   * Enable CDC unittest (CASSANDRA-14141)"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "0a6b6f506b012d4d491757d6216c5fa1c53bedc9",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518281893,
            "hunks": 39,
            "message": "Merge branch 'cassandra-3.0' into cassandra-3.11",
            "diff": [
                "diff --cc CHANGES.txt",
                "index c38b69b171,a492c42a04..5b49f486eb",
                "--- a/CHANGES.txt",
                "+++ b/CHANGES.txt",
                "@@@ -52,2 -31,3 +52,3 @@@ Merged from 2.2",
                "  Merged from 2.1:",
                " - * More PEP8 compliance for cqlsh (CASSANDRA-14021)",
                "++ * Protect against overflow of local expiration time (CASSANDRA-14092)",
                "   * RPM package spec: fix permissions for installed jars and config files (CASSANDRA-14181)",
                "diff --cc NEWS.txt",
                "index f4b15e70f9,f574c339bc..fb1dafe96e",
                "--- a/NEWS.txt",
                "+++ b/NEWS.txt",
                "@@@ -20,18 -40,10 +40,19 @@@ Upgradin",
                "  ---------",
                "+    - See MAXIMUM TTL EXPIRATION DATE NOTICE above.",
                " -   - Cassandra is now relying on the JVM options to properly shutdown on OutOfMemoryError. By default it will",
                " -     rely on the OnOutOfMemoryError option as the ExitOnOutOfMemoryError and CrashOnOutOfMemoryError options",
                " -     are not supported by the older 1.7 and 1.8 JVMs. A warning will be logged at startup if none of those JVM",
                " -     options are used. See CASSANDRA-13006 for more details.",
                " -   - Cassandra is not logging anymore by default an Heap histogram on OutOfMemoryError. To enable that behavior",
                " -     set the 'cassandra.printHeapHistogramOnOutOfMemoryError' System property to 'true'. See CASSANDRA-13006",
                " -     for more details.",
                " +    - Cassandra is now relying on the JVM options to properly shutdown on OutOfMemoryError. By default it will",
                " +      rely on the OnOutOfMemoryError option as the ExitOnOutOfMemoryError and CrashOnOutOfMemoryError options",
                " +      are not supported by the older 1.7 and 1.8 JVMs. A warning will be logged at startup if none of those JVM",
                " +      options are used. See CASSANDRA-13006 for more details",
                " +    - Cassandra is not logging anymore by default an Heap histogram on OutOfMemoryError. To enable that behavior",
                " +      set the 'cassandra.printHeapHistogramOnOutOfMemoryError' System property to 'true'. See CASSANDRA-13006",
                " +      for more details.",
                " +    - Upgrades from 3.0 might have produced unnecessary schema migrations while",
                " +      there was at least one 3.0 node in the cluster. It is therefore highly",
                " +      recommended to upgrade from 3.0 to at least 3.11.2. The root cause of",
                " +      this schema mismatch was a difference in the way how schema digests were computed",
                " +      in 3.0 and 3.11.2. To mitigate this issue, 3.11.2 and newer announce",
                " +      3.0 compatible digests as long as there is at least one 3.0 node in the",
                " +      cluster. Once all nodes have been upgraded, the \"real\" schema version will be",
                " +      announced. Note: this fix is only necessary in 3.11.2 and therefore only applies",
                " +      to 3.11. (CASSANDRA-14109)",
                "diff --cc src/java/org/apache/cassandra/cql3/Attributes.java",
                "index d915560c5a,832d0a780b..d4e230f78b",
                "--- a/src/java/org/apache/cassandra/cql3/Attributes.java",
                "+++ b/src/java/org/apache/cassandra/cql3/Attributes.java",
                "@@@ -23,3 -24,3 +24,4 @@@ import org.apache.cassandra.config.CFMe",
                "  import org.apache.cassandra.cql3.functions.Function;",
                "+ import org.apache.cassandra.db.ExpirationDateOverflowHandling;",
                " +import org.apache.cassandra.db.LivenessInfo;",
                "  import org.apache.cassandra.db.marshal.Int32Type;",
                "@@@ -109,5 -116,2 +114,5 @@@ public class Attribute",
                " +        if (tval == ByteBufferUtil.UNSET_BYTE_BUFFER)",
                "-             return defaultTimeToLive;",
                "++            return metadata.params.defaultTimeToLive;",
                " +",
                "          try",
                "@@@ -128,5 -132,4 +133,7 @@@",
                "-         if (defaultTimeToLive != LivenessInfo.NO_TTL && ttl == LivenessInfo.NO_TTL)",
                "++        if (metadata.params.defaultTimeToLive != LivenessInfo.NO_TTL && ttl == LivenessInfo.NO_TTL)",
                " +            return LivenessInfo.NO_TTL;",
                " +",
                "+         ExpirationDateOverflowHandling.maybeApplyExpirationDateOverflowPolicy(metadata, ttl, false);",
                "+ ",
                "          return ttl;",
                "diff --cc src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "index ed107d768e,bc1150430f..f8fa548eb7",
                "--- a/src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "@@@ -71,6 -75,6 +75,6 @@@ public class Scrubber implements Closea",
                "      {",
                "--         public int compare(Partition r1, Partition r2)",
                "--         {",
                "--             return r1.partitionKey().compareTo(r2.partitionKey());",
                "--         }",
                "++        public int compare(Partition r1, Partition r2)",
                "++        {",
                "++            return r1.partitionKey().compareTo(r2.partitionKey());",
                "++        }",
                "      };",
                "@@@ -114,4 -128,4 +126,4 @@@",
                "          this.expectedBloomFilterSize = Math.max(",
                "--            cfs.metadata.params.minIndexInterval,",
                "--            hasIndexFile ? SSTableReader.getApproximateKeyCount(toScrub) : 0);",
                "++        cfs.metadata.params.minIndexInterval,",
                "++        hasIndexFile ? SSTableReader.getApproximateKeyCount(toScrub) : 0);",
                "@@@ -126,4 -140,4 +138,4 @@@",
                "          this.indexFile = hasIndexFile",
                "--                ? RandomAccessReader.open(new File(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX)))",
                "--                : null;",
                "++                         ? RandomAccessReader.open(new File(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX)))",
                "++                         : null;",
                "@@@ -205,4 -222,4 +220,4 @@@",
                "                          throw new IOError(new IOException(String.format(\"Key from data file (%s) does not match key from index file (%s)\",",
                "--                                //ByteBufferUtil.bytesToHex(key.getKey()), ByteBufferUtil.bytesToHex(currentIndexKey))));",
                "--                                \"_too big_\", ByteBufferUtil.bytesToHex(currentIndexKey))));",
                "++                                                                        //ByteBufferUtil.bytesToHex(key.getKey()), ByteBufferUtil.bytesToHex(currentIndexKey))));",
                "++                                                                        \"_too big_\", ByteBufferUtil.bytesToHex(currentIndexKey))));",
                "                      }",
                "@@@ -227,3 -244,3 +242,3 @@@",
                "                          outputHandler.output(String.format(\"Retrying from row index; data is %s bytes starting at %s\",",
                "--                                                  dataSizeFromIndex, dataStartFromIndex));",
                "++                                                           dataSizeFromIndex, dataStartFromIndex));",
                "                          key = sstable.decorateKey(currentIndexKey);",
                "@@@ -335,2 -354,14 +352,14 @@@",
                "+     /**",
                "+      * Only wrap with {@link FixNegativeLocalDeletionTimeIterator} if {@link #reinsertOverflowedTTLRows} option",
                "+      * is specified",
                "+      */",
                "+     private UnfilteredRowIterator getIterator(DecoratedKey key)",
                "+     {",
                " -        RowMergingSSTableIterator rowMergingIterator = new RowMergingSSTableIterator(sstable, dataFile, key);",
                "++        RowMergingSSTableIterator rowMergingIterator = new RowMergingSSTableIterator(SSTableIdentityIterator.create(sstable, dataFile, key));",
                "+         return reinsertOverflowedTTLRows ? new FixNegativeLocalDeletionTimeIterator(rowMergingIterator,",
                "+                                                                                     outputHandler,",
                "+                                                                                     negativeLocalDeletionInfoMetrics) : rowMergingIterator;",
                "+     }",
                "+ ",
                "      private void updateIndexKey()",
                "@@@ -344,4 -375,4 +373,4 @@@",
                "              nextRowPositionFromIndex = !indexAvailable()",
                "--                    ? dataFile.length()",
                "-                     : rowIndexEntrySerializer.deserializePositionAndSkip(indexFile);",
                " -                    : rowIndexEntrySerializer.deserialize(indexFile).position;",
                "++                                       ? dataFile.length()",
                "++                                       : rowIndexEntrySerializer.deserializePositionAndSkip(indexFile);",
                "          }",
                "@@@ -624,2 -802,2 +804,3 @@@",
                "      }",
                "++",
                "  }",
                "diff --cc src/java/org/apache/cassandra/db/rows/BufferCell.java",
                "index 9b31c16c4f,df2619c7ce..b62d95aaec",
                "--- a/src/java/org/apache/cassandra/db/rows/BufferCell.java",
                "+++ b/src/java/org/apache/cassandra/db/rows/BufferCell.java",
                "@@@ -21,3 -22,5 +21,4 @@@ import java.nio.ByteBuffer",
                " -import org.apache.cassandra.config.*;",
                " -import org.apache.cassandra.db.*;",
                " -import org.apache.cassandra.db.context.CounterContext;",
                " +import org.apache.cassandra.config.ColumnDefinition;",
                "++import org.apache.cassandra.db.ExpirationDateOverflowHandling;",
                "  import org.apache.cassandra.db.marshal.ByteType;",
                "diff --cc src/java/org/apache/cassandra/db/rows/Cell.java",
                "index 19d1f30fd0,c69e11f65c..1205b7d801",
                "--- a/src/java/org/apache/cassandra/db/rows/Cell.java",
                "+++ b/src/java/org/apache/cassandra/db/rows/Cell.java",
                "@@@ -23,3 -23,9 +23,10 @@@ import java.util.Comparator",
                "+ import com.google.common.annotations.VisibleForTesting;",
                "+ import org.slf4j.Logger;",
                "+ import org.slf4j.LoggerFactory;",
                "+ ",
                "+ import org.apache.cassandra.config.CFMetaData;",
                "+ import org.apache.cassandra.config.ColumnDefinition;",
                "+ import org.apache.cassandra.cql3.Attributes;",
                " +import org.apache.cassandra.config.*;",
                "  import org.apache.cassandra.db.*;",
                "diff --cc src/java/org/apache/cassandra/db/rows/NativeCell.java",
                "index 593033206a,0000000000..31ce0b7700",
                "mode 100644,000000..100644",
                "--- a/src/java/org/apache/cassandra/db/rows/NativeCell.java",
                "+++ b/src/java/org/apache/cassandra/db/rows/NativeCell.java",
                "@@@ -1,156 -1,0 +1,161 @@@",
                " +/*",
                " + * Licensed to the Apache Software Foundation (ASF) under one",
                " + * or more contributor license agreements.  See the NOTICE file",
                " + * distributed with this work for additional information",
                " + * regarding copyright ownership.  The ASF licenses this file",
                " + * to you under the Apache License, Version 2.0 (the",
                " + * \"License\"); you may not use this file except in compliance",
                " + * with the License.  You may obtain a copy of the License at",
                " + *",
                " + *     http://www.apache.org/licenses/LICENSE-2.0",
                " + *",
                " + * Unless required by applicable law or agreed to in writing, software",
                " + * distributed under the License is distributed on an \"AS IS\" BASIS,",
                " + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                " + * See the License for the specific language governing permissions and",
                " + * limitations under the License.",
                " + */",
                " +package org.apache.cassandra.db.rows;",
                " +",
                " +import java.nio.ByteBuffer;",
                " +import java.nio.ByteOrder;",
                " +",
                " +import org.apache.cassandra.config.ColumnDefinition;",
                " +import org.apache.cassandra.utils.ObjectSizes;",
                " +import org.apache.cassandra.utils.concurrent.OpOrder;",
                " +import org.apache.cassandra.utils.memory.MemoryUtil;",
                " +import org.apache.cassandra.utils.memory.NativeAllocator;",
                " +",
                " +public class NativeCell extends AbstractCell",
                " +{",
                " +    private static final long EMPTY_SIZE = ObjectSizes.measure(new NativeCell());",
                " +",
                " +    private static final long HAS_CELLPATH = 0;",
                " +    private static final long TIMESTAMP = 1;",
                " +    private static final long TTL = 9;",
                " +    private static final long DELETION = 13;",
                " +    private static final long LENGTH = 17;",
                " +    private static final long VALUE = 21;",
                " +",
                " +    private final long peer;",
                " +",
                " +    private NativeCell()",
                " +    {",
                " +        super(null);",
                " +        this.peer = 0;",
                " +    }",
                " +",
                " +    public NativeCell(NativeAllocator allocator,",
                " +                      OpOrder.Group writeOp,",
                " +                      Cell cell)",
                " +    {",
                " +        this(allocator,",
                " +             writeOp,",
                " +             cell.column(),",
                " +             cell.timestamp(),",
                " +             cell.ttl(),",
                " +             cell.localDeletionTime(),",
                " +             cell.value(),",
                " +             cell.path());",
                " +    }",
                " +",
                " +    public NativeCell(NativeAllocator allocator,",
                " +                      OpOrder.Group writeOp,",
                " +                      ColumnDefinition column,",
                " +                      long timestamp,",
                " +                      int ttl,",
                " +                      int localDeletionTime,",
                " +                      ByteBuffer value,",
                " +                      CellPath path)",
                " +    {",
                " +        super(column);",
                " +        long size = simpleSize(value.remaining());",
                " +",
                " +        assert value.order() == ByteOrder.BIG_ENDIAN;",
                " +        assert column.isComplex() == (path != null);",
                " +        if (path != null)",
                " +        {",
                " +            assert path.size() == 1;",
                " +            size += 4 + path.get(0).remaining();",
                " +        }",
                " +",
                " +        if (size > Integer.MAX_VALUE)",
                " +            throw new IllegalStateException();",
                " +",
                " +        // cellpath? : timestamp : ttl : localDeletionTime : length : <data> : [cell path length] : [<cell path data>]",
                " +        peer = allocator.allocate((int) size, writeOp);",
                " +        MemoryUtil.setByte(peer + HAS_CELLPATH, (byte)(path == null ? 0 : 1));",
                " +        MemoryUtil.setLong(peer + TIMESTAMP, timestamp);",
                " +        MemoryUtil.setInt(peer + TTL, ttl);",
                " +        MemoryUtil.setInt(peer + DELETION, localDeletionTime);",
                " +        MemoryUtil.setInt(peer + LENGTH, value.remaining());",
                " +        MemoryUtil.setBytes(peer + VALUE, value);",
                " +",
                " +        if (path != null)",
                " +        {",
                " +            ByteBuffer pathbuffer = path.get(0);",
                " +            assert pathbuffer.order() == ByteOrder.BIG_ENDIAN;",
                " +",
                " +            long offset = peer + VALUE + value.remaining();",
                " +            MemoryUtil.setInt(offset, pathbuffer.remaining());",
                " +            MemoryUtil.setBytes(offset + 4, pathbuffer);",
                " +        }",
                " +    }",
                " +",
                " +    private static long simpleSize(int length)",
                " +    {",
                " +        return VALUE + length;",
                " +    }",
                " +",
                " +    public long timestamp()",
                " +    {",
                " +        return MemoryUtil.getLong(peer + TIMESTAMP);",
                " +    }",
                " +",
                " +    public int ttl()",
                " +    {",
                " +        return MemoryUtil.getInt(peer + TTL);",
                " +    }",
                " +",
                " +    public int localDeletionTime()",
                " +    {",
                " +        return MemoryUtil.getInt(peer + DELETION);",
                " +    }",
                " +",
                " +    public ByteBuffer value()",
                " +    {",
                " +        int length = MemoryUtil.getInt(peer + LENGTH);",
                " +        return MemoryUtil.getByteBuffer(peer + VALUE, length, ByteOrder.BIG_ENDIAN);",
                " +    }",
                " +",
                " +    public CellPath path()",
                " +    {",
                " +        if (MemoryUtil.getByte(peer+ HAS_CELLPATH) == 0)",
                " +            return null;",
                " +",
                " +        long offset = peer + VALUE + MemoryUtil.getInt(peer + LENGTH);",
                " +        int size = MemoryUtil.getInt(offset);",
                " +        return CellPath.create(MemoryUtil.getByteBuffer(offset + 4, size, ByteOrder.BIG_ENDIAN));",
                " +    }",
                " +",
                " +    public Cell withUpdatedValue(ByteBuffer newValue)",
                " +    {",
                " +        throw new UnsupportedOperationException();",
                " +    }",
                " +",
                "++    public Cell withUpdatedTimestampAndLocalDeletionTime(long newTimestamp, int newLocalDeletionTime)",
                "++    {",
                "++        return new BufferCell(column, newTimestamp, ttl(), newLocalDeletionTime, value(), path());",
                "++    }",
                "++",
                " +    public Cell withUpdatedColumn(ColumnDefinition column)",
                " +    {",
                " +        return new BufferCell(column, timestamp(), ttl(), localDeletionTime(), value(), path());",
                " +    }",
                " +",
                " +    public long unsharedHeapSizeExcludingData()",
                " +    {",
                " +        return EMPTY_SIZE;",
                " +    }",
                " +",
                " +}",
                "diff --cc src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "index 54b340ee56,4778d7263c..ead8fc57d4",
                "--- a/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "+++ b/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "@@@ -268,2 -276,3 +271,3 @@@ public class StandaloneScrubbe",
                "              options.addOption(\"n\",  NO_VALIDATE_OPTION,    \"do not validate columns using column validator\");",
                " -            options.addOption(\"r\", REINSERT_OVERFLOWED_TTL_OPTION, REINSERT_OVERFLOWED_TTL_OPTION_DESCRIPTION);",
                "++            options.addOption(\"r\",  REINSERT_OVERFLOWED_TTL_OPTION,    \"Reinsert found rows with overflowed TTL affected by CASSANDRA-14092\");",
                "              return options;",
                "diff --cc src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "index 2345a8533f,ead2fd4c37..812202dc25",
                "--- a/src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "+++ b/src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "@@@ -50,2 -51,7 +50,7 @@@ public class Scrub extends NodeToolCm",
                "+     @Option(title = \"reinsert_overflowed_ttl\",",
                "+     name = {\"r\", \"--reinsert-overflowed-ttl\"},",
                " -    description = StandaloneScrubber.REINSERT_OVERFLOWED_TTL_OPTION_DESCRIPTION)",
                "++    description = \"Reinsert found rows with overflowed TTL affected by CASSANDRA-14092\")",
                "+     private boolean reinsertOverflowedTTL = false;",
                "+ ",
                "      @Option(title = \"jobs\",",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-Data.db",
                "index 0000000000,e7a72da607..cb96af36d2",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-Digest.crc32",
                "index 0000000000,a3c633a846..44c47fba84",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table1/mc-1-big-Digest.crc32",
                "+++ b/test/data/negative-local-expiration-test/table1/mc-1-big-Digest.crc32",
                "@@@ -1,0 -1,1 +1,1 @@@",
                " -203700622",
                "++4223695539",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-Statistics.db",
                "index 0000000000,faf367b5fb..ebcf4c8395",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-TOC.txt",
                "index 0000000000,45113dc798..831e376353",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table1/mc-1-big-TOC.txt",
                "+++ b/test/data/negative-local-expiration-test/table1/mc-1-big-TOC.txt",
                "@@@ -1,0 -1,8 +1,8 @@@",
                "++Digest.crc32",
                "+ CompressionInfo.db",
                "++Index.db",
                "++TOC.txt",
                "+ Data.db",
                " -Summary.db",
                " -Filter.db",
                "+ Statistics.db",
                " -TOC.txt",
                " -Digest.crc32",
                " -Index.db",
                "++Filter.db",
                "++Summary.db",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-Data.db",
                "index 0000000000,c1de5721bc..8f41a217b1",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-Digest.crc32",
                "index 0000000000,0403b5bdcd..da919fe8af",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table2/mc-1-big-Digest.crc32",
                "+++ b/test/data/negative-local-expiration-test/table2/mc-1-big-Digest.crc32",
                "@@@ -1,0 -1,1 +1,1 @@@",
                " -82785930",
                "++2886964045",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-Statistics.db",
                "index 0000000000,e9d65771bf..549dabef16",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-TOC.txt",
                "index 0000000000,45113dc798..831e376353",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table2/mc-1-big-TOC.txt",
                "+++ b/test/data/negative-local-expiration-test/table2/mc-1-big-TOC.txt",
                "@@@ -1,0 -1,8 +1,8 @@@",
                "++Digest.crc32",
                "+ CompressionInfo.db",
                "++Index.db",
                "++TOC.txt",
                "+ Data.db",
                " -Summary.db",
                " -Filter.db",
                "+ Statistics.db",
                " -TOC.txt",
                " -Digest.crc32",
                " -Index.db",
                "++Filter.db",
                "++Summary.db",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-Data.db",
                "index 0000000000,e96f77253e..008d3e8f47",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-Digest.crc32",
                "index 0000000000,459804bdf3..0bdc0bfc1f",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table3/mc-1-big-Digest.crc32",
                "+++ b/test/data/negative-local-expiration-test/table3/mc-1-big-Digest.crc32",
                "@@@ -1,0 -1,1 +1,1 @@@",
                " -3064924389",
                "++3254141434",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-Statistics.db",
                "index 0000000000,1ee01e674a..62bf84eadd",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-TOC.txt",
                "index 0000000000,f4455377aa..831e376353",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table3/mc-1-big-TOC.txt",
                "+++ b/test/data/negative-local-expiration-test/table3/mc-1-big-TOC.txt",
                "@@@ -1,0 -1,8 +1,8 @@@",
                " -Summary.db",
                " -TOC.txt",
                " -Filter.db",
                " -Index.db",
                "+ Digest.crc32",
                "+ CompressionInfo.db",
                "++Index.db",
                "++TOC.txt",
                "+ Data.db",
                "+ Statistics.db",
                "++Filter.db",
                "++Summary.db",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-Data.db",
                "index 0000000000,a22a7a3044..128ea47153",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-Digest.crc32",
                "index 0000000000,db7a6c765c..9d522094ef",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table4/mc-1-big-Digest.crc32",
                "+++ b/test/data/negative-local-expiration-test/table4/mc-1-big-Digest.crc32",
                "@@@ -1,0 -1,1 +1,1 @@@",
                " -1803989939",
                "++3231150985",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-Statistics.db",
                "index 0000000000,4ee9294b5a..4eee729f2e",
                "mode 000000,100644..100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-TOC.txt",
                "index 0000000000,f4455377aa..831e376353",
                "mode 000000,100644..100644",
                "--- a/test/data/negative-local-expiration-test/table4/mc-1-big-TOC.txt",
                "+++ b/test/data/negative-local-expiration-test/table4/mc-1-big-TOC.txt",
                "@@@ -1,0 -1,8 +1,8 @@@",
                " -Summary.db",
                " -TOC.txt",
                " -Filter.db",
                " -Index.db",
                "+ Digest.crc32",
                "+ CompressionInfo.db",
                "++Index.db",
                "++TOC.txt",
                "+ Data.db",
                "+ Statistics.db",
                "++Filter.db",
                "++Summary.db",
                "diff --cc test/unit/org/apache/cassandra/db/CellTest.java",
                "index ea009f6bd5,22f1b781fe..c68b4ecb7a",
                "--- a/test/unit/org/apache/cassandra/db/CellTest.java",
                "+++ b/test/unit/org/apache/cassandra/db/CellTest.java",
                "@@@ -8,5 -8,5 +8,5 @@@",
                "   * with the License.  You may obtain a copy of the License at",
                "-- * ",
                "++ *",
                "   *   http://www.apache.org/licenses/LICENSE-2.0",
                "-- * ",
                "++ *",
                "   * Unless required by applicable law or agreed to in writing,",
                "@@@ -153,6 -146,6 +153,6 @@@ public class CellTes",
                "          // Invalid ttl",
                " -        assertInvalid(BufferCell.expiring(c, 0, -4, 4, ByteBufferUtil.bytes(4)));",
                " -        // Invalid local deletion times",
                " -        assertInvalid(BufferCell.expiring(c, 0, 4, -4, ByteBufferUtil.bytes(4)));",
                " -        assertInvalid(BufferCell.expiring(c, 0, 4, Cell.NO_DELETION_TIME, ByteBufferUtil.bytes(4)));",
                " +        assertInvalid(BufferCell.expiring(c, 0, -4, 4, bbs(4)));",
                "-         // Invalid local deletion times",
                "-         assertInvalid(BufferCell.expiring(c, 0, 4, -5, bbs(4)));",
                "-         assertInvalid(BufferCell.expiring(c, 0, 4, Cell.NO_DELETION_TIME, bbs(4)));",
                "++        // Cells with overflowed localExpirationTime are valid after CASSANDRA-14092",
                "++        assertValid(BufferCell.expiring(c, 0, 4, -5, bbs(4)));",
                "++        assertValid(BufferCell.expiring(c, 0, 4, Cell.NO_DELETION_TIME, bbs(4)));",
                "@@@ -162,72 -155,3 +162,72 @@@",
                "          // Invalid cell path (int values should be 0 or 4 bytes)",
                " -        assertInvalid(BufferCell.live(fakeMetadata, c, 0, ByteBufferUtil.bytes(4), CellPath.create(ByteBufferUtil.bytes((long)4))));",
                " +        assertInvalid(BufferCell.live(c, 0, ByteBufferUtil.bytes(4), CellPath.create(ByteBufferUtil.bytes((long)4))));",
                " +    }",
                " +",
                " +    @Test",
                " +    public void testValidateNonFrozenUDT()",
                " +    {",
                " +        FieldIdentifier f1 = field(\"f1\");  // has field position 0",
                " +        FieldIdentifier f2 = field(\"f2\");  // has field position 1",
                " +        UserType udt = new UserType(\"ks\",",
                " +                                    bb(\"myType\"),",
                " +                                    asList(f1, f2),",
                " +                                    asList(Int32Type.instance, UTF8Type.instance),",
                " +                                    true);",
                " +        ColumnDefinition c;",
                " +",
                " +        // Valid cells",
                " +        c = fakeColumn(\"c\", udt);",
                " +        assertValid(BufferCell.live(c, 0, bb(1), CellPath.create(bbs(0))));",
                " +        assertValid(BufferCell.live(c, 0, bb(\"foo\"), CellPath.create(bbs(1))));",
                " +        assertValid(BufferCell.expiring(c, 0, 4, 4, bb(1), CellPath.create(bbs(0))));",
                " +        assertValid(BufferCell.expiring(c, 0, 4, 4, bb(\"foo\"), CellPath.create(bbs(1))));",
                " +        assertValid(BufferCell.tombstone(c, 0, 4, CellPath.create(bbs(0))));",
                " +",
                " +        // Invalid value (text in an int field)",
                " +        assertInvalid(BufferCell.live(c, 0, bb(\"foo\"), CellPath.create(bbs(0))));",
                " +",
                " +        // Invalid ttl",
                " +        assertInvalid(BufferCell.expiring(c, 0, -4, 4, bb(1), CellPath.create(bbs(0))));",
                "-         // Invalid local deletion times",
                "-         assertInvalid(BufferCell.expiring(c, 0, 4, -5, bb(1), CellPath.create(bbs(0))));",
                "-         assertInvalid(BufferCell.expiring(c, 0, 4, Cell.NO_DELETION_TIME, bb(1), CellPath.create(bbs(0))));",
                "++        // Cells with overflowed localExpirationTime are valid after CASSANDRA-14092",
                "++        assertValid(BufferCell.expiring(c, 0, 4, -5, bb(1), CellPath.create(bbs(0))));",
                "++        assertValid((BufferCell.expiring(c, 0, 4, Cell.NO_DELETION_TIME, bb(1), CellPath.create(bbs(0)))));",
                " +",
                " +        // Invalid cell path (int values should be 0 or 2 bytes)",
                " +        assertInvalid(BufferCell.live(c, 0, bb(1), CellPath.create(ByteBufferUtil.bytes((long)4))));",
                " +    }",
                " +",
                " +    @Test",
                " +    public void testValidateFrozenUDT()",
                " +    {",
                " +        FieldIdentifier f1 = field(\"f1\");  // has field position 0",
                " +        FieldIdentifier f2 = field(\"f2\");  // has field position 1",
                " +        UserType udt = new UserType(\"ks\",",
                " +                                    bb(\"myType\"),",
                " +                                    asList(f1, f2),",
                " +                                    asList(Int32Type.instance, UTF8Type.instance),",
                " +                                    false);",
                " +",
                " +        ColumnDefinition c = fakeColumn(\"c\", udt);",
                " +        ByteBuffer val = udt(bb(1), bb(\"foo\"));",
                " +",
                " +        // Valid cells",
                " +        assertValid(BufferCell.live(c, 0, val));",
                " +        assertValid(BufferCell.live(c, 0, val));",
                " +        assertValid(BufferCell.expiring(c, 0, 4, 4, val));",
                " +        assertValid(BufferCell.expiring(c, 0, 4, 4, val));",
                " +        assertValid(BufferCell.tombstone(c, 0, 4));",
                " +        // fewer values than types is accepted",
                " +        assertValid(BufferCell.live(c, 0, udt(bb(1))));",
                " +",
                " +        // Invalid values",
                " +        // invalid types",
                " +        assertInvalid(BufferCell.live(c, 0, udt(bb(\"foo\"), bb(1))));",
                " +        // too many types",
                " +        assertInvalid(BufferCell.live(c, 0, udt(bb(1), bb(\"foo\"), bb(\"bar\"))));",
                " +",
                " +        // Invalid ttl",
                " +        assertInvalid(BufferCell.expiring(c, 0, -4, 4, val));",
                "-         // Invalid local deletion times",
                "-         assertInvalid(BufferCell.expiring(c, 0, 4, -5, val));",
                "-         assertInvalid(BufferCell.expiring(c, 0, 4, Cell.NO_DELETION_TIME, val));",
                "++        // Cells with overflowed localExpirationTime are valid after CASSANDRA-14092",
                "++        assertValid(BufferCell.expiring(c, 0, 4, -5, val));",
                "++        assertValid(BufferCell.expiring(c, 0, 4, Cell.NO_DELETION_TIME, val));",
                "      }"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "735f1a417f4646fbb4dd9308055dd0880770aff8",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516624644,
            "hunks": 0,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "6034c268c389fc40a2f96c2746a09997906b93ba",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518282085,
            "hunks": 27,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [
                "diff --cc src/java/org/apache/cassandra/cql3/Attributes.java",
                "index d915560c5a,d4e230f78b..262a5104e4",
                "--- a/src/java/org/apache/cassandra/cql3/Attributes.java",
                "+++ b/src/java/org/apache/cassandra/cql3/Attributes.java",
                "@@@ -22,3 -22,5 +22,4 @@@ import java.util.List",
                " -import org.apache.cassandra.config.CFMetaData;",
                "  import org.apache.cassandra.cql3.functions.Function;",
                "+ import org.apache.cassandra.db.ExpirationDateOverflowHandling;",
                "  import org.apache.cassandra.db.LivenessInfo;",
                "@@@ -27,2 -29,2 +28,3 @@@ import org.apache.cassandra.db.marshal.",
                "  import org.apache.cassandra.exceptions.InvalidRequestException;",
                "++import org.apache.cassandra.schema.TableMetadata;",
                "  import org.apache.cassandra.serializers.MarshalException;",
                "@@@ -100,3 -102,3 +102,3 @@@ public class Attribute",
                "-     public int getTimeToLive(QueryOptions options, int defaultTimeToLive) throws InvalidRequestException",
                " -    public int getTimeToLive(QueryOptions options, CFMetaData metadata) throws InvalidRequestException",
                "++    public int getTimeToLive(QueryOptions options, TableMetadata metadata) throws InvalidRequestException",
                "      {",
                "diff --cc src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java",
                "index f0cfd0dff8,8a896e9846..31aa80cbea",
                "--- a/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java",
                "+++ b/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java",
                "@@@ -217,3 -205,3 +217,3 @@@ public abstract class ModificationState",
                "      {",
                "-         return attrs.getTimeToLive(options, metadata().params.defaultTimeToLive);",
                " -        return attrs.getTimeToLive(options, cfm);",
                "++        return attrs.getTimeToLive(options, metadata);",
                "      }",
                "diff --cc src/java/org/apache/cassandra/db/ExpirationDateOverflowHandling.java",
                "index 0000000000,852dcb1f7d..fea729f404",
                "mode 000000,100644..100644",
                "--- a/src/java/org/apache/cassandra/db/ExpirationDateOverflowHandling.java",
                "+++ b/src/java/org/apache/cassandra/db/ExpirationDateOverflowHandling.java",
                "@@@ -1,0 -1,121 +1,121 @@@",
                "+ /*",
                "+  * Licensed to the Apache Software Foundation (ASF) under one",
                "+  * or more contributor license agreements.  See the NOTICE file",
                "+  * distributed with this work for additional information",
                "+  * regarding copyright ownership.  The ASF licenses this file",
                "+  * to you under the Apache License, Version 2.0 (the",
                "+  * \"License\"); you may not use this file except in compliance",
                "+  * with the License.  You may obtain a copy of the License at",
                "+  *",
                "+  *     http://www.apache.org/licenses/LICENSE-2.0",
                "+  *",
                "+  * Unless required by applicable law or agreed to in writing, software",
                "+  * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "+  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "+  * See the License for the specific language governing permissions and",
                "+  * limitations under the License.",
                "+  */",
                "+ ",
                "+ package org.apache.cassandra.db;",
                "+ ",
                "+ import java.util.concurrent.TimeUnit;",
                "+ ",
                "+ import com.google.common.annotations.VisibleForTesting;",
                "+ import org.slf4j.Logger;",
                "+ import org.slf4j.LoggerFactory;",
                "+ import org.slf4j.helpers.MessageFormatter;",
                "+ ",
                " -import org.apache.cassandra.config.CFMetaData;",
                "+ import org.apache.cassandra.cql3.Attributes;",
                "+ import org.apache.cassandra.db.rows.BufferCell;",
                "+ import org.apache.cassandra.db.rows.Cell;",
                "+ import org.apache.cassandra.exceptions.InvalidRequestException;",
                "++import org.apache.cassandra.schema.TableMetadata;",
                "+ import org.apache.cassandra.service.ClientWarn;",
                "+ import org.apache.cassandra.utils.NoSpamLogger;",
                "+ ",
                "+ public class ExpirationDateOverflowHandling",
                "+ {",
                "+     private static final Logger logger = LoggerFactory.getLogger(Attributes.class);",
                "+ ",
                "+     private static final int EXPIRATION_OVERFLOW_WARNING_INTERVAL_MINUTES = Integer.getInteger(\"cassandra.expiration_overflow_warning_interval_minutes\", 5);",
                "+ ",
                "+     public enum ExpirationDateOverflowPolicy",
                "+     {",
                "+         REJECT, CAP_NOWARN, CAP",
                "+     }",
                "+ ",
                "+     @VisibleForTesting",
                "+     public static ExpirationDateOverflowPolicy policy;",
                "+ ",
                "+     static {",
                "+         String policyAsString = System.getProperty(\"cassandra.expiration_date_overflow_policy\", ExpirationDateOverflowPolicy.REJECT.name());",
                "+         try",
                "+         {",
                "+             policy = ExpirationDateOverflowPolicy.valueOf(policyAsString.toUpperCase());",
                "+         }",
                "+         catch (RuntimeException e)",
                "+         {",
                "+             logger.warn(\"Invalid expiration date overflow policy: {}. Using default: {}\", policyAsString, ExpirationDateOverflowPolicy.REJECT.name());",
                "+             policy = ExpirationDateOverflowPolicy.REJECT;",
                "+         }",
                "+     }",
                "+ ",
                "+     public static final String MAXIMUM_EXPIRATION_DATE_EXCEEDED_WARNING = \"Request on table {}.{} with {}ttl of {} seconds exceeds maximum supported expiration \" +",
                "+                                                                           \"date of 2038-01-19T03:14:06+00:00 and will have its expiration capped to that date. \" +",
                "+                                                                           \"In order to avoid this use a lower TTL or upgrade to a version where this limitation \" +",
                "+                                                                           \"is fixed. See CASSANDRA-14092 for more details.\";",
                "+ ",
                "+     public static final String MAXIMUM_EXPIRATION_DATE_EXCEEDED_REJECT_MESSAGE = \"Request on table %s.%s with %sttl of %d seconds exceeds maximum supported expiration \" +",
                "+                                                                                  \"date of 2038-01-19T03:14:06+00:00. In order to avoid this use a lower TTL, change \" +",
                "+                                                                                  \"the expiration date overflow policy or upgrade to a version where this limitation \" +",
                "+                                                                                  \"is fixed. See CASSANDRA-14092 for more details.\";",
                "+ ",
                " -    public static void maybeApplyExpirationDateOverflowPolicy(CFMetaData metadata, int ttl, boolean isDefaultTTL) throws InvalidRequestException",
                "++    public static void maybeApplyExpirationDateOverflowPolicy(TableMetadata metadata, int ttl, boolean isDefaultTTL) throws InvalidRequestException",
                "+     {",
                "+         if (ttl == BufferCell.NO_TTL)",
                "+             return;",
                "+ ",
                "+         // Check for localExpirationTime overflow (CASSANDRA-14092)",
                "+         int nowInSecs = (int)(System.currentTimeMillis() / 1000);",
                "+         if (ttl + nowInSecs < 0)",
                "+         {",
                "+             switch (policy)",
                "+             {",
                "+                 case CAP:",
                " -                    ClientWarn.instance.warn(MessageFormatter.arrayFormat(MAXIMUM_EXPIRATION_DATE_EXCEEDED_WARNING, new Object[] { metadata.ksName,",
                " -                                                                                                                                   metadata.cfName,",
                "++                    ClientWarn.instance.warn(MessageFormatter.arrayFormat(MAXIMUM_EXPIRATION_DATE_EXCEEDED_WARNING, new Object[] { metadata.keyspace,",
                "++                                                                                                                                   metadata.name,",
                "+                                                                                                                                    isDefaultTTL? \"default \" : \"\", ttl })",
                "+                                                              .getMessage());",
                "+                 case CAP_NOWARN:",
                "+                     /**",
                "+                      * Capping at this stage is basically not rejecting the request. The actual capping is done",
                "+                      * by {@link #computeLocalExpirationTime(int, int)}, which converts the negative TTL",
                "+                      * to {@link org.apache.cassandra.db.BufferExpiringCell#MAX_DELETION_TIME}",
                "+                      */",
                "+                     NoSpamLogger.log(logger, NoSpamLogger.Level.WARN, EXPIRATION_OVERFLOW_WARNING_INTERVAL_MINUTES, TimeUnit.MINUTES, MAXIMUM_EXPIRATION_DATE_EXCEEDED_WARNING,",
                " -                                     metadata.ksName, metadata.cfName, isDefaultTTL? \"default \" : \"\", ttl);",
                "++                                     metadata.keyspace, metadata.name, isDefaultTTL? \"default \" : \"\", ttl);",
                "+                     return;",
                "+ ",
                "+                 default:",
                " -                    throw new InvalidRequestException(String.format(MAXIMUM_EXPIRATION_DATE_EXCEEDED_REJECT_MESSAGE, metadata.ksName, metadata.cfName,",
                "++                    throw new InvalidRequestException(String.format(MAXIMUM_EXPIRATION_DATE_EXCEEDED_REJECT_MESSAGE, metadata.keyspace, metadata.name,",
                "+                                                                     isDefaultTTL? \"default \" : \"\", ttl));",
                "+             }",
                "+         }",
                "+     }",
                "+ ",
                "+     /**",
                "+      * This method computes the {@link Cell#localDeletionTime()}, maybe capping to the maximum representable value",
                "+      * which is {@link Cell#MAX_DELETION_TIME}.",
                "+      *",
                "+      * Please note that the {@link ExpirationDateOverflowHandling.ExpirationDateOverflowPolicy} is applied",
                " -     * during {@link ExpirationDateOverflowHandling#maybeApplyExpirationDateOverflowPolicy(CFMetaData, int, boolean)},",
                "++     * during {@link ExpirationDateOverflowHandling#maybeApplyExpirationDateOverflowPolicy(org.apache.cassandra.schema.TableMetadata, int, boolean)},",
                "+      * so if the request was not denied it means its expiration date should be capped.",
                "+      *",
                "+      * See CASSANDRA-14092",
                "+      */",
                "+     public static int computeLocalExpirationTime(int nowInSec, int timeToLive)",
                "+     {",
                "+         int localExpirationTime = nowInSec + timeToLive;",
                "+         return localExpirationTime >= 0? localExpirationTime : Cell.MAX_DELETION_TIME;",
                "+     }",
                "+ }",
                "diff --cc src/java/org/apache/cassandra/db/LivenessInfo.java",
                "index a8c33b070e,c2a2291c95..1340c00caf",
                "--- a/src/java/org/apache/cassandra/db/LivenessInfo.java",
                "+++ b/src/java/org/apache/cassandra/db/LivenessInfo.java",
                "@@@ -20,5 -20,6 +20,6 @@@ package org.apache.cassandra.db",
                "  import java.util.Objects;",
                " -import java.security.MessageDigest;",
                " -import org.apache.cassandra.config.CFMetaData;",
                " +import com.google.common.hash.Hasher;",
                " +",
                "+ import org.apache.cassandra.db.rows.Cell;",
                "  import org.apache.cassandra.serializers.MarshalException;",
                "diff --cc src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "index c5e59581bf,809e741957..01fc188abc",
                "--- a/src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "@@@ -970,3 -979,3 +978,3 @@@ public class CompactionManager implemen",
                "-     private void scrubOne(ColumnFamilyStore cfs, LifecycleTransaction modifier, boolean skipCorrupted, boolean checkData)",
                " -    private void scrubOne(ColumnFamilyStore cfs, LifecycleTransaction modifier, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTL) throws IOException",
                "++    private void scrubOne(ColumnFamilyStore cfs, LifecycleTransaction modifier, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTL)",
                "      {",
                "diff --cc src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "index 42dd614cdb,f8fa548eb7..ce749f4deb",
                "--- a/src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "@@@ -81,3 -84,9 +85,9 @@@ public class Scrubber implements Closea",
                "      {",
                "-         this(cfs, transaction, skipCorrupted, new OutputHandler.LogOutput(), checkData);",
                "+         this(cfs, transaction, skipCorrupted, checkData, false);",
                "+     }",
                "+ ",
                "+     public Scrubber(ColumnFamilyStore cfs, LifecycleTransaction transaction, boolean skipCorrupted, boolean checkData,",
                " -                    boolean reinsertOverflowedTTLRows) throws IOException",
                "++                    boolean reinsertOverflowedTTLRows)",
                "+     {",
                "+         this(cfs, transaction, skipCorrupted, new OutputHandler.LogOutput(), checkData, reinsertOverflowedTTLRows);",
                "      }",
                "@@@ -89,3 -98,4 +99,4 @@@",
                "                      OutputHandler outputHandler,",
                "-                     boolean checkData)",
                "+                     boolean checkData,",
                " -                    boolean reinsertOverflowedTTLRows) throws IOException",
                "++                    boolean reinsertOverflowedTTLRows)",
                "      {",
                "@@@ -96,3 -106,4 +107,4 @@@",
                "          this.skipCorrupted = skipCorrupted;",
                "+         this.reinsertOverflowedTTLRows = reinsertOverflowedTTLRows;",
                " -        this.rowIndexEntrySerializer = sstable.descriptor.version.getSSTableFormat().getIndexSerializer(sstable.metadata,",
                " +        this.rowIndexEntrySerializer = sstable.descriptor.version.getSSTableFormat().getIndexSerializer(cfs.metadata(),",
                "                                                                                                          sstable.descriptor.version,",
                "@@@ -310,4 -326,4 +327,4 @@@",
                "          // to the outOfOrder set that will be later written to a new SSTable.",
                "-         OrderCheckerIterator sstableIterator = new OrderCheckerIterator(new RowMergingSSTableIterator(SSTableIdentityIterator.create(sstable, dataFile, key)),",
                "+         OrderCheckerIterator sstableIterator = new OrderCheckerIterator(getIterator(key),",
                " -                                                                        cfs.metadata.comparator);",
                " +                                                                        cfs.metadata().comparator);",
                "@@@ -623,2 -656,121 +657,121 @@@",
                "          }",
                "+     }",
                "+ ",
                "+     /**",
                "+      * This iterator converts negative {@link AbstractCell#localDeletionTime()} into {@link AbstractCell#MAX_DELETION_TIME}",
                "+      *",
                "+      * This is to recover entries with overflowed localExpirationTime due to CASSANDRA-14092",
                "+      */",
                "+     private static final class FixNegativeLocalDeletionTimeIterator extends AbstractIterator<Unfiltered> implements UnfilteredRowIterator",
                "+     {",
                "+         /**",
                "+          * The decorated iterator.",
                "+          */",
                "+         private final UnfilteredRowIterator iterator;",
                "+ ",
                "+         private final OutputHandler outputHandler;",
                "+         private final NegativeLocalDeletionInfoMetrics negativeLocalExpirationTimeMetrics;",
                "+ ",
                "+         public FixNegativeLocalDeletionTimeIterator(UnfilteredRowIterator iterator, OutputHandler outputHandler,",
                "+                                                     NegativeLocalDeletionInfoMetrics negativeLocalDeletionInfoMetrics)",
                "+         {",
                "+             this.iterator = iterator;",
                "+             this.outputHandler = outputHandler;",
                "+             this.negativeLocalExpirationTimeMetrics = negativeLocalDeletionInfoMetrics;",
                "+         }",
                "+ ",
                " -        public CFMetaData metadata()",
                "++        public TableMetadata metadata()",
                "+         {",
                "+             return iterator.metadata();",
                "+         }",
                "+ ",
                "+         public boolean isReverseOrder()",
                "+         {",
                "+             return iterator.isReverseOrder();",
                "+         }",
                "+ ",
                " -        public PartitionColumns columns()",
                "++        public RegularAndStaticColumns columns()",
                "+         {",
                "+             return iterator.columns();",
                "+         }",
                "+ ",
                "+         public DecoratedKey partitionKey()",
                "+         {",
                "+             return iterator.partitionKey();",
                "+         }",
                "+ ",
                "+         public Row staticRow()",
                "+         {",
                "+             return iterator.staticRow();",
                "+         }",
                "+ ",
                "+         @Override",
                "+         public boolean isEmpty()",
                "+         {",
                "+             return iterator.isEmpty();",
                "+         }",
                "+ ",
                "+         public void close()",
                "+         {",
                "+             iterator.close();",
                "+         }",
                "+ ",
                "+         public DeletionTime partitionLevelDeletion()",
                "+         {",
                "+             return iterator.partitionLevelDeletion();",
                "+         }",
                "+ ",
                "+         public EncodingStats stats()",
                "+         {",
                "+             return iterator.stats();",
                "+         }",
                "+ ",
                "+         protected Unfiltered computeNext()",
                "+         {",
                "+             if (!iterator.hasNext())",
                "+                 return endOfData();",
                "+ ",
                "+             Unfiltered next = iterator.next();",
                "+             if (!next.isRow())",
                "+                 return next;",
                "+ ",
                "+             if (hasNegativeLocalExpirationTime((Row) next))",
                "+             {",
                "+                 outputHandler.debug(String.format(\"Found row with negative local expiration time: %s\", next.toString(metadata(), false)));",
                "+                 negativeLocalExpirationTimeMetrics.fixedRows++;",
                "+                 return fixNegativeLocalExpirationTime((Row) next);",
                "+             }",
                "+ ",
                "+             return next;",
                "+         }",
                "+ ",
                "+         private boolean hasNegativeLocalExpirationTime(Row next)",
                "+         {",
                "+             Row row = next;",
                "+             if (row.primaryKeyLivenessInfo().isExpiring() && row.primaryKeyLivenessInfo().localExpirationTime() < 0)",
                "+             {",
                "+                 return true;",
                "+             }",
                "+ ",
                "+             for (ColumnData cd : row)",
                "+             {",
                "+                 if (cd.column().isSimple())",
                "+                 {",
                "+                     Cell cell = (Cell)cd;",
                "+                     if (cell.isExpiring() && cell.localDeletionTime() < 0)",
                "+                         return true;",
                "+                 }",
                "+                 else",
                "+                 {",
                "+                     ComplexColumnData complexData = (ComplexColumnData)cd;",
                "+                     for (Cell cell : complexData)",
                "+                     {",
                "+                         if (cell.isExpiring() && cell.localDeletionTime() < 0)",
                "+                             return true;",
                "+                     }",
                "+                 }",
                "+             }",
                "+ ",
                "+             return false;",
                "+         }",
                "diff --cc src/java/org/apache/cassandra/db/rows/BufferCell.java",
                "index 76c6d3ebe2,b62d95aaec..8bf8f7de67",
                "--- a/src/java/org/apache/cassandra/db/rows/BufferCell.java",
                "+++ b/src/java/org/apache/cassandra/db/rows/BufferCell.java",
                "@@@ -21,3 -21,4 +21,4 @@@ import java.nio.ByteBuffer",
                " -import org.apache.cassandra.config.ColumnDefinition;",
                "+ import org.apache.cassandra.db.ExpirationDateOverflowHandling;",
                " +import org.apache.cassandra.schema.ColumnMetadata;",
                "  import org.apache.cassandra.db.marshal.ByteType;",
                "diff --cc src/java/org/apache/cassandra/db/rows/NativeCell.java",
                "index 10ccf889a8,31ce0b7700..c4cb6c1342",
                "--- a/src/java/org/apache/cassandra/db/rows/NativeCell.java",
                "+++ b/src/java/org/apache/cassandra/db/rows/NativeCell.java",
                "@@@ -145,3 -145,8 +145,8 @@@ public class NativeCell extends Abstrac",
                "+     public Cell withUpdatedTimestampAndLocalDeletionTime(long newTimestamp, int newLocalDeletionTime)",
                "+     {",
                "+         return new BufferCell(column, newTimestamp, ttl(), newLocalDeletionTime, value(), path());",
                "+     }",
                "+ ",
                " -    public Cell withUpdatedColumn(ColumnDefinition column)",
                " +    public Cell withUpdatedColumn(ColumnMetadata column)",
                "      {"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "1cb91eaaaad8169a7b680f1f6ab6b1418ce56e61",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516367698,
            "hunks": 0,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "d0b7566ff8386c4246ec24232256e7860338b724",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516367660,
            "hunks": 0,
            "message": "Merge branch 'cassandra-3.0' into cassandra-3.11",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "a8ce4b6cd801f55d01a7400c9455220b47390d40",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1517888499,
            "hunks": 0,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "c231ed5be0f47b030181185f544132523a2cf908",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518281727,
            "hunks": 152,
            "message": "Merge branch 'cassandra-2.2' into cassandra-3.0",
            "diff": [
                "diff --cc CASSANDRA-14092.txt",
                "index 0000000000,5ac872c662..f95380b583",
                "mode 000000,100644..100644",
                "--- a/CASSANDRA-14092.txt",
                "+++ b/CASSANDRA-14092.txt",
                "@@@ -1,0 -1,81 +1,81 @@@",
                "+ CASSANDRA-14092: MAXIMUM TTL EXPIRATION DATE",
                "+ ---------------------------------------------",
                "+ ",
                "+ The maximum expiration timestamp that can be represented by the storage engine is",
                "+ 2038-01-19T03:14:06+00:00, which means that INSERTS using TTL that would expire",
                "+ after this date are not currently supported.",
                "+ ",
                "+ # Expiration Date Overflow Policy",
                "+ ",
                "+ We plan to lift this limitation in newer versions, but while the fix is not available,",
                "+ operators can decide which policy to apply when dealing with inserts with TTL exceeding",
                "+ the maximum supported expiration date:",
                "+   -     REJECT: this is the default policy and will reject any requests with expiration",
                "+                 date timestamp after 2038-01-19T03:14:06+00:00.",
                "+   -        CAP: any insert with TTL expiring after 2038-01-19T03:14:06+00:00 will expire on",
                "+                 2038-01-19T03:14:06+00:00 and the client will receive a warning.",
                "+   - CAP_NOWARN: same as previous, except that the client warning will not be emitted.",
                "+ ",
                "+ These policies may be specified via the -Dcassandra.expiration_date_overflow_policy=POLICY",
                " -startup option.",
                "++startup option in the jvm.options configuration file.",
                "+ ",
                "+ # Potential data loss on earlier versions",
                "+ ",
                "+ Prior to 3.0.16 (3.0.X) and 3.11.2 (3.11.x), there was no protection against",
                "+ INSERTS with TTL expiring after the maximum supported date, causing the expiration",
                "+ time field to overflow and the records to expire immediately. Expired records due",
                "+ to overflow will not be queryable and will be permanently removed after a compaction.",
                "+ ",
                "+ 2.1.X, 2.2.X and earlier series are not subject to this bug when assertions are enabled",
                "+ since an AssertionError is thrown during INSERT when the expiration time field overflows",
                "+ on these versions. When assertions are disabled then it is possible to INSERT entries",
                "+ with overflowed local expiration time and even the earlier versions are subject to data",
                "+ loss due to this bug.",
                "+ ",
                "+ This issue only affected INSERTs with very large TTLs, close to the maximum allowed value",
                "+ of 630720000 seconds (20 years), starting from 2018-01-19T03:14:06+00:00. As time progresses,",
                "+ the maximum supported TTL will be gradually reduced as the maximum expiration date approaches.",
                "+ For instance, a user on an affected version on 2028-01-19T03:14:06 with a TTL of 10 years",
                "+ will be affected by this bug, so we urge users of very large TTLs to upgrade to a version",
                "+ where this issue is addressed as soon as possible.",
                "+ ",
                "+ # Data Recovery",
                "+ ",
                "+ SSTables from Cassandra versions prior to 2.1.20/2.2.12/3.0.16/3.11.2 containing entries",
                "+ with overflowed expiration time that were backed up or did not go through compaction can",
                "+ be recovered by reinserting overflowed entries with a valid expiration time and a higher",
                "+ timestamp, since tombstones may have been generated with the original timestamp.",
                "+ ",
                "+ To find out if an SSTable has an entry with overflowed expiration, inspect it with the",
                " -sstable2json tool and look for a negative \"local deletion time\" field. SSTables in this",
                " -condition should be backed up immediately, as they are subject to data loss during",
                "++'sstablemetadata' tool and look for a negative \"Minimum local deletion time\" field. SSTables",
                "++in this condition should be backed up immediately, as they are subject to data loss during",
                "+ compaction.",
                "+ ",
                "+ A \"--reinsert-overflowed-ttl\" option was added to scrub to rewrite SSTables containing",
                "+ rows with overflowed expiration time with the maximum expiration date of",
                "+ 2038-01-19T03:14:06+00:00 and the original timestamp + 1 (ms). Two methods are offered",
                "+ for recovery of SSTables via scrub:",
                "+ ",
                "+ - Offline scrub:",
                "+    - Clone the data directory tree to another location, keeping only the folders and the",
                "+      contents of the system tables.",
                "+    - Clone the configuration directory to another location, setting the data_file_directories",
                "+      property to the cloned data directory in the cloned cassandra.yaml.",
                "+    - Copy the affected SSTables to the cloned data location of the affected table.",
                "+    - Set the environment variable CASSANDRA_CONF=<cloned configuration directory>.",
                "+    - Execute \"sstablescrub --reinsert-overflowed-ttl <keyspace> <table>\".",
                "+          WARNING: not specifying --reinsert-overflowed-ttl is equivalent to a single-sstable",
                "+          compaction, so the data with overflowed will be removed - make sure to back up",
                "+          your SSTables before running scrub.",
                "+    - Once the scrub is completed, copy the resulting SSTables to the original data directory.",
                "+    - Execute \"nodetool refresh\" in a live node to load the recovered SSTables.",
                "+ ",
                "+ - Online scrub:",
                "+    - Disable compaction on the node with \"nodetool disableautocompaction\" - this step is crucial",
                "+      as otherwise, the data may be removed permanently during compaction.",
                "+    - Copy the SSTables containing entries with overflowed expiration time to the data directory.",
                "+    - run \"nodetool refresh\" to load the SSTables.",
                "+    - run \"nodetool scrub --reinsert-overflowed-ttl <keyspace> <table>\".",
                "+    - Re-enable compactions after verifying that scrub recovered the missing entries.",
                "+ ",
                " -See https://issues.apache.org/jira/browse/CASSANDRA-14092 for more details about this issue.",
                "++See https://issues.apache.org/jira/browse/CASSANDRA-14092 for more details about this issue.",
                "diff --cc CHANGES.txt",
                "index f42f3f45cb,82da6ad26e..a492c42a04",
                "--- a/CHANGES.txt",
                "+++ b/CHANGES.txt",
                "@@@ -1,27 -1,2 +1,28 @@@",
                " -2.2.12",
                " +3.0.16",
                "++ * Protect against overflow of local expiration time (CASSANDRA-14092)",
                " + * Use the correct digest file and reload sstable metadata in nodetool verify (CASSANDRA-14217)",
                " + * Handle failure when mutating repaired status in Verifier (CASSANDRA-13933)",
                " + * Close socket on error during connect on OutboundTcpConnection (CASSANDRA-9630)",
                " + * Set encoding for javadoc generation (CASSANDRA-14154)",
                " + * Fix index target computation for dense composite tables with dropped compact storage (CASSANDRA-14104)",
                " + * Improve commit log chain marker updating (CASSANDRA-14108)",
                " + * Extra range tombstone bound creates double rows (CASSANDRA-14008)",
                " + * Fix SStable ordering by max timestamp in SinglePartitionReadCommand (CASSANDRA-14010)",
                " + * Accept role names containing forward-slash (CASSANDRA-14088)",
                " + * Optimize CRC check chance probability calculations (CASSANDRA-14094)",
                " + * Fix cleanup on keyspace with no replicas (CASSANDRA-13526)",
                " + * Fix updating base table rows with TTL not removing materialized view entries (CASSANDRA-14071)",
                " + * Reduce garbage created by DynamicSnitch (CASSANDRA-14091)",
                " + * More frequent commitlog chained markers (CASSANDRA-13987)",
                " + * Fix serialized size of DataLimits (CASSANDRA-14057)",
                " + * Add flag to allow dropping oversized read repair mutations (CASSANDRA-13975)",
                " + * Fix SSTableLoader logger message (CASSANDRA-14003)",
                " + * Fix repair race that caused gossip to block (CASSANDRA-13849)",
                " + * Tracing interferes with digest requests when using RandomPartitioner (CASSANDRA-13964)",
                " + * Add flag to disable materialized views, and warnings on creation (CASSANDRA-13959)",
                " + * Don't let user drop or generally break tables in system_distributed (CASSANDRA-13813)",
                " + * Provide a JMX call to sync schema with local storage (CASSANDRA-13954)",
                " + * Mishandling of cells for removed/dropped columns when reading legacy files (CASSANDRA-13939)",
                " + * Deserialise sstable metadata in nodetool verify (CASSANDRA-13922)",
                " +Merged from 2.2:",
                "   * Fix the inspectJvmOptions startup check (CASSANDRA-14112)",
                "diff --cc NEWS.txt",
                "index 1bbe1aa301,4fe3508bea..f574c339bc",
                "--- a/NEWS.txt",
                "+++ b/NEWS.txt",
                "@@@ -15,67 -35,3 +35,68 @@@ using the provided 'sstableupgrade' too",
                " -2.2.12",
                " +3.0.16",
                " +=====",
                " +",
                " +Upgrading",
                " +---------",
                "++   - See MAXIMUM TTL EXPIRATION DATE NOTICE above.",
                " +   - Cassandra is now relying on the JVM options to properly shutdown on OutOfMemoryError. By default it will",
                " +     rely on the OnOutOfMemoryError option as the ExitOnOutOfMemoryError and CrashOnOutOfMemoryError options",
                " +     are not supported by the older 1.7 and 1.8 JVMs. A warning will be logged at startup if none of those JVM",
                " +     options are used. See CASSANDRA-13006 for more details.",
                " +   - Cassandra is not logging anymore by default an Heap histogram on OutOfMemoryError. To enable that behavior",
                " +     set the 'cassandra.printHeapHistogramOnOutOfMemoryError' System property to 'true'. See CASSANDRA-13006",
                " +     for more details.",
                " +",
                " +Materialized Views",
                " +-------------------",
                " +   - Following a discussion regarding concerns about the design and safety of Materialized Views, the C* development",
                " +     community no longer recommends them for production use, and considers them experimental. Warnings messages will",
                " +     now be logged when they are created. (See https://www.mail-archive.com/dev@cassandra.apache.org/msg11511.html)",
                " +   - An 'enable_materialized_views' flag has been added to cassandra.yaml to allow operators to prevent creation of",
                " +     views",
                " +",
                " +3.0.15",
                " +=====",
                " +",
                " +Upgrading",
                " +---------",
                " +   - Nothing specific to this release, but please see previous upgrading sections,",
                " +     especially if you are upgrading from 2.2.",
                " +",
                " +Compact Storage",
                " +---------------",
                " +    - Starting version 4.0, Thrift and COMPACT STORAGE is no longer supported.",
                " +      'ALTER ... DROP COMPACT STORAGE' statement makes Compact Tables CQL-compatible,",
                " +      exposing internal structure of Thrift/Compact Tables. You can find more details",
                " +      on exposed internal structure under: ",
                " +      http://cassandra.apache.org/doc/latest/cql/appendices.html#appendix-c-dropping-compact-storage",
                " +",
                " +      For uninterrupted cluster upgrades, drivers now support 'NO_COMPACT' startup option.",
                " +      Supplying this flag will have same effect as 'DROP COMPACT STORAGE', but only for the",
                " +      current connection.",
                " +",
                " +      In order to upgrade, clients supporting a non-compact schema view can be rolled out",
                " +      gradually. When all the clients are updated 'ALTER ... DROP COMPACT STORAGE' can be",
                " +      executed. After dropping compact storage, \u00e2\u0080\u0099NO_COMPACT' option will have no effect",
                " +      after that.",
                " +",
                " +Materialized Views",
                " +-------------------",
                " +    - Cassandra will no longer allow dropping columns on tables with Materialized Views.",
                " +    - A change was made in the way the Materialized View timestamp is computed, which",
                " +      may cause an old deletion to a base column which is view primary key (PK) column",
                " +      to not be reflected in the view when repairing the base table post-upgrade. This",
                " +      condition is only possible when a column deletion to an MV primary key (PK) column",
                " +      not present in the base table PK (via UPDATE base SET view_pk_col = null or DELETE",
                " +      view_pk_col FROM base) is missed before the upgrade and received by repair after the upgrade.",
                " +      If such column deletions are done on a view PK column which is not a base PK, it's advisable",
                " +      to run repair on the base table of all nodes prior to the upgrade. Alternatively it's possible",
                " +      to fix potential inconsistencies by running repair on the views after upgrade or drop and",
                " +      re-create the views. See CASSANDRA-11500 for more details.",
                " +    - Removal of columns not selected in the Materialized View (via UPDATE base SET unselected_column",
                " +      = null or DELETE unselected_column FROM base) may not be properly reflected in the view in some",
                " +      situations so we advise against doing deletions on base columns not selected in views",
                " +      until this is fixed on CASSANDRA-13826.",
                " +",
                " +3.0.14",
                "  ======",
                "diff --cc conf/jvm.options",
                "index a7b3bd87f3,0000000000..eb2ad19e43",
                "mode 100644,000000..100644",
                "--- a/conf/jvm.options",
                "+++ b/conf/jvm.options",
                "@@@ -1,108 -1,0 +1,119 @@@",
                " +###########################################################################",
                " +#                             jvm.options                                 #",
                " +#                                                                         #",
                " +# - all flags defined here will be used by cassandra to startup the JVM   #",
                " +# - one flag should be specified per line                                 #",
                " +# - lines that do not start with '-' will be ignored                      #",
                " +# - only static flags are accepted (no variables or parameters)           #",
                " +# - dynamic flags will be appended to these on cassandra-env              #",
                " +###########################################################################",
                " +",
                " +#################",
                " +# HEAP SETTINGS #",
                " +#################",
                " +",
                " +# Heap size is automatically calculated by cassandra-env based on this",
                " +# formula: max(min(1/2 ram, 1024MB), min(1/4 ram, 8GB))",
                " +# That is:",
                " +# - calculate 1/2 ram and cap to 1024MB",
                " +# - calculate 1/4 ram and cap to 8192MB",
                " +# - pick the max",
                " +#",
                " +# For production use you may wish to adjust this for your environment.",
                " +# If that's the case, uncomment the -Xmx and Xms options below to override the",
                " +# automatic calculation of JVM heap memory.",
                " +#",
                " +# It is recommended to set min (-Xms) and max (-Xmx) heap sizes to",
                " +# the same value to avoid stop-the-world GC pauses during resize, and",
                " +# so that we can lock the heap in memory on startup to prevent any",
                " +# of it from being swapped out.",
                " +#-Xms4G",
                " +#-Xmx4G",
                " +",
                " +# Young generation size is automatically calculated by cassandra-env",
                " +# based on this formula: min(100 * num_cores, 1/4 * heap size)",
                " +#",
                " +# The main trade-off for the young generation is that the larger it",
                " +# is, the longer GC pause times will be. The shorter it is, the more",
                " +# expensive GC will be (usually).",
                " +#",
                " +# It is not recommended to set the young generation size if using the",
                " +# G1 GC, since that will override the target pause-time goal.",
                " +# More info: http://www.oracle.com/technetwork/articles/java/g1gc-1984535.html",
                " +#",
                " +# The example below assumes a modern 8-core+ machine for decent",
                " +# times. If in doubt, and if you do not particularly want to tweak, go",
                " +# 100 MB per physical CPU core.",
                " +#-Xmn800M",
                " +",
                "++###################################",
                "++# EXPIRATION DATE OVERFLOW POLICY #",
                "++###################################",
                "++",
                "++# Defines how to handle INSERT requests with TTL exceeding the maximum supported expiration date:",
                "++# * REJECT: this is the default policy and will reject any requests with expiration date timestamp after 2038-01-19T03:14:06+00:00.",
                "++# * CAP: any insert with TTL expiring after 2038-01-19T03:14:06+00:00 will expire on 2038-01-19T03:14:06+00:00 and the client will receive a warning.",
                "++# * CAP_NOWARN: same as previous, except that the client warning will not be emitted.",
                "++#",
                "++#-Dcassandra.expiration_date_overflow_policy=REJECT",
                "++",
                " +#################",
                " +#  GC SETTINGS  #",
                " +#################",
                " +",
                " +### CMS Settings",
                " +",
                " +-XX:+UseParNewGC",
                " +-XX:+UseConcMarkSweepGC",
                " +-XX:+CMSParallelRemarkEnabled",
                " +-XX:SurvivorRatio=8",
                " +-XX:MaxTenuringThreshold=1",
                " +-XX:CMSInitiatingOccupancyFraction=75",
                " +-XX:+UseCMSInitiatingOccupancyOnly",
                " +-XX:CMSWaitDuration=10000",
                " +-XX:+CMSParallelInitialMarkEnabled",
                " +-XX:+CMSEdenChunksRecordAlways",
                " +# some JVMs will fill up their heap when accessed via JMX, see CASSANDRA-6541",
                " +-XX:+CMSClassUnloadingEnabled",
                " +",
                " +### G1 Settings (experimental, comment previous section and uncomment section below to enable)",
                " +",
                " +## Use the Hotspot garbage-first collector.",
                " +#-XX:+UseG1GC",
                " +#",
                " +## Have the JVM do less remembered set work during STW, instead",
                " +## preferring concurrent GC. Reduces p99.9 latency.",
                " +#-XX:G1RSetUpdatingPauseTimePercent=5",
                " +#",
                " +## Main G1GC tunable: lowering the pause target will lower throughput and vise versa.",
                " +## 200ms is the JVM default and lowest viable setting",
                " +## 1000ms increases throughput. Keep it smaller than the timeouts in cassandra.yaml.",
                " +#-XX:MaxGCPauseMillis=500",
                " +",
                " +## Optional G1 Settings",
                " +",
                " +# Save CPU time on large (>= 16GB) heaps by delaying region scanning",
                " +# until the heap is 70% full. The default in Hotspot 8u40 is 40%.",
                " +#-XX:InitiatingHeapOccupancyPercent=70",
                " +",
                " +# For systems with > 8 cores, the default ParallelGCThreads is 5/8 the number of logical cores.",
                " +# Otherwise equal to the number of cores when 8 or less.",
                " +# Machines with > 10 cores should try setting these to <= full cores.",
                " +#-XX:ParallelGCThreads=16",
                " +# By default, ConcGCThreads is 1/4 of ParallelGCThreads.",
                " +# Setting both to the same value can reduce STW durations.",
                " +#-XX:ConcGCThreads=16",
                " +",
                " +### GC logging options -- uncomment to enable",
                " +",
                " +-XX:+PrintGCDetails",
                " +-XX:+PrintGCDateStamps",
                " +-XX:+PrintHeapAtGC",
                " +-XX:+PrintTenuringDistribution",
                " +-XX:+PrintGCApplicationStoppedTime",
                " +-XX:+PrintPromotionFailure",
                " +#-XX:PrintFLSStatistics=1",
                " +#-Xloggc:/var/log/cassandra/gc.log",
                " +-XX:+UseGCLogFileRotation",
                " +-XX:NumberOfGCLogFiles=10",
                " +-XX:GCLogFileSize=10M",
                "diff --cc src/java/org/apache/cassandra/cql3/Attributes.java",
                "index 4ed0f833b9,84f423acb6..832d0a780b",
                "--- a/src/java/org/apache/cassandra/cql3/Attributes.java",
                "+++ b/src/java/org/apache/cassandra/cql3/Attributes.java",
                "@@@ -20,4 -20,9 +20,3 @@@ package org.apache.cassandra.cql3",
                "  import java.nio.ByteBuffer;",
                "--import java.util.Collections;",
                " -import java.util.concurrent.TimeUnit;",
                " -",
                " -import com.google.common.collect.Iterables;",
                " -import com.google.common.annotations.VisibleForTesting;",
                " -import org.slf4j.Logger;",
                " -import org.slf4j.LoggerFactory;",
                " +import java.util.List;",
                "@@@ -26,2 -30,3 +24,3 @@@ import org.apache.cassandra.config.CFMe",
                "  import org.apache.cassandra.cql3.functions.Function;",
                " -import org.apache.cassandra.db.ExpiringCell;",
                "++import org.apache.cassandra.db.ExpirationDateOverflowHandling;",
                "  import org.apache.cassandra.db.marshal.Int32Type;",
                "@@@ -105,3 -142,6 +104,6 @@@ public class Attribute",
                "          if (timeToLive == null)",
                "-             return 0;",
                "+         {",
                " -            maybeApplyExpirationDateOverflowPolicy(metadata, metadata.getDefaultTimeToLive(), true);",
                " -            return metadata.getDefaultTimeToLive();",
                "++            ExpirationDateOverflowHandling.maybeApplyExpirationDateOverflowPolicy(metadata, metadata.params.defaultTimeToLive, true);",
                "++            return metadata.params.defaultTimeToLive;",
                "+         }",
                "@@@ -130,2 -170,4 +132,4 @@@",
                " -        maybeApplyExpirationDateOverflowPolicy(metadata, ttl, false);",
                "++        ExpirationDateOverflowHandling.maybeApplyExpirationDateOverflowPolicy(metadata, ttl, false);",
                "+ ",
                "          return ttl;",
                "diff --cc src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "index 536681f6ba,45908deb6a..b5946bbc1b",
                "--- a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "+++ b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java",
                "@@@ -1381,9 -1612,8 +1381,9 @@@ public class ColumnFamilyStore implemen",
                "-     public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, int jobs) throws ExecutionException, InterruptedException",
                " -    public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs) throws ExecutionException, InterruptedException",
                "++    public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTL, int jobs) throws ExecutionException, InterruptedException",
                "      {",
                "-         return scrub(disableSnapshot, skipCorrupted, false, checkData, jobs);",
                " -        return scrub(disableSnapshot, skipCorrupted, false, checkData, reinsertOverflowedTTLRows, jobs);",
                "++        return scrub(disableSnapshot, skipCorrupted, reinsertOverflowedTTL, false, checkData, jobs);",
                "      }",
                " -    public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean alwaysFail, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs) throws ExecutionException, InterruptedException",
                " +    @VisibleForTesting",
                "-     public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean alwaysFail, boolean checkData, int jobs) throws ExecutionException, InterruptedException",
                "++    public CompactionManager.AllSSTableOpStatus scrub(boolean disableSnapshot, boolean skipCorrupted, boolean reinsertOverflowedTTL, boolean alwaysFail, boolean checkData, int jobs) throws ExecutionException, InterruptedException",
                "      {",
                "@@@ -1395,3 -1625,3 +1395,3 @@@",
                "          {",
                "-             return CompactionManager.instance.performScrub(ColumnFamilyStore.this, skipCorrupted, checkData, jobs);",
                " -            return CompactionManager.instance.performScrub(ColumnFamilyStore.this, skipCorrupted, checkData, reinsertOverflowedTTLRows, jobs);",
                "++            return CompactionManager.instance.performScrub(ColumnFamilyStore.this, skipCorrupted, checkData, reinsertOverflowedTTL, jobs);",
                "          }",
                "diff --cc src/java/org/apache/cassandra/db/ExpirationDateOverflowHandling.java",
                "index 0000000000,0000000000..852dcb1f7d",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/src/java/org/apache/cassandra/db/ExpirationDateOverflowHandling.java",
                "@@@ -1,0 -1,0 +1,121 @@@",
                "++/*",
                "++ * Licensed to the Apache Software Foundation (ASF) under one",
                "++ * or more contributor license agreements.  See the NOTICE file",
                "++ * distributed with this work for additional information",
                "++ * regarding copyright ownership.  The ASF licenses this file",
                "++ * to you under the Apache License, Version 2.0 (the",
                "++ * \"License\"); you may not use this file except in compliance",
                "++ * with the License.  You may obtain a copy of the License at",
                "++ *",
                "++ *     http://www.apache.org/licenses/LICENSE-2.0",
                "++ *",
                "++ * Unless required by applicable law or agreed to in writing, software",
                "++ * distributed under the License is distributed on an \"AS IS\" BASIS,",
                "++ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                "++ * See the License for the specific language governing permissions and",
                "++ * limitations under the License.",
                "++ */",
                "++",
                "++package org.apache.cassandra.db;",
                "++",
                "++import java.util.concurrent.TimeUnit;",
                "++",
                "++import com.google.common.annotations.VisibleForTesting;",
                "++import org.slf4j.Logger;",
                "++import org.slf4j.LoggerFactory;",
                "++import org.slf4j.helpers.MessageFormatter;",
                "++",
                "++import org.apache.cassandra.config.CFMetaData;",
                "++import org.apache.cassandra.cql3.Attributes;",
                "++import org.apache.cassandra.db.rows.BufferCell;",
                "++import org.apache.cassandra.db.rows.Cell;",
                "++import org.apache.cassandra.exceptions.InvalidRequestException;",
                "++import org.apache.cassandra.service.ClientWarn;",
                "++import org.apache.cassandra.utils.NoSpamLogger;",
                "++",
                "++public class ExpirationDateOverflowHandling",
                "++{",
                "++    private static final Logger logger = LoggerFactory.getLogger(Attributes.class);",
                "++",
                "++    private static final int EXPIRATION_OVERFLOW_WARNING_INTERVAL_MINUTES = Integer.getInteger(\"cassandra.expiration_overflow_warning_interval_minutes\", 5);",
                "++",
                "++    public enum ExpirationDateOverflowPolicy",
                "++    {",
                "++        REJECT, CAP_NOWARN, CAP",
                "++    }",
                "++",
                "++    @VisibleForTesting",
                "++    public static ExpirationDateOverflowPolicy policy;",
                "++",
                "++    static {",
                "++        String policyAsString = System.getProperty(\"cassandra.expiration_date_overflow_policy\", ExpirationDateOverflowPolicy.REJECT.name());",
                "++        try",
                "++        {",
                "++            policy = ExpirationDateOverflowPolicy.valueOf(policyAsString.toUpperCase());",
                "++        }",
                "++        catch (RuntimeException e)",
                "++        {",
                "++            logger.warn(\"Invalid expiration date overflow policy: {}. Using default: {}\", policyAsString, ExpirationDateOverflowPolicy.REJECT.name());",
                "++            policy = ExpirationDateOverflowPolicy.REJECT;",
                "++        }",
                "++    }",
                "++",
                "++    public static final String MAXIMUM_EXPIRATION_DATE_EXCEEDED_WARNING = \"Request on table {}.{} with {}ttl of {} seconds exceeds maximum supported expiration \" +",
                "++                                                                          \"date of 2038-01-19T03:14:06+00:00 and will have its expiration capped to that date. \" +",
                "++                                                                          \"In order to avoid this use a lower TTL or upgrade to a version where this limitation \" +",
                "++                                                                          \"is fixed. See CASSANDRA-14092 for more details.\";",
                "++",
                "++    public static final String MAXIMUM_EXPIRATION_DATE_EXCEEDED_REJECT_MESSAGE = \"Request on table %s.%s with %sttl of %d seconds exceeds maximum supported expiration \" +",
                "++                                                                                 \"date of 2038-01-19T03:14:06+00:00. In order to avoid this use a lower TTL, change \" +",
                "++                                                                                 \"the expiration date overflow policy or upgrade to a version where this limitation \" +",
                "++                                                                                 \"is fixed. See CASSANDRA-14092 for more details.\";",
                "++",
                "++    public static void maybeApplyExpirationDateOverflowPolicy(CFMetaData metadata, int ttl, boolean isDefaultTTL) throws InvalidRequestException",
                "++    {",
                "++        if (ttl == BufferCell.NO_TTL)",
                "++            return;",
                "++",
                "++        // Check for localExpirationTime overflow (CASSANDRA-14092)",
                "++        int nowInSecs = (int)(System.currentTimeMillis() / 1000);",
                "++        if (ttl + nowInSecs < 0)",
                "++        {",
                "++            switch (policy)",
                "++            {",
                "++                case CAP:",
                "++                    ClientWarn.instance.warn(MessageFormatter.arrayFormat(MAXIMUM_EXPIRATION_DATE_EXCEEDED_WARNING, new Object[] { metadata.ksName,",
                "++                                                                                                                                   metadata.cfName,",
                "++                                                                                                                                   isDefaultTTL? \"default \" : \"\", ttl })",
                "++                                                             .getMessage());",
                "++                case CAP_NOWARN:",
                "++                    /**",
                "++                     * Capping at this stage is basically not rejecting the request. The actual capping is done",
                "++                     * by {@link #computeLocalExpirationTime(int, int)}, which converts the negative TTL",
                "++                     * to {@link org.apache.cassandra.db.BufferExpiringCell#MAX_DELETION_TIME}",
                "++                     */",
                "++                    NoSpamLogger.log(logger, NoSpamLogger.Level.WARN, EXPIRATION_OVERFLOW_WARNING_INTERVAL_MINUTES, TimeUnit.MINUTES, MAXIMUM_EXPIRATION_DATE_EXCEEDED_WARNING,",
                "++                                     metadata.ksName, metadata.cfName, isDefaultTTL? \"default \" : \"\", ttl);",
                "++                    return;",
                "++",
                "++                default:",
                "++                    throw new InvalidRequestException(String.format(MAXIMUM_EXPIRATION_DATE_EXCEEDED_REJECT_MESSAGE, metadata.ksName, metadata.cfName,",
                "++                                                                    isDefaultTTL? \"default \" : \"\", ttl));",
                "++            }",
                "++        }",
                "++    }",
                "++",
                "++    /**",
                "++     * This method computes the {@link Cell#localDeletionTime()}, maybe capping to the maximum representable value",
                "++     * which is {@link Cell#MAX_DELETION_TIME}.",
                "++     *",
                "++     * Please note that the {@link ExpirationDateOverflowHandling.ExpirationDateOverflowPolicy} is applied",
                "++     * during {@link ExpirationDateOverflowHandling#maybeApplyExpirationDateOverflowPolicy(CFMetaData, int, boolean)},",
                "++     * so if the request was not denied it means its expiration date should be capped.",
                "++     *",
                "++     * See CASSANDRA-14092",
                "++     */",
                "++    public static int computeLocalExpirationTime(int nowInSec, int timeToLive)",
                "++    {",
                "++        int localExpirationTime = nowInSec + timeToLive;",
                "++        return localExpirationTime >= 0? localExpirationTime : Cell.MAX_DELETION_TIME;",
                "++    }",
                "++}",
                "diff --cc src/java/org/apache/cassandra/db/LegacyLayout.java",
                "index 2117dd6e37,0000000000..912d5910bf",
                "mode 100644,000000..100644",
                "--- a/src/java/org/apache/cassandra/db/LegacyLayout.java",
                "+++ b/src/java/org/apache/cassandra/db/LegacyLayout.java",
                "@@@ -1,2504 -1,0 +1,2509 @@@",
                " +/*",
                " + * Licensed to the Apache Software Foundation (ASF) under one",
                " + * or more contributor license agreements.  See the NOTICE file",
                " + * distributed with this work for additional information",
                " + * regarding copyright ownership.  The ASF licenses this file",
                " + * to you under the Apache License, Version 2.0 (the",
                " + * \"License\"); you may not use this file except in compliance",
                " + * with the License.  You may obtain a copy of the License at",
                " + *",
                " + *     http://www.apache.org/licenses/LICENSE-2.0",
                " + *",
                " + * Unless required by applicable law or agreed to in writing, software",
                " + * distributed under the License is distributed on an \"AS IS\" BASIS,",
                " + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                " + * See the License for the specific language governing permissions and",
                " + * limitations under the License.",
                " + */",
                " +package org.apache.cassandra.db;",
                " +",
                " +import java.io.DataInput;",
                " +import java.io.IOException;",
                " +import java.io.IOError;",
                " +import java.nio.ByteBuffer;",
                " +import java.security.MessageDigest;",
                " +import java.util.*;",
                " +",
                " +import org.apache.cassandra.cql3.SuperColumnCompatibility;",
                "++import org.apache.cassandra.thrift.Column;",
                " +import org.apache.cassandra.utils.AbstractIterator;",
                " +import com.google.common.collect.Iterators;",
                " +import com.google.common.collect.Lists;",
                " +import com.google.common.collect.PeekingIterator;",
                " +",
                " +import org.apache.cassandra.config.CFMetaData;",
                " +import org.apache.cassandra.config.ColumnDefinition;",
                " +import org.apache.cassandra.db.filter.ColumnFilter;",
                " +import org.apache.cassandra.db.filter.DataLimits;",
                " +import org.apache.cassandra.db.rows.*;",
                " +import org.apache.cassandra.db.partitions.*;",
                " +import org.apache.cassandra.db.context.CounterContext;",
                " +import org.apache.cassandra.db.marshal.*;",
                " +import org.apache.cassandra.io.util.DataInputPlus;",
                " +import org.apache.cassandra.io.util.DataOutputPlus;",
                " +import org.apache.cassandra.net.MessagingService;",
                " +import org.apache.cassandra.utils.*;",
                " +import org.slf4j.Logger;",
                " +import org.slf4j.LoggerFactory;",
                " +",
                " +import static org.apache.cassandra.utils.ByteBufferUtil.bytes;",
                " +",
                " +/**",
                " + * Functions to deal with the old format.",
                " + */",
                " +public abstract class LegacyLayout",
                " +{",
                " +    private static final Logger logger = LoggerFactory.getLogger(LegacyLayout.class);",
                " +",
                " +    public final static int MAX_CELL_NAME_LENGTH = FBUtilities.MAX_UNSIGNED_SHORT;",
                " +",
                " +    public final static int STATIC_PREFIX = 0xFFFF;",
                " +",
                " +    public final static int DELETION_MASK        = 0x01;",
                " +    public final static int EXPIRATION_MASK      = 0x02;",
                " +    public final static int COUNTER_MASK         = 0x04;",
                " +    public final static int COUNTER_UPDATE_MASK  = 0x08;",
                " +    private final static int RANGE_TOMBSTONE_MASK = 0x10;",
                " +",
                " +    private LegacyLayout() {}",
                " +",
                " +    public static AbstractType<?> makeLegacyComparator(CFMetaData metadata)",
                " +    {",
                " +        ClusteringComparator comparator = metadata.comparator;",
                " +        if (!metadata.isCompound())",
                " +        {",
                " +            assert comparator.size() == 1;",
                " +            return comparator.subtype(0);",
                " +        }",
                " +",
                " +        boolean hasCollections = metadata.hasCollectionColumns() || metadata.hasDroppedCollectionColumns();",
                " +        List<AbstractType<?>> types = new ArrayList<>(comparator.size() + (metadata.isDense() ? 0 : 1) + (hasCollections ? 1 : 0));",
                " +",
                " +        types.addAll(comparator.subtypes());",
                " +",
                " +        if (!metadata.isDense())",
                " +        {",
                " +            types.add(UTF8Type.instance);",
                " +",
                " +            if (hasCollections)",
                " +            {",
                " +                Map<ByteBuffer, CollectionType> defined = new HashMap<>();",
                " +",
                " +                for (CFMetaData.DroppedColumn def : metadata.getDroppedColumns().values())",
                " +                    if (def.type instanceof CollectionType && def.type.isMultiCell())",
                " +                        defined.put(bytes(def.name), (CollectionType) def.type);",
                " +",
                " +                for (ColumnDefinition def : metadata.partitionColumns())",
                " +                    if (def.type instanceof CollectionType && def.type.isMultiCell())",
                " +                        defined.put(def.name.bytes, (CollectionType) def.type);",
                " +",
                " +                types.add(ColumnToCollectionType.getInstance(defined));",
                " +            }",
                " +        }",
                " +        return CompositeType.getInstance(types);",
                " +    }",
                " +",
                " +    public static LegacyCellName decodeCellName(CFMetaData metadata, ByteBuffer superColumnName, ByteBuffer cellname)",
                " +    throws UnknownColumnException",
                " +    {",
                " +        assert cellname != null;",
                " +        if (metadata.isSuper())",
                " +        {",
                " +            assert superColumnName != null;",
                " +            return decodeForSuperColumn(metadata, new Clustering(superColumnName), cellname);",
                " +        }",
                " +",
                " +        assert superColumnName == null;",
                " +        return decodeCellName(metadata, cellname);",
                " +    }",
                " +",
                " +    private static LegacyCellName decodeForSuperColumn(CFMetaData metadata, Clustering clustering, ByteBuffer subcol)",
                " +    {",
                " +        ColumnDefinition def = metadata.getColumnDefinition(subcol);",
                " +        if (def != null)",
                " +        {",
                " +            // it's a statically defined subcolumn",
                " +            return new LegacyCellName(clustering, def, null);",
                " +        }",
                " +",
                " +        def = metadata.compactValueColumn();",
                " +        assert def != null && def.type instanceof MapType;",
                " +        return new LegacyCellName(clustering, def, subcol);",
                " +    }",
                " +",
                " +    public static LegacyCellName decodeCellName(CFMetaData metadata, ByteBuffer cellname) throws UnknownColumnException",
                " +    {",
                " +        return decodeCellName(metadata, cellname, false);",
                " +    }",
                " +",
                " +    public static LegacyCellName decodeCellName(CFMetaData metadata, ByteBuffer cellname, boolean readAllAsDynamic) throws UnknownColumnException",
                " +    {",
                " +        Clustering clustering = decodeClustering(metadata, cellname);",
                " +",
                " +        if (metadata.isSuper())",
                " +            return decodeForSuperColumn(metadata, clustering, CompositeType.extractComponent(cellname, 1));",
                " +",
                " +        if (metadata.isDense() || (metadata.isCompactTable() && readAllAsDynamic))",
                " +            return new LegacyCellName(clustering, metadata.compactValueColumn(), null);",
                " +",
                " +        ByteBuffer column = metadata.isCompound() ? CompositeType.extractComponent(cellname, metadata.comparator.size()) : cellname;",
                " +        if (column == null)",
                " +        {",
                " +            // Tables for composite 2ndary indexes used to be compound but dense, but we've transformed them into regular tables",
                " +            // (non compact ones) but with no regular column (i.e. we only care about the clustering). So we'll get here",
                " +            // in that case, and what we want to return is basically a row marker.",
                " +            if (metadata.partitionColumns().isEmpty())",
                " +                return new LegacyCellName(clustering, null, null);",
                " +",
                " +            // Otherwise, we shouldn't get there",
                " +            throw new IllegalArgumentException(\"No column name component found in cell name\");",
                " +        }",
                " +",
                " +        // Row marker, this is ok",
                " +        if (!column.hasRemaining())",
                " +            return new LegacyCellName(clustering, null, null);",
                " +",
                " +        ColumnDefinition def = metadata.getColumnDefinition(column);",
                " +        if ((def == null) || def.isPrimaryKeyColumn())",
                " +        {",
                " +            // If it's a compact table, it means the column is in fact a \"dynamic\" one",
                " +            if (metadata.isCompactTable())",
                " +                return new LegacyCellName(new Clustering(column), metadata.compactValueColumn(), null);",
                " +",
                " +            if (def == null)",
                " +                throw new UnknownColumnException(metadata, column);",
                " +            else",
                " +                throw new IllegalArgumentException(\"Cannot add primary key column to partition update\");",
                " +        }",
                " +",
                " +        ByteBuffer collectionElement = metadata.isCompound() ? CompositeType.extractComponent(cellname, metadata.comparator.size() + 1) : null;",
                " +",
                " +        // Note that because static compact columns are translated to static defs in the new world order, we need to force a static",
                " +        // clustering if the definition is static (as it might not be in this case).",
                " +        return new LegacyCellName(def.isStatic() ? Clustering.STATIC_CLUSTERING : clustering, def, collectionElement);",
                " +    }",
                " +",
                " +    public static LegacyBound decodeBound(CFMetaData metadata, ByteBuffer bound, boolean isStart)",
                " +    {",
                " +        if (!bound.hasRemaining())",
                " +            return isStart ? LegacyBound.BOTTOM : LegacyBound.TOP;",
                " +",
                " +        if (!metadata.isCompound())",
                " +        {",
                " +            // The non compound case is a lot easier, in that there is no EOC nor collection to worry about, so dealing",
                " +            // with that first.",
                " +            return new LegacyBound(isStart ? Slice.Bound.inclusiveStartOf(bound) : Slice.Bound.inclusiveEndOf(bound), false, null);",
                " +        }",
                " +",
                " +        int clusteringSize = metadata.comparator.size();",
                " +",
                " +        List<ByteBuffer> components = CompositeType.splitName(bound);",
                " +        byte eoc = CompositeType.lastEOC(bound);",
                " +",
                " +        // There can be  more components than the clustering size only in the case this is the bound of a collection",
                " +        // range tombstone. In which case, there is exactly one more component, and that component is the name of the",
                " +        // collection being selected/deleted.",
                " +        assert components.size() <= clusteringSize || (!metadata.isCompactTable() && components.size() == clusteringSize + 1);",
                " +",
                " +        ColumnDefinition collectionName = null;",
                " +        if (components.size() > clusteringSize)",
                " +            collectionName = metadata.getColumnDefinition(components.remove(clusteringSize));",
                " +",
                " +        boolean isInclusive;",
                " +        if (isStart)",
                " +        {",
                " +            isInclusive = eoc <= 0;",
                " +        }",
                " +        else",
                " +        {",
                " +            isInclusive = eoc >= 0;",
                " +",
                " +            // for an end bound, if we only have a prefix of all the components and the final EOC is zero,",
                " +            // then it should only match up to the prefix but no further, that is, it is an inclusive bound",
                " +            // of the exact prefix but an exclusive bound of anything beyond it, so adding an empty",
                " +            // composite value ensures this behavior, see CASSANDRA-12423 for more details",
                " +            if (eoc == 0 && components.size() < clusteringSize)",
                " +            {",
                " +                components.add(ByteBufferUtil.EMPTY_BYTE_BUFFER);",
                " +                isInclusive = false;",
                " +            }",
                " +        }",
                " +",
                " +        Slice.Bound.Kind boundKind = Slice.Bound.boundKind(isStart, isInclusive);",
                " +        Slice.Bound sb = Slice.Bound.create(boundKind, components.toArray(new ByteBuffer[components.size()]));",
                " +        return new LegacyBound(sb, metadata.isCompound() && CompositeType.isStaticName(bound), collectionName);",
                " +    }",
                " +",
                " +    public static ByteBuffer encodeBound(CFMetaData metadata, Slice.Bound bound, boolean isStart)",
                " +    {",
                " +        if (bound == Slice.Bound.BOTTOM || bound == Slice.Bound.TOP || metadata.comparator.size() == 0)",
                " +            return ByteBufferUtil.EMPTY_BYTE_BUFFER;",
                " +",
                " +        ClusteringPrefix clustering = bound.clustering();",
                " +",
                " +        if (!metadata.isCompound())",
                " +        {",
                " +            assert clustering.size() == 1;",
                " +            return clustering.get(0);",
                " +        }",
                " +",
                " +        CompositeType ctype = CompositeType.getInstance(metadata.comparator.subtypes());",
                " +        CompositeType.Builder builder = ctype.builder();",
                " +        for (int i = 0; i < clustering.size(); i++)",
                " +            builder.add(clustering.get(i));",
                " +",
                " +        if (isStart)",
                " +            return bound.isInclusive() ? builder.build() : builder.buildAsEndOfRange();",
                " +        else",
                " +            return bound.isInclusive() ? builder.buildAsEndOfRange() : builder.build();",
                " +    }",
                " +",
                " +    public static ByteBuffer encodeCellName(CFMetaData metadata, ClusteringPrefix clustering, ByteBuffer columnName, ByteBuffer collectionElement)",
                " +    {",
                " +        boolean isStatic = clustering == Clustering.STATIC_CLUSTERING;",
                " +",
                " +        if (!metadata.isCompound())",
                " +        {",
                " +            if (isStatic)",
                " +                return columnName;",
                " +",
                " +            assert clustering.size() == 1 : \"Expected clustering size to be 1, but was \" + clustering.size();",
                " +            return clustering.get(0);",
                " +        }",
                " +",
                " +        // We use comparator.size() rather than clustering.size() because of static clusterings",
                " +        int clusteringSize = metadata.comparator.size();",
                " +        int size = clusteringSize + (metadata.isDense() ? 0 : 1) + (collectionElement == null ? 0 : 1);",
                " +        if (metadata.isSuper())",
                " +            size = clusteringSize + 1;",
                " +        ByteBuffer[] values = new ByteBuffer[size];",
                " +        for (int i = 0; i < clusteringSize; i++)",
                " +        {",
                " +            if (isStatic)",
                " +            {",
                " +                values[i] = ByteBufferUtil.EMPTY_BYTE_BUFFER;",
                " +                continue;",
                " +            }",
                " +",
                " +            ByteBuffer v = clustering.get(i);",
                " +            // we can have null (only for dense compound tables for backward compatibility reasons) but that",
                " +            // means we're done and should stop there as far as building the composite is concerned.",
                " +            if (v == null)",
                " +                return CompositeType.build(Arrays.copyOfRange(values, 0, i));",
                " +",
                " +            values[i] = v;",
                " +        }",
                " +",
                " +        if (metadata.isSuper())",
                " +        {",
                " +            // We need to set the \"column\" (in thrift terms) name, i.e. the value corresponding to the subcomparator.",
                " +            // What it is depends if this a cell for a declared \"static\" column or a \"dynamic\" column part of the",
                " +            // super-column internal map.",
                " +            assert columnName != null; // This should never be null for supercolumns, see decodeForSuperColumn() above",
                " +            values[clusteringSize] = columnName.equals(SuperColumnCompatibility.SUPER_COLUMN_MAP_COLUMN)",
                " +                                   ? collectionElement",
                " +                                   : columnName;",
                " +        }",
                " +        else",
                " +        {",
                " +            if (!metadata.isDense())",
                " +                values[clusteringSize] = columnName;",
                " +            if (collectionElement != null)",
                " +                values[clusteringSize + 1] = collectionElement;",
                " +        }",
                " +",
                " +        return CompositeType.build(isStatic, values);",
                " +    }",
                " +",
                " +    public static Clustering decodeClustering(CFMetaData metadata, ByteBuffer value)",
                " +    {",
                " +        int csize = metadata.comparator.size();",
                " +        if (csize == 0)",
                " +            return Clustering.EMPTY;",
                " +",
                " +        if (metadata.isCompound() && CompositeType.isStaticName(value))",
                " +            return Clustering.STATIC_CLUSTERING;",
                " +",
                " +        List<ByteBuffer> components = metadata.isCompound()",
                " +                                    ? CompositeType.splitName(value)",
                " +                                    : Collections.singletonList(value);",
                " +",
                " +        return new Clustering(components.subList(0, Math.min(csize, components.size())).toArray(new ByteBuffer[csize]));",
                " +    }",
                " +",
                " +    public static ByteBuffer encodeClustering(CFMetaData metadata, ClusteringPrefix clustering)",
                " +    {",
                " +        if (clustering.size() == 0)",
                " +            return ByteBufferUtil.EMPTY_BYTE_BUFFER;",
                " +",
                " +        if (!metadata.isCompound())",
                " +        {",
                " +            assert clustering.size() == 1;",
                " +            return clustering.get(0);",
                " +        }",
                " +",
                " +        ByteBuffer[] values = new ByteBuffer[clustering.size()];",
                " +        for (int i = 0; i < clustering.size(); i++)",
                " +            values[i] = clustering.get(i);",
                " +        return CompositeType.build(values);",
                " +    }",
                " +",
                " +    /**",
                " +     * The maximum number of cells to include per partition when converting to the old format.",
                " +     * <p>",
                " +     * We already apply the limit during the actual query, but for queries that counts cells and not rows (thrift queries",
                " +     * and distinct queries as far as old nodes are concerned), we may still include a little bit more than requested",
                " +     * because {@link DataLimits} always include full rows. So if the limit ends in the middle of a queried row, the",
                " +     * full row will be part of our result. This would confuse old nodes however so we make sure to truncate it to",
                " +     * what's expected before writting it on the wire.",
                " +     *",
                " +     * @param command the read commmand for which to determine the maximum cells per partition. This can be {@code null}",
                " +     * in which case {@code Integer.MAX_VALUE} is returned.",
                " +     * @return the maximum number of cells per partition that should be enforced according to the read command if",
                " +     * post-query limitation are in order (see above). This will be {@code Integer.MAX_VALUE} if no such limits are",
                " +     * necessary.",
                " +     */",
                " +    private static int maxCellsPerPartition(ReadCommand command)",
                " +    {",
                " +        if (command == null)",
                " +            return Integer.MAX_VALUE;",
                " +",
                " +        DataLimits limits = command.limits();",
                " +",
                " +        // There is 2 types of DISTINCT queries: those that includes only the partition key, and those that include static columns.",
                " +        // On old nodes, the latter expects the first row in term of CQL count, which is what we already have and there is no additional",
                " +        // limit to apply. The former however expect only one cell per partition and rely on it (See CASSANDRA-10762).",
                " +        if (limits.isDistinct())",
                " +            return command.columnFilter().fetchedColumns().statics.isEmpty() ? 1 : Integer.MAX_VALUE;",
                " +",
                " +        switch (limits.kind())",
                " +        {",
                " +            case THRIFT_LIMIT:",
                " +            case SUPER_COLUMN_COUNTING_LIMIT:",
                " +                return limits.perPartitionCount();",
                " +            default:",
                " +                return Integer.MAX_VALUE;",
                " +        }",
                " +    }",
                " +",
                " +    // For serializing to old wire format",
                " +    public static LegacyUnfilteredPartition fromUnfilteredRowIterator(ReadCommand command, UnfilteredRowIterator iterator)",
                " +    {",
                " +        // we need to extract the range tombstone so materialize the partition. Since this is",
                " +        // used for the on-wire format, this is not worst than it used to be.",
                " +        final ImmutableBTreePartition partition = ImmutableBTreePartition.create(iterator);",
                " +        DeletionInfo info = partition.deletionInfo();",
                " +        Pair<LegacyRangeTombstoneList, Iterator<LegacyCell>> pair = fromRowIterator(partition.metadata(), partition.iterator(), partition.staticRow());",
                " +",
                " +        LegacyLayout.LegacyRangeTombstoneList rtl = pair.left;",
                " +",
                " +        // Processing the cell iterator results in the LegacyRangeTombstoneList being populated, so we do this",
                " +        // before we use the LegacyRangeTombstoneList at all",
                " +        List<LegacyLayout.LegacyCell> cells = Lists.newArrayList(pair.right);",
                " +",
                " +        int maxCellsPerPartition = maxCellsPerPartition(command);",
                " +        if (cells.size() > maxCellsPerPartition)",
                " +            cells = cells.subList(0, maxCellsPerPartition);",
                " +",
                " +        // The LegacyRangeTombstoneList already has range tombstones for the single-row deletions and complex",
                " +        // deletions.  Go through our normal range tombstones and add then to the LegacyRTL so that the range",
                " +        // tombstones all get merged and sorted properly.",
                " +        if (info.hasRanges())",
                " +        {",
                " +            Iterator<RangeTombstone> rangeTombstoneIterator = info.rangeIterator(false);",
                " +            while (rangeTombstoneIterator.hasNext())",
                " +            {",
                " +                RangeTombstone rt = rangeTombstoneIterator.next();",
                " +                Slice slice = rt.deletedSlice();",
                " +                LegacyLayout.LegacyBound start = new LegacyLayout.LegacyBound(slice.start(), false, null);",
                " +                LegacyLayout.LegacyBound end = new LegacyLayout.LegacyBound(slice.end(), false, null);",
                " +                rtl.add(start, end, rt.deletionTime().markedForDeleteAt(), rt.deletionTime().localDeletionTime());",
                " +            }",
                " +        }",
                " +",
                " +        return new LegacyUnfilteredPartition(info.getPartitionDeletion(), rtl, cells);",
                " +    }",
                " +",
                " +    public static void serializeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, DataOutputPlus out, int version) throws IOException",
                " +    {",
                " +        assert version < MessagingService.VERSION_30;",
                " +",
                " +        out.writeBoolean(true);",
                " +",
                " +        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);",
                " +",
                " +        UUIDSerializer.serializer.serialize(partition.metadata().cfId, out, version);",
                " +        DeletionTime.serializer.serialize(legacyPartition.partitionDeletion, out);",
                " +",
                " +        legacyPartition.rangeTombstones.serialize(out, partition.metadata());",
                " +",
                " +        // begin cell serialization",
                " +        out.writeInt(legacyPartition.cells.size());",
                " +        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)",
                " +        {",
                " +            ByteBufferUtil.writeWithShortLength(cell.name.encode(partition.metadata()), out);",
                " +            out.writeByte(cell.serializationFlags());",
                " +            if (cell.isExpiring())",
                " +            {",
                " +                out.writeInt(cell.ttl);",
                " +                out.writeInt(cell.localDeletionTime);",
                " +            }",
                " +            else if (cell.isTombstone())",
                " +            {",
                " +                out.writeLong(cell.timestamp);",
                " +                out.writeInt(TypeSizes.sizeof(cell.localDeletionTime));",
                " +                out.writeInt(cell.localDeletionTime);",
                " +                continue;",
                " +            }",
                " +            else if (cell.isCounterUpdate())",
                " +            {",
                " +                out.writeLong(cell.timestamp);",
                " +                long count = CounterContext.instance().getLocalCount(cell.value);",
                " +                ByteBufferUtil.writeWithLength(ByteBufferUtil.bytes(count), out);",
                " +                continue;",
                " +            }",
                " +            else if (cell.isCounter())",
                " +            {",
                " +                out.writeLong(Long.MIN_VALUE);  // timestampOfLastDelete (not used, and MIN_VALUE is the default)",
                " +            }",
                " +",
                " +            out.writeLong(cell.timestamp);",
                " +            ByteBufferUtil.writeWithLength(cell.value, out);",
                " +        }",
                " +    }",
                " +",
                " +    // For the old wire format",
                " +    // Note: this can return null if an empty partition is serialized!",
                " +    public static UnfilteredRowIterator deserializeLegacyPartition(DataInputPlus in, int version, SerializationHelper.Flag flag, ByteBuffer key) throws IOException",
                " +    {",
                " +        assert version < MessagingService.VERSION_30;",
                " +",
                " +        // This is only used in mutation, and mutation have never allowed \"null\" column families",
                " +        boolean present = in.readBoolean();",
                " +        if (!present)",
                " +            return null;",
                " +",
                " +        CFMetaData metadata = CFMetaData.serializer.deserialize(in, version);",
                " +        LegacyDeletionInfo info = LegacyDeletionInfo.deserialize(metadata, in);",
                " +        int size = in.readInt();",
                " +        Iterator<LegacyCell> cells = deserializeCells(metadata, in, flag, size);",
                " +        SerializationHelper helper = new SerializationHelper(metadata, version, flag);",
                " +        return onWireCellstoUnfilteredRowIterator(metadata, metadata.partitioner.decorateKey(key), info, cells, false, helper);",
                " +    }",
                " +",
                " +    // For the old wire format",
                " +    public static long serializedSizeAsLegacyPartition(ReadCommand command, UnfilteredRowIterator partition, int version)",
                " +    {",
                " +        assert version < MessagingService.VERSION_30;",
                " +",
                " +        if (partition.isEmpty())",
                " +            return TypeSizes.sizeof(false);",
                " +",
                " +        long size = TypeSizes.sizeof(true);",
                " +",
                " +        LegacyLayout.LegacyUnfilteredPartition legacyPartition = LegacyLayout.fromUnfilteredRowIterator(command, partition);",
                " +",
                " +        size += UUIDSerializer.serializer.serializedSize(partition.metadata().cfId, version);",
                " +        size += DeletionTime.serializer.serializedSize(legacyPartition.partitionDeletion);",
                " +        size += legacyPartition.rangeTombstones.serializedSize(partition.metadata());",
                " +",
                " +        // begin cell serialization",
                " +        size += TypeSizes.sizeof(legacyPartition.cells.size());",
                " +        for (LegacyLayout.LegacyCell cell : legacyPartition.cells)",
                " +        {",
                " +            size += ByteBufferUtil.serializedSizeWithShortLength(cell.name.encode(partition.metadata()));",
                " +            size += 1;  // serialization flags",
                " +            if (cell.isExpiring())",
                " +            {",
                " +                size += TypeSizes.sizeof(cell.ttl);",
                " +                size += TypeSizes.sizeof(cell.localDeletionTime);",
                " +            }",
                " +            else if (cell.isTombstone())",
                " +            {",
                " +                size += TypeSizes.sizeof(cell.timestamp);",
                " +                // localDeletionTime replaces cell.value as the body",
                " +                size += TypeSizes.sizeof(TypeSizes.sizeof(cell.localDeletionTime));",
                " +                size += TypeSizes.sizeof(cell.localDeletionTime);",
                " +                continue;",
                " +            }",
                " +            else if (cell.isCounterUpdate())",
                " +            {",
                " +                size += TypeSizes.sizeof(cell.timestamp);",
                " +                long count = CounterContext.instance().getLocalCount(cell.value);",
                " +                size += ByteBufferUtil.serializedSizeWithLength(ByteBufferUtil.bytes(count));",
                " +                continue;",
                " +            }",
                " +            else if (cell.isCounter())",
                " +            {",
                " +                size += TypeSizes.sizeof(Long.MIN_VALUE);  // timestampOfLastDelete",
                " +            }",
                " +",
                " +            size += TypeSizes.sizeof(cell.timestamp);",
                " +            size += ByteBufferUtil.serializedSizeWithLength(cell.value);",
                " +        }",
                " +",
                " +        return size;",
                " +    }",
                " +",
                " +    // For thrift sake",
                " +    public static UnfilteredRowIterator toUnfilteredRowIterator(CFMetaData metadata,",
                " +                                                                DecoratedKey key,",
                " +                                                                LegacyDeletionInfo delInfo,",
                " +                                                                Iterator<LegacyCell> cells)",
                " +    {",
                " +        SerializationHelper helper = new SerializationHelper(metadata, 0, SerializationHelper.Flag.LOCAL);",
                " +        return toUnfilteredRowIterator(metadata, key, delInfo, cells, false, helper);",
                " +    }",
                " +",
                " +    // For deserializing old wire format",
                " +    public static UnfilteredRowIterator onWireCellstoUnfilteredRowIterator(CFMetaData metadata,",
                " +                                                                           DecoratedKey key,",
                " +                                                                           LegacyDeletionInfo delInfo,",
                " +                                                                           Iterator<LegacyCell> cells,",
                " +                                                                           boolean reversed,",
                " +                                                                           SerializationHelper helper)",
                " +    {",
                " +",
                " +        // If the table is a static compact, the \"column_metadata\" are now internally encoded as",
                " +        // static. This has already been recognized by decodeCellName, but it means the cells",
                " +        // provided are not in the expected order (the \"static\" cells are not necessarily at the front).",
                " +        // So sort them to make sure toUnfilteredRowIterator works as expected.",
                " +        // Further, if the query is reversed, then the on-wire format still has cells in non-reversed",
                " +        // order, but we need to have them reverse in the final UnfilteredRowIterator. So reverse them.",
                " +        if (metadata.isStaticCompactTable() || reversed)",
                " +        {",
                " +            List<LegacyCell> l = new ArrayList<>();",
                " +            Iterators.addAll(l, cells);",
                " +            Collections.sort(l, legacyCellComparator(metadata, reversed));",
                " +            cells = l.iterator();",
                " +        }",
                " +",
                " +        return toUnfilteredRowIterator(metadata, key, delInfo, cells, reversed, helper);",
                " +    }",
                " +",
                " +    private static UnfilteredRowIterator toUnfilteredRowIterator(CFMetaData metadata,",
                " +                                                                 DecoratedKey key,",
                " +                                                                 LegacyDeletionInfo delInfo,",
                " +                                                                 Iterator<LegacyCell> cells,",
                " +                                                                 boolean reversed,",
                " +                                                                 SerializationHelper helper)",
                " +    {",
                " +        // A reducer that basically does nothing, we know the 2 merged iterators can't have conflicting atoms (since we merge cells with range tombstones).",
                " +        MergeIterator.Reducer<LegacyAtom, LegacyAtom> reducer = new MergeIterator.Reducer<LegacyAtom, LegacyAtom>()",
                " +        {",
                " +            private LegacyAtom atom;",
                " +",
                " +            public void reduce(int idx, LegacyAtom current)",
                " +            {",
                " +                // We're merging cell with range tombstones, so we should always only have a single atom to reduce.",
                " +                assert atom == null;",
                " +                atom = current;",
                " +            }",
                " +",
                " +            protected LegacyAtom getReduced()",
                " +            {",
                " +                return atom;",
                " +            }",
                " +",
                " +            protected void onKeyChange()",
                " +            {",
                " +                atom = null;",
                " +            }",
                " +        };",
                " +        List<Iterator<LegacyAtom>> iterators = Arrays.asList(asLegacyAtomIterator(cells), asLegacyAtomIterator(delInfo.inRowRangeTombstones()));",
                " +        PeekingIterator<LegacyAtom> atoms = Iterators.peekingIterator(MergeIterator.get(iterators, legacyAtomComparator(metadata), reducer));",
                " +",
                " +        // Check if we have some static",
                " +        Row staticRow = atoms.hasNext() && atoms.peek().isStatic()",
                " +                      ? getNextRow(CellGrouper.staticGrouper(metadata, helper), atoms)",
                " +                      : Rows.EMPTY_STATIC_ROW;",
                " +",
                " +        Iterator<Row> rows = convertToRows(new CellGrouper(metadata, helper), atoms);",
                " +        Iterator<RangeTombstone> ranges = delInfo.deletionInfo.rangeIterator(reversed);",
                " +        return new RowAndDeletionMergeIterator(metadata,",
                " +                                               key,",
                " +                                               delInfo.deletionInfo.getPartitionDeletion(),",
                " +                                               ColumnFilter.all(metadata),",
                " +                                               staticRow,",
                " +                                               reversed,",
                " +                                               EncodingStats.NO_STATS,",
                " +                                               rows,",
                " +                                               ranges,",
                " +                                               true);",
                " +    }",
                " +",
                " +    public static Row extractStaticColumns(CFMetaData metadata, DataInputPlus in, Columns statics) throws IOException",
                " +    {",
                " +        assert !statics.isEmpty();",
                " +        assert metadata.isCompactTable();",
                " +",
                " +        if (metadata.isSuper())",
                " +            // TODO: there is in practice nothing to do here, but we need to handle the column_metadata for super columns somewhere else",
                " +            throw new UnsupportedOperationException();",
                " +",
                " +        Set<ByteBuffer> columnsToFetch = new HashSet<>(statics.size());",
                " +        for (ColumnDefinition column : statics)",
                " +            columnsToFetch.add(column.name.bytes);",
                " +",
                " +        Row.Builder builder = BTreeRow.unsortedBuilder(FBUtilities.nowInSeconds());",
                " +        builder.newRow(Clustering.STATIC_CLUSTERING);",
                " +",
                " +        boolean foundOne = false;",
                " +        LegacyAtom atom;",
                " +        while ((atom = readLegacyAtomSkippingUnknownColumn(metadata,in)) != null)",
                " +        {",
                " +            if (atom.isCell())",
                " +            {",
                " +                LegacyCell cell = atom.asCell();",
                " +                if (!columnsToFetch.contains(cell.name.encode(metadata)))",
                " +                    continue;",
                " +",
                " +                foundOne = true;",
                " +                builder.addCell(new BufferCell(cell.name.column, cell.timestamp, cell.ttl, cell.localDeletionTime, cell.value, null));",
                " +            }",
                " +            else",
                " +            {",
                " +                LegacyRangeTombstone tombstone = atom.asRangeTombstone();",
                " +                // TODO: we need to track tombstones and potentially ignore cells that are",
                " +                // shadowed (or even better, replace them by tombstones).",
                " +                throw new UnsupportedOperationException();",
                " +            }",
                " +        }",
                " +",
                " +        return foundOne ? builder.build() : Rows.EMPTY_STATIC_ROW;",
                " +    }",
                " +",
                " +    private static LegacyAtom readLegacyAtomSkippingUnknownColumn(CFMetaData metadata, DataInputPlus in)",
                " +    throws IOException",
                " +    {",
                " +        while (true)",
                " +        {",
                " +            try",
                " +            {",
                " +                return readLegacyAtom(metadata, in, false);",
                " +            }",
                " +            catch (UnknownColumnException e)",
                " +            {",
                " +                // Simply skip, as the method name implies.",
                " +            }",
                " +        }",
                " +",
                " +    }",
                " +",
                " +    private static Row getNextRow(CellGrouper grouper, PeekingIterator<? extends LegacyAtom> cells)",
                " +    {",
                " +        if (!cells.hasNext())",
                " +            return null;",
                " +",
                " +        grouper.reset();",
                " +        while (cells.hasNext() && grouper.addAtom(cells.peek()))",
                " +        {",
                " +            // We've added the cell already in the grouper, so just skip it",
                " +            cells.next();",
                " +        }",
                " +        return grouper.getRow();",
                " +    }",
                " +",
                " +    @SuppressWarnings(\"unchecked\")",
                " +    private static Iterator<LegacyAtom> asLegacyAtomIterator(Iterator<? extends LegacyAtom> iter)",
                " +    {",
                " +        return (Iterator<LegacyAtom>)iter;",
                " +    }",
                " +",
                " +    private static Iterator<Row> convertToRows(final CellGrouper grouper, final PeekingIterator<LegacyAtom> atoms)",
                " +    {",
                " +        return new AbstractIterator<Row>()",
                " +        {",
                " +            protected Row computeNext()",
                " +            {",
                " +                if (!atoms.hasNext())",
                " +                    return endOfData();",
                " +",
                " +                return getNextRow(grouper, atoms);",
                " +            }",
                " +        };",
                " +    }",
                " +",
                " +    public static Pair<LegacyRangeTombstoneList, Iterator<LegacyCell>> fromRowIterator(final RowIterator iterator)",
                " +    {",
                " +        return fromRowIterator(iterator.metadata(), iterator, iterator.staticRow());",
                " +    }",
                " +",
                " +    private static Pair<LegacyRangeTombstoneList, Iterator<LegacyCell>> fromRowIterator(final CFMetaData metadata, final Iterator<Row> iterator, final Row staticRow)",
                " +    {",
                " +        LegacyRangeTombstoneList deletions = new LegacyRangeTombstoneList(new LegacyBoundComparator(metadata.comparator), 10);",
                " +        Iterator<LegacyCell> cells = new AbstractIterator<LegacyCell>()",
                " +        {",
                " +            private Iterator<LegacyCell> currentRow = initializeRow();",
                " +",
                " +            private Iterator<LegacyCell> initializeRow()",
                " +            {",
                " +                if (staticRow == null || staticRow.isEmpty())",
                " +                    return Collections.<LegacyLayout.LegacyCell>emptyIterator();",
                " +",
                " +                Pair<LegacyRangeTombstoneList, Iterator<LegacyCell>> row = fromRow(metadata, staticRow);",
                " +                deletions.addAll(row.left);",
                " +                return row.right;",
                " +            }",
                " +",
                " +            protected LegacyCell computeNext()",
                " +            {",
                " +                while (true)",
                " +                {",
                " +                    if (currentRow.hasNext())",
                " +                        return currentRow.next();",
                " +",
                " +                    if (!iterator.hasNext())",
                " +                        return endOfData();",
                " +",
                " +                    Pair<LegacyRangeTombstoneList, Iterator<LegacyCell>> row = fromRow(metadata, iterator.next());",
                " +                    deletions.addAll(row.left);",
                " +                    currentRow = row.right;",
                " +                }",
                " +            }",
                " +        };",
                " +",
                " +        return Pair.create(deletions, cells);",
                " +    }",
                " +",
                " +    private static Pair<LegacyRangeTombstoneList, Iterator<LegacyCell>> fromRow(final CFMetaData metadata, final Row row)",
                " +    {",
                " +        // convert any complex deletions or row deletion into normal range tombstones so that we can build and send a proper RangeTombstoneList",
                " +        // to legacy nodes",
                " +        LegacyRangeTombstoneList deletions = new LegacyRangeTombstoneList(new LegacyBoundComparator(metadata.comparator), 10);",
                " +",
                " +        if (!row.deletion().isLive())",
                " +        {",
                " +            Clustering clustering = row.clustering();",
                " +            Slice.Bound startBound = Slice.Bound.inclusiveStartOf(clustering);",
                " +            Slice.Bound endBound = Slice.Bound.inclusiveEndOf(clustering);",
                " +",
                " +            LegacyBound start = new LegacyLayout.LegacyBound(startBound, false, null);",
                " +            LegacyBound end = new LegacyLayout.LegacyBound(endBound, false, null);",
                " +",
                " +            deletions.add(start, end, row.deletion().time().markedForDeleteAt(), row.deletion().time().localDeletionTime());",
                " +        }",
                " +",
                " +        for (ColumnData cd : row)",
                " +        {",
                " +            ColumnDefinition col = cd.column();",
                " +            if (col.isSimple())",
                " +                continue;",
                " +",
                " +            DeletionTime delTime = ((ComplexColumnData)cd).complexDeletion();",
                " +            if (!delTime.isLive())",
                " +            {",
                " +                Clustering clustering = row.clustering();",
                " +",
                " +                Slice.Bound startBound = Slice.Bound.inclusiveStartOf(clustering);",
                " +                Slice.Bound endBound = Slice.Bound.inclusiveEndOf(clustering);",
                " +",
                " +                LegacyLayout.LegacyBound start = new LegacyLayout.LegacyBound(startBound, col.isStatic(), col);",
                " +                LegacyLayout.LegacyBound end = new LegacyLayout.LegacyBound(endBound, col.isStatic(), col);",
                " +",
                " +                deletions.add(start, end, delTime.markedForDeleteAt(), delTime.localDeletionTime());",
                " +            }",
                " +        }",
                " +",
                " +        Iterator<LegacyCell> cells = new AbstractIterator<LegacyCell>()",
                " +        {",
                " +            private final Iterator<Cell> cells = row.cellsInLegacyOrder(metadata, false).iterator();",
                " +            // we don't have (and shouldn't have) row markers for compact tables.",
                " +            private boolean hasReturnedRowMarker = metadata.isCompactTable();",
                " +",
                " +            protected LegacyCell computeNext()",
                " +            {",
                " +                if (!hasReturnedRowMarker)",
                " +                {",
                " +                    hasReturnedRowMarker = true;",
                " +",
                " +                    // don't include a row marker if there's no timestamp on the primary key; this is the 3.0+ equivalent",
                " +                    // of a row marker",
                " +                    if (!row.primaryKeyLivenessInfo().isEmpty())",
                " +                    {",
                " +                        LegacyCellName cellName = new LegacyCellName(row.clustering(), null, null);",
                " +                        LivenessInfo info = row.primaryKeyLivenessInfo();",
                " +                        return new LegacyCell(info.isExpiring() ? LegacyCell.Kind.EXPIRING : LegacyCell.Kind.REGULAR, cellName, ByteBufferUtil.EMPTY_BYTE_BUFFER, info.timestamp(), info.localExpirationTime(), info.ttl());",
                " +                    }",
                " +                }",
                " +",
                " +                if (!cells.hasNext())",
                " +                    return endOfData();",
                " +",
                " +                return makeLegacyCell(row.clustering(), cells.next());",
                " +            }",
                " +        };",
                " +        return Pair.create(deletions, cells);",
                " +    }",
                " +",
                " +    private static LegacyCell makeLegacyCell(Clustering clustering, Cell cell)",
                " +    {",
                " +        LegacyCell.Kind kind;",
                " +        if (cell.isCounterCell())",
                " +            kind = LegacyCell.Kind.COUNTER;",
                " +        else if (cell.isTombstone())",
                " +            kind = LegacyCell.Kind.DELETED;",
                " +        else if (cell.isExpiring())",
                " +            kind = LegacyCell.Kind.EXPIRING;",
                " +        else",
                " +            kind = LegacyCell.Kind.REGULAR;",
                " +",
                " +        CellPath path = cell.path();",
                " +        assert path == null || path.size() == 1;",
                " +        LegacyCellName name = new LegacyCellName(clustering, cell.column(), path == null ? null : path.get(0));",
                " +        return new LegacyCell(kind, name, cell.value(), cell.timestamp(), cell.localDeletionTime(), cell.ttl());",
                " +    }",
                " +",
                " +    public static RowIterator toRowIterator(final CFMetaData metadata,",
                " +                                            final DecoratedKey key,",
                " +                                            final Iterator<LegacyCell> cells,",
                " +                                            final int nowInSec)",
                " +    {",
                " +        SerializationHelper helper = new SerializationHelper(metadata, 0, SerializationHelper.Flag.LOCAL);",
                " +        return UnfilteredRowIterators.filter(toUnfilteredRowIterator(metadata, key, LegacyDeletionInfo.live(), cells, false, helper), nowInSec);",
                " +    }",
                " +",
                " +    public static Comparator<LegacyCell> legacyCellComparator(CFMetaData metadata)",
                " +    {",
                " +        return legacyCellComparator(metadata, false);",
                " +    }",
                " +",
                " +    public static Comparator<LegacyCell> legacyCellComparator(final CFMetaData metadata, final boolean reversed)",
                " +    {",
                " +        final Comparator<LegacyCellName> cellNameComparator = legacyCellNameComparator(metadata, reversed);",
                " +        return new Comparator<LegacyCell>()",
                " +        {",
                " +            public int compare(LegacyCell cell1, LegacyCell cell2)",
                " +            {",
                " +                LegacyCellName c1 = cell1.name;",
                " +                LegacyCellName c2 = cell2.name;",
                " +",
                " +                int c = cellNameComparator.compare(c1, c2);",
                " +                if (c != 0)",
                " +                    return c;",
                " +",
                " +                // The actual sorting when the cellname is equal doesn't matter, we just want to make",
                " +                // sure the cells are not considered equal.",
                " +                if (cell1.timestamp != cell2.timestamp)",
                " +                    return cell1.timestamp < cell2.timestamp ? -1 : 1;",
                " +",
                " +                if (cell1.localDeletionTime != cell2.localDeletionTime)",
                " +                    return cell1.localDeletionTime < cell2.localDeletionTime ? -1 : 1;",
                " +",
                " +                return cell1.value.compareTo(cell2.value);",
                " +            }",
                " +        };",
                " +    }",
                " +",
                " +    // Note that this doesn't exactly compare cells as they were pre-3.0 because within a row they sort columns like",
                " +    // in 3.0, that is, with simple columns before complex columns. In other words, this comparator makes sure cells",
                " +    // are in the proper order to convert them to actual 3.0 rows.",
                " +    public static Comparator<LegacyCellName> legacyCellNameComparator(final CFMetaData metadata, final boolean reversed)",
                " +    {",
                " +        return new Comparator<LegacyCellName>()",
                " +        {",
                " +            public int compare(LegacyCellName c1, LegacyCellName c2)",
                " +            {",
                " +                // Compare clustering first",
                " +                if (c1.clustering == Clustering.STATIC_CLUSTERING)",
                " +                {",
                " +                    if (c2.clustering != Clustering.STATIC_CLUSTERING)",
                " +                        return -1;",
                " +                }",
                " +                else if (c2.clustering == Clustering.STATIC_CLUSTERING)",
                " +                {",
                " +                    return 1;",
                " +                }",
                " +                else",
                " +                {",
                " +                    int c = metadata.comparator.compare(c1.clustering, c2.clustering);",
                " +                    if (c != 0)",
                " +                        return reversed ? -c : c;",
                " +                }",
                " +",
                " +                // Note that when reversed, we only care about the clustering being reversed, so it's ok",
                " +                // not to take reversed into account below.",
                " +",
                " +                // Then check the column name",
                " +                if (c1.column != c2.column)",
                " +                {",
                " +                    // A null for the column means it's a row marker",
                " +                    if (c1.column == null)",
                " +                        return -1;",
                " +                    if (c2.column == null)",
                " +                        return 1;",
                " +",
                " +                    assert c1.column.isRegular() || c1.column.isStatic();",
                " +                    assert c2.column.isRegular() || c2.column.isStatic();",
                " +                    int cmp = c1.column.compareTo(c2.column);",
                " +                    if (cmp != 0)",
                " +                        return cmp;",
                " +                }",
                " +",
                " +                assert (c1.collectionElement == null) == (c2.collectionElement == null);",
                " +",
                " +                if (c1.collectionElement != null)",
                " +                {",
                " +                    AbstractType<?> colCmp = ((CollectionType)c1.column.type).nameComparator();",
                " +                    return colCmp.compare(c1.collectionElement, c2.collectionElement);",
                " +                }",
                " +                return 0;",
                " +            }",
                " +        };",
                " +    }",
                " +",
                " +    private static boolean equalValues(ClusteringPrefix c1, ClusteringPrefix c2, ClusteringComparator comparator)",
                " +    {",
                " +        assert c1.size() == c2.size();",
                " +        for (int i = 0; i < c1.size(); i++)",
                " +        {",
                " +            if (comparator.compareComponent(i, c1.get(i), c2.get(i)) != 0)",
                " +                return false;",
                " +        }",
                " +        return true;",
                " +    }",
                " +",
                " +    private static Comparator<LegacyAtom> legacyAtomComparator(CFMetaData metadata)",
                " +    {",
                " +        return (o1, o2) ->",
                " +        {",
                " +            // First we want to compare by clustering, but we have to be careful with range tombstone, because",
                " +            // we can have collection deletion and we want those to sort properly just before the column they",
                " +            // delete, not before the whole row.",
                " +            // We also want to special case static so they sort before any non-static. Note in particular that",
                " +            // this special casing is important in the case of one of the Atom being Slice.Bound.BOTTOM: we want",
                " +            // it to sort after the static as we deal with static first in toUnfilteredAtomIterator and having",
                " +            // Slice.Bound.BOTTOM first would mess that up (note that static deletion is handled through a specific",
                " +            // static tombstone, see LegacyDeletionInfo.add()).",
                " +            if (o1.isStatic() != o2.isStatic())",
                " +                return o1.isStatic() ? -1 : 1;",
                " +",
                " +            ClusteringPrefix c1 = o1.clustering();",
                " +            ClusteringPrefix c2 = o2.clustering();",
                " +",
                " +            int clusteringComparison;",
                " +            if (c1.size() != c2.size() || (o1.isCell() == o2.isCell()) || !equalValues(c1, c2, metadata.comparator))",
                " +            {",
                " +                clusteringComparison = metadata.comparator.compare(c1, c2);",
                " +            }",
                " +            else",
                " +            {",
                " +                // one is a cell and one is a range tombstone, and both have the same prefix size (that is, the",
                " +                // range tombstone is either a row deletion or a collection deletion).",
                " +                LegacyRangeTombstone rt = o1.isCell() ? o2.asRangeTombstone() : o1.asRangeTombstone();",
                " +                clusteringComparison = rt.isCollectionTombstone()",
                " +                                       ? 0",
                " +                                       : metadata.comparator.compare(c1, c2);",
                " +            }",
                " +",
                " +            // Note that if both are range tombstones and have the same clustering, then they are equal.",
                " +            if (clusteringComparison != 0)",
                " +                return clusteringComparison;",
                " +",
                " +            if (o1.isCell())",
                " +            {",
                " +                LegacyCell cell1 = o1.asCell();",
                " +                if (o2.isCell())",
                " +                {",
                " +                    LegacyCell cell2 = o2.asCell();",
                " +                    // Check for row marker cells",
                " +                    if (cell1.name.column == null)",
                " +                        return cell2.name.column == null ? 0 : -1;",
                " +                    return cell2.name.column == null ? 1 : cell1.name.column.compareTo(cell2.name.column);",
                " +                }",
                " +",
                " +                LegacyRangeTombstone rt2 = o2.asRangeTombstone();",
                " +                assert rt2.isCollectionTombstone(); // otherwise, we shouldn't have got a clustering equality",
                " +                if (cell1.name.column == null)",
                " +                    return -1;",
                " +                int cmp = cell1.name.column.compareTo(rt2.start.collectionName);",
                " +                // If both are for the same column, then the RT should come first",
                " +                return cmp == 0 ? 1 : cmp;",
                " +            }",
                " +            else",
                " +            {",
                " +                assert o2.isCell();",
                " +                LegacyCell cell2 = o2.asCell();",
                " +",
                " +                LegacyRangeTombstone rt1 = o1.asRangeTombstone();",
                " +                assert rt1.isCollectionTombstone(); // otherwise, we shouldn't have got a clustering equality",
                " +",
                " +                if (cell2.name.column == null)",
                " +                    return 1;",
                " +",
                " +                int cmp = rt1.start.collectionName.compareTo(cell2.name.column);",
                " +                // If both are for the same column, then the RT should come first",
                " +                return cmp == 0 ? -1 : cmp;",
                " +            }",
                " +        };",
                " +    }",
                " +",
                " +    public static LegacyAtom readLegacyAtom(CFMetaData metadata, DataInputPlus in, boolean readAllAsDynamic)",
                " +    throws IOException, UnknownColumnException",
                " +    {",
                " +        ByteBuffer cellname = ByteBufferUtil.readWithShortLength(in);",
                " +        if (!cellname.hasRemaining())",
                " +            return null; // END_OF_ROW",
                " +",
                " +        try",
                " +        {",
                " +            int b = in.readUnsignedByte();",
                " +            return (b & RANGE_TOMBSTONE_MASK) != 0",
                " +                   ? readLegacyRangeTombstoneBody(metadata, in, cellname)",
                " +                   : readLegacyCellBody(metadata, in, cellname, b, SerializationHelper.Flag.LOCAL, readAllAsDynamic);",
                " +        }",
                " +        catch (UnknownColumnException e)",
                " +        {",
                " +            // We legitimately can get here in 2 cases:",
                " +            // 1) for system tables, because we've unceremoniously removed columns (without registering them as dropped)",
                " +            // 2) for dropped columns.",
                " +            // In any other case, there is a mismatch between the schema and the data, and we complain loudly in",
                " +            // that case. Note that if we are in a legit case of an unknown column, we want to simply skip that cell,",
                " +            // but we don't do this here and re-throw the exception because the calling code sometimes has to know",
                " +            // about this happening. This does mean code calling this method should handle this case properly.",
                " +            if (!metadata.ksName.equals(SystemKeyspace.NAME) && metadata.getDroppedColumnDefinition(e.columnName) == null)",
                " +                throw new IllegalStateException(String.format(\"Got cell for unknown column %s in sstable of %s.%s: \" +",
                " +                                                              \"This suggest a problem with the schema which doesn't list \" +",
                " +                                                              \"this column. Even if that column was dropped, it should have \" +",
                " +                                                              \"been listed as such\", metadata.ksName, metadata.cfName, UTF8Type.instance.compose(e.columnName)), e);",
                " +",
                " +            throw e;",
                " +        }",
                " +    }",
                " +",
                " +    public static LegacyCell readLegacyCell(CFMetaData metadata, DataInput in, SerializationHelper.Flag flag) throws IOException, UnknownColumnException",
                " +    {",
                " +        ByteBuffer cellname = ByteBufferUtil.readWithShortLength(in);",
                " +        int b = in.readUnsignedByte();",
                " +        return readLegacyCellBody(metadata, in, cellname, b, flag, false);",
                " +    }",
                " +",
                " +    public static LegacyCell readLegacyCellBody(CFMetaData metadata, DataInput in, ByteBuffer cellname, int mask, SerializationHelper.Flag flag, boolean readAllAsDynamic)",
                " +    throws IOException, UnknownColumnException",
                " +    {",
                " +        // Note that we want to call decodeCellName only after we've deserialized other parts, since it can throw",
                " +        // and we want to throw only after having deserialized the full cell.",
                " +        if ((mask & COUNTER_MASK) != 0)",
                " +        {",
                " +            in.readLong(); // timestampOfLastDelete: this has been unused for a long time so we ignore it",
                " +            long ts = in.readLong();",
                " +            ByteBuffer value = ByteBufferUtil.readWithLength(in);",
                " +            if (flag == SerializationHelper.Flag.FROM_REMOTE || (flag == SerializationHelper.Flag.LOCAL && CounterContext.instance().shouldClearLocal(value)))",
                " +                value = CounterContext.instance().clearAllLocal(value);",
                " +            return new LegacyCell(LegacyCell.Kind.COUNTER, decodeCellName(metadata, cellname, readAllAsDynamic), value, ts, Cell.NO_DELETION_TIME, Cell.NO_TTL);",
                " +        }",
                " +        else if ((mask & EXPIRATION_MASK) != 0)",
                " +        {",
                " +            int ttl = in.readInt();",
                " +            int expiration = in.readInt();",
                " +            long ts = in.readLong();",
                " +            ByteBuffer value = ByteBufferUtil.readWithLength(in);",
                " +            return new LegacyCell(LegacyCell.Kind.EXPIRING, decodeCellName(metadata, cellname, readAllAsDynamic), value, ts, expiration, ttl);",
                " +        }",
                " +        else",
                " +        {",
                " +            long ts = in.readLong();",
                " +            ByteBuffer value = ByteBufferUtil.readWithLength(in);",
                " +            LegacyCellName name = decodeCellName(metadata, cellname, readAllAsDynamic);",
                " +            return (mask & COUNTER_UPDATE_MASK) != 0",
                " +                ? new LegacyCell(LegacyCell.Kind.COUNTER, name, CounterContext.instance().createUpdate(ByteBufferUtil.toLong(value)), ts, Cell.NO_DELETION_TIME, Cell.NO_TTL)",
                " +                : ((mask & DELETION_MASK) == 0",
                " +                        ? new LegacyCell(LegacyCell.Kind.REGULAR, name, value, ts, Cell.NO_DELETION_TIME, Cell.NO_TTL)",
                " +                        : new LegacyCell(LegacyCell.Kind.DELETED, name, ByteBufferUtil.EMPTY_BYTE_BUFFER, ts, ByteBufferUtil.toInt(value), Cell.NO_TTL));",
                " +        }",
                " +    }",
                " +",
                " +    public static LegacyRangeTombstone readLegacyRangeTombstoneBody(CFMetaData metadata, DataInputPlus in, ByteBuffer boundname) throws IOException",
                " +    {",
                " +        LegacyBound min = decodeBound(metadata, boundname, true);",
                " +        LegacyBound max = decodeBound(metadata, ByteBufferUtil.readWithShortLength(in), false);",
                " +        DeletionTime dt = DeletionTime.serializer.deserialize(in);",
                " +        return new LegacyRangeTombstone(min, max, dt);",
                " +    }",
                " +",
                " +    public static Iterator<LegacyCell> deserializeCells(final CFMetaData metadata,",
                " +                                                        final DataInput in,",
                " +                                                        final SerializationHelper.Flag flag,",
                " +                                                        final int size)",
                " +    {",
                " +        return new AbstractIterator<LegacyCell>()",
                " +        {",
                " +            private int i = 0;",
                " +",
                " +            protected LegacyCell computeNext()",
                " +            {",
                " +                if (i >= size)",
                " +                    return endOfData();",
                " +",
                " +                ++i;",
                " +                try",
                " +                {",
                " +                    return readLegacyCell(metadata, in, flag);",
                " +                }",
                " +                catch (UnknownColumnException e)",
                " +                {",
                " +                    // We can get there if we read a cell for a dropped column, and if that is the case,",
                " +                    // then simply ignore the cell is fine. But also not that we ignore if it's the",
                " +                    // system keyspace because for those table we actually remove columns without registering",
                " +                    // them in the dropped columns",
                " +                    if (metadata.ksName.equals(SystemKeyspace.NAME) || metadata.getDroppedColumnDefinition(e.columnName) != null)",
                " +                        return computeNext();",
                " +                    else",
                " +                        throw new IOError(e);",
                " +                }",
                " +                catch (IOException e)",
                " +                {",
                " +                    throw new IOError(e);",
                " +                }",
                " +            }",
                " +        };",
                " +    }",
                " +",
                " +    public static class CellGrouper",
                " +    {",
                " +        /**",
                " +         * The fake TTL used for expired rows that have been compacted.",
                " +         */",
                " +        private static final int FAKE_TTL = 1;",
                " +",
                " +        public final CFMetaData metadata;",
                " +        private final boolean isStatic;",
                " +        private final SerializationHelper helper;",
                " +        private final Row.Builder builder;",
                " +        private Clustering clustering;",
                " +",
                " +        private LegacyRangeTombstone rowDeletion;",
                " +        private LegacyRangeTombstone collectionDeletion;",
                " +",
                " +        public CellGrouper(CFMetaData metadata, SerializationHelper helper)",
                " +        {",
                " +            this(metadata, helper, false);",
                " +        }",
                " +",
                " +        private CellGrouper(CFMetaData metadata, SerializationHelper helper, boolean isStatic)",
                " +        {",
                " +            this.metadata = metadata;",
                " +            this.isStatic = isStatic;",
                " +            this.helper = helper;",
                " +            // We cannot use a sorted builder because we don't have exactly the same ordering in 3.0 and pre-3.0. More precisely, within a row, we",
                " +            // store all simple columns before the complex ones in 3.0, which we use to sort everything sorted by the column name before. Note however",
                " +            // that the unsorted builder won't have to reconcile cells, so the exact value we pass for nowInSec doesn't matter.",
                " +            this.builder = BTreeRow.unsortedBuilder(FBUtilities.nowInSeconds());",
                " +        }",
                " +",
                " +        public static CellGrouper staticGrouper(CFMetaData metadata, SerializationHelper helper)",
                " +        {",
                " +            return new CellGrouper(metadata, helper, true);",
                " +        }",
                " +",
                " +        public void reset()",
                " +        {",
                " +            this.clustering = null;",
                " +            this.rowDeletion = null;",
                " +            this.collectionDeletion = null;",
                " +        }",
                " +",
                " +        public boolean addAtom(LegacyAtom atom)",
                " +        {",
                " +            return atom.isCell()",
                " +                 ? addCell(atom.asCell())",
                " +                 : addRangeTombstone(atom.asRangeTombstone());",
                " +        }",
                " +",
                " +        public boolean addCell(LegacyCell cell)",
                " +        {",
                " +            if (clustering == null)",
                " +            {",
                " +                clustering = cell.name.clustering;",
                " +                assert !isStatic || clustering == Clustering.STATIC_CLUSTERING;",
                " +                builder.newRow(clustering);",
                " +            }",
                " +            else if (!clustering.equals(cell.name.clustering))",
                " +            {",
                " +                return false;",
                " +            }",
                " +",
                " +            // Ignore shadowed cells",
                " +            if (rowDeletion != null && rowDeletion.deletionTime.deletes(cell.timestamp))",
                " +                return true;",
                " +",
                " +            ColumnDefinition column = cell.name.column;",
                " +            if (column == null)",
                " +            {",
                " +                // It's the row marker",
                " +                assert !cell.value.hasRemaining();",
                " +                // In 2.1, the row marker expired cell might have been converted into a deleted one by compaction.",
                " +                // If we do not set the primary key liveness info for this row and it does not contains any regular columns",
                " +                // the row will be empty. To avoid that, we reuse the localDeletionTime but use a fake TTL.",
                " +                // The only time in 2.x that we actually delete a row marker is in 2i tables, so in that case we do",
                " +                // want to actually propagate the row deletion. (CASSANDRA-13320)",
                " +                if (!cell.isTombstone())",
                " +                    builder.addPrimaryKeyLivenessInfo(LivenessInfo.create(cell.timestamp, cell.ttl, cell.localDeletionTime));",
                " +                else if (metadata.isIndex())",
                " +                    builder.addRowDeletion(Row.Deletion.regular(new DeletionTime(cell.timestamp, cell.localDeletionTime)));",
                " +                else",
                " +                    builder.addPrimaryKeyLivenessInfo(LivenessInfo.create(cell.timestamp, FAKE_TTL, cell.localDeletionTime));",
                " +            }",
                " +            else",
                " +            {",
                " +                if (collectionDeletion != null && collectionDeletion.start.collectionName.name.equals(column.name) && collectionDeletion.deletionTime.deletes(cell.timestamp))",
                " +                    return true;",
                " +",
                " +                if (helper.includes(column))",
                " +                {",
                " +                    CellPath path = null;",
                " +                    if (column.isComplex())",
                " +                    {",
                " +                        // Recalling startOfComplexColumn for every cell is a big inefficient, but it's ok in practice",
                " +                        // and it's simpler. And since 1) this only matter for super column selection in thrift in",
                " +                        // practice and 2) is only used during upgrade, it's probably worth keeping things simple.",
                " +                        helper.startOfComplexColumn(column);",
                " +                        path = cell.name.collectionElement == null ? null : CellPath.create(cell.name.collectionElement);",
                " +                        if (!helper.includes(path))",
                " +                            return true;",
                " +                    }",
                " +                    Cell c = new BufferCell(column, cell.timestamp, cell.ttl, cell.localDeletionTime, cell.value, path);",
                " +                    if (!helper.isDropped(c, column.isComplex()))",
                " +                        builder.addCell(c);",
                " +                    if (column.isComplex())",
                " +                    {",
                " +                        helper.endOfComplexColumn();",
                " +                    }",
                " +                }",
                " +            }",
                " +            return true;",
                " +        }",
                " +",
                " +        private boolean addRangeTombstone(LegacyRangeTombstone tombstone)",
                " +        {",
                " +            if (tombstone.isRowDeletion(metadata))",
                " +                return addRowTombstone(tombstone);",
                " +            else if (tombstone.isCollectionTombstone())",
                " +                return addCollectionTombstone(tombstone);",
                " +            else",
                " +                return addGenericRangeTombstone(tombstone);",
                " +        }",
                " +",
                " +        private boolean addRowTombstone(LegacyRangeTombstone tombstone)",
                " +        {",
                " +            if (clustering != null)",
                " +            {",
                " +                // If we're already in the row, there might be a chance that there were two range tombstones",
                " +                // written, as 2.x storage format does not guarantee just one range tombstone, unlike 3.x.",
                " +                // We have to make sure that clustering matches, which would mean that tombstone is for the",
                " +                // same row.",
                " +                if (rowDeletion != null && clustering.equals(tombstone.start.getAsClustering(metadata)))",
                " +                {",
                " +                    // If the tombstone superceeds the previous delete, we discard the previous one",
                " +                    if (tombstone.deletionTime.supersedes(rowDeletion.deletionTime))",
                " +                    {",
                " +                        builder.addRowDeletion(Row.Deletion.regular(tombstone.deletionTime));",
                " +                        rowDeletion = tombstone;",
                " +                    }",
                " +                    return true;",
                " +                }",
                " +",
                " +                // If we're already within a row and there was no delete written before that one, it can't be the same one",
                " +                return false;",
                " +            }",
                " +",
                " +            clustering = tombstone.start.getAsClustering(metadata);",
                " +            builder.newRow(clustering);",
                " +            builder.addRowDeletion(Row.Deletion.regular(tombstone.deletionTime));",
                " +            rowDeletion = tombstone;",
                " +",
                " +            return true;",
                " +        }",
                " +",
                " +        private boolean addCollectionTombstone(LegacyRangeTombstone tombstone)",
                " +        {",
                " +            if (!helper.includes(tombstone.start.collectionName))",
                " +                return false; // see CASSANDRA-13109",
                " +",
                " +            if (clustering == null)",
                " +            {",
                " +                clustering = tombstone.start.getAsClustering(metadata);",
                " +                builder.newRow(clustering);",
                " +            }",
                " +            else if (!clustering.equals(tombstone.start.getAsClustering(metadata)))",
                " +            {",
                " +                return false;",
                " +            }",
                " +",
                " +            builder.addComplexDeletion(tombstone.start.collectionName, tombstone.deletionTime);",
                " +            if (rowDeletion == null || tombstone.deletionTime.supersedes(rowDeletion.deletionTime))",
                " +                collectionDeletion = tombstone;",
                " +",
                " +            return true;",
                " +        }",
                " +",
                " +        private boolean addGenericRangeTombstone(LegacyRangeTombstone tombstone)",
                " +        {",
                " +            /*",
                " +             * We can see a non-collection, non-row deletion in two scenarios:",
                " +             *",
                " +             * 1. Most commonly, the tombstone's start bound is bigger than current row's clustering, which means that",
                " +             *    the current row is over, and we should move on to the next row or RT;",
                " +             *",
                " +             * 2. Less commonly, the tombstone's start bound is smaller than current row's clustering, which means that",
                " +             *    we've crossed an index boundary and are seeing a non-closed RT from the previous block, repeated;",
                " +             *    we should ignore it and stay in the current row.",
                " +             *",
                " +             *  In either case, clustering should be non-null, or we shouldn't have gotten to this method at all",
                " +             *  However, to be absolutely SURE we're in case two above, we check here.",
                " +             */",
                " +            return clustering != null && metadata.comparator.compare(clustering, tombstone.start.bound.clustering()) > 0;",
                " +        }",
                " +",
                " +        public Row getRow()",
                " +        {",
                " +            return builder.build();",
                " +        }",
                " +    }",
                " +",
                " +    public static class LegacyUnfilteredPartition",
                " +    {",
                " +        public final DeletionTime partitionDeletion;",
                " +        public final LegacyRangeTombstoneList rangeTombstones;",
                " +        public final List<LegacyCell> cells;",
                " +",
                " +        private LegacyUnfilteredPartition(DeletionTime partitionDeletion, LegacyRangeTombstoneList rangeTombstones, List<LegacyCell> cells)",
                " +        {",
                " +            this.partitionDeletion = partitionDeletion;",
                " +            this.rangeTombstones = rangeTombstones;",
                " +            this.cells = cells;",
                " +        }",
                " +",
                " +        public void digest(CFMetaData metadata, MessageDigest digest)",
                " +        {",
                " +            for (LegacyCell cell : cells)",
                " +            {",
                " +                digest.update(cell.name.encode(metadata).duplicate());",
                " +",
                " +                if (cell.isCounter())",
                " +                    CounterContext.instance().updateDigest(digest, cell.value);",
                " +                else",
                " +                    digest.update(cell.value.duplicate());",
                " +",
                " +                FBUtilities.updateWithLong(digest, cell.timestamp);",
                " +                FBUtilities.updateWithByte(digest, cell.serializationFlags());",
                " +",
                " +                if (cell.isExpiring())",
                " +                    FBUtilities.updateWithInt(digest, cell.ttl);",
                " +",
                " +                if (cell.isCounter())",
                " +                {",
                " +                    // Counters used to have the timestampOfLastDelete field, which we stopped using long ago and has been hard-coded",
                " +                    // to Long.MIN_VALUE but was still taken into account in 2.2 counter digests (to maintain backward compatibility",
                " +                    // in the first place).",
                " +                    FBUtilities.updateWithLong(digest, Long.MIN_VALUE);",
                " +                }",
                " +            }",
                " +",
                " +            if (partitionDeletion.markedForDeleteAt() != Long.MIN_VALUE)",
                " +                digest.update(ByteBufferUtil.bytes(partitionDeletion.markedForDeleteAt()));",
                " +",
                " +            if (!rangeTombstones.isEmpty())",
                " +                rangeTombstones.updateDigest(digest);",
                " +        }",
                " +    }",
                " +",
                " +    public static class LegacyCellName",
                " +    {",
                " +        public final Clustering clustering;",
                " +        public final ColumnDefinition column;",
                " +        public final ByteBuffer collectionElement;",
                " +",
                " +        private LegacyCellName(Clustering clustering, ColumnDefinition column, ByteBuffer collectionElement)",
                " +        {",
                " +            this.clustering = clustering;",
                " +            this.column = column;",
                " +            this.collectionElement = collectionElement;",
                " +        }",
                " +",
                " +        public ByteBuffer encode(CFMetaData metadata)",
                " +        {",
                " +            return encodeCellName(metadata, clustering, column == null ? ByteBufferUtil.EMPTY_BYTE_BUFFER : column.name.bytes, collectionElement);",
                " +        }",
                " +",
                " +        public ByteBuffer superColumnSubName()",
                " +        {",
                " +            assert collectionElement != null;",
                " +            return collectionElement;",
                " +        }",
                " +",
                " +        public ByteBuffer superColumnName()",
                " +        {",
                " +            return clustering.get(0);",
                " +        }",
                " +",
                " +        @Override",
                " +        public String toString()",
                " +        {",
                " +            StringBuilder sb = new StringBuilder();",
                " +            for (int i = 0; i < clustering.size(); i++)",
                " +                sb.append(i > 0 ? \":\" : \"\").append(clustering.get(i) == null ? \"null\" : ByteBufferUtil.bytesToHex(clustering.get(i)));",
                " +            return String.format(\"Cellname(clustering=%s, column=%s, collElt=%s)\", sb.toString(), column == null ? \"null\" : column.name, collectionElement == null ? \"null\" : ByteBufferUtil.bytesToHex(collectionElement));",
                " +        }",
                " +    }",
                " +",
                " +    public static class LegacyBound",
                " +    {",
                " +        public static final LegacyBound BOTTOM = new LegacyBound(Slice.Bound.BOTTOM, false, null);",
                " +        public static final LegacyBound TOP = new LegacyBound(Slice.Bound.TOP, false, null);",
                " +",
                " +        public final Slice.Bound bound;",
                " +        public final boolean isStatic;",
                " +        public final ColumnDefinition collectionName;",
                " +",
                " +        public LegacyBound(Slice.Bound bound, boolean isStatic, ColumnDefinition collectionName)",
                " +        {",
                " +            this.bound = bound;",
                " +            this.isStatic = isStatic;",
                " +            this.collectionName = collectionName;",
                " +        }",
                " +",
                " +        public Clustering getAsClustering(CFMetaData metadata)",
                " +        {",
                " +            if (isStatic)",
                " +                return Clustering.STATIC_CLUSTERING;",
                " +",
                " +            assert bound.size() == metadata.comparator.size();",
                " +            ByteBuffer[] values = new ByteBuffer[bound.size()];",
                " +            for (int i = 0; i < bound.size(); i++)",
                " +                values[i] = bound.get(i);",
                " +            return new Clustering(values);",
                " +        }",
                " +",
                " +        @Override",
                " +        public String toString()",
                " +        {",
                " +            StringBuilder sb = new StringBuilder();",
                " +            sb.append(bound.kind()).append('(');",
                " +            for (int i = 0; i < bound.size(); i++)",
                " +                sb.append(i > 0 ? \":\" : \"\").append(bound.get(i) == null ? \"null\" : ByteBufferUtil.bytesToHex(bound.get(i)));",
                " +            sb.append(')');",
                " +            return String.format(\"Bound(%s, collection=%s)\", sb.toString(), collectionName == null ? \"null\" : collectionName.name);",
                " +        }",
                " +    }",
                " +",
                " +    public interface LegacyAtom",
                " +    {",
                " +        public boolean isCell();",
                " +",
                " +        public ClusteringPrefix clustering();",
                " +        public boolean isStatic();",
                " +",
                " +        public LegacyCell asCell();",
                " +        public LegacyRangeTombstone asRangeTombstone();",
                " +    }",
                " +",
                " +    /**",
                " +     * A legacy cell.",
                " +     * <p>",
                " +     * This is used as a temporary object to facilitate dealing with the legacy format, this",
                " +     * is not meant to be optimal.",
                " +     */",
                " +    public static class LegacyCell implements LegacyAtom",
                " +    {",
                " +        private final static int DELETION_MASK        = 0x01;",
                " +        private final static int EXPIRATION_MASK      = 0x02;",
                " +        private final static int COUNTER_MASK         = 0x04;",
                " +        private final static int COUNTER_UPDATE_MASK  = 0x08;",
                " +        private final static int RANGE_TOMBSTONE_MASK = 0x10;",
                " +",
                " +        public enum Kind { REGULAR, EXPIRING, DELETED, COUNTER }",
                " +",
                " +        public final Kind kind;",
                " +",
                " +        public final LegacyCellName name;",
                " +        public final ByteBuffer value;",
                " +",
                " +        public final long timestamp;",
                " +        public final int localDeletionTime;",
                " +        public final int ttl;",
                " +",
                " +        private LegacyCell(Kind kind, LegacyCellName name, ByteBuffer value, long timestamp, int localDeletionTime, int ttl)",
                " +        {",
                " +            this.kind = kind;",
                " +            this.name = name;",
                " +            this.value = value;",
                " +            this.timestamp = timestamp;",
                " +            this.localDeletionTime = localDeletionTime;",
                " +            this.ttl = ttl;",
                " +        }",
                " +",
                " +        public static LegacyCell regular(CFMetaData metadata, ByteBuffer superColumnName, ByteBuffer name, ByteBuffer value, long timestamp)",
                " +        throws UnknownColumnException",
                " +        {",
                " +            return new LegacyCell(Kind.REGULAR, decodeCellName(metadata, superColumnName, name), value, timestamp, Cell.NO_DELETION_TIME, Cell.NO_TTL);",
                " +        }",
                " +",
                " +        public static LegacyCell expiring(CFMetaData metadata, ByteBuffer superColumnName, ByteBuffer name, ByteBuffer value, long timestamp, int ttl, int nowInSec)",
                " +        throws UnknownColumnException",
                " +        {",
                "-             return new LegacyCell(Kind.EXPIRING, decodeCellName(metadata, superColumnName, name), value, timestamp, nowInSec + ttl, ttl);",
                "++            /*",
                "++             * CASSANDRA-14092: Max expiration date capping is maybe performed here, expiration overflow policy application",
                "++             * is done at {@link org.apache.cassandra.thrift.ThriftValidation#validateTtl(CFMetaData, Column)}",
                "++             */",
                "++            return new LegacyCell(Kind.EXPIRING, decodeCellName(metadata, superColumnName, name), value, timestamp, ExpirationDateOverflowHandling.computeLocalExpirationTime(nowInSec, ttl), ttl);",
                " +        }",
                " +",
                " +        public static LegacyCell tombstone(CFMetaData metadata, ByteBuffer superColumnName, ByteBuffer name, long timestamp, int nowInSec)",
                " +        throws UnknownColumnException",
                " +        {",
                " +            return new LegacyCell(Kind.DELETED, decodeCellName(metadata, superColumnName, name), ByteBufferUtil.EMPTY_BYTE_BUFFER, timestamp, nowInSec, LivenessInfo.NO_TTL);",
                " +        }",
                " +",
                " +        public static LegacyCell counterUpdate(CFMetaData metadata, ByteBuffer superColumnName, ByteBuffer name, long value)",
                " +        throws UnknownColumnException",
                " +        {",
                " +            // See UpdateParameters.addCounter() for more details on this",
                " +            ByteBuffer counterValue = CounterContext.instance().createUpdate(value);",
                " +            return counter(decodeCellName(metadata, superColumnName, name), counterValue);",
                " +        }",
                " +",
                " +        public static LegacyCell counter(LegacyCellName name, ByteBuffer value)",
                " +        {",
                " +            return new LegacyCell(Kind.COUNTER, name, value, FBUtilities.timestampMicros(), Cell.NO_DELETION_TIME, Cell.NO_TTL);",
                " +        }",
                " +",
                " +        public byte serializationFlags()",
                " +        {",
                " +            if (isExpiring())",
                " +                return EXPIRATION_MASK;",
                " +            if (isTombstone())",
                " +                return DELETION_MASK;",
                " +            if (isCounterUpdate())",
                " +                return COUNTER_UPDATE_MASK;",
                " +            if (isCounter())",
                " +                return COUNTER_MASK;",
                " +            return 0;",
                " +        }",
                " +",
                " +        public boolean isCounterUpdate()",
                " +        {",
                " +            // See UpdateParameters.addCounter() for more details on this",
                " +            return isCounter() && CounterContext.instance().isUpdate(value);",
                " +        }",
                " +",
                " +        public ClusteringPrefix clustering()",
                " +        {",
                " +            return name.clustering;",
                " +        }",
                " +",
                " +        public boolean isStatic()",
                " +        {",
                " +            return name.clustering == Clustering.STATIC_CLUSTERING;",
                " +        }",
                " +",
                " +        public boolean isCell()",
                " +        {",
                " +            return true;",
                " +        }",
                " +",
                " +        public LegacyCell asCell()",
                " +        {",
                " +            return this;",
                " +        }",
                " +",
                " +        public LegacyRangeTombstone asRangeTombstone()",
                " +        {",
                " +            throw new UnsupportedOperationException();",
                " +        }",
                " +",
                " +        public boolean isCounter()",
                " +        {",
                " +            return kind == Kind.COUNTER;",
                " +        }",
                " +",
                " +        public boolean isExpiring()",
                " +        {",
                " +            return kind == Kind.EXPIRING;",
                " +        }",
                " +",
                " +        public boolean isTombstone()",
                " +        {",
                " +            return kind == Kind.DELETED;",
                " +        }",
                " +",
                " +        public boolean isLive(int nowInSec)",
                " +        {",
                " +            if (isTombstone())",
                " +                return false;",
                " +",
                " +            return !isExpiring() || nowInSec < localDeletionTime;",
                " +        }",
                " +",
                " +        @Override",
                " +        public String toString()",
                " +        {",
                " +            return String.format(\"LegacyCell(%s, name=%s, v=%s, ts=%s, ldt=%s, ttl=%s)\", kind, name, ByteBufferUtil.bytesToHex(value), timestamp, localDeletionTime, ttl);",
                " +        }",
                " +    }",
                " +",
                " +    /**",
                " +     * A legacy range tombstone.",
                " +     * <p>",
                " +     * This is used as a temporary object to facilitate dealing with the legacy format, this",
                " +     * is not meant to be optimal.",
                " +     */",
                " +    public static class LegacyRangeTombstone implements LegacyAtom",
                " +    {",
                " +        public final LegacyBound start;",
                " +        public final LegacyBound stop;",
                " +        public final DeletionTime deletionTime;",
                " +",
                " +        public LegacyRangeTombstone(LegacyBound start, LegacyBound stop, DeletionTime deletionTime)",
                " +        {",
                " +            // Because of the way RangeTombstoneList work, we can have a tombstone where only one of",
                " +            // the bound has a collectionName. That happens if we have a big tombstone A (spanning one",
                " +            // or multiple rows) and a collection tombstone B. In that case, RangeTombstoneList will",
                " +            // split this into 3 RTs: the first one from the beginning of A to the beginning of B,",
                " +            // then B, then a third one from the end of B to the end of A. To make this simpler, if",
                " +            // we detect that case we transform the 1st and 3rd tombstone so they don't end in the middle",
                " +            // of a row (which is still correct).",
                " +            if ((start.collectionName == null) != (stop.collectionName == null))",
                " +            {",
                " +                if (start.collectionName == null)",
                " +                    stop = new LegacyBound(stop.bound, stop.isStatic, null);",
                " +                else",
                " +                    start = new LegacyBound(start.bound, start.isStatic, null);",
                " +            }",
                " +            else if (!Objects.equals(start.collectionName, stop.collectionName))",
                " +            {",
                " +                // We're in the similar but slightly more complex case where on top of the big tombstone",
                " +                // A, we have 2 (or more) collection tombstones B and C within A. So we also end up with",
                " +                // a tombstone that goes between the end of B and the start of C.",
                " +                start = new LegacyBound(start.bound, start.isStatic, null);",
                " +                stop = new LegacyBound(stop.bound, stop.isStatic, null);",
                " +            }",
                " +",
                " +            this.start = start;",
                " +            this.stop = stop;",
                " +            this.deletionTime = deletionTime;",
                " +        }",
                " +",
                " +        public ClusteringPrefix clustering()",
                " +        {",
                " +            return start.bound;",
                " +        }",
                " +",
                " +        public LegacyRangeTombstone withNewStart(LegacyBound newStart)",
                " +        {",
                " +            return new LegacyRangeTombstone(newStart, stop, deletionTime);",
                " +        }",
                " +",
                " +        public LegacyRangeTombstone withNewEnd(LegacyBound newStop)",
                " +        {",
                " +            return new LegacyRangeTombstone(start, newStop, deletionTime);",
                " +        }",
                " +",
                " +        public boolean isCell()",
                " +        {",
                " +            return false;",
                " +        }",
                " +",
                " +        public boolean isStatic()",
                " +        {",
                " +            return start.isStatic || stop.isStatic;",
                " +        }",
                " +",
                " +        public LegacyCell asCell()",
                " +        {",
                " +            throw new UnsupportedOperationException();",
                " +        }",
                " +",
                " +        public LegacyRangeTombstone asRangeTombstone()",
                " +        {",
                " +            return this;",
                " +        }",
                " +",
                " +        public boolean isCollectionTombstone()",
                " +        {",
                " +            return start.collectionName != null;",
                " +        }",
                " +",
                " +        public boolean isRowDeletion(CFMetaData metadata)",
                " +        {",
                " +            if (start.collectionName != null",
                " +                || stop.collectionName != null",
                " +                || start.bound.size() != metadata.comparator.size()",
                " +                || stop.bound.size() != metadata.comparator.size())",
                " +                return false;",
                " +",
                " +            for (int i = 0; i < start.bound.size(); i++)",
                " +                if (!Objects.equals(start.bound.get(i), stop.bound.get(i)))",
                " +                    return false;",
                " +            return true;",
                " +        }",
                " +",
                " +        @Override",
                " +        public String toString()",
                " +        {",
                " +            return String.format(\"RT(%s-%s, %s)\", start, stop, deletionTime);",
                " +        }",
                " +    }",
                " +",
                " +    public static class LegacyDeletionInfo",
                " +    {",
                " +        public final MutableDeletionInfo deletionInfo;",
                " +        public final List<LegacyRangeTombstone> inRowTombstones = new ArrayList<>();",
                " +",
                " +        private LegacyDeletionInfo(MutableDeletionInfo deletionInfo)",
                " +        {",
                " +            this.deletionInfo = deletionInfo;",
                " +        }",
                " +",
                " +        public static LegacyDeletionInfo live()",
                " +        {",
                " +            return new LegacyDeletionInfo(MutableDeletionInfo.live());",
                " +        }",
                " +",
                " +        public void add(DeletionTime topLevel)",
                " +        {",
                " +            deletionInfo.add(topLevel);",
                " +        }",
                " +",
                " +        private static Slice.Bound staticBound(CFMetaData metadata, boolean isStart)",
                " +        {",
                " +            // In pre-3.0 nodes, static row started by a clustering with all empty values so we",
                " +            // preserve that here. Note that in practice, it doesn't really matter since the rest",
                " +            // of the code will ignore the bound for RT that have their static flag set.",
                " +            ByteBuffer[] values = new ByteBuffer[metadata.comparator.size()];",
                " +            for (int i = 0; i < values.length; i++)",
                " +                values[i] = ByteBufferUtil.EMPTY_BYTE_BUFFER;",
                " +            return isStart",
                " +                 ? Slice.Bound.inclusiveStartOf(values)",
                " +                 : Slice.Bound.inclusiveEndOf(values);",
                " +        }",
                " +",
                " +        public void add(CFMetaData metadata, LegacyRangeTombstone tombstone)",
                " +        {",
                " +            if (metadata.hasStaticColumns())",
                " +            {",
                " +                /*",
                " +                 * For table having static columns we have to deal with the following cases:",
                " +                 *  1. the end of the tombstone is static (in which case either the start is static or is BOTTOM, which is the same",
                " +                 *     for our consideration). This mean that either the range only delete the static row, or that it's a collection",
                " +                 *     tombstone of a static collection. In both case, we just add the tombstone to the inRowTombstones.",
                " +                 *  2. only the start is static. There is then 2 subcase: either the start is inclusive, and that mean we include the",
                " +                 *     static row and more (so we add an inRowTombstone for the static and deal with the rest normally). Or the start",
                " +                 *     is exclusive, and that means we explicitely exclude the static (in which case we can just add the tombstone",
                " +                 *     as if it started at BOTTOM).",
                " +                 *  3. none of the bound are static but the start is BOTTOM. This means we intended to delete the static row so we",
                " +                 *     need to add it to the inRowTombstones (and otherwise handle the range normally).",
                " +                 */",
                " +                if (tombstone.stop.isStatic)",
                " +                {",
                " +                    // If the start is BOTTOM, we replace it by the beginning of the starting row so as to not confuse the",
                " +                    // RangeTombstone.isRowDeletion() method",
                " +                    if (tombstone.start == LegacyBound.BOTTOM)",
                " +                        tombstone = tombstone.withNewStart(new LegacyBound(staticBound(metadata, true), true, null));",
                " +                    inRowTombstones.add(tombstone);",
                " +                    return;",
                " +                }",
                " +",
                " +                if (tombstone.start.isStatic)",
                " +                {",
                " +                    if (tombstone.start.bound.isInclusive())",
                " +                        inRowTombstones.add(tombstone.withNewEnd(new LegacyBound(staticBound(metadata, false), true, null)));",
                " +",
                " +                    tombstone = tombstone.withNewStart(LegacyBound.BOTTOM);",
                " +                }",
                " +                else if (tombstone.start == LegacyBound.BOTTOM)",
                " +                {",
                " +                    inRowTombstones.add(new LegacyRangeTombstone(new LegacyBound(staticBound(metadata, true), true, null),",
                " +                                                                 new LegacyBound(staticBound(metadata, false), true, null),",
                " +                                                                 tombstone.deletionTime));",
                " +                }",
                " +            }",
                " +",
                " +            if (tombstone.isCollectionTombstone() || tombstone.isRowDeletion(metadata))",
                " +                inRowTombstones.add(tombstone);",
                " +            else",
                " +                add(metadata, new RangeTombstone(Slice.make(tombstone.start.bound, tombstone.stop.bound), tombstone.deletionTime));",
                " +        }",
                " +",
                " +        public void add(CFMetaData metadata, RangeTombstone tombstone)",
                " +        {",
                " +            deletionInfo.add(tombstone, metadata.comparator);",
                " +        }",
                " +",
                " +        public Iterator<LegacyRangeTombstone> inRowRangeTombstones()",
                " +        {",
                " +            return inRowTombstones.iterator();",
                " +        }",
                " +",
                " +        public static LegacyDeletionInfo deserialize(CFMetaData metadata, DataInputPlus in) throws IOException",
                " +        {",
                " +            DeletionTime topLevel = DeletionTime.serializer.deserialize(in);",
                " +",
                " +            int rangeCount = in.readInt();",
                " +            if (rangeCount == 0)",
                " +                return new LegacyDeletionInfo(new MutableDeletionInfo(topLevel));",
                " +",
                " +            LegacyDeletionInfo delInfo = new LegacyDeletionInfo(new MutableDeletionInfo(topLevel));",
                " +            for (int i = 0; i < rangeCount; i++)",
                " +            {",
                " +                LegacyBound start = decodeBound(metadata, ByteBufferUtil.readWithShortLength(in), true);",
                " +                LegacyBound end = decodeBound(metadata, ByteBufferUtil.readWithShortLength(in), false);",
                " +                int delTime =  in.readInt();",
                " +                long markedAt = in.readLong();",
                " +",
                " +                delInfo.add(metadata, new LegacyRangeTombstone(start, end, new DeletionTime(markedAt, delTime)));",
                " +            }",
                " +            return delInfo;",
                " +        }",
                " +    }",
                " +",
                " +    /**",
                " +     * A helper class for LegacyRangeTombstoneList.  This replaces the Comparator<Composite> that RTL used before 3.0.",
                " +     */",
                " +    private static class LegacyBoundComparator implements Comparator<LegacyBound>",
                " +    {",
                " +        ClusteringComparator clusteringComparator;",
                " +",
                " +        public LegacyBoundComparator(ClusteringComparator clusteringComparator)",
                " +        {",
                " +            this.clusteringComparator = clusteringComparator;",
                " +        }",
                " +",
                " +        public int compare(LegacyBound a, LegacyBound b)",
                " +        {",
                " +            // In the legacy sorting, BOTTOM comes before anything else",
                " +            if (a == LegacyBound.BOTTOM)",
                " +                return b == LegacyBound.BOTTOM ? 0 : -1;",
                " +            if (b == LegacyBound.BOTTOM)",
                " +                return 1;",
                " +",
                " +            // Excluding BOTTOM, statics are always before anything else.",
                " +            if (a.isStatic != b.isStatic)",
                " +                return a.isStatic ? -1 : 1;",
                " +",
                " +            // We have to be careful with bound comparison because of collections. Namely, if the 2 bounds represent the",
                " +            // same prefix, then we should take the collectionName into account before taking the bounds kind",
                " +            // (ClusteringPrefix.Kind). This means we can't really call ClusteringComparator.compare() directly.",
                " +            // For instance, if",
                " +            //    a is (bound=INCL_START_BOUND('x'), collectionName='d')",
                " +            //    b is (bound=INCL_END_BOUND('x'),   collectionName='c')",
                " +            // Ten b < a since the element 'c' of collection 'x' comes before element 'd', but calling",
                " +            // clusteringComparator.compare(a.bound, b.bound) returns -1.",
                " +            // See CASSANDRA-13125 for details.",
                " +            int sa = a.bound.size();",
                " +            int sb = b.bound.size();",
                " +            for (int i = 0; i < Math.min(sa, sb); i++)",
                " +            {",
                " +                int cmp = clusteringComparator.compareComponent(i, a.bound.get(i), b.bound.get(i));",
                " +                if (cmp != 0)",
                " +                    return cmp;",
                " +            }",
                " +",
                " +            if (sa != sb)",
                " +                return sa < sb ? a.bound.kind().comparedToClustering : -b.bound.kind().comparedToClustering;",
                " +",
                " +            // Both bound represent the same prefix, compare the collection names",
                " +            // If one has a collection name and the other doesn't, the other comes before as it points to the beginning of the row.",
                " +            if ((a.collectionName == null) != (b.collectionName == null))",
                " +                return a.collectionName == null ? -1 : 1;",
                " +",
                " +            // If they both have a collection, compare that first",
                " +            if (a.collectionName != null)",
                " +            {",
                " +                int cmp = UTF8Type.instance.compare(a.collectionName.name.bytes, b.collectionName.name.bytes);",
                " +                if (cmp != 0)",
                " +                    return cmp;",
                " +            }",
                " +",
                " +            // Lastly, if everything so far is equal, compare their clustering kind",
                " +            return ClusteringPrefix.Kind.compare(a.bound.kind(), b.bound.kind());",
                " +        }",
                " +    }",
                " +",
                " +    /**",
                " +     * Almost an entire copy of RangeTombstoneList from C* 2.1.  The main difference is that LegacyBoundComparator",
                " +     * is used in place of Comparator<Composite> (because Composite doesn't exist any more).",
                " +     *",
                " +     * This class is needed to allow us to convert single-row deletions and complex deletions into range tombstones",
                " +     * and properly merge them into the normal set of range tombstones.",
                " +     */",
                " +    public static class LegacyRangeTombstoneList",
                " +    {",
                " +        private final LegacyBoundComparator comparator;",
                " +",
                " +        // Note: we don't want to use a List for the markedAts and delTimes to avoid boxing. We could",
                " +        // use a List for starts and ends, but having arrays everywhere is almost simpler.",
                " +        LegacyBound[] starts;",
                " +        LegacyBound[] ends;",
                " +        private long[] markedAts;",
                " +        private int[] delTimes;",
                " +",
                " +        private int size;",
                " +",
                " +        private LegacyRangeTombstoneList(LegacyBoundComparator comparator, LegacyBound[] starts, LegacyBound[] ends, long[] markedAts, int[] delTimes, int size)",
                " +        {",
                " +            assert starts.length == ends.length && starts.length == markedAts.length && starts.length == delTimes.length;",
                " +            this.comparator = comparator;",
                " +            this.starts = starts;",
                " +            this.ends = ends;",
                " +            this.markedAts = markedAts;",
                " +            this.delTimes = delTimes;",
                " +            this.size = size;",
                " +        }",
                " +",
                " +        public LegacyRangeTombstoneList(LegacyBoundComparator comparator, int capacity)",
                " +        {",
                " +            this(comparator, new LegacyBound[capacity], new LegacyBound[capacity], new long[capacity], new int[capacity], 0);",
                " +        }",
                " +",
                " +        @Override",
                " +        public String toString()",
                " +        {",
                " +            StringBuilder sb = new StringBuilder();",
                " +            sb.append('[');",
                " +            for (int i = 0; i < size; i++)",
                " +            {",
                " +                if (i > 0)",
                " +                    sb.append(',');",
                " +                sb.append('(').append(starts[i]).append(\", \").append(ends[i]).append(')');",
                " +            }",
                " +            return sb.append(']').toString();",
                " +        }",
                " +",
                " +        public boolean isEmpty()",
                " +        {",
                " +            return size == 0;",
                " +        }",
                " +",
                " +        public int size()",
                " +        {",
                " +            return size;",
                " +        }",
                " +",
                " +        /**",
                " +         * Adds a new range tombstone.",
                " +         *",
                " +         * This method will be faster if the new tombstone sort after all the currently existing ones (this is a common use case),",
                " +         * but it doesn't assume it.",
                " +         */",
                " +        public void add(LegacyBound start, LegacyBound end, long markedAt, int delTime)",
                " +        {",
                " +            if (isEmpty())",
                " +            {",
                " +                addInternal(0, start, end, markedAt, delTime);",
                " +                return;",
                " +            }",
                " +",
                " +            int c = comparator.compare(ends[size-1], start);",
                " +",
                " +            // Fast path if we add in sorted order",
                " +            if (c <= 0)",
                " +            {",
                " +                addInternal(size, start, end, markedAt, delTime);",
                " +            }",
                " +            else",
                " +            {",
                " +                // Note: insertFrom expect i to be the insertion point in term of interval ends",
                " +                int pos = Arrays.binarySearch(ends, 0, size, start, comparator);",
                " +                insertFrom((pos >= 0 ? pos : -pos-1), start, end, markedAt, delTime);",
                " +            }",
                " +        }",
                " +",
                " +        /*",
                " +         * Inserts a new element starting at index i. This method assumes that:",
                " +         *    ends[i-1] <= start <= ends[i]",
                " +         *",
                " +         * A RangeTombstoneList is a list of range [s_0, e_0]...[s_n, e_n] such that:",
                " +         *   - s_i <= e_i",
                " +         *   - e_i <= s_i+1",
                " +         *   - if s_i == e_i and e_i == s_i+1 then s_i+1 < e_i+1",
                " +         * Basically, range are non overlapping except for their bound and in order. And while",
                " +         * we allow ranges with the same value for the start and end, we don't allow repeating",
                " +         * such range (so we can't have [0, 0][0, 0] even though it would respect the first 2",
                " +         * conditions).",
                " +         *",
                " +         */",
                " +",
                " +        /**",
                " +         * Adds all the range tombstones of {@code tombstones} to this RangeTombstoneList.",
                " +         */",
                " +        public void addAll(LegacyRangeTombstoneList tombstones)",
                " +        {",
                " +            if (tombstones.isEmpty())",
                " +                return;",
                " +",
                " +            if (isEmpty())",
                " +            {",
                " +                copyArrays(tombstones, this);",
                " +                return;",
                " +            }",
                " +",
                " +            /*",
                " +             * We basically have 2 techniques we can use here: either we repeatedly call add() on tombstones values,",
                " +             * or we do a merge of both (sorted) lists. If this lists is bigger enough than the one we add, then",
                " +             * calling add() will be faster, otherwise it's merging that will be faster.",
                " +             *",
                " +             * Let's note that during memtables updates, it might not be uncommon that a new update has only a few range",
                " +             * tombstones, while the CF we're adding it to (the one in the memtable) has many. In that case, using add() is",
                " +             * likely going to be faster.",
                " +             *",
                " +             * In other cases however, like when diffing responses from multiple nodes, the tombstone lists we \"merge\" will",
                " +             * be likely sized, so using add() might be a bit inefficient.",
                " +             *",
                " +             * Roughly speaking (this ignore the fact that updating an element is not exactly constant but that's not a big",
                " +             * deal), if n is the size of this list and m is tombstones size, merging is O(n+m) while using add() is O(m*log(n)).",
                " +             *",
                " +             * But let's not crank up a logarithm computation for that. Long story short, merging will be a bad choice only",
                " +             * if this list size is lot bigger that the other one, so let's keep it simple.",
                " +             */",
                " +            if (size > 10 * tombstones.size)",
                " +            {",
                " +                for (int i = 0; i < tombstones.size; i++)",
                " +                    add(tombstones.starts[i], tombstones.ends[i], tombstones.markedAts[i], tombstones.delTimes[i]);",
                " +            }",
                " +            else",
                " +            {",
                " +                int i = 0;",
                " +                int j = 0;",
                " +                while (i < size && j < tombstones.size)",
                " +                {",
                " +                    if (comparator.compare(tombstones.starts[j], ends[i]) <= 0)",
                " +                    {",
                " +                        insertFrom(i, tombstones.starts[j], tombstones.ends[j], tombstones.markedAts[j], tombstones.delTimes[j]);",
                " +                        j++;",
                " +                    }",
                " +                    else",
                " +                    {",
                " +                        i++;",
                " +                    }",
                " +                }",
                " +                // Addds the remaining ones from tombstones if any (note that addInternal will increment size if relevant).",
                " +                for (; j < tombstones.size; j++)",
                " +                    addInternal(size, tombstones.starts[j], tombstones.ends[j], tombstones.markedAts[j], tombstones.delTimes[j]);",
                " +            }",
                " +        }",
                " +",
                " +        private static void copyArrays(LegacyRangeTombstoneList src, LegacyRangeTombstoneList dst)",
                " +        {",
                " +            dst.grow(src.size);",
                " +            System.arraycopy(src.starts, 0, dst.starts, 0, src.size);",
                " +            System.arraycopy(src.ends, 0, dst.ends, 0, src.size);",
                " +            System.arraycopy(src.markedAts, 0, dst.markedAts, 0, src.size);",
                " +            System.arraycopy(src.delTimes, 0, dst.delTimes, 0, src.size);",
                " +            dst.size = src.size;",
                " +        }",
                " +",
                " +        private void insertFrom(int i, LegacyBound start, LegacyBound end, long markedAt, int delTime)",
                " +        {",
                " +            while (i < size)",
                " +            {",
                " +                assert i == 0 || comparator.compare(ends[i-1], start) <= 0;",
                " +",
                " +                int c = comparator.compare(start, ends[i]);",
                " +                assert c <= 0;",
                " +                if (c == 0)",
                " +                {",
                " +                    // If start == ends[i], then we can insert from the next one (basically the new element",
                " +                    // really start at the next element), except for the case where starts[i] == ends[i].",
                " +                    // In this latter case, if we were to move to next element, we could end up with ...[x, x][x, x]...",
                " +                    if (comparator.compare(starts[i], ends[i]) == 0)",
                " +                    {",
                " +                        // The current element cover a single value which is equal to the start of the inserted",
                " +                        // element. If the inserted element overwrites the current one, just remove the current",
                " +                        // (it's included in what we insert) and proceed with the insert.",
                " +                        if (markedAt > markedAts[i])",
                " +                        {",
                " +                            removeInternal(i);",
                " +                            continue;",
                " +                        }",
                " +",
                " +                        // Otherwise (the current singleton interval override the new one), we want to leave the",
                " +                        // current element and move to the next, unless start == end since that means the new element",
                " +                        // is in fact fully covered by the current one (so we're done)",
                " +                        if (comparator.compare(start, end) == 0)",
                " +                            return;",
                " +                    }",
                " +                    i++;",
                " +                    continue;",
                " +                }",
                " +",
                " +                // Do we overwrite the current element?",
                " +                if (markedAt > markedAts[i])",
                " +                {",
                " +                    // We do overwrite.",
                " +",
                " +                    // First deal with what might come before the newly added one.",
                " +                    if (comparator.compare(starts[i], start) < 0)",
                " +                    {",
                " +                        addInternal(i, starts[i], start, markedAts[i], delTimes[i]);",
                " +                        i++;",
                " +                        // We don't need to do the following line, but in spirit that's what we want to do",
                " +                        // setInternal(i, start, ends[i], markedAts, delTime])",
                " +                    }",
                " +",
                " +                    // now, start <= starts[i]",
                " +",
                " +                    // Does the new element stops before/at the current one,",
                " +                    int endCmp = comparator.compare(end, starts[i]);",
                " +                    if (endCmp <= 0)",
                " +                    {",
                " +                        // Here start <= starts[i] and end <= starts[i]",
                " +                        // This means the current element is before the current one. However, one special",
                " +                        // case is if end == starts[i] and starts[i] == ends[i]. In that case,",
                " +                        // the new element entirely overwrite the current one and we can just overwrite",
                " +                        if (endCmp == 0 && comparator.compare(starts[i], ends[i]) == 0)",
                " +                            setInternal(i, start, end, markedAt, delTime);",
                " +                        else",
                " +                            addInternal(i, start, end, markedAt, delTime);",
                " +                        return;",
                " +                    }",
                " +",
                " +                    // Do we overwrite the current element fully?",
                " +                    int cmp = comparator.compare(ends[i], end);",
                " +                    if (cmp <= 0)",
                " +                    {",
                " +                        // We do overwrite fully:",
                " +                        // update the current element until it's end and continue",
                " +                        // on with the next element (with the new inserted start == current end).",
                " +",
                " +                        // If we're on the last element, we can optimize",
                " +                        if (i == size-1)",
                " +                        {",
                " +                            setInternal(i, start, end, markedAt, delTime);",
                " +                            return;",
                " +                        }",
                " +",
                " +                        setInternal(i, start, ends[i], markedAt, delTime);",
                " +                        if (cmp == 0)",
                " +                            return;",
                " +",
                " +                        start = ends[i];",
                " +                        i++;",
                " +                    }",
                " +                    else",
                " +                    {",
                " +                        // We don't ovewrite fully. Insert the new interval, and then update the now next",
                " +                        // one to reflect the not overwritten parts. We're then done.",
                " +                        addInternal(i, start, end, markedAt, delTime);",
                " +                        i++;",
                " +                        setInternal(i, end, ends[i], markedAts[i], delTimes[i]);",
                " +                        return;",
                " +                    }",
                " +                }",
                " +                else",
                " +                {",
                " +                    // we don't overwrite the current element",
                " +",
                " +                    // If the new interval starts before the current one, insert that new interval",
                " +                    if (comparator.compare(start, starts[i]) < 0)",
                " +                    {",
                " +                        // If we stop before the start of the current element, just insert the new",
                " +                        // interval and we're done; otherwise insert until the beginning of the",
                " +                        // current element",
                " +                        if (comparator.compare(end, starts[i]) <= 0)",
                " +                        {",
                " +                            addInternal(i, start, end, markedAt, delTime);",
                " +                            return;",
                " +                        }",
                " +                        addInternal(i, start, starts[i], markedAt, delTime);",
                " +                        i++;",
                " +                    }",
                " +",
                " +                    // After that, we're overwritten on the current element but might have",
                " +                    // some residual parts after ...",
                " +",
                " +                    // ... unless we don't extend beyond it.",
                " +                    if (comparator.compare(end, ends[i]) <= 0)",
                " +                        return;",
                " +",
                " +                    start = ends[i];",
                " +                    i++;",
                " +                }",
                " +            }",
                " +",
                " +            // If we got there, then just insert the remainder at the end",
                " +            addInternal(i, start, end, markedAt, delTime);",
                " +        }",
                " +",
                " +        private int capacity()",
                " +        {",
                " +            return starts.length;",
                " +        }",
                " +",
                " +        private void addInternal(int i, LegacyBound start, LegacyBound end, long markedAt, int delTime)",
                " +        {",
                " +            assert i >= 0;",
                " +",
                " +            if (size == capacity())",
                " +                growToFree(i);",
                " +            else if (i < size)",
                " +                moveElements(i);",
                " +",
                " +            setInternal(i, start, end, markedAt, delTime);",
                " +            size++;",
                " +        }",
                " +",
                " +        private void removeInternal(int i)",
                " +        {",
                " +            assert i >= 0;",
                " +",
                " +            System.arraycopy(starts, i+1, starts, i, size - i - 1);",
                " +            System.arraycopy(ends, i+1, ends, i, size - i - 1);",
                " +            System.arraycopy(markedAts, i+1, markedAts, i, size - i - 1);",
                " +            System.arraycopy(delTimes, i+1, delTimes, i, size - i - 1);",
                " +",
                " +            --size;",
                " +            starts[size] = null;",
                " +            ends[size] = null;",
                " +        }",
                " +",
                " +        /*",
                " +         * Grow the arrays, leaving index i \"free\" in the process.",
                " +         */",
                " +        private void growToFree(int i)",
                " +        {",
                " +            int newLength = (capacity() * 3) / 2 + 1;",
                " +            grow(i, newLength);",
                " +        }",
                " +",
                " +        /*",
                " +         * Grow the arrays to match newLength capacity.",
                " +         */",
                " +        private void grow(int newLength)",
                " +        {",
                " +            if (capacity() < newLength)",
                " +                grow(-1, newLength);",
                " +        }",
                " +",
                " +        private void grow(int i, int newLength)",
                " +        {",
                " +            starts = grow(starts, size, newLength, i);",
                " +            ends = grow(ends, size, newLength, i);",
                " +            markedAts = grow(markedAts, size, newLength, i);",
                " +            delTimes = grow(delTimes, size, newLength, i);",
                " +        }",
                " +",
                " +        private static LegacyBound[] grow(LegacyBound[] a, int size, int newLength, int i)",
                " +        {",
                " +            if (i < 0 || i >= size)",
                " +                return Arrays.copyOf(a, newLength);",
                " +",
                " +            LegacyBound[] newA = new LegacyBound[newLength];",
                " +            System.arraycopy(a, 0, newA, 0, i);",
                " +            System.arraycopy(a, i, newA, i+1, size - i);",
                " +            return newA;",
                " +        }",
                " +",
                " +        private static long[] grow(long[] a, int size, int newLength, int i)",
                " +        {",
                " +            if (i < 0 || i >= size)",
                " +                return Arrays.copyOf(a, newLength);",
                " +",
                " +            long[] newA = new long[newLength];",
                " +            System.arraycopy(a, 0, newA, 0, i);",
                " +            System.arraycopy(a, i, newA, i+1, size - i);",
                " +            return newA;",
                " +        }",
                " +",
                " +        private static int[] grow(int[] a, int size, int newLength, int i)",
                " +        {",
                " +            if (i < 0 || i >= size)",
                " +                return Arrays.copyOf(a, newLength);",
                " +",
                " +            int[] newA = new int[newLength];",
                " +            System.arraycopy(a, 0, newA, 0, i);",
                " +            System.arraycopy(a, i, newA, i+1, size - i);",
                " +            return newA;",
                " +        }",
                " +",
                " +        /*",
                " +         * Move elements so that index i is \"free\", assuming the arrays have at least one free slot at the end.",
                " +         */",
                " +        private void moveElements(int i)",
                " +        {",
                " +            if (i >= size)",
                " +                return;",
                " +",
                " +            System.arraycopy(starts, i, starts, i+1, size - i);",
                " +            System.arraycopy(ends, i, ends, i+1, size - i);",
                " +            System.arraycopy(markedAts, i, markedAts, i+1, size - i);",
                " +            System.arraycopy(delTimes, i, delTimes, i+1, size - i);",
                " +            // we set starts[i] to null to indicate the position is now empty, so that we update boundaryHeapSize",
                " +            // when we set it",
                " +            starts[i] = null;",
                " +        }",
                " +",
                " +        private void setInternal(int i, LegacyBound start, LegacyBound end, long markedAt, int delTime)",
                " +        {",
                " +            starts[i] = start;",
                " +            ends[i] = end;",
                " +            markedAts[i] = markedAt;",
                " +            delTimes[i] = delTime;",
                " +        }",
                " +",
                " +        public void updateDigest(MessageDigest digest)",
                " +        {",
                " +            ByteBuffer longBuffer = ByteBuffer.allocate(8);",
                " +            for (int i = 0; i < size; i++)",
                " +            {",
                " +                for (int j = 0; j < starts[i].bound.size(); j++)",
                " +                    digest.update(starts[i].bound.get(j).duplicate());",
                " +                if (starts[i].collectionName != null)",
                " +                    digest.update(starts[i].collectionName.name.bytes.duplicate());",
                " +                for (int j = 0; j < ends[i].bound.size(); j++)",
                " +                    digest.update(ends[i].bound.get(j).duplicate());",
                " +                if (ends[i].collectionName != null)",
                " +                    digest.update(ends[i].collectionName.name.bytes.duplicate());",
                " +",
                " +                longBuffer.putLong(0, markedAts[i]);",
                " +                digest.update(longBuffer.array(), 0, 8);",
                " +            }",
                " +        }",
                " +",
                " +        public void serialize(DataOutputPlus out, CFMetaData metadata) throws IOException",
                " +        {",
                " +            out.writeInt(size);",
                " +            if (size == 0)",
                " +                return;",
                " +",
                " +            if (metadata.isCompound())",
                " +                serializeCompound(out, metadata.isDense());",
                " +            else",
                " +                serializeSimple(out);",
                " +        }",
                " +",
                " +        private void serializeCompound(DataOutputPlus out, boolean isDense) throws IOException",
                " +        {",
                " +            List<AbstractType<?>> types = new ArrayList<>(comparator.clusteringComparator.subtypes());",
                " +",
                " +            if (!isDense)",
                " +                types.add(UTF8Type.instance);",
                " +",
                " +            CompositeType type = CompositeType.getInstance(types);",
                " +",
                " +            for (int i = 0; i < size; i++)",
                " +            {",
                " +                LegacyBound start = starts[i];",
                " +                LegacyBound end = ends[i];",
                " +",
                " +                CompositeType.Builder startBuilder = type.builder();",
                " +                CompositeType.Builder endBuilder = type.builder();",
                " +                for (int j = 0; j < start.bound.clustering().size(); j++)",
                " +                {",
                " +                    startBuilder.add(start.bound.get(j));",
                " +                    endBuilder.add(end.bound.get(j));",
                " +                }",
                " +",
                " +                if (start.collectionName != null)",
                " +                    startBuilder.add(start.collectionName.name.bytes);",
                " +                if (end.collectionName != null)",
                " +                    endBuilder.add(end.collectionName.name.bytes);",
                " +",
                " +                ByteBufferUtil.writeWithShortLength(startBuilder.build(), out);",
                " +                ByteBufferUtil.writeWithShortLength(endBuilder.buildAsEndOfRange(), out);",
                " +",
                " +                out.writeInt(delTimes[i]);",
                " +                out.writeLong(markedAts[i]);",
                " +            }",
                " +        }",
                " +",
                " +        private void serializeSimple(DataOutputPlus out) throws IOException",
                " +        {",
                " +            List<AbstractType<?>> types = new ArrayList<>(comparator.clusteringComparator.subtypes());",
                " +            assert types.size() == 1 : types;",
                " +",
                " +            for (int i = 0; i < size; i++)",
                " +            {",
                " +                LegacyBound start = starts[i];",
                " +                LegacyBound end = ends[i];",
                " +",
                " +                ClusteringPrefix startClustering = start.bound.clustering();",
                " +                ClusteringPrefix endClustering = end.bound.clustering();",
                " +",
                " +                assert startClustering.size() == 1;",
                " +                assert endClustering.size() == 1;",
                " +",
                " +                ByteBufferUtil.writeWithShortLength(startClustering.get(0), out);",
                " +                ByteBufferUtil.writeWithShortLength(endClustering.get(0), out);",
                " +",
                " +                out.writeInt(delTimes[i]);",
                " +                out.writeLong(markedAts[i]);",
                " +            }",
                " +        }",
                " +",
                " +        public long serializedSize(CFMetaData metadata)",
                " +        {",
                " +            long size = 0;",
                " +            size += TypeSizes.sizeof(this.size);",
                " +",
                " +            if (this.size == 0)",
                " +                return size;",
                " +",
                " +            if (metadata.isCompound())",
                " +                return size + serializedSizeCompound(metadata.isDense());",
                " +            else",
                " +                return size + serializedSizeSimple();",
                " +        }",
                " +",
                " +        private long serializedSizeCompound(boolean isDense)",
                " +        {",
                " +            long size = 0;",
                " +            List<AbstractType<?>> types = new ArrayList<>(comparator.clusteringComparator.subtypes());",
                " +            if (!isDense)",
                " +                types.add(UTF8Type.instance);",
                " +            CompositeType type = CompositeType.getInstance(types);",
                " +",
                " +            for (int i = 0; i < this.size; i++)",
                " +            {",
                " +                LegacyBound start = starts[i];",
                " +                LegacyBound end = ends[i];",
                " +",
                " +                CompositeType.Builder startBuilder = type.builder();",
                " +                CompositeType.Builder endBuilder = type.builder();",
                " +                for (int j = 0; j < start.bound.clustering().size(); j++)",
                " +                {",
                " +                    startBuilder.add(start.bound.get(j));",
                " +                    endBuilder.add(end.bound.get(j));",
                " +                }",
                " +",
                " +                if (start.collectionName != null)",
                " +                    startBuilder.add(start.collectionName.name.bytes);",
                " +                if (end.collectionName != null)",
                " +                    endBuilder.add(end.collectionName.name.bytes);",
                " +",
                " +                size += ByteBufferUtil.serializedSizeWithShortLength(startBuilder.build());",
                " +                size += ByteBufferUtil.serializedSizeWithShortLength(endBuilder.buildAsEndOfRange());",
                " +",
                " +                size += TypeSizes.sizeof(delTimes[i]);",
                " +                size += TypeSizes.sizeof(markedAts[i]);",
                " +            }",
                " +            return size;",
                " +        }",
                " +",
                " +        private long serializedSizeSimple()",
                " +        {",
                " +            long size = 0;",
                " +            List<AbstractType<?>> types = new ArrayList<>(comparator.clusteringComparator.subtypes());",
                " +            assert types.size() == 1 : types;",
                " +",
                " +            for (int i = 0; i < this.size; i++)",
                " +            {",
                " +                LegacyBound start = starts[i];",
                " +                LegacyBound end = ends[i];",
                " +",
                " +                ClusteringPrefix startClustering = start.bound.clustering();",
                " +                ClusteringPrefix endClustering = end.bound.clustering();",
                " +",
                " +                assert startClustering.size() == 1;",
                " +                assert endClustering.size() == 1;",
                " +",
                " +                size += ByteBufferUtil.serializedSizeWithShortLength(startClustering.get(0));",
                " +                size += ByteBufferUtil.serializedSizeWithShortLength(endClustering.get(0));",
                " +",
                " +                size += TypeSizes.sizeof(delTimes[i]);",
                " +                size += TypeSizes.sizeof(markedAts[i]);",
                " +            }",
                " +            return size;",
                " +        }",
                " +    }",
                " +}",
                "diff --cc src/java/org/apache/cassandra/db/LivenessInfo.java",
                "index 89e0578911,0000000000..f6c9b629f8",
                "mode 100644,000000..100644",
                "--- a/src/java/org/apache/cassandra/db/LivenessInfo.java",
                "+++ b/src/java/org/apache/cassandra/db/LivenessInfo.java",
                "@@@ -1,369 -1,0 +1,375 @@@",
                " +/*",
                " + * Licensed to the Apache Software Foundation (ASF) under one",
                " + * or more contributor license agreements.  See the NOTICE file",
                " + * distributed with this work for additional information",
                " + * regarding copyright ownership.  The ASF licenses this file",
                " + * to you under the Apache License, Version 2.0 (the",
                " + * \"License\"); you may not use this file except in compliance",
                " + * with the License.  You may obtain a copy of the License at",
                " + *",
                " + *     http://www.apache.org/licenses/LICENSE-2.0",
                " + *",
                " + * Unless required by applicable law or agreed to in writing, software",
                " + * distributed under the License is distributed on an \"AS IS\" BASIS,",
                " + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                " + * See the License for the specific language governing permissions and",
                " + * limitations under the License.",
                " + */",
                " +package org.apache.cassandra.db;",
                " +",
                " +import java.util.Objects;",
                " +import java.security.MessageDigest;",
                " +",
                " +import org.apache.cassandra.config.CFMetaData;",
                "++import org.apache.cassandra.db.rows.Cell;",
                " +import org.apache.cassandra.serializers.MarshalException;",
                " +import org.apache.cassandra.utils.FBUtilities;",
                " +",
                " +/**",
                " + * Stores the information relating to the liveness of the primary key columns of a row.",
                " + * <p>",
                " + * A {@code LivenessInfo} can first be empty. If it isn't, it contains at least a timestamp,",
                " + * which is the timestamp for the row primary key columns. On top of that, the info can be",
                " + * ttl'ed, in which case the {@code LivenessInfo} also has both a ttl and a local expiration time.",
                " + * <p>",
                " + * Please note that if a liveness info is ttl'ed, that expiration is <b>only</b> an expiration",
                " + * of the liveness info itself (so, of the timestamp), and once the info expires it becomes",
                " + * {@code EMPTY}. But if a row has a liveness info which expires, the rest of the row data is",
                " + * unaffected (of course, the rest of said row data might be ttl'ed on its own but this is",
                " + * separate).",
                " + */",
                " +public class LivenessInfo",
                " +{",
                " +    public static final long NO_TIMESTAMP = Long.MIN_VALUE;",
                "-     public static final int NO_TTL = 0;",
                "++    public static final int NO_TTL = Cell.NO_TTL;",
                " +    /**",
                " +     * Used as flag for representing an expired liveness.",
                " +     *",
                " +     * TTL per request is at most 20 yrs, so this shouldn't conflict",
                " +     * (See {@link org.apache.cassandra.cql3.Attributes#MAX_TTL})",
                " +     */",
                " +    public static final int EXPIRED_LIVENESS_TTL = Integer.MAX_VALUE;",
                "-     public static final int NO_EXPIRATION_TIME = Integer.MAX_VALUE;",
                "++    public static final int NO_EXPIRATION_TIME = Cell.NO_DELETION_TIME;",
                " +",
                " +    public static final LivenessInfo EMPTY = new LivenessInfo(NO_TIMESTAMP);",
                " +",
                " +    protected final long timestamp;",
                " +",
                " +    protected LivenessInfo(long timestamp)",
                " +    {",
                " +        this.timestamp = timestamp;",
                " +    }",
                " +",
                " +    public static LivenessInfo create(CFMetaData metadata, long timestamp, int nowInSec)",
                " +    {",
                " +        int defaultTTL = metadata.params.defaultTimeToLive;",
                " +        if (defaultTTL != NO_TTL)",
                " +            return expiring(timestamp, defaultTTL, nowInSec);",
                " +",
                " +        return new LivenessInfo(timestamp);",
                " +    }",
                " +",
                " +    public static LivenessInfo expiring(long timestamp, int ttl, int nowInSec)",
                " +    {",
                " +        assert ttl != EXPIRED_LIVENESS_TTL;",
                "-         return new ExpiringLivenessInfo(timestamp, ttl, nowInSec + ttl);",
                "++        return new ExpiringLivenessInfo(timestamp, ttl, ExpirationDateOverflowHandling.computeLocalExpirationTime(nowInSec, ttl));",
                " +    }",
                " +",
                " +    public static LivenessInfo create(CFMetaData metadata, long timestamp, int ttl, int nowInSec)",
                " +    {",
                " +        return ttl == NO_TTL",
                " +             ? create(metadata, timestamp, nowInSec)",
                " +             : expiring(timestamp, ttl, nowInSec);",
                " +    }",
                " +",
                " +    // Note that this ctor ignores the default table ttl and takes the expiration time, not the current time.",
                " +    // Use when you know that's what you want.",
                " +    public static LivenessInfo create(long timestamp, int ttl, int localExpirationTime)",
                " +    {",
                " +        if (ttl == EXPIRED_LIVENESS_TTL)",
                " +            return new ExpiredLivenessInfo(timestamp, ttl, localExpirationTime);",
                " +        return ttl == NO_TTL ? new LivenessInfo(timestamp) : new ExpiringLivenessInfo(timestamp, ttl, localExpirationTime);",
                " +    }",
                " +",
                " +    /**",
                " +     * Whether this liveness info is empty (has no timestamp).",
                " +     *",
                " +     * @return whether this liveness info is empty or not.",
                " +     */",
                " +    public boolean isEmpty()",
                " +    {",
                " +        return timestamp == NO_TIMESTAMP;",
                " +    }",
                " +",
                " +    /**",
                " +     * The timestamp for this liveness info.",
                " +     *",
                " +     * @return the liveness info timestamp (or {@link #NO_TIMESTAMP} if the info is empty).",
                " +     */",
                " +    public long timestamp()",
                " +    {",
                " +        return timestamp;",
                " +    }",
                " +",
                " +    /**",
                " +     * Whether the info has a ttl.",
                " +     */",
                " +    public boolean isExpiring()",
                " +    {",
                " +        return false;",
                " +    }",
                " +",
                " +    /**",
                " +     * The ttl (if any) on the row primary key columns or {@link #NO_TTL} if it is not",
                " +     * expiring.",
                " +     *",
                " +     * Please note that this value is the TTL that was set originally and is thus not",
                " +     * changing.",
                " +     */",
                " +    public int ttl()",
                " +    {",
                " +        return NO_TTL;",
                " +    }",
                " +",
                " +    /**",
                " +     * The expiration time (in seconds) if the info is expiring ({@link #NO_EXPIRATION_TIME} otherwise).",
                " +     *",
                " +     */",
                " +    public int localExpirationTime()",
                " +    {",
                " +        return NO_EXPIRATION_TIME;",
                " +    }",
                " +",
                " +    /**",
                " +     * Whether that info is still live.",
                " +     *",
                " +     * A {@code LivenessInfo} is live if it is either not expiring, or if its expiration time if after",
                " +     * {@code nowInSec}.",
                " +     *",
                " +     * @param nowInSec the current time in seconds.",
                " +     * @return whether this liveness info is live or not.",
                " +     */",
                " +    public boolean isLive(int nowInSec)",
                " +    {",
                " +        return !isEmpty();",
                " +    }",
                " +",
                " +    /**",
                " +     * Adds this liveness information to the provided digest.",
                " +     *",
                " +     * @param digest the digest to add this liveness information to.",
                " +     */",
                " +    public void digest(MessageDigest digest)",
                " +    {",
                " +        FBUtilities.updateWithLong(digest, timestamp());",
                " +    }",
                " +",
                " +    /**",
                " +     * Validate the data contained by this liveness information.",
                " +     *",
                " +     * @throws MarshalException if some of the data is corrupted.",
                " +     */",
                " +    public void validate()",
                " +    {",
                " +    }",
                " +",
                " +    /**",
                " +     * The size of the (useful) data this liveness information contains.",
                " +     *",
                " +     * @return the size of the data this liveness information contains.",
                " +     */",
                " +    public int dataSize()",
                " +    {",
                " +        return TypeSizes.sizeof(timestamp());",
                " +    }",
                " +",
                " +    /**",
                " +     * Whether this liveness information supersedes another one (that is",
                " +     * whether is has a greater timestamp than the other or not).",
                " +     *",
                " +     * </br>",
                " +     *",
                " +     * If timestamps are the same and none of them are expired livenessInfo,",
                " +     * livenessInfo with greater TTL supersedes another. It also means, if timestamps are the same,",
                " +     * ttl superseders no-ttl. This is the same rule as {@link Conflicts#resolveRegular}",
                " +     *",
                " +     * If timestamps are the same and one of them is expired livenessInfo. Expired livenessInfo",
                " +     * supersedes, ie. tombstone supersedes.",
                " +     *",
                " +     * If timestamps are the same and both of them are expired livenessInfo(Ideally it shouldn't happen),",
                " +     * greater localDeletionTime wins.",
                " +     *",
                " +     * @param other",
                " +     *            the {@code LivenessInfo} to compare this info to.",
                " +     *",
                " +     * @return whether this {@code LivenessInfo} supersedes {@code other}.",
                " +     */",
                " +    public boolean supersedes(LivenessInfo other)",
                " +    {",
                " +        if (timestamp != other.timestamp)",
                " +            return timestamp > other.timestamp;",
                " +        if (isExpired() ^ other.isExpired())",
                " +            return isExpired();",
                " +        if (isExpiring() == other.isExpiring())",
                " +            return localExpirationTime() > other.localExpirationTime();",
                " +        return isExpiring();",
                " +    }",
                " +",
                " +    protected boolean isExpired()",
                " +    {",
                " +        return false;",
                " +    }",
                " +",
                " +    /**",
                " +     * Returns a copy of this liveness info updated with the provided timestamp.",
                " +     *",
                " +     * @param newTimestamp the timestamp for the returned info.",
                " +     * @return if this liveness info has a timestamp, a copy of it with {@code newTimestamp}",
                " +     * as timestamp. If it has no timestamp however, this liveness info is returned",
                " +     * unchanged.",
                " +     */",
                " +    public LivenessInfo withUpdatedTimestamp(long newTimestamp)",
                " +    {",
                " +        return new LivenessInfo(newTimestamp);",
                " +    }",
                " +",
                "++    public LivenessInfo withUpdatedTimestampAndLocalDeletionTime(long newTimestamp, int newLocalDeletionTime)",
                "++    {",
                "++        return LivenessInfo.create(newTimestamp, ttl(), newLocalDeletionTime);",
                "++    }",
                "++",
                " +    @Override",
                " +    public String toString()",
                " +    {",
                " +        return String.format(\"[ts=%d]\", timestamp);",
                " +    }",
                " +",
                " +    @Override",
                " +    public boolean equals(Object other)",
                " +    {",
                " +        if(!(other instanceof LivenessInfo))",
                " +            return false;",
                " +",
                " +        LivenessInfo that = (LivenessInfo)other;",
                " +        return this.timestamp() == that.timestamp()",
                " +            && this.ttl() == that.ttl()",
                " +            && this.localExpirationTime() == that.localExpirationTime();",
                " +    }",
                " +",
                " +    @Override",
                " +    public int hashCode()",
                " +    {",
                " +        return Objects.hash(timestamp(), ttl(), localExpirationTime());",
                " +    }",
                " +",
                " +    /**",
                " +     * Effectively acts as a PK tombstone. This is used for Materialized Views to shadow",
                " +     * updated entries while co-existing with row tombstones.",
                " +     *",
                " +     * See {@link org.apache.cassandra.db.view.ViewUpdateGenerator#deleteOldEntryInternal}.",
                " +     */",
                " +    private static class ExpiredLivenessInfo extends ExpiringLivenessInfo",
                " +    {",
                " +        private ExpiredLivenessInfo(long timestamp, int ttl, int localExpirationTime)",
                " +        {",
                " +            super(timestamp, ttl, localExpirationTime);",
                " +            assert ttl == EXPIRED_LIVENESS_TTL;",
                " +            assert timestamp != NO_TIMESTAMP;",
                " +        }",
                " +",
                " +        @Override",
                " +        public boolean isExpired()",
                " +        {",
                " +            return true;",
                " +        }",
                " +",
                " +        @Override",
                " +        public boolean isLive(int nowInSec)",
                " +        {",
                " +            // used as tombstone to shadow entire PK",
                " +            return false;",
                " +        }",
                " +",
                " +        @Override",
                " +        public LivenessInfo withUpdatedTimestamp(long newTimestamp)",
                " +        {",
                " +            return new ExpiredLivenessInfo(newTimestamp, ttl(), localExpirationTime());",
                " +        }",
                " +    }",
                " +",
                " +    private static class ExpiringLivenessInfo extends LivenessInfo",
                " +    {",
                " +        private final int ttl;",
                " +        private final int localExpirationTime;",
                " +",
                " +        private ExpiringLivenessInfo(long timestamp, int ttl, int localExpirationTime)",
                " +        {",
                " +            super(timestamp);",
                " +            assert ttl != NO_TTL && localExpirationTime != NO_EXPIRATION_TIME;",
                " +            this.ttl = ttl;",
                " +            this.localExpirationTime = localExpirationTime;",
                " +        }",
                " +",
                " +        @Override",
                " +        public int ttl()",
                " +        {",
                " +            return ttl;",
                " +        }",
                " +",
                " +        @Override",
                " +        public int localExpirationTime()",
                " +        {",
                " +            return localExpirationTime;",
                " +        }",
                " +",
                " +        @Override",
                " +        public boolean isExpiring()",
                " +        {",
                " +            return true;",
                " +        }",
                " +",
                " +        @Override",
                " +        public boolean isLive(int nowInSec)",
                " +        {",
                " +            return nowInSec < localExpirationTime;",
                " +        }",
                " +",
                " +        @Override",
                " +        public void digest(MessageDigest digest)",
                " +        {",
                " +            super.digest(digest);",
                " +            FBUtilities.updateWithInt(digest, localExpirationTime);",
                " +            FBUtilities.updateWithInt(digest, ttl);",
                " +        }",
                " +",
                " +        @Override",
                " +        public void validate()",
                " +        {",
                " +            if (ttl < 0)",
                " +                throw new MarshalException(\"A TTL should not be negative\");",
                " +            if (localExpirationTime < 0)",
                " +                throw new MarshalException(\"A local expiration time should not be negative\");",
                " +        }",
                " +",
                " +        @Override",
                " +        public int dataSize()",
                " +        {",
                " +            return super.dataSize()",
                " +                 + TypeSizes.sizeof(ttl)",
                " +                 + TypeSizes.sizeof(localExpirationTime);",
                " +",
                " +        }",
                " +",
                " +        @Override",
                " +        public LivenessInfo withUpdatedTimestamp(long newTimestamp)",
                " +        {",
                " +            return new ExpiringLivenessInfo(newTimestamp, ttl, localExpirationTime);",
                " +        }",
                " +",
                " +        @Override",
                " +        public String toString()",
                " +        {",
                " +            return String.format(\"[ts=%d ttl=%d, let=%d]\", timestamp, ttl, localExpirationTime);",
                " +        }",
                " +    }",
                " +}",
                "diff --cc src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "index eaf6dab595,d90abe9fe7..1d5466795e",
                "--- a/src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/CompactionManager.java",
                "@@@ -365,3 -350,9 +365,11 @@@ public class CompactionManager implemen",
                "--    public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData, int jobs)",
                "++    public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData,",
                "++                                           int jobs)",
                "+     throws InterruptedException, ExecutionException",
                "+     {",
                "+         return performScrub(cfs, skipCorrupted, checkData, false, jobs);",
                "+     }",
                "+ ",
                " -    public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData, final boolean reinsertOverflowedTTLRows, int jobs)",
                "++    public AllSSTableOpStatus performScrub(final ColumnFamilyStore cfs, final boolean skipCorrupted, final boolean checkData,",
                "++                                           final boolean reinsertOverflowedTTL, int jobs)",
                "      throws InterruptedException, ExecutionException",
                "@@@ -379,3 -370,3 +387,3 @@@",
                "              {",
                "-                 scrubOne(cfs, input, skipCorrupted, checkData);",
                " -                scrubOne(cfs, input, skipCorrupted, checkData, reinsertOverflowedTTLRows);",
                "++                scrubOne(cfs, input, skipCorrupted, checkData, reinsertOverflowedTTL);",
                "              }",
                "@@@ -747,3 -738,3 +755,3 @@@",
                "-     private void scrubOne(ColumnFamilyStore cfs, LifecycleTransaction modifier, boolean skipCorrupted, boolean checkData) throws IOException",
                " -    private void scrubOne(ColumnFamilyStore cfs, LifecycleTransaction modifier, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows) throws IOException",
                "++    private void scrubOne(ColumnFamilyStore cfs, LifecycleTransaction modifier, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTL) throws IOException",
                "      {",
                "@@@ -751,3 -742,3 +759,3 @@@",
                "-         try (Scrubber scrubber = new Scrubber(cfs, modifier, skipCorrupted, checkData))",
                " -        try (Scrubber scrubber = new Scrubber(cfs, modifier, skipCorrupted, checkData, reinsertOverflowedTTLRows))",
                "++        try (Scrubber scrubber = new Scrubber(cfs, modifier, skipCorrupted, checkData, reinsertOverflowedTTL))",
                "          {",
                "diff --cc src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "index c8e0c53323,affee119fd..bc1150430f",
                "--- a/src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "+++ b/src/java/org/apache/cassandra/db/compaction/Scrubber.java",
                "@@@ -39,2 -45,2 +39,3 @@@ import org.apache.cassandra.utils.*",
                "  import org.apache.cassandra.utils.concurrent.Refs;",
                "++import org.apache.cassandra.utils.memory.HeapAllocator;",
                "@@@ -67,5 -78,6 +69,7 @@@ public class Scrubber implements Closea",
                " -    private final OutputHandler outputHandler;",
                "+     private NegativeLocalDeletionInfoMetrics negativeLocalDeletionInfoMetrics = new NegativeLocalDeletionInfoMetrics();",
                "+ ",
                " -    private static final Comparator<Row> rowComparator = new Comparator<Row>()",
                " +    private final OutputHandler outputHandler;",
                " +",
                " +    private static final Comparator<Partition> partitionComparator = new Comparator<Partition>()",
                "      {",
                "@@@ -80,3 -92,9 +84,9 @@@",
                "      {",
                "-         this(cfs, transaction, skipCorrupted, new OutputHandler.LogOutput(), checkData);",
                " -        this(cfs, transaction, skipCorrupted, new OutputHandler.LogOutput(), checkData, false);",
                "++        this(cfs, transaction, skipCorrupted, checkData, false);",
                "+     }",
                "+ ",
                "+     public Scrubber(ColumnFamilyStore cfs, LifecycleTransaction transaction, boolean skipCorrupted, boolean checkData,",
                "+                     boolean reinsertOverflowedTTLRows) throws IOException",
                "+     {",
                "+         this(cfs, transaction, skipCorrupted, new OutputHandler.LogOutput(), checkData, reinsertOverflowedTTLRows);",
                "      }",
                "@@@ -84,7 -102,4 +94,8 @@@",
                "      @SuppressWarnings(\"resource\")",
                " -    public Scrubber(ColumnFamilyStore cfs, LifecycleTransaction transaction, boolean skipCorrupted, OutputHandler outputHandler, boolean checkData,",
                " +    public Scrubber(ColumnFamilyStore cfs,",
                " +                    LifecycleTransaction transaction,",
                " +                    boolean skipCorrupted,",
                " +                    OutputHandler outputHandler,",
                "-                     boolean checkData) throws IOException",
                "++                    boolean checkData,",
                "+                     boolean reinsertOverflowedTTLRows) throws IOException",
                "      {",
                "@@@ -95,6 -110,5 +106,6 @@@",
                "          this.skipCorrupted = skipCorrupted;",
                " -        this.rowIndexEntrySerializer = sstable.descriptor.version.getSSTableFormat().getIndexSerializer(sstable.metadata);",
                "+         this.reinsertOverflowedTTLRows = reinsertOverflowedTTLRows;",
                " -",
                " +        this.rowIndexEntrySerializer = sstable.descriptor.version.getSSTableFormat().getIndexSerializer(sstable.metadata,",
                " +                                                                                                        sstable.descriptor.version,",
                " +                                                                                                        sstable.header);",
                "- ",
                "          List<SSTableReader> toScrub = Collections.singletonList(sstable);",
                "@@@ -298,8 -333,2 +313,9 @@@",
                "          }",
                " +        else",
                " +        {",
                "-             outputHandler.output(\"Scrub of \" + sstable + \" complete: \" + goodRows + \" rows in new sstable and \" + emptyRows + \" empty (tombstoned) rows dropped\");",
                " +            if (badRows > 0)",
                "-                 outputHandler.warn(\"Unable to recover \" + badRows + \" rows that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  You can also run nodetool repair to transfer the data from a healthy replica, if any\");",
                "++                outputHandler.warn(\"No valid rows found while scrubbing \" + sstable + \"; it is marked for deletion now. If you want to attempt manual recovery, you can find a copy in the pre-scrub snapshot\");",
                "++            else",
                "++                outputHandler.output(\"Scrub of \" + sstable + \" complete; looks like all \" + emptyRows + \" rows were tombstoned\");",
                " +        }",
                "      }",
                "@@@ -309,7 -338,12 +325,7 @@@",
                "      {",
                " -        // OrderCheckerIterator will check, at iteration time, that the cells are in the proper order. If it detects",
                " -        // that one cell is out of order, it will stop returning them. The remaining cells will be sorted and added",
                " -        // to the outOfOrderRows that will be later written to a new SSTable.",
                " -        OrderCheckerIterator atoms = new OrderCheckerIterator(getIterator(key),",
                " -                                                              cfs.metadata.comparator.onDiskAtomComparator());",
                " -        if (prevKey != null && prevKey.compareTo(key) > 0)",
                " -        {",
                " -            saveOutOfOrderRow(prevKey, key, atoms);",
                " -            return false;",
                " -        }",
                " +        // OrderCheckerIterator will check, at iteration time, that the rows are in the proper order. If it detects",
                " +        // that one row is out of order, it will stop returning them. The remaining rows will be sorted and added",
                " +        // to the outOfOrder set that will be later written to a new SSTable.",
                "-         OrderCheckerIterator sstableIterator = new OrderCheckerIterator(new RowMergingSSTableIterator(sstable, dataFile, key),",
                "++        OrderCheckerIterator sstableIterator = new OrderCheckerIterator(getIterator(key),",
                " +                                                                        cfs.metadata.comparator);",
                "@@@ -338,2 -361,14 +354,14 @@@",
                "+     /**",
                "+      * Only wrap with {@link FixNegativeLocalDeletionTimeIterator} if {@link #reinsertOverflowedTTLRows} option",
                "+      * is specified",
                "+      */",
                " -    private OnDiskAtomIterator getIterator(DecoratedKey key)",
                "++    private UnfilteredRowIterator getIterator(DecoratedKey key)",
                "+     {",
                " -        SSTableIdentityIterator sstableIdentityIterator = new SSTableIdentityIterator(sstable, dataFile, key, checkData);",
                " -        return reinsertOverflowedTTLRows ? new FixNegativeLocalDeletionTimeIterator(sstableIdentityIterator,",
                "++        RowMergingSSTableIterator rowMergingIterator = new RowMergingSSTableIterator(sstable, dataFile, key);",
                "++        return reinsertOverflowedTTLRows ? new FixNegativeLocalDeletionTimeIterator(rowMergingIterator,",
                "+                                                                                     outputHandler,",
                " -                                                                                    negativeLocalDeletionInfoMetrics) : sstableIdentityIterator;",
                "++                                                                                    negativeLocalDeletionInfoMetrics) : rowMergingIterator;",
                "+     }",
                "+ ",
                "      private void updateIndexKey()",
                "@@@ -479,44 -557,7 +507,50 @@@",
                "+     public class NegativeLocalDeletionInfoMetrics",
                "+     {",
                "+         public volatile int fixedRows = 0;",
                "+     }",
                "+ ",
                " +    /**",
                " +     * During 2.x migration, under some circumstances rows might have gotten duplicated.",
                " +     * Merging iterator merges rows with same clustering.",
                " +     *",
                " +     * For more details, refer to CASSANDRA-12144.",
                " +     */",
                " +    private static class RowMergingSSTableIterator extends SSTableIdentityIterator",
                " +    {",
                " +        RowMergingSSTableIterator(SSTableReader sstable, RandomAccessReader file, DecoratedKey key)",
                " +        {",
                " +            super(sstable, file, key);",
                " +        }",
                " +",
                " +        @Override",
                " +        protected Unfiltered doCompute()",
                " +        {",
                " +            if (!iterator.hasNext())",
                " +                return endOfData();",
                " +",
                " +            Unfiltered next = iterator.next();",
                " +            if (!next.isRow())",
                " +                return next;",
                " +",
                " +            while (iterator.hasNext())",
                " +            {",
                " +                Unfiltered peek = iterator.peek();",
                " +                // If there was a duplicate row, merge it.",
                " +                if (next.clustering().equals(peek.clustering()) && peek.isRow())",
                " +                {",
                " +                    iterator.next(); // Make sure that the peeked item was consumed.",
                " +                    next = Rows.merge((Row) next, (Row) peek, FBUtilities.nowInSeconds());",
                " +                }",
                " +                else",
                " +                {",
                " +                    break;",
                " +                }",
                " +            }",
                " +",
                " +            return next;",
                " +        }",
                "++",
                " +    }",
                " +",
                "      /**",
                "@@@ -620,3 -646,59 +654,149 @@@",
                "          }",
                "+     }",
                "+ ",
                "+     /**",
                " -     * This iterator converts negative {@link BufferExpiringCell#getLocalDeletionTime()} into {@link BufferExpiringCell#MAX_DELETION_TIME}",
                "++     * This iterator converts negative {@link AbstractCell#localDeletionTime()} into {@link AbstractCell#MAX_DELETION_TIME}",
                "+      *",
                "+      * This is to recover entries with overflowed localExpirationTime due to CASSANDRA-14092",
                "+      */",
                " -    private static final class FixNegativeLocalDeletionTimeIterator extends AbstractIterator<OnDiskAtom> implements OnDiskAtomIterator",
                "++    private static final class FixNegativeLocalDeletionTimeIterator extends AbstractIterator<Unfiltered> implements UnfilteredRowIterator",
                "+     {",
                "+         /**",
                "+          * The decorated iterator.",
                "+          */",
                " -        private final OnDiskAtomIterator iterator;",
                "++        private final UnfilteredRowIterator iterator;",
                "+ ",
                "+         private final OutputHandler outputHandler;",
                "+         private final NegativeLocalDeletionInfoMetrics negativeLocalExpirationTimeMetrics;",
                "+ ",
                " -        public FixNegativeLocalDeletionTimeIterator(OnDiskAtomIterator iterator, OutputHandler outputHandler,",
                "++        public FixNegativeLocalDeletionTimeIterator(UnfilteredRowIterator iterator, OutputHandler outputHandler,",
                "+                                                     NegativeLocalDeletionInfoMetrics negativeLocalDeletionInfoMetrics)",
                "+         {",
                "+             this.iterator = iterator;",
                "+             this.outputHandler = outputHandler;",
                "+             this.negativeLocalExpirationTimeMetrics = negativeLocalDeletionInfoMetrics;",
                "+         }",
                "+ ",
                " -        public ColumnFamily getColumnFamily()",
                "++        public CFMetaData metadata()",
                "+         {",
                " -            return iterator.getColumnFamily();",
                "++            return iterator.metadata();",
                "+         }",
                "+ ",
                " -        public DecoratedKey getKey()",
                "++        public boolean isReverseOrder()",
                "+         {",
                " -            return iterator.getKey();",
                "++            return iterator.isReverseOrder();",
                "+         }",
                "+ ",
                " -        public void close() throws IOException",
                "++        public PartitionColumns columns()",
                "+         {",
                " -            iterator.close();",
                "++            return iterator.columns();",
                "++        }",
                "++",
                "++        public DecoratedKey partitionKey()",
                "++        {",
                "++            return iterator.partitionKey();",
                "++        }",
                "++",
                "++        public Row staticRow()",
                "++        {",
                "++            return iterator.staticRow();",
                "+         }",
                "+ ",
                "+         @Override",
                " -        protected OnDiskAtom computeNext()",
                "++        public boolean isEmpty()",
                "++        {",
                "++            return iterator.isEmpty();",
                "++        }",
                "++",
                "++        public void close()",
                "++        {",
                "++            iterator.close();",
                "++        }",
                "++",
                "++        public DeletionTime partitionLevelDeletion()",
                "++        {",
                "++            return iterator.partitionLevelDeletion();",
                "++        }",
                "++",
                "++        public EncodingStats stats()",
                "++        {",
                "++            return iterator.stats();",
                "++        }",
                "++",
                "++        protected Unfiltered computeNext()",
                "+         {",
                "+             if (!iterator.hasNext())",
                "+                 return endOfData();",
                "+ ",
                " -            OnDiskAtom next = iterator.next();",
                "++            Unfiltered next = iterator.next();",
                "++            if (!next.isRow())",
                "++                return next;",
                "+ ",
                " -            if (next instanceof ExpiringCell && next.getLocalDeletionTime() < 0)",
                "++            if (hasNegativeLocalExpirationTime((Row) next))",
                "+             {",
                " -                outputHandler.debug(String.format(\"Found cell with negative local expiration time: %s\", ((ExpiringCell) next).getString(getColumnFamily().getComparator()), getColumnFamily()));",
                "++                outputHandler.debug(String.format(\"Found row with negative local expiration time: %s\", next.toString(metadata(), false)));",
                "+                 negativeLocalExpirationTimeMetrics.fixedRows++;",
                " -                next = ((Cell) next).localCopy(getColumnFamily().metadata(), HeapAllocator.instance).withUpdatedTimestampAndLocalDeletionTime(next.timestamp() + 1, BufferExpiringCell.MAX_DELETION_TIME);",
                "++                return fixNegativeLocalExpirationTime((Row) next);",
                "+             }",
                "+ ",
                "+             return next;",
                "+         }",
                "++",
                "++        private boolean hasNegativeLocalExpirationTime(Row next)",
                "++        {",
                "++            Row row = next;",
                "++            if (row.primaryKeyLivenessInfo().isExpiring() && row.primaryKeyLivenessInfo().localExpirationTime() < 0)",
                "++            {",
                "++                return true;",
                "++            }",
                "++",
                "++            for (ColumnData cd : row)",
                "++            {",
                "++                if (cd.column().isSimple())",
                "++                {",
                "++                    Cell cell = (Cell)cd;",
                "++                    if (cell.isExpiring() && cell.localDeletionTime() < 0)",
                "++                        return true;",
                "++                }",
                "++                else",
                "++                {",
                "++                    ComplexColumnData complexData = (ComplexColumnData)cd;",
                "++                    for (Cell cell : complexData)",
                "++                    {",
                "++                        if (cell.isExpiring() && cell.localDeletionTime() < 0)",
                "++                            return true;",
                "++                    }",
                "++                }",
                "++            }",
                "++",
                "++            return false;",
                "++        }",
                " +",
                "++        private Unfiltered fixNegativeLocalExpirationTime(Row row)",
                "++        {",
                "++            Row.Builder builder = HeapAllocator.instance.cloningBTreeRowBuilder();",
                "++            builder.newRow(row.clustering());",
                "++            builder.addPrimaryKeyLivenessInfo(row.primaryKeyLivenessInfo().isExpiring() && row.primaryKeyLivenessInfo().localExpirationTime() < 0 ?",
                "++                                              row.primaryKeyLivenessInfo().withUpdatedTimestampAndLocalDeletionTime(row.primaryKeyLivenessInfo().timestamp() + 1, AbstractCell.MAX_DELETION_TIME)",
                "++                                              :row.primaryKeyLivenessInfo());",
                "++            builder.addRowDeletion(row.deletion());",
                "++            for (ColumnData cd : row)",
                "++            {",
                "++                if (cd.column().isSimple())",
                "++                {",
                "++                    Cell cell = (Cell)cd;",
                "++                    builder.addCell(cell.isExpiring() && cell.localDeletionTime() < 0 ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp() + 1, AbstractCell.MAX_DELETION_TIME) : cell);",
                "++                }",
                "++                else",
                "++                {",
                "++                    ComplexColumnData complexData = (ComplexColumnData)cd;",
                "++                    builder.addComplexDeletion(complexData.column(), complexData.complexDeletion());",
                "++                    for (Cell cell : complexData)",
                "++                    {",
                "++                        builder.addCell(cell.isExpiring() && cell.localDeletionTime() < 0 ? cell.withUpdatedTimestampAndLocalDeletionTime(cell.timestamp() + 1, AbstractCell.MAX_DELETION_TIME) : cell);",
                "++                    }",
                "++                }",
                "++            }",
                "++            return builder.build();",
                "++        }",
                "      }",
                "diff --cc src/java/org/apache/cassandra/db/rows/BufferCell.java",
                "index 82ae02ca68,0000000000..df2619c7ce",
                "mode 100644,000000..100644",
                "--- a/src/java/org/apache/cassandra/db/rows/BufferCell.java",
                "+++ b/src/java/org/apache/cassandra/db/rows/BufferCell.java",
                "@@@ -1,365 -1,0 +1,370 @@@",
                " +/*",
                " + * Licensed to the Apache Software Foundation (ASF) under one",
                " + * or more contributor license agreements.  See the NOTICE file",
                " + * distributed with this work for additional information",
                " + * regarding copyright ownership.  The ASF licenses this file",
                " + * to you under the Apache License, Version 2.0 (the",
                " + * \"License\"); you may not use this file except in compliance",
                " + * with the License.  You may obtain a copy of the License at",
                " + *",
                " + *     http://www.apache.org/licenses/LICENSE-2.0",
                " + *",
                " + * Unless required by applicable law or agreed to in writing, software",
                " + * distributed under the License is distributed on an \"AS IS\" BASIS,",
                " + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                " + * See the License for the specific language governing permissions and",
                " + * limitations under the License.",
                " + */",
                " +package org.apache.cassandra.db.rows;",
                " +",
                " +import java.io.IOException;",
                " +import java.nio.ByteBuffer;",
                " +",
                " +import org.apache.cassandra.config.*;",
                " +import org.apache.cassandra.db.*;",
                " +import org.apache.cassandra.db.context.CounterContext;",
                " +import org.apache.cassandra.db.marshal.ByteType;",
                " +import org.apache.cassandra.io.util.DataInputPlus;",
                " +import org.apache.cassandra.io.util.DataOutputPlus;",
                " +import org.apache.cassandra.utils.ByteBufferUtil;",
                " +import org.apache.cassandra.utils.ObjectSizes;",
                " +import org.apache.cassandra.utils.FBUtilities;",
                " +import org.apache.cassandra.utils.memory.AbstractAllocator;",
                " +",
                " +public class BufferCell extends AbstractCell",
                " +{",
                " +    private static final long EMPTY_SIZE = ObjectSizes.measure(new BufferCell(ColumnDefinition.regularDef(\"\", \"\", \"\", ByteType.instance), 0L, 0, 0, ByteBufferUtil.EMPTY_BYTE_BUFFER, null));",
                " +",
                " +    private final long timestamp;",
                " +    private final int ttl;",
                " +    private final int localDeletionTime;",
                " +",
                " +    private final ByteBuffer value;",
                " +    private final CellPath path;",
                " +",
                " +    public BufferCell(ColumnDefinition column, long timestamp, int ttl, int localDeletionTime, ByteBuffer value, CellPath path)",
                " +    {",
                " +        super(column);",
                " +        assert column.isComplex() == (path != null);",
                " +        this.timestamp = timestamp;",
                " +        this.ttl = ttl;",
                " +        this.localDeletionTime = localDeletionTime;",
                " +        this.value = value;",
                " +        this.path = path;",
                " +    }",
                " +",
                " +    public static BufferCell live(CFMetaData metadata, ColumnDefinition column, long timestamp, ByteBuffer value)",
                " +    {",
                " +        return live(metadata, column, timestamp, value, null);",
                " +    }",
                " +",
                " +    public static BufferCell live(CFMetaData metadata, ColumnDefinition column, long timestamp, ByteBuffer value, CellPath path)",
                " +    {",
                " +        if (metadata.params.defaultTimeToLive != NO_TTL)",
                " +            return expiring(column, timestamp, metadata.params.defaultTimeToLive, FBUtilities.nowInSeconds(), value, path);",
                " +",
                " +        return new BufferCell(column, timestamp, NO_TTL, NO_DELETION_TIME, value, path);",
                " +    }",
                " +",
                " +    public static BufferCell expiring(ColumnDefinition column, long timestamp, int ttl, int nowInSec, ByteBuffer value)",
                " +    {",
                " +        return expiring(column, timestamp, ttl, nowInSec, value, null);",
                " +    }",
                " +",
                " +    public static BufferCell expiring(ColumnDefinition column, long timestamp, int ttl, int nowInSec, ByteBuffer value, CellPath path)",
                " +    {",
                " +        assert ttl != NO_TTL;",
                "-         return new BufferCell(column, timestamp, ttl, nowInSec + ttl, value, path);",
                "++        return new BufferCell(column, timestamp, ttl, ExpirationDateOverflowHandling.computeLocalExpirationTime(nowInSec, ttl), value, path);",
                " +    }",
                " +",
                " +    public static BufferCell tombstone(ColumnDefinition column, long timestamp, int nowInSec)",
                " +    {",
                " +        return tombstone(column, timestamp, nowInSec, null);",
                " +    }",
                " +",
                " +    public static BufferCell tombstone(ColumnDefinition column, long timestamp, int nowInSec, CellPath path)",
                " +    {",
                " +        return new BufferCell(column, timestamp, NO_TTL, nowInSec, ByteBufferUtil.EMPTY_BYTE_BUFFER, path);",
                " +    }",
                " +",
                " +    public boolean isCounterCell()",
                " +    {",
                " +        return !isTombstone() && column.isCounterColumn();",
                " +    }",
                " +",
                " +    public boolean isLive(int nowInSec)",
                " +    {",
                " +        return localDeletionTime == NO_DELETION_TIME || (ttl != NO_TTL && nowInSec < localDeletionTime);",
                " +    }",
                " +",
                " +    public boolean isTombstone()",
                " +    {",
                " +        return localDeletionTime != NO_DELETION_TIME && ttl == NO_TTL;",
                " +    }",
                " +",
                " +    public boolean isExpiring()",
                " +    {",
                " +        return ttl != NO_TTL;",
                " +    }",
                " +",
                " +    public long timestamp()",
                " +    {",
                " +        return timestamp;",
                " +    }",
                " +",
                " +    public int ttl()",
                " +    {",
                " +        return ttl;",
                " +    }",
                " +",
                " +    public int localDeletionTime()",
                " +    {",
                " +        return localDeletionTime;",
                " +    }",
                " +",
                " +    public ByteBuffer value()",
                " +    {",
                " +        return value;",
                " +    }",
                " +",
                " +    public CellPath path()",
                " +    {",
                " +        return path;",
                " +    }",
                " +",
                " +    public Cell withUpdatedColumn(ColumnDefinition newColumn)",
                " +    {",
                " +        return new BufferCell(newColumn, timestamp, ttl, localDeletionTime, value, path);",
                " +    }",
                " +",
                " +    public Cell withUpdatedValue(ByteBuffer newValue)",
                " +    {",
                " +        return new BufferCell(column, timestamp, ttl, localDeletionTime, newValue, path);",
                " +    }",
                " +",
                "++    public Cell withUpdatedTimestampAndLocalDeletionTime(long newTimestamp, int newLocalDeletionTime)",
                "++    {",
                "++        return new BufferCell(column, newTimestamp, ttl, newLocalDeletionTime, value, path);",
                "++    }",
                "++",
                " +    public Cell copy(AbstractAllocator allocator)",
                " +    {",
                " +        if (!value.hasRemaining())",
                " +            return this;",
                " +",
                " +        return new BufferCell(column, timestamp, ttl, localDeletionTime, allocator.clone(value), path == null ? null : path.copy(allocator));",
                " +    }",
                " +",
                " +    public Cell markCounterLocalToBeCleared()",
                " +    {",
                " +        if (!isCounterCell())",
                " +            return this;",
                " +",
                " +        ByteBuffer marked = CounterContext.instance().markLocalToBeCleared(value());",
                " +        return marked == value() ? this : new BufferCell(column, timestamp, ttl, localDeletionTime, marked, path);",
                " +    }",
                " +",
                " +    public Cell purge(DeletionPurger purger, int nowInSec)",
                " +    {",
                " +        if (!isLive(nowInSec))",
                " +        {",
                " +            if (purger.shouldPurge(timestamp, localDeletionTime))",
                " +                return null;",
                " +",
                " +            // We slightly hijack purging to convert expired but not purgeable columns to tombstones. The reason we do that is",
                " +            // that once a column has expired it is equivalent to a tombstone but actually using a tombstone is more compact since",
                " +            // we don't keep the column value. The reason we do it here is that 1) it's somewhat related to dealing with tombstones",
                " +            // so hopefully not too surprising and 2) we want to this and purging at the same places, so it's simpler/more efficient",
                " +            // to do both here.",
                " +            if (isExpiring())",
                " +            {",
                " +                // Note that as long as the expiring column and the tombstone put together live longer than GC grace seconds,",
                " +                // we'll fulfil our responsibility to repair. See discussion at",
                " +                // http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/repair-compaction-and-tombstone-rows-td7583481.html",
                " +                return BufferCell.tombstone(column, timestamp, localDeletionTime - ttl, path).purge(purger, nowInSec);",
                " +            }",
                " +        }",
                " +        return this;",
                " +    }",
                " +",
                " +    public Cell updateAllTimestamp(long newTimestamp)",
                " +    {",
                " +        return new BufferCell(column, isTombstone() ? newTimestamp - 1 : newTimestamp, ttl, localDeletionTime, value, path);",
                " +    }",
                " +",
                " +    public int dataSize()",
                " +    {",
                " +        return TypeSizes.sizeof(timestamp)",
                " +             + TypeSizes.sizeof(ttl)",
                " +             + TypeSizes.sizeof(localDeletionTime)",
                " +             + value.remaining()",
                " +             + (path == null ? 0 : path.dataSize());",
                " +    }",
                " +",
                " +    public long unsharedHeapSizeExcludingData()",
                " +    {",
                " +        return EMPTY_SIZE + ObjectSizes.sizeOnHeapExcludingData(value) + (path == null ? 0 : path.unsharedHeapSizeExcludingData());",
                " +    }",
                " +",
                " +    /**",
                " +     * The serialization format for cell is:",
                " +     *     [ flags ][ timestamp ][ deletion time ][    ttl    ][ path size ][ path ][ value size ][ value ]",
                " +     *     [   1b  ][ 8b (vint) ][   4b (vint)   ][ 4b (vint) ][ 4b (vint) ][  arb ][  4b (vint) ][  arb  ]",
                " +     *",
                " +     * where not all field are always present (in fact, only the [ flags ] are guaranteed to be present). The fields have the following",
                " +     * meaning:",
                " +     *   - [ flags ] is the cell flags. It is a byte for which each bit represents a flag whose meaning is explained below (*_MASK constants)",
                " +     *   - [ timestamp ] is the cell timestamp. Present unless the cell has the USE_TIMESTAMP_MASK.",
                " +     *   - [ deletion time]: the local deletion time for the cell. Present if either the cell is deleted (IS_DELETED_MASK)",
                " +     *       or it is expiring (IS_EXPIRING_MASK) but doesn't have the USE_ROW_TTL_MASK.",
                " +     *   - [ ttl ]: the ttl for the cell. Present if the row is expiring (IS_EXPIRING_MASK) but doesn't have the",
                " +     *       USE_ROW_TTL_MASK.",
                " +     *   - [ value size ] is the size of the [ value ] field. It's present unless either the cell has the HAS_EMPTY_VALUE_MASK, or the value",
                " +     *       for columns of this type have a fixed length.",
                " +     *   - [ path size ] is the size of the [ path ] field. Present iff this is the cell of a complex column.",
                " +     *   - [ value ]: the cell value, unless it has the HAS_EMPTY_VALUE_MASK.",
                " +     *   - [ path ]: the cell path if the column this is a cell of is complex.",
                " +     */",
                " +    static class Serializer implements Cell.Serializer",
                " +    {",
                " +        private final static int IS_DELETED_MASK             = 0x01; // Whether the cell is a tombstone or not.",
                " +        private final static int IS_EXPIRING_MASK            = 0x02; // Whether the cell is expiring.",
                " +        private final static int HAS_EMPTY_VALUE_MASK        = 0x04; // Wether the cell has an empty value. This will be the case for tombstone in particular.",
                " +        private final static int USE_ROW_TIMESTAMP_MASK      = 0x08; // Wether the cell has the same timestamp than the row this is a cell of.",
                " +        private final static int USE_ROW_TTL_MASK            = 0x10; // Wether the cell has the same ttl than the row this is a cell of.",
                " +",
                " +        public void serialize(Cell cell, ColumnDefinition column, DataOutputPlus out, LivenessInfo rowLiveness, SerializationHeader header) throws IOException",
                " +        {",
                " +            assert cell != null;",
                " +            boolean hasValue = cell.value().hasRemaining();",
                " +            boolean isDeleted = cell.isTombstone();",
                " +            boolean isExpiring = cell.isExpiring();",
                " +            boolean useRowTimestamp = !rowLiveness.isEmpty() && cell.timestamp() == rowLiveness.timestamp();",
                " +            boolean useRowTTL = isExpiring && rowLiveness.isExpiring() && cell.ttl() == rowLiveness.ttl() && cell.localDeletionTime() == rowLiveness.localExpirationTime();",
                " +            int flags = 0;",
                " +            if (!hasValue)",
                " +                flags |= HAS_EMPTY_VALUE_MASK;",
                " +",
                " +            if (isDeleted)",
                " +                flags |= IS_DELETED_MASK;",
                " +            else if (isExpiring)",
                " +                flags |= IS_EXPIRING_MASK;",
                " +",
                " +            if (useRowTimestamp)",
                " +                flags |= USE_ROW_TIMESTAMP_MASK;",
                " +            if (useRowTTL)",
                " +                flags |= USE_ROW_TTL_MASK;",
                " +",
                " +            out.writeByte((byte)flags);",
                " +",
                " +            if (!useRowTimestamp)",
                " +                header.writeTimestamp(cell.timestamp(), out);",
                " +",
                " +            if ((isDeleted || isExpiring) && !useRowTTL)",
                " +                header.writeLocalDeletionTime(cell.localDeletionTime(), out);",
                " +            if (isExpiring && !useRowTTL)",
                " +                header.writeTTL(cell.ttl(), out);",
                " +",
                " +            if (column.isComplex())",
                " +                column.cellPathSerializer().serialize(cell.path(), out);",
                " +",
                " +            if (hasValue)",
                " +                header.getType(column).writeValue(cell.value(), out);",
                " +        }",
                " +",
                " +        public Cell deserialize(DataInputPlus in, LivenessInfo rowLiveness, ColumnDefinition column, SerializationHeader header, SerializationHelper helper) throws IOException",
                " +        {",
                " +            int flags = in.readUnsignedByte();",
                " +            boolean hasValue = (flags & HAS_EMPTY_VALUE_MASK) == 0;",
                " +            boolean isDeleted = (flags & IS_DELETED_MASK) != 0;",
                " +            boolean isExpiring = (flags & IS_EXPIRING_MASK) != 0;",
                " +            boolean useRowTimestamp = (flags & USE_ROW_TIMESTAMP_MASK) != 0;",
                " +            boolean useRowTTL = (flags & USE_ROW_TTL_MASK) != 0;",
                " +",
                " +            long timestamp = useRowTimestamp ? rowLiveness.timestamp() : header.readTimestamp(in);",
                " +",
                " +            int localDeletionTime = useRowTTL",
                " +                                  ? rowLiveness.localExpirationTime()",
                " +                                  : (isDeleted || isExpiring ? header.readLocalDeletionTime(in) : NO_DELETION_TIME);",
                " +",
                " +            int ttl = useRowTTL ? rowLiveness.ttl() : (isExpiring ? header.readTTL(in) : NO_TTL);",
                " +",
                " +            CellPath path = column.isComplex()",
                " +                          ? column.cellPathSerializer().deserialize(in)",
                " +                          : null;",
                " +",
                " +            boolean isCounter = localDeletionTime == NO_DELETION_TIME && column.type.isCounter();",
                " +",
                " +            ByteBuffer value = ByteBufferUtil.EMPTY_BYTE_BUFFER;",
                " +            if (hasValue)",
                " +            {",
                " +                if (helper.canSkipValue(column) || (path != null && helper.canSkipValue(path)))",
                " +                {",
                " +                    header.getType(column).skipValue(in);",
                " +                }",
                " +                else",
                " +                {",
                " +                    value = header.getType(column).readValue(in, DatabaseDescriptor.getMaxValueSize());",
                " +                    if (isCounter)",
                " +                        value = helper.maybeClearCounterValue(value);",
                " +                }",
                " +            }",
                " +",
                " +            return new BufferCell(column, timestamp, ttl, localDeletionTime, value, path);",
                " +        }",
                " +",
                " +        public long serializedSize(Cell cell, ColumnDefinition column, LivenessInfo rowLiveness, SerializationHeader header)",
                " +        {",
                " +            long size = 1; // flags",
                " +            boolean hasValue = cell.value().hasRemaining();",
                " +            boolean isDeleted = cell.isTombstone();",
                " +            boolean isExpiring = cell.isExpiring();",
                " +            boolean useRowTimestamp = !rowLiveness.isEmpty() && cell.timestamp() == rowLiveness.timestamp();",
                " +            boolean useRowTTL = isExpiring && rowLiveness.isExpiring() && cell.ttl() == rowLiveness.ttl() && cell.localDeletionTime() == rowLiveness.localExpirationTime();",
                " +",
                " +            if (!useRowTimestamp)",
                " +                size += header.timestampSerializedSize(cell.timestamp());",
                " +",
                " +            if ((isDeleted || isExpiring) && !useRowTTL)",
                " +                size += header.localDeletionTimeSerializedSize(cell.localDeletionTime());",
                " +            if (isExpiring && !useRowTTL)",
                " +                size += header.ttlSerializedSize(cell.ttl());",
                " +",
                " +            if (column.isComplex())",
                " +                size += column.cellPathSerializer().serializedSize(cell.path());",
                " +",
                " +            if (hasValue)",
                " +                size += header.getType(column).writtenLength(cell.value());",
                " +",
                " +            return size;",
                " +        }",
                " +",
                " +        // Returns if the skipped cell was an actual cell (i.e. it had its presence flag).",
                " +        public boolean skip(DataInputPlus in, ColumnDefinition column, SerializationHeader header) throws IOException",
                " +        {",
                " +            int flags = in.readUnsignedByte();",
                " +            boolean hasValue = (flags & HAS_EMPTY_VALUE_MASK) == 0;",
                " +            boolean isDeleted = (flags & IS_DELETED_MASK) != 0;",
                " +            boolean isExpiring = (flags & IS_EXPIRING_MASK) != 0;",
                " +            boolean useRowTimestamp = (flags & USE_ROW_TIMESTAMP_MASK) != 0;",
                " +            boolean useRowTTL = (flags & USE_ROW_TTL_MASK) != 0;",
                " +",
                " +            if (!useRowTimestamp)",
                " +                header.skipTimestamp(in);",
                " +",
                " +            if (!useRowTTL && (isDeleted || isExpiring))",
                " +                header.skipLocalDeletionTime(in);",
                " +",
                " +            if (!useRowTTL && isExpiring)",
                " +                header.skipTTL(in);",
                " +",
                " +            if (column.isComplex())",
                " +                column.cellPathSerializer().skip(in);",
                " +",
                " +            if (hasValue)",
                " +                header.getType(column).skipValue(in);",
                " +",
                " +            return true;",
                " +        }",
                " +    }",
                " +}",
                "diff --cc src/java/org/apache/cassandra/db/rows/Cell.java",
                "index d10cc74286,0000000000..c69e11f65c",
                "mode 100644,000000..100644",
                "--- a/src/java/org/apache/cassandra/db/rows/Cell.java",
                "+++ b/src/java/org/apache/cassandra/db/rows/Cell.java",
                "@@@ -1,157 -1,0 +1,166 @@@",
                " +/*",
                " + * Licensed to the Apache Software Foundation (ASF) under one",
                " + * or more contributor license agreements.  See the NOTICE file",
                " + * distributed with this work for additional information",
                " + * regarding copyright ownership.  The ASF licenses this file",
                " + * to you under the Apache License, Version 2.0 (the",
                " + * \"License\"); you may not use this file except in compliance",
                " + * with the License.  You may obtain a copy of the License at",
                " + *",
                " + *     http://www.apache.org/licenses/LICENSE-2.0",
                " + *",
                " + * Unless required by applicable law or agreed to in writing, software",
                " + * distributed under the License is distributed on an \"AS IS\" BASIS,",
                " + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",
                " + * See the License for the specific language governing permissions and",
                " + * limitations under the License.",
                " + */",
                " +package org.apache.cassandra.db.rows;",
                " +",
                " +import java.io.IOException;",
                " +import java.nio.ByteBuffer;",
                " +import java.util.Comparator;",
                " +",
                "++import com.google.common.annotations.VisibleForTesting;",
                "++import org.slf4j.Logger;",
                "++import org.slf4j.LoggerFactory;",
                "++",
                "++import org.apache.cassandra.config.CFMetaData;",
                " +import org.apache.cassandra.config.ColumnDefinition;",
                "++import org.apache.cassandra.cql3.Attributes;",
                " +import org.apache.cassandra.db.*;",
                " +import org.apache.cassandra.io.util.DataOutputPlus;",
                " +import org.apache.cassandra.io.util.DataInputPlus;",
                " +import org.apache.cassandra.utils.memory.AbstractAllocator;",
                " +",
                " +/**",
                " + * A cell is our atomic unit for a single value of a single column.",
                " + * <p>",
                " + * A cell always holds at least a timestamp that gives us how the cell reconcile. We then",
                " + * have 3 main types of cells:",
                " + *   1) live regular cells: those will also have a value and, if for a complex column, a path.",
                " + *   2) expiring cells: on top of regular cells, those have a ttl and a local deletion time (when they are expired).",
                " + *   3) tombstone cells: those won't have value, but they have a local deletion time (when the tombstone was created).",
                " + */",
                " +public abstract class Cell extends ColumnData",
                " +{",
                " +    public static final int NO_TTL = 0;",
                " +    public static final int NO_DELETION_TIME = Integer.MAX_VALUE;",
                "++    public static final int MAX_DELETION_TIME = Integer.MAX_VALUE - 1;",
                " +",
                " +    public final static Comparator<Cell> comparator = (c1, c2) ->",
                " +    {",
                " +        int cmp = c1.column().compareTo(c2.column());",
                " +        if (cmp != 0)",
                " +            return cmp;",
                " +",
                " +        Comparator<CellPath> pathComparator = c1.column().cellPathComparator();",
                " +        return pathComparator == null ? 0 : pathComparator.compare(c1.path(), c2.path());",
                " +    };",
                " +",
                " +    public static final Serializer serializer = new BufferCell.Serializer();",
                " +",
                " +    protected Cell(ColumnDefinition column)",
                " +    {",
                " +        super(column);",
                " +    }",
                " +",
                " +    /**",
                " +     * Whether the cell is a counter cell or not.",
                " +     *",
                " +     * @return whether the cell is a counter cell or not.",
                " +     */",
                " +    public abstract boolean isCounterCell();",
                " +",
                " +    /**",
                " +     * The cell value.",
                " +     *",
                " +     * @return the cell value.",
                " +     */",
                " +    public abstract ByteBuffer value();",
                " +",
                " +    /**",
                " +     * The cell timestamp.",
                " +     * <p>",
                " +     * @return the cell timestamp.",
                " +     */",
                " +    public abstract long timestamp();",
                " +",
                " +    /**",
                " +     * The cell ttl.",
                " +     *",
                " +     * @return the cell ttl, or {@code NO_TTL} if the cell isn't an expiring one.",
                " +     */",
                " +    public abstract int ttl();",
                " +",
                " +    /**",
                " +     * The cell local deletion time.",
                " +     *",
                " +     * @return the cell local deletion time, or {@code NO_DELETION_TIME} if the cell is neither",
                " +     * a tombstone nor an expiring one.",
                " +     */",
                " +    public abstract int localDeletionTime();",
                " +",
                " +    /**",
                " +     * Whether the cell is a tombstone or not.",
                " +     *",
                " +     * @return whether the cell is a tombstone or not.",
                " +     */",
                " +    public abstract boolean isTombstone();",
                " +",
                " +    /**",
                " +     * Whether the cell is an expiring one or not.",
                " +     * <p>",
                " +     * Note that this only correspond to whether the cell liveness info",
                " +     * have a TTL or not, but doesn't tells whether the cell is already expired",
                " +     * or not. You should use {@link #isLive} for that latter information.",
                " +     *",
                " +     * @return whether the cell is an expiring one or not.",
                " +     */",
                " +    public abstract boolean isExpiring();",
                " +",
                " +    /**",
                " +     * Whether the cell is live or not given the current time.",
                " +     *",
                " +     * @param nowInSec the current time in seconds. This is used to",
                " +     * decide if an expiring cell is expired or live.",
                " +     * @return whether the cell is live or not at {@code nowInSec}.",
                " +     */",
                " +    public abstract boolean isLive(int nowInSec);",
                " +",
                " +    /**",
                " +     * For cells belonging to complex types (non-frozen collection and UDT), the",
                " +     * path to the cell.",
                " +     *",
                " +     * @return the cell path for cells of complex column, and {@code null} for other cells.",
                " +     */",
                " +    public abstract CellPath path();",
                " +",
                " +    public abstract Cell withUpdatedColumn(ColumnDefinition newColumn);",
                " +",
                " +    public abstract Cell withUpdatedValue(ByteBuffer newValue);",
                " +",
                "++    public abstract Cell withUpdatedTimestampAndLocalDeletionTime(long newTimestamp, int newLocalDeletionTime);",
                "++",
                " +    public abstract Cell copy(AbstractAllocator allocator);",
                " +",
                " +    @Override",
                " +    // Overrides super type to provide a more precise return type.",
                " +    public abstract Cell markCounterLocalToBeCleared();",
                " +",
                " +    @Override",
                " +    // Overrides super type to provide a more precise return type.",
                " +    public abstract Cell purge(DeletionPurger purger, int nowInSec);",
                " +",
                " +    public interface Serializer",
                " +    {",
                " +        public void serialize(Cell cell, ColumnDefinition column, DataOutputPlus out, LivenessInfo rowLiveness, SerializationHeader header) throws IOException;",
                " +",
                " +        public Cell deserialize(DataInputPlus in, LivenessInfo rowLiveness, ColumnDefinition column, SerializationHeader header, SerializationHelper helper) throws IOException;",
                " +",
                " +        public long serializedSize(Cell cell, ColumnDefinition column, LivenessInfo rowLiveness, SerializationHeader header);",
                " +",
                " +        // Returns if the skipped cell was an actual cell (i.e. it had its presence flag).",
                " +        public boolean skip(DataInputPlus in, ColumnDefinition column, SerializationHeader header) throws IOException;",
                " +    }",
                " +}",
                "diff --cc src/java/org/apache/cassandra/service/StorageService.java",
                "index e5a50dd0a6,2c9ac4d272..cf8e257936",
                "--- a/src/java/org/apache/cassandra/service/StorageService.java",
                "+++ b/src/java/org/apache/cassandra/service/StorageService.java",
                "@@@ -2717,3 -2611,9 +2717,8 @@@ public class StorageService extends Not",
                " -    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException",
                " +    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, int jobs, String keyspaceName, String... tables) throws IOException, ExecutionException, InterruptedException",
                "+     {",
                " -        return scrub(disableSnapshot, skipCorrupted, checkData, false, jobs, keyspaceName, columnFamilies);",
                "++        return scrub(disableSnapshot, skipCorrupted, checkData, false, jobs, keyspaceName, tables);",
                "+     }",
                "+ ",
                " -    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows,",
                " -                     int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException",
                "++    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTL, int jobs, String keyspaceName, String... tables) throws IOException, ExecutionException, InterruptedException",
                "      {",
                "@@@ -2722,3 -2622,3 +2727,3 @@@",
                "          {",
                "-             CompactionManager.AllSSTableOpStatus oneStatus = cfStore.scrub(disableSnapshot, skipCorrupted, checkData, jobs);",
                " -            CompactionManager.AllSSTableOpStatus oneStatus = cfStore.scrub(disableSnapshot, skipCorrupted, checkData, reinsertOverflowedTTLRows, jobs);",
                "++            CompactionManager.AllSSTableOpStatus oneStatus = cfStore.scrub(disableSnapshot, skipCorrupted, reinsertOverflowedTTL, checkData, jobs);",
                "              if (oneStatus != CompactionManager.AllSSTableOpStatus.SUCCESSFUL)",
                "diff --cc src/java/org/apache/cassandra/service/StorageServiceMBean.java",
                "index 7344ca8c77,f336bcc495..10d47f716f",
                "--- a/src/java/org/apache/cassandra/service/StorageServiceMBean.java",
                "+++ b/src/java/org/apache/cassandra/service/StorageServiceMBean.java",
                "@@@ -266,5 -266,8 +266,8 @@@ public interface StorageServiceMBean ex",
                "      @Deprecated",
                " -    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException;",
                " +    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, String keyspaceName, String... tableNames) throws IOException, ExecutionException, InterruptedException;",
                "+     @Deprecated",
                "      public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException;",
                " -public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException;",
                "++    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTL, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException;",
                "+ ",
                "      /**",
                "diff --cc src/java/org/apache/cassandra/thrift/ThriftValidation.java",
                "index 6ad791d888,8bdf9dcb1e..2ab0330078",
                "--- a/src/java/org/apache/cassandra/thrift/ThriftValidation.java",
                "+++ b/src/java/org/apache/cassandra/thrift/ThriftValidation.java",
                "@@@ -376,4 -359,5 +376,5 @@@ public class ThriftValidatio",
                " -            if (column.ttl > ExpiringCell.MAX_TTL)",
                " -                throw new org.apache.cassandra.exceptions.InvalidRequestException(String.format(\"ttl is too large. requested (%d) maximum (%d)\", column.ttl, ExpiringCell.MAX_TTL));",
                " -            Attributes.maybeApplyExpirationDateOverflowPolicy(metadata, column.ttl, false);",
                " +            if (column.ttl > Attributes.MAX_TTL)",
                " +                throw new org.apache.cassandra.exceptions.InvalidRequestException(String.format(\"ttl is too large. requested (%d) maximum (%d)\", column.ttl, Attributes.MAX_TTL));",
                "++            ExpirationDateOverflowHandling.maybeApplyExpirationDateOverflowPolicy(metadata, column.ttl, false);",
                "          }",
                "@@@ -381,2 -365,3 +382,3 @@@",
                "          {",
                " -            Attributes.maybeApplyExpirationDateOverflowPolicy(metadata, metadata.getDefaultTimeToLive(), true);",
                "++            ExpirationDateOverflowHandling.maybeApplyExpirationDateOverflowPolicy(metadata, metadata.params.defaultTimeToLive, true);",
                "              // if it's not set, then it should be zero -- here we are just checking to make sure Thrift doesn't change that contract with us.",
                "diff --cc src/java/org/apache/cassandra/tools/NodeProbe.java",
                "index 172b505614,17bef02b12..0d3c0789d3",
                "--- a/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "+++ b/src/java/org/apache/cassandra/tools/NodeProbe.java",
                "@@@ -253,5 -238,5 +253,5 @@@ public class NodeProbe implements AutoC",
                "-     public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, int jobs, String keyspaceName, String... tables) throws IOException, ExecutionException, InterruptedException",
                " -    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException",
                "++    public int scrub(boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTL, int jobs, String keyspaceName, String... tables) throws IOException, ExecutionException, InterruptedException",
                "      {",
                "-         return ssProxy.scrub(disableSnapshot, skipCorrupted, checkData, jobs, keyspaceName, tables);",
                " -        return ssProxy.scrub(disableSnapshot, skipCorrupted, checkData, reinsertOverflowedTTLRows, jobs, keyspaceName, columnFamilies);",
                "++        return ssProxy.scrub(disableSnapshot, skipCorrupted, checkData, reinsertOverflowedTTL, jobs, keyspaceName, tables);",
                "      }",
                "@@@ -290,6 -269,6 +290,6 @@@",
                "-     public void scrub(PrintStream out, boolean disableSnapshot, boolean skipCorrupted, boolean checkData, int jobs, String keyspaceName, String... tables) throws IOException, ExecutionException, InterruptedException",
                " -    public void scrub(PrintStream out, boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTLRows, int jobs, String keyspaceName, String... columnFamilies) throws IOException, ExecutionException, InterruptedException",
                "++    public void scrub(PrintStream out, boolean disableSnapshot, boolean skipCorrupted, boolean checkData, boolean reinsertOverflowedTTL, int jobs, String keyspaceName, String... tables) throws IOException, ExecutionException, InterruptedException",
                "      {",
                "          checkJobs(out, jobs);",
                "-         switch (scrub(disableSnapshot, skipCorrupted, checkData, jobs, keyspaceName, tables))",
                " -        if (scrub(disableSnapshot, skipCorrupted, checkData, reinsertOverflowedTTLRows, jobs, keyspaceName, columnFamilies) != 0)",
                "++        switch (ssProxy.scrub(disableSnapshot, skipCorrupted, checkData, reinsertOverflowedTTL, jobs, keyspaceName, tables))",
                "          {",
                "diff --cc src/java/org/apache/cassandra/tools/SSTableMetadataViewer.java",
                "index 6076e324a8,831901439c..19af957228",
                "--- a/src/java/org/apache/cassandra/tools/SSTableMetadataViewer.java",
                "+++ b/src/java/org/apache/cassandra/tools/SSTableMetadataViewer.java",
                "@@@ -92,2 -67,2 +92,3 @@@ public class SSTableMetadataViewe",
                "                      out.printf(\"Maximum timestamp: %s%n\", stats.maxTimestamp);",
                "++                    out.printf(\"SSTable min local deletion time: %s%n\", stats.minLocalDeletionTime);",
                "                      out.printf(\"SSTable max local deletion time: %s%n\", stats.maxLocalDeletionTime);",
                "diff --cc src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "index 424943031c,f5e84c5f0e..4778d7263c",
                "--- a/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "+++ b/src/java/org/apache/cassandra/tools/StandaloneScrubber.java",
                "@@@ -124,3 -131,3 +131,3 @@@ public class StandaloneScrubbe",
                "                          txn.obsoleteOriginals(); // make sure originals are deleted and avoid NPE if index is missing, CASSANDRA-9591",
                "-                         try (Scrubber scrubber = new Scrubber(cfs, txn, options.skipCorrupted, handler, !options.noValidate))",
                " -                        try (Scrubber scrubber = new Scrubber(cfs, txn, options.skipCorrupted, handler, !options.noValidate, options.reinsertOverflowedTTL))",
                "++                        try (Scrubber scrubber = new Scrubber(cfs, txn, options.skipCorrupted, handler, !options.noValidate, options.reinserOverflowedTTL))",
                "                          {",
                "@@@ -201,2 -209,3 +208,3 @@@",
                "          public boolean noValidate;",
                " -        public boolean reinsertOverflowedTTL;",
                "++        public boolean reinserOverflowedTTL;",
                "@@@ -241,2 -250,3 +249,3 @@@",
                "                  opts.noValidate = cmd.hasOption(NO_VALIDATE_OPTION);",
                " -                opts.reinsertOverflowedTTL = cmd.hasOption(REINSERT_OVERFLOWED_TTL_OPTION);",
                "++                opts.reinserOverflowedTTL = cmd.hasOption(REINSERT_OVERFLOWED_TTL_OPTION);",
                "diff --cc src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "index 2345a8533f,50224a0137..ead2fd4c37",
                "--- a/src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "+++ b/src/java/org/apache/cassandra/tools/nodetool/Scrub.java",
                "@@@ -50,7 -51,12 +51,12 @@@ public class Scrub extends NodeToolCm",
                " -    @Option(title = \"jobs\",",
                " -            name = {\"-j\", \"--jobs\"},",
                " -            description = \"Number of sstables to scrub simultanously, set to 0 to use all available compaction threads\")",
                " -    private int jobs = 2;",
                " -",
                "+     @Option(title = \"reinsert_overflowed_ttl\",",
                "+     name = {\"r\", \"--reinsert-overflowed-ttl\"},",
                "+     description = StandaloneScrubber.REINSERT_OVERFLOWED_TTL_OPTION_DESCRIPTION)",
                "+     private boolean reinsertOverflowedTTL = false;",
                "+ ",
                " +    @Option(title = \"jobs\",",
                " +            name = {\"-j\", \"--jobs\"},",
                " +            description = \"Number of sstables to scrub simultanously, set to 0 to use all available compaction threads\")",
                " +    private int jobs = 2;",
                " +",
                "      @Override",
                "@@@ -65,5 -71,4 +71,5 @@@",
                "              {",
                "-                 probe.scrub(System.out, disableSnapshot, skipCorrupted, !noValidation, jobs, keyspace, tableNames);",
                " -                probe.scrub(System.out, disableSnapshot, skipCorrupted, !noValidation, reinsertOverflowedTTL, jobs, keyspace, cfnames);",
                " -            } catch (IllegalArgumentException e)",
                "++                probe.scrub(System.out, disableSnapshot, skipCorrupted, !noValidation, reinsertOverflowedTTL, jobs, keyspace, tableNames);",
                " +            }",
                " +            catch (IllegalArgumentException e)",
                "              {",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-CompressionInfo.db",
                "index 0000000000,0000000000..d759cec5c2",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-Data.db",
                "index 0000000000,0000000000..e7a72da607",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-Digest.crc32",
                "index 0000000000,0000000000..a3c633a846",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table1/mc-1-big-Digest.crc32",
                "@@@ -1,0 -1,0 +1,1 @@@",
                "++203700622",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-Filter.db",
                "index 0000000000,0000000000..a397f351d9",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-Index.db",
                "index 0000000000,0000000000..d7427249d0",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-Statistics.db",
                "index 0000000000,0000000000..faf367b5fb",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-Summary.db",
                "index 0000000000,0000000000..66cf70f7c7",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table1/mc-1-big-TOC.txt",
                "index 0000000000,0000000000..45113dc798",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table1/mc-1-big-TOC.txt",
                "@@@ -1,0 -1,0 +1,8 @@@",
                "++CompressionInfo.db",
                "++Data.db",
                "++Summary.db",
                "++Filter.db",
                "++Statistics.db",
                "++TOC.txt",
                "++Digest.crc32",
                "++Index.db",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-CompressionInfo.db",
                "index 0000000000,0000000000..1759c09c63",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-Data.db",
                "index 0000000000,0000000000..c1de5721bc",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-Digest.crc32",
                "index 0000000000,0000000000..0403b5bdcd",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table2/mc-1-big-Digest.crc32",
                "@@@ -1,0 -1,0 +1,1 @@@",
                "++82785930",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-Filter.db",
                "index 0000000000,0000000000..a397f351d9",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-Index.db",
                "index 0000000000,0000000000..a0477eb8ee",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-Statistics.db",
                "index 0000000000,0000000000..e9d65771bf",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-Summary.db",
                "index 0000000000,0000000000..66cf70f7c7",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table2/mc-1-big-TOC.txt",
                "index 0000000000,0000000000..45113dc798",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table2/mc-1-big-TOC.txt",
                "@@@ -1,0 -1,0 +1,8 @@@",
                "++CompressionInfo.db",
                "++Data.db",
                "++Summary.db",
                "++Filter.db",
                "++Statistics.db",
                "++TOC.txt",
                "++Digest.crc32",
                "++Index.db",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-CompressionInfo.db",
                "index 0000000000,0000000000..b4de068379",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-Data.db",
                "index 0000000000,0000000000..e96f77253e",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-Digest.crc32",
                "index 0000000000,0000000000..459804bdf3",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table3/mc-1-big-Digest.crc32",
                "@@@ -1,0 -1,0 +1,1 @@@",
                "++3064924389",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-Filter.db",
                "index 0000000000,0000000000..a397f351d9",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-Index.db",
                "index 0000000000,0000000000..807a27b8bd",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-Statistics.db",
                "index 0000000000,0000000000..1ee01e674a",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-Summary.db",
                "index 0000000000,0000000000..66cf70f7c7",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table3/mc-1-big-TOC.txt",
                "index 0000000000,0000000000..f4455377aa",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table3/mc-1-big-TOC.txt",
                "@@@ -1,0 -1,0 +1,8 @@@",
                "++Summary.db",
                "++TOC.txt",
                "++Filter.db",
                "++Index.db",
                "++Digest.crc32",
                "++CompressionInfo.db",
                "++Data.db",
                "++Statistics.db",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-CompressionInfo.db",
                "index 0000000000,0000000000..5d22c044a4",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-Data.db",
                "index 0000000000,0000000000..a22a7a3044",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-Digest.crc32",
                "index 0000000000,0000000000..db7a6c765c",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table4/mc-1-big-Digest.crc32",
                "@@@ -1,0 -1,0 +1,1 @@@",
                "++1803989939",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-Filter.db",
                "index 0000000000,0000000000..a397f351d9",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-Index.db",
                "index 0000000000,0000000000..6397b5e0a9",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-Statistics.db",
                "index 0000000000,0000000000..4ee9294b5a",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-Summary.db",
                "index 0000000000,0000000000..66cf70f7c7",
                "new file mode 100644",
                "Binary files differ",
                "diff --cc test/data/negative-local-expiration-test/table4/mc-1-big-TOC.txt",
                "index 0000000000,0000000000..f4455377aa",
                "new file mode 100644",
                "--- /dev/null",
                "+++ b/test/data/negative-local-expiration-test/table4/mc-1-big-TOC.txt",
                "@@@ -1,0 -1,0 +1,8 @@@",
                "++Summary.db",
                "++TOC.txt",
                "++Filter.db",
                "++Index.db",
                "++Digest.crc32",
                "++CompressionInfo.db",
                "++Data.db",
                "++Statistics.db",
                "diff --cc test/unit/org/apache/cassandra/cql3/validation/operations/TTLTest.java",
                "index 9f375d43bf,b1eaac10b0..fc709748f5",
                "--- a/test/unit/org/apache/cassandra/cql3/validation/operations/TTLTest.java",
                "+++ b/test/unit/org/apache/cassandra/cql3/validation/operations/TTLTest.java",
                "@@@ -7,2 -13,7 +13,7 @@@ import org.apache.cassandra.cql3.Attrib",
                "  import org.apache.cassandra.cql3.CQLTester;",
                "+ import org.apache.cassandra.cql3.UntypedResultSet;",
                " -import org.apache.cassandra.db.BufferExpiringCell;",
                "+ import org.apache.cassandra.db.ColumnFamilyStore;",
                " -import org.apache.cassandra.db.ExpiringCell;",
                "++import org.apache.cassandra.db.ExpirationDateOverflowHandling;",
                "+ import org.apache.cassandra.db.Keyspace;",
                "++import org.apache.cassandra.db.rows.AbstractCell;",
                "  import org.apache.cassandra.exceptions.InvalidRequestException;",
                "@@@ -12,2 -25,10 +25,10 @@@ public class TTLTest extends CQLTeste",
                "  {",
                "+     public static String NEGATIVE_LOCAL_EXPIRATION_TEST_DIR = \"test/data/negative-local-expiration-test/%s\";",
                "+ ",
                " -    public static int MAX_TTL = ExpiringCell.MAX_TTL;",
                "++    public static int MAX_TTL = Attributes.MAX_TTL;",
                "+ ",
                "+     public static final String SIMPLE_NOCLUSTERING = \"table1\";",
                "+     public static final String SIMPLE_CLUSTERING = \"table2\";",
                "+     public static final String COMPLEX_NOCLUSTERING = \"table3\";",
                "+     public static final String COMPLEX_CLUSTERING = \"table4\";",
                "@@@ -17,6 -38,4 +38,4 @@@",
                "          createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int)\");",
                "-         // insert",
                "-         execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL ?\", Attributes.MAX_TTL); // max ttl",
                "-         int ttl = execute(\"SELECT ttl(i) FROM %s\").one().getInt(\"ttl(i)\");",
                "-         assertTrue(ttl > Attributes.MAX_TTL - 10);",
                "+         // insert with low TTL should not be denied",
                " -        execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL ?\", 10); // max ttl",
                "++        execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL ?\", 10);",
                "@@@ -43,6 -62,4 +62,4 @@@",
                "-         // update",
                "-         execute(\"UPDATE %s USING TTL ? SET i = 1 WHERE k = 2\", Attributes.MAX_TTL); // max ttl",
                "-         ttl = execute(\"SELECT ttl(i) FROM %s\").one().getInt(\"ttl(i)\");",
                "-         assertTrue(ttl > Attributes.MAX_TTL - 10);",
                "+         // insert with low TTL should not be denied",
                " -        execute(\"UPDATE %s USING TTL ? SET i = 1 WHERE k = 2\", 5); // max ttl",
                "++        execute(\"UPDATE %s USING TTL ? SET i = 1 WHERE k = 2\", 5);",
                "@@@ -97,8 -115,291 +114,282 @@@",
                "-         createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\" + Attributes.MAX_TTL);",
                "+         // table with default low TTL should not be denied",
                "+         createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\" + 5);",
                "+         execute(\"INSERT INTO %s (k, i) VALUES (1, 1)\");",
                "+     }",
                "+ ",
                "+     @Test",
                " -    public void testRejectExpirationDateOverflowPolicy() throws Throwable",
                "++    public void testCapWarnExpirationOverflowPolicy() throws Throwable",
                "++    {",
                "++        // We don't test that the actual warn is logged here, only on dtest",
                "++        testCapExpirationDateOverflowPolicy(ExpirationDateOverflowHandling.ExpirationDateOverflowPolicy.CAP);",
                "++    }",
                "++",
                "++    @Test",
                "++    public void testCapNoWarnExpirationOverflowPolicy() throws Throwable",
                "++    {",
                "++        testCapExpirationDateOverflowPolicy(ExpirationDateOverflowHandling.ExpirationDateOverflowPolicy.CAP_NOWARN);",
                "++    }",
                "++",
                "++    @Test",
                "++    public void testCapNoWarnExpirationOverflowPolicyDefaultTTL() throws Throwable",
                "++    {",
                "++        ExpirationDateOverflowHandling.policy = ExpirationDateOverflowHandling.policy.CAP_NOWARN;",
                "++        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\" + MAX_TTL);",
                " +        execute(\"INSERT INTO %s (k, i) VALUES (1, 1)\");",
                "-         int ttl = execute(\"SELECT ttl(i) FROM %s\").one().getInt(\"ttl(i)\");",
                "-         assertTrue(ttl > 10000 - 10); // within 10 second",
                "++        checkTTLIsCapped(\"i\");",
                "++        ExpirationDateOverflowHandling.policy = ExpirationDateOverflowHandling.policy.REJECT;",
                "++    }",
                "++",
                "++    @Test",
                "++    public void testRejectExpirationOverflowPolicy() throws Throwable",
                "+     {",
                " -        Attributes.policy = Attributes.ExpirationDateOverflowPolicy.REJECT;",
                "++        //ExpirationDateOverflowHandling.expirationDateOverflowPolicy = ExpirationDateOverflowHandling.expirationDateOverflowPolicy.REJECT;",
                "+         createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int)\");",
                "+         try",
                "+         {",
                "+             execute(\"INSERT INTO %s (k, i) VALUES (1, 1) USING TTL \" + MAX_TTL);",
                "+         }",
                "+         catch (InvalidRequestException e)",
                "+         {",
                "+             assertTrue(e.getMessage().contains(\"exceeds maximum supported expiration date\"));",
                "+         }",
                "+         try",
                "+         {",
                "+             createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\" + MAX_TTL);",
                "+             execute(\"INSERT INTO %s (k, i) VALUES (1, 1)\");",
                "+         }",
                "+         catch (InvalidRequestException e)",
                "+         {",
                "+             assertTrue(e.getMessage().contains(\"exceeds maximum supported expiration date\"));",
                "+         }",
                "+     }",
                "+ ",
                "+     @Test",
                " -    public void testCapExpirationDatePolicyDefaultTTL() throws Throwable",
                "++    public void testRecoverOverflowedExpirationWithScrub() throws Throwable",
                "+     {",
                " -        Attributes.policy = Attributes.ExpirationDateOverflowPolicy.CAP;",
                " -        createTable(\"CREATE TABLE %s (k int PRIMARY KEY, i int) WITH default_time_to_live=\" + MAX_TTL);",
                " -        execute(\"INSERT INTO %s (k, i) VALUES (1, 1)\");",
                " -        checkTTLIsCapped(\"i\");",
                " -        Attributes.policy = Attributes.ExpirationDateOverflowPolicy.REJECT;",
                "++        baseTestRecoverOverflowedExpiration(false, false);",
                "++        baseTestRecoverOverflowedExpiration(true, false);",
                "++        baseTestRecoverOverflowedExpiration(true, true);",
                "+     }",
                "+ ",
                " -    @Test",
                " -    public void testCapExpirationDatePolicyPerRequest() throws Throwable",
                "++    public void testCapExpirationDateOverflowPolicy(ExpirationDateOverflowHandling.ExpirationDateOverflowPolicy policy) throws Throwable",
                "+     {",
                " -        // Test cap policy",
                " -        Attributes.policy = Attributes.ExpirationDateOverflowPolicy.CAP;",
                "++        ExpirationDateOverflowHandling.policy = policy;",
                "+ ",
                "+         // simple column, clustering, flush",
                " -        baseCapExpirationDateOverflowTest(true, true, true);",
                "++        testCapExpirationDateOverflowPolicy(true, true, true);",
                "+         // simple column, clustering, noflush",
                " -        baseCapExpirationDateOverflowTest(true, true, false);",
                "++        testCapExpirationDateOverflowPolicy(true, true, false);",
                "+         // simple column, noclustering, flush",
                " -        baseCapExpirationDateOverflowTest(true, false, true);",
                "++        testCapExpirationDateOverflowPolicy(true, false, true);",
                "+         // simple column, noclustering, noflush",
                " -        baseCapExpirationDateOverflowTest(true, false, false);",
                "++        testCapExpirationDateOverflowPolicy(true, false, false);",
                "+         // complex column, clustering, flush",
                " -        baseCapExpirationDateOverflowTest(false, true, true);",
                "++        testCapExpirationDateOverflowPolicy(false, true, true);",
                "+         // complex column, clustering, noflush",
                " -        baseCapExpirationDateOverflowTest(false, true, false);",
                "++        testCapExpirationDateOverflowPolicy(false, true, false);",
                "+         // complex column, noclustering, flush",
                " -        baseCapExpirationDateOverflowTest(false, false, true);",
                "++        testCapExpirationDateOverflowPolicy(false, false, true);",
                "+         // complex column, noclustering, noflush",
                " -        baseCapExpirationDateOverflowTest(false, false, false);",
                " -        // complex column, noclustering, flush",
                " -        baseCapExpirationDateOverflowTest(false, false, false);",
                "++        testCapExpirationDateOverflowPolicy(false, false, false);",
                "+ ",
                "+         // Return to previous policy",
                " -        Attributes.policy = Attributes.ExpirationDateOverflowPolicy.REJECT;",
                " -    }",
                " -",
                " -    @Test",
                " -    public void testRecoverOverflowedExpirationWithScrub() throws Throwable",
                " -    {",
                " -        baseTestRecoverOverflowedExpiration(false, false);",
                " -        baseTestRecoverOverflowedExpiration(true, false);",
                " -        baseTestRecoverOverflowedExpiration(true, true);",
                "++        ExpirationDateOverflowHandling.policy = ExpirationDateOverflowHandling.ExpirationDateOverflowPolicy.REJECT;",
                "+     }",
                "+ ",
                " -    public void baseCapExpirationDateOverflowTest(boolean simple, boolean clustering, boolean flush) throws Throwable",
                "++    public void testCapExpirationDateOverflowPolicy(boolean simple, boolean clustering, boolean flush) throws Throwable",
                "+     {",
                "+         // Create Table",
                " -        if (simple)",
                " -        {",
                " -            if (clustering)",
                " -                createTable(\"create table %s (k int, a int, b int, primary key(k, a))\");",
                " -            else",
                " -                createTable(\"create table %s (k int primary key, a int, b int)\");",
                " -        }",
                " -        else",
                " -        {",
                " -            if (clustering)",
                " -                createTable(\"create table %s (k int, a int, b set<text>, primary key(k, a))\");",
                " -            else",
                " -                createTable(\"create table %s (k int primary key, a int, b set<text>)\");",
                " -        }",
                "++        createTable(simple, clustering);",
                "+ ",
                "+         // Insert data with INSERT and UPDATE",
                "+         if (simple)",
                "+         {",
                " -            execute(\"INSERT INTO %s (k, a, b) VALUES (?, ?, ?) USING TTL \" + MAX_TTL, 2, 2, 2);",
                "++            execute(\"INSERT INTO %s (k, a) VALUES (?, ?) USING TTL \" + MAX_TTL, 2, 2);",
                "+             if (clustering)",
                "+                 execute(\"UPDATE %s USING TTL \" + MAX_TTL + \" SET b = 1 WHERE k = 1 AND a = 1;\");",
                "+             else",
                "+                 execute(\"UPDATE %s USING TTL \" + MAX_TTL + \" SET a = 1, b = 1 WHERE k = 1;\");",
                "+         }",
                "+         else",
                "+         {",
                "+             execute(\"INSERT INTO %s (k, a, b) VALUES (?, ?, ?) USING TTL \" + MAX_TTL, 2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\"));",
                "+             if (clustering)",
                "+                 execute(\"UPDATE  %s USING TTL \" + MAX_TTL + \" SET b = ? WHERE k = 1 AND a = 1;\", set(\"v11\", \"v12\", \"v13\", \"v14\"));",
                "+             else",
                "+                 execute(\"UPDATE  %s USING TTL \" + MAX_TTL + \" SET a = 1, b = ? WHERE k = 1;\", set(\"v11\", \"v12\", \"v13\", \"v14\"));",
                "+         }",
                "+ ",
                "+         // Maybe Flush",
                "+         Keyspace ks = Keyspace.open(keyspace());",
                "+         if (flush)",
                "+             FBUtilities.waitOnFutures(ks.flush());",
                "+ ",
                "+         // Verify data",
                "+         verifyData(simple);",
                "+ ",
                "+         // Maybe major compact",
                "+         if (flush)",
                "+         {",
                "+             // Major compact and check data is still present",
                "+             ks.getColumnFamilyStore(currentTable()).forceMajorCompaction();",
                "+ ",
                "+             // Verify data again",
                "+             verifyData(simple);",
                "+         }",
                "+     }",
                "+ ",
                "+     public void baseTestRecoverOverflowedExpiration(boolean runScrub, boolean reinsertOverflowedTTL) throws Throwable",
                "+     {",
                "+         // simple column, clustering",
                "+         testRecoverOverflowedExpirationWithScrub(true, true, runScrub, reinsertOverflowedTTL);",
                "+         // simple column, noclustering",
                "+         testRecoverOverflowedExpirationWithScrub(true, false, runScrub, reinsertOverflowedTTL);",
                "+         // complex column, clustering",
                "+         testRecoverOverflowedExpirationWithScrub(false, true, runScrub, reinsertOverflowedTTL);",
                "+         // complex column, noclustering",
                "+         testRecoverOverflowedExpirationWithScrub(false, false, runScrub, reinsertOverflowedTTL);",
                "+     }",
                "+ ",
                "++    private void createTable(boolean simple, boolean clustering)",
                "++    {",
                "++        if (simple)",
                "++        {",
                "++            if (clustering)",
                "++                createTable(\"create table %s (k int, a int, b int, primary key(k, a))\");",
                "++            else",
                "++                createTable(\"create table %s (k int primary key, a int, b int)\");",
                "++        }",
                "++        else",
                "++        {",
                "++            if (clustering)",
                "++                createTable(\"create table %s (k int, a int, b set<text>, primary key(k, a))\");",
                "++            else",
                "++                createTable(\"create table %s (k int primary key, a int, b set<text>)\");",
                "++        }",
                "++    }",
                "++",
                "+     private void verifyData(boolean simple) throws Throwable",
                "+     {",
                "+         if (simple)",
                "+         {",
                " -            assertRows(execute(\"SELECT * from %s\"), row(1, 1, 1), row(2, 2, 2));",
                "++            assertRows(execute(\"SELECT * from %s\"), row(1, 1, 1), row(2, 2, null));",
                "+         }",
                "+         else",
                "+         {",
                "+             assertRows(execute(\"SELECT * from %s\"), row(1, 1, set(\"v11\", \"v12\", \"v13\", \"v14\")), row(2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\")));",
                "+         }",
                "+         // Cannot retrieve TTL from collections",
                "+         if (simple)",
                "+             checkTTLIsCapped(\"b\");",
                "      }",
                "+     /**",
                " -     * Verify that the computed TTL is approximately equal to the maximum allowed ttl given the",
                " -     * {@link ExpiringCell#getLocalDeletionTime()} field limitation (CASSANDRA-14092)",
                "++     * Verify that the computed TTL is equal to the maximum allowed ttl given the",
                "++     * {@link AbstractCell#localDeletionTime()} field limitation (CASSANDRA-14092)",
                "+      */",
                "+     private void checkTTLIsCapped(String field) throws Throwable",
                "+     {",
                "+ ",
                "+         // TTL is computed dynamically from row expiration time, so if it is",
                "+         // equal or higher to the minimum max TTL we compute before the query",
                "+         // we are fine.",
                "+         int minMaxTTL = computeMaxTTL();",
                " -        UntypedResultSet execute = execute(\"SELECT ttl(\" + field + \") FROM %s\");",
                "++        UntypedResultSet execute = execute(\"SELECT ttl(\" + field + \") FROM %s WHERE k = 1\");",
                "+         for (UntypedResultSet.Row row : execute)",
                "+         {",
                "+             int ttl = row.getInt(\"ttl(\" + field + \")\");",
                "+             assertTrue(ttl >= minMaxTTL);",
                "+         }",
                "+     }",
                "+ ",
                "+     /**",
                "+      * The max TTL is computed such that the TTL summed with the current time is equal to the maximum",
                " -     * allowed expiration time {@link BufferExpiringCell#getLocalDeletionTime()} (2038-01-19T03:14:06+00:00)",
                "++     * allowed expiration time {@link org.apache.cassandra.db.rows.Cell#MAX_DELETION_TIME} (2038-01-19T03:14:06+00:00)",
                "+      */",
                "+     private int computeMaxTTL()",
                "+     {",
                "+         int nowInSecs = (int) (System.currentTimeMillis() / 1000);",
                " -        return BufferExpiringCell.MAX_DELETION_TIME - nowInSecs;",
                "++        return AbstractCell.MAX_DELETION_TIME - nowInSecs;",
                "+     }",
                "+ ",
                "+     public void testRecoverOverflowedExpirationWithScrub(boolean simple, boolean clustering, boolean runScrub, boolean reinsertOverflowedTTL) throws Throwable",
                "+     {",
                "+         if (reinsertOverflowedTTL)",
                "+         {",
                "+             assert runScrub;",
                "+         }",
                "+ ",
                "+         createTable(simple, clustering);",
                "+ ",
                "+         Keyspace keyspace = Keyspace.open(KEYSPACE);",
                "+         ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(currentTable());",
                "+ ",
                " -        assertEquals(0, cfs.getSSTables().size());",
                "++        assertEquals(0, cfs.getLiveSSTables().size());",
                "+ ",
                "+         copySSTablesToTableDir(currentTable(), simple, clustering);",
                "+ ",
                "+         cfs.loadNewSSTables();",
                "+ ",
                "+         if (runScrub)",
                "+         {",
                " -            cfs.scrub(true, false, false, reinsertOverflowedTTL, 1);",
                "++            cfs.scrub(true, false, true, reinsertOverflowedTTL, 1);",
                "+         }",
                "+ ",
                "+         if (reinsertOverflowedTTL)",
                "+         {",
                "+             if (simple)",
                " -                assertRows(execute(\"SELECT * from %s\"), row(1, 1, 1), row(2, 2, 2));",
                "++                assertRows(execute(\"SELECT * from %s\"), row(1, 1, 1), row(2, 2, null));",
                "+             else",
                "+                 assertRows(execute(\"SELECT * from %s\"), row(1, 1, set(\"v11\", \"v12\", \"v13\", \"v14\")), row(2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\")));",
                "+ ",
                "+             cfs.forceMajorCompaction();",
                "+ ",
                "+             if (simple)",
                " -                assertRows(execute(\"SELECT * from %s\"), row(1, 1, 1), row(2, 2, 2));",
                "++                assertRows(execute(\"SELECT * from %s\"), row(1, 1, 1), row(2, 2, null));",
                "+             else",
                "+                 assertRows(execute(\"SELECT * from %s\"), row(1, 1, set(\"v11\", \"v12\", \"v13\", \"v14\")), row(2, 2, set(\"v21\", \"v22\", \"v23\", \"v24\")));",
                "+         }",
                "+         else",
                "+         {",
                "+             assertEmpty(execute(\"SELECT * from %s\"));",
                "+         }",
                "+     }",
                "+ ",
                "+     private void copySSTablesToTableDir(String table, boolean simple, boolean clustering) throws IOException",
                "+     {",
                " -        File destDir = Keyspace.open(keyspace()).getColumnFamilyStore(table).directories.getCFDirectories().iterator().next();",
                "++        File destDir = Keyspace.open(keyspace()).getColumnFamilyStore(table).getDirectories().getCFDirectories().iterator().next();",
                "+         File sourceDir = getTableDir(table, simple, clustering);",
                "+         for (File file : sourceDir.listFiles())",
                "+         {",
                "+             copyFile(file, destDir);",
                "+         }",
                "+     }",
                "+ ",
                "+     private static File getTableDir(String table, boolean simple, boolean clustering)",
                "+     {",
                "+         return new File(String.format(NEGATIVE_LOCAL_EXPIRATION_TEST_DIR, getTableName(simple, clustering)));",
                "+     }",
                "+ ",
                " -    private void createTable(boolean simple, boolean clustering)",
                " -    {",
                " -        if (simple)",
                " -        {",
                " -            if (clustering)",
                " -                createTable(\"create table %s (k int, a int, b int, primary key(k, a))\");",
                " -            else",
                " -                createTable(\"create table %s (k int primary key, a int, b int)\");",
                " -        }",
                " -        else",
                " -        {",
                " -            if (clustering)",
                " -                createTable(\"create table %s (k int, a int, b set<text>, primary key(k, a))\");",
                " -            else",
                " -                createTable(\"create table %s (k int primary key, a int, b set<text>)\");",
                " -        }",
                " -    }",
                " -",
                " -    private static File getTableDir(boolean simple, boolean clustering)",
                " -    {",
                " -        return new File(String.format(NEGATIVE_LOCAL_EXPIRATION_TEST_DIR, getTableName(simple, clustering)));",
                " -    }",
                " -",
                "+     private static void copyFile(File src, File dest) throws IOException",
                "+     {",
                "+         byte[] buf = new byte[65536];",
                "+         if (src.isFile())",
                "+         {",
                "+             File target = new File(dest, src.getName());",
                "+             int rd;",
                "+             FileInputStream is = new FileInputStream(src);",
                "+             FileOutputStream os = new FileOutputStream(target);",
                "+             while ((rd = is.read(buf)) >= 0)",
                "+                 os.write(buf, 0, rd);",
                "+         }",
                "+     }",
                "+ ",
                "+     public static String getTableName(boolean simple, boolean clustering)",
                "+     {",
                "+         if (simple)",
                "+             return clustering ? SIMPLE_CLUSTERING : SIMPLE_NOCLUSTERING;",
                "+         else",
                "+             return clustering ? COMPLEX_CLUSTERING : COMPLEX_NOCLUSTERING;",
                "+     }",
                "  }",
                "diff --cc test/unit/org/apache/cassandra/db/ScrubTest.java",
                "index 08336a16ad,9b1ede421d..fc2faeaa4f",
                "--- a/test/unit/org/apache/cassandra/db/ScrubTest.java",
                "+++ b/test/unit/org/apache/cassandra/db/ScrubTest.java",
                "@@@ -119,3 -136,3 +119,3 @@@ public class ScrubTes",
                "--        CompactionManager.instance.performScrub(cfs, false, true, 2);",
                "++        CompactionManager.instance.performScrub(cfs, false, true, false, 2);",
                "@@@ -621,3 -767,3 +621,3 @@@",
                "                  }",
                "-                 CompactionManager.AllSSTableOpStatus result = indexCfs.scrub(false, false, true, true, 0);",
                " -                CompactionManager.AllSSTableOpStatus result = indexCfs.scrub(false, false, true, true, true, 0);",
                "++                CompactionManager.AllSSTableOpStatus result = indexCfs.scrub(false, false, false, true, false,0);",
                "                  assertEquals(failure ?",
                "@@@ -631,115 -777,5 +631,115 @@@",
                "          // check index is still working",
                " -        rows = cfs.search(Util.range(\"\", \"\"), Arrays.asList(expr), new IdentityQueryFilter(), numRows);",
                " -        assertNotNull(rows);",
                " -        assertEquals(numRows / 2, rows.size());",
                " +        assertOrdered(Util.cmd(cfs).filterOn(colName, Operator.EQ, 1L).build(), numRows / 2);",
                " +    }",
                " +",
                " +    private static SSTableMultiWriter createTestWriter(Descriptor descriptor, long keyCount, CFMetaData metadata, LifecycleTransaction txn)",
                " +    {",
                " +        SerializationHeader header = new SerializationHeader(true, metadata, metadata.partitionColumns(), EncodingStats.NO_STATS);",
                " +        MetadataCollector collector = new MetadataCollector(metadata.comparator).sstableLevel(0);",
                " +        return new TestMultiWriter(new TestWriter(descriptor, keyCount, 0, metadata, collector, header, txn));",
                " +    }",
                " +",
                " +    private static class TestMultiWriter extends SimpleSSTableMultiWriter",
                " +    {",
                " +        TestMultiWriter(SSTableWriter writer)",
                " +        {",
                " +            super(writer);",
                " +        }",
                " +    }",
                " +",
                " +    /**",
                " +     * Test writer that allows to write out of order SSTable.",
                " +     */",
                " +    private static class TestWriter extends BigTableWriter",
                " +    {",
                " +        TestWriter(Descriptor descriptor, long keyCount, long repairedAt, CFMetaData metadata,",
                " +                   MetadataCollector collector, SerializationHeader header, LifecycleTransaction txn)",
                " +        {",
                " +            super(descriptor, keyCount, repairedAt, metadata, collector, header, txn);",
                " +        }",
                " +",
                " +        @Override",
                " +        protected long beforeAppend(DecoratedKey decoratedKey)",
                " +        {",
                " +            return dataFile.position();",
                " +        }",
                " +    }",
                " +",
                " +    /**",
                " +     * Tests with invalid sstables (containing duplicate entries in 2.0 and 3.0 storage format),",
                " +     * that were caused by upgrading from 2.x with duplicate range tombstones.",
                " +     *",
                " +     * See CASSANDRA-12144 for details.",
                " +     */",
                " +    @Test",
                " +    public void testFilterOutDuplicates() throws Exception",
                " +    {",
                " +        DatabaseDescriptor.setPartitionerUnsafe(Murmur3Partitioner.instance);",
                " +        QueryProcessor.process(String.format(\"CREATE TABLE \\\"%s\\\".cf_with_duplicates_3_0 (a int, b int, c int, PRIMARY KEY (a, b))\", KEYSPACE), ConsistencyLevel.ONE);",
                " +",
                " +        Keyspace keyspace = Keyspace.open(KEYSPACE);",
                " +        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(\"cf_with_duplicates_3_0\");",
                " +",
                " +        Path legacySSTableRoot = Paths.get(System.getProperty(INVALID_LEGACY_SSTABLE_ROOT_PROP),",
                " +                                           \"Keyspace1\",",
                " +                                           \"cf_with_duplicates_3_0\");",
                " +",
                " +        for (String filename : new String[]{ \"mb-3-big-CompressionInfo.db\",",
                " +                                             \"mb-3-big-Digest.crc32\",",
                " +                                             \"mb-3-big-Index.db\",",
                " +                                             \"mb-3-big-Summary.db\",",
                " +                                             \"mb-3-big-Data.db\",",
                " +                                             \"mb-3-big-Filter.db\",",
                " +                                             \"mb-3-big-Statistics.db\",",
                " +                                             \"mb-3-big-TOC.txt\" })",
                " +        {",
                " +            Files.copy(Paths.get(legacySSTableRoot.toString(), filename), cfs.getDirectories().getDirectoryForNewSSTables().toPath().resolve(filename));",
                " +        }",
                " +",
                " +        cfs.loadNewSSTables();",
                " +",
                "-         cfs.scrub(true, true, true, 1);",
                "++        cfs.scrub(true, true, false, false, false, 1);",
                " +",
                " +        UntypedResultSet rs = QueryProcessor.executeInternal(String.format(\"SELECT * FROM \\\"%s\\\".cf_with_duplicates_3_0\", KEYSPACE));",
                " +        assertEquals(1, rs.size());",
                " +        QueryProcessor.executeInternal(String.format(\"DELETE FROM \\\"%s\\\".cf_with_duplicates_3_0 WHERE a=1 AND b =2\", KEYSPACE));",
                " +        rs = QueryProcessor.executeInternal(String.format(\"SELECT * FROM \\\"%s\\\".cf_with_duplicates_3_0\", KEYSPACE));",
                " +        assertEquals(0, rs.size());",
                " +    }",
                " +",
                " +    @Test",
                " +    public void testUpgradeSstablesWithDuplicates() throws Exception",
                " +    {",
                " +        DatabaseDescriptor.setPartitionerUnsafe(Murmur3Partitioner.instance);",
                " +        String cf = \"cf_with_duplicates_2_0\";",
                " +        QueryProcessor.process(String.format(\"CREATE TABLE \\\"%s\\\".%s (a int, b int, c int, PRIMARY KEY (a, b))\", KEYSPACE, cf), ConsistencyLevel.ONE);",
                " +",
                " +        Keyspace keyspace = Keyspace.open(KEYSPACE);",
                " +        ColumnFamilyStore cfs = keyspace.getColumnFamilyStore(cf);",
                " +",
                " +        Path legacySSTableRoot = Paths.get(System.getProperty(INVALID_LEGACY_SSTABLE_ROOT_PROP),",
                " +                                           \"Keyspace1\",",
                " +                                           cf);",
                " +",
                " +        for (String filename : new String[]{ \"lb-1-big-CompressionInfo.db\",",
                " +                                             \"lb-1-big-Data.db\",",
                " +                                             \"lb-1-big-Digest.adler32\",",
                " +                                             \"lb-1-big-Filter.db\",",
                " +                                             \"lb-1-big-Index.db\",",
                " +                                             \"lb-1-big-Statistics.db\",",
                " +                                             \"lb-1-big-Summary.db\",",
                " +                                             \"lb-1-big-TOC.txt\" })",
                " +        {",
                " +            Files.copy(Paths.get(legacySSTableRoot.toString(), filename), cfs.getDirectories().getDirectoryForNewSSTables().toPath().resolve(filename));",
                " +        }",
                " +",
                " +        cfs.loadNewSSTables();",
                " +",
                " +        cfs.sstablesRewrite(true, 1);",
                " +",
                " +        UntypedResultSet rs = QueryProcessor.executeInternal(String.format(\"SELECT * FROM \\\"%s\\\".%s\", KEYSPACE, cf));",
                " +        assertEquals(1, rs.size());",
                " +        QueryProcessor.executeInternal(String.format(\"DELETE FROM \\\"%s\\\".%s WHERE a=1 AND b =2\", KEYSPACE, cf));",
                " +        rs = QueryProcessor.executeInternal(String.format(\"SELECT * FROM \\\"%s\\\".%s\", KEYSPACE, cf));",
                " +        assertEquals(0, rs.size());",
                "      }"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-3.0.16",
                "cassandra-3.0.17",
                "cassandra-3.0.18",
                "cassandra-3.0.19",
                "cassandra-3.0.20",
                "cassandra-3.0.21",
                "cassandra-3.0.22",
                "cassandra-3.0.23",
                "cassandra-3.0.24",
                "cassandra-3.0.25",
                "cassandra-3.0.26",
                "cassandra-3.0.27",
                "cassandra-3.0.28",
                "cassandra-3.0.29",
                "cassandra-3.0.30",
                "cassandra-3.11.10",
                "cassandra-3.11.11",
                "cassandra-3.11.12",
                "cassandra-3.11.13",
                "cassandra-3.11.14",
                "cassandra-3.11.15",
                "cassandra-3.11.16",
                "cassandra-3.11.17",
                "cassandra-3.11.2",
                "cassandra-3.11.3",
                "cassandra-3.11.4",
                "cassandra-3.11.5",
                "cassandra-3.11.6",
                "cassandra-3.11.7",
                "cassandra-3.11.8",
                "cassandra-3.11.9",
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "9635c6041a1b70dbefc6ca2b5b58b08717077300",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1516627583,
            "hunks": 1,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [
                "diff --cc CHANGES.txt",
                "index fd0acb9be9,81b358d158..b34cb1bd4d",
                "--- a/CHANGES.txt",
                "+++ b/CHANGES.txt",
                "@@@ -224,3 -47,4 +224,4 @@@ Merged from 2.2",
                "  Merged from 2.1:",
                "+  * RPM package spec: fix permissions for installed jars and config files (CASSANDRA-14181)",
                " - * More PEP8 compiance for cqlsh (CASSANDRA-14021)",
                " + * More PEP8 compliance for cqlsh"
            ],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        },
        {
            "commit_id": "66b2b3f5f39fa9568f17569b74aec2b48e206efe",
            "repository": "https://github.com/apache/cassandra",
            "timestamp": 1518369515,
            "hunks": 0,
            "message": "Merge branch 'cassandra-3.11' into trunk",
            "diff": [],
            "changed_files": [],
            "message_reference_content": [],
            "jira_refs": {},
            "ghissue_refs": {},
            "cve_refs": [],
            "twins": [],
            "tags": [
                "cassandra-4.0-alpha1",
                "cassandra-4.0-alpha2",
                "cassandra-4.0-alpha3",
                "cassandra-4.0-alpha4",
                "cassandra-4.0-beta1",
                "cassandra-4.0-beta2",
                "cassandra-4.0-beta3",
                "cassandra-4.0-beta4",
                "cassandra-4.0-rc1",
                "cassandra-4.0-rc2",
                "cassandra-4.0.0",
                "cassandra-4.0.1",
                "cassandra-4.0.10",
                "cassandra-4.0.11",
                "cassandra-4.0.12",
                "cassandra-4.0.13",
                "cassandra-4.0.2",
                "cassandra-4.0.3",
                "cassandra-4.0.4",
                "cassandra-4.0.5",
                "cassandra-4.0.6",
                "cassandra-4.0.7",
                "cassandra-4.0.8",
                "cassandra-4.0.9",
                "cassandra-4.1-alpha1",
                "cassandra-4.1-beta1",
                "cassandra-4.1-rc1",
                "cassandra-4.1.0",
                "cassandra-4.1.1",
                "cassandra-4.1.2",
                "cassandra-4.1.3",
                "cassandra-4.1.4",
                "cassandra-4.1.5",
                "cassandra-5.0-alpha1",
                "cassandra-5.0-alpha2",
                "cassandra-5.0-beta1"
            ],
            "matched_rules": []
        }
    ]
}
